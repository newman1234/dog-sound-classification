{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"papermill":{"duration":2433.885786,"end_time":"2020-08-14T11:07:00.653314","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-08-14T10:26:26.767528","version":"2.1.0"},"colab":{"name":"introduction-to-sound-event-detection.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ck66G62eCBLM","executionInfo":{"status":"ok","timestamp":1623286706986,"user_tz":-480,"elapsed":69236,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e90b6e3d-5c68-4043-c04c-a01855e5eae9"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/Colab\\ Notebooks/esun_tbrain/dog_sound"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/MyDrive/Colab Notebooks/esun_tbrain/dog_sound\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3_Ga8Nshm-s","executionInfo":{"status":"ok","timestamp":1623286706987,"user_tz":-480,"elapsed":9,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"385a0f9e-4c94-41d1-c20a-b997365d0d5c"},"source":["%cd /gdrive/MyDrive/Colab\\ Notebooks/esun_tbrain/dog_sound"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/MyDrive/Colab Notebooks/esun_tbrain/dog_sound\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvlv1nFNCBu0","executionInfo":{"status":"ok","timestamp":1623286803960,"user_tz":-480,"elapsed":96978,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e27a0603-cf77-466c-957d-1bb892457888"},"source":["!mkdir -p /content/data/ && unzip data/train.zip -d /content/data/ && unzip data/public_test.zip -d /content/data/ && unzip data/private_test.zip -d /content/data/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","  inflating: /content/data/private_test/private_12035.wav  \n","  inflating: /content/data/private_test/private_05549.wav  \n","  inflating: /content/data/private_test/private_06040.wav  \n","  inflating: /content/data/private_test/private_09373.wav  \n","  inflating: /content/data/private_test/private_16253.wav  \n","  inflating: /content/data/private_test/private_19160.wav  \n","  inflating: /content/data/private_test/private_00431.wav  \n","  inflating: /content/data/private_test/private_10622.wav  \n","  inflating: /content/data/private_test/private_07438.wav  \n","  inflating: /content/data/private_test/private_10144.wav  \n","  inflating: /content/data/private_test/private_00357.wav  \n","  inflating: /content/data/private_test/private_18518.wav  \n","  inflating: /content/data/private_test/private_16535.wav  \n","  inflating: /content/data/private_test/private_19606.wav  \n","  inflating: /content/data/private_test/private_01049.wav  \n","  inflating: /content/data/private_test/private_06726.wav  \n","  inflating: /content/data/private_test/private_09415.wav  \n","  inflating: /content/data/private_test/private_03886.wav  \n","  inflating: /content/data/private_test/private_12753.wav  \n","  inflating: /content/data/private_test/private_02540.wav  \n","  inflating: /content/data/private_test/private_14322.wav  \n","  inflating: /content/data/private_test/private_04131.wav  \n","  inflating: /content/data/private_test/private_12747.wav  \n","  inflating: /content/data/private_test/private_03892.wav  \n","  inflating: /content/data/private_test/private_02554.wav  \n","  inflating: /content/data/private_test/private_15028.wav  \n","  inflating: /content/data/private_test/private_14336.wav  \n","  inflating: /content/data/private_test/private_13459.wav  \n","  inflating: /content/data/private_test/private_04125.wav  \n","  inflating: /content/data/private_test/private_10150.wav  \n","  inflating: /content/data/private_test/private_00343.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00343.wav  \n","  inflating: /content/data/private_test/private_16521.wav  \n","  inflating: /content/data/private_test/private_19612.wav  \n","  inflating: /content/data/private_test/private_06732.wav  \n","  inflating: /content/data/private_test/private_09401.wav  \n","  inflating: /content/data/private_test/private_06054.wav  \n","  inflating: /content/data/private_test/private_09367.wav  \n","  inflating: /content/data/private_test/private_11528.wav  \n","  inflating: /content/data/private_test/private_16247.wav  \n","  inflating: /content/data/private_test/private_19174.wav  \n","  inflating: /content/data/private_test/private_17159.wav  \n","  inflating: /content/data/private_test/private_00425.wav  \n","  inflating: /content/data/private_test/private_10636.wav  \n","  inflating: /content/data/private_test/private_08079.wav  \n","  inflating: /content/data/private_test/private_04643.wav  \n","  inflating: /content/data/private_test/private_15996.wav  \n","  inflating: /content/data/private_test/private_14450.wav  \n","  inflating: /content/data/private_test/private_02232.wav  \n","  inflating: /content/data/private_test/private_12021.wav  \n","  inflating: /content/data/private_test/private_11500.wav  \n","  inflating: /content/data/private_test/private_01713.wav  \n","  inflating: /content/data/private_test/private_18242.wav  \n","  inflating: /content/data/private_test/private_17171.wav  \n","  inflating: /content/data/private_test/private_08051.wav  \n","  inflating: /content/data/private_test/private_07362.wav  \n","  inflating: /content/data/private_test/private_13317.wav  \n","  inflating: /content/data/private_test/private_14478.wav  \n","  inflating: /content/data/private_test/private_03104.wav  \n","  inflating: /content/data/private_test/private_15766.wav  \n","  inflating: /content/data/private_test/private_05575.wav  \n","  inflating: /content/data/private_test/private_12009.wav  \n","  inflating: /content/data/private_test/private_05213.wav  \n","  inflating: /content/data/private_test/private_15000.wav  \n","  inflating: /content/data/private_test/private_03662.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03662.wav  \n","  inflating: /content/data/private_test/private_13471.wav  \n","  inflating: /content/data/private_test/private_08737.wav  \n","  inflating: /content/data/private_test/private_10178.wav  \n","  inflating: /content/data/private_test/private_07404.wav  \n","  inflating: /content/data/private_test/private_18524.wav  \n","  inflating: /content/data/private_test/private_17617.wav  \n","  inflating: /content/data/private_test/private_01075.wav  \n","  inflating: /content/data/private_test/private_16509.wav  \n","  inflating: /content/data/private_test/private_11266.wav  \n","  inflating: /content/data/private_test/private_09429.wav  \n","  inflating: /content/data/private_test/private_08723.wav  \n","  inflating: /content/data/private_test/private_07410.wav  \n","  inflating: /content/data/private_test/private_18530.wav  \n","  inflating: /content/data/private_test/private_17603.wav  \n","  inflating: /content/data/private_test/private_01061.wav  \n","  inflating: /content/data/private_test/private_11272.wav  \n","  inflating: /content/data/private_test/private_05207.wav  \n","  inflating: /content/data/private_test/private_15014.wav  \n","  inflating: /content/data/private_test/private_02568.wav  \n","  inflating: /content/data/private_test/private_03676.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03676.wav  \n","  inflating: /content/data/private_test/private_04119.wav  \n","  inflating: /content/data/private_test/private_13465.wav  \n","  inflating: /content/data/private_test/private_13303.wav  \n","  inflating: /content/data/private_test/private_03110.wav  \n","  inflating: /content/data/private_test/private_15772.wav  \n","  inflating: /content/data/private_test/private_05561.wav  \n","  inflating: /content/data/private_test/private_11514.wav  \n","  inflating: /content/data/private_test/private_06068.wav  \n","  inflating: /content/data/private_test/private_01707.wav  \n","  inflating: /content/data/private_test/private_19148.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19148.wav  \n","  inflating: /content/data/private_test/private_18256.wav  \n","  inflating: /content/data/private_test/private_00419.wav  \n","  inflating: /content/data/private_test/private_17165.wav  \n","  inflating: /content/data/private_test/private_08045.wav  \n","  inflating: /content/data/private_test/private_07376.wav  \n","  inflating: /content/data/private_test/private_01934.wav  \n","  inflating: /content/data/private_test/private_06083.wav  \n","  inflating: /content/data/private_test/private_16290.wav  \n","  inflating: /content/data/private_test/private_10839.wav  \n","  inflating: /content/data/private_test/private_15799.wav  \n","  inflating: /content/data/private_test/private_15941.wav  \n","  inflating: /content/data/private_test/private_04694.wav  \n","  inflating: /content/data/private_test/private_14487.wav  \n","  inflating: /content/data/private_test/private_12948.wav  \n","  inflating: /content/data/private_test/private_03845.wav  \n","  inflating: /content/data/private_test/private_12790.wav  \n","  inflating: /content/data/private_test/private_02583.wav  \n","  inflating: /content/data/private_test/private_08910.wav  \n","  inflating: /content/data/private_test/private_17830.wav  \n","  inflating: /content/data/private_test/private_11299.wav  \n","  inflating: /content/data/private_test/private_10187.wav  \n","  inflating: /content/data/private_test/private_00394.wav  \n","  inflating: /content/data/private_test/private_08904.wav  \n","  inflating: /content/data/private_test/private_17824.wav  \n","  inflating: /content/data/private_test/private_19809.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19809.wav  \n","  inflating: /content/data/private_test/private_10193.wav  \n","  inflating: /content/data/private_test/private_06929.wav  \n","  inflating: /content/data/private_test/private_00380.wav  \n","  inflating: /content/data/private_test/private_03689.wav  \n","  inflating: /content/data/private_test/private_12784.wav  \n","  inflating: /content/data/private_test/private_03851.wav  \n","  inflating: /content/data/private_test/private_02597.wav  \n","  inflating: /content/data/private_test/private_04858.wav  \n","  inflating: /content/data/private_test/private_04680.wav  \n","  inflating: /content/data/private_test/private_15955.wav  \n","  inflating: /content/data/private_test/private_14493.wav  \n","  inflating: /content/data/private_test/private_07389.wav  \n","  inflating: /content/data/private_test/private_01920.wav  \n","  inflating: /content/data/private_test/private_06097.wav  \n","  inflating: /content/data/private_test/private_16284.wav  \n","  inflating: /content/data/private_test/private_04870.wav  \n","  inflating: /content/data/private_test/private_18281.wav  \n","  inflating: /content/data/private_test/private_01908.wav  \n","  inflating: /content/data/private_test/private_08092.wav  \n","  inflating: /content/data/private_test/private_10805.wav  \n","  inflating: /content/data/private_test/private_19821.wav  \n","  inflating: /content/data/private_test/private_06901.wav  \n","  inflating: /content/data/private_test/private_12974.wav  \n","  inflating: /content/data/private_test/private_03879.wav  \n","  inflating: /content/data/private_test/private_12960.wav  \n","  inflating: /content/data/private_test/private_08938.wav  \n","  inflating: /content/data/private_test/private_17818.wav  \n","  inflating: /content/data/private_test/private_19835.wav  \n","  inflating: /content/data/private_test/private_06915.wav  \n","  inflating: /content/data/private_test/private_18295.wav  \n","  inflating: /content/data/private_test/private_08086.wav  \n","  inflating: /content/data/private_test/private_09398.wav  \n","  inflating: /content/data/private_test/private_10811.wav  \n","  inflating: /content/data/private_test/private_04864.wav  \n","  inflating: /content/data/private_test/private_15969.wav  \n","  inflating: /content/data/private_test/private_12141.wav  \n","  inflating: /content/data/private_test/private_02352.wav  \n","  inflating: /content/data/private_test/private_14530.wav  \n","  inflating: /content/data/private_test/private_04723.wav  \n","  inflating: /content/data/private_test/private_01883.wav  \n","  inflating: /content/data/private_test/private_10756.wav  \n","  inflating: /content/data/private_test/private_08119.wav  \n","  inflating: /content/data/private_test/private_17039.wav  \n","  inflating: /content/data/private_test/private_00545.wav  \n","  inflating: /content/data/private_test/private_16327.wav  \n","  inflating: /content/data/private_test/private_19014.wav  \n","  inflating: /content/data/private_test/private_06134.wav  \n","  inflating: /content/data/private_test/private_09207.wav  \n","  inflating: /content/data/private_test/private_11448.wav  \n","  inflating: /content/data/private_test/private_17987.wav  \n","  inflating: /content/data/private_test/private_06652.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06652.wav  \n","  inflating: /content/data/private_test/private_09561.wav  \n","  inflating: /content/data/private_test/private_16441.wav  \n","  inflating: /content/data/private_test/private_19772.wav  \n","  inflating: /content/data/private_test/private_00223.wav  \n","  inflating: /content/data/private_test/private_10030.wav  \n","  inflating: /content/data/private_test/private_13539.wav  \n","  inflating: /content/data/private_test/private_04045.wav  \n","  inflating: /content/data/private_test/private_14256.wav  \n","  inflating: /content/data/private_test/private_02434.wav  \n","  inflating: /content/data/private_test/private_15148.wav  \n","  inflating: /content/data/private_test/private_12627.wav  \n","  inflating: /content/data/private_test/private_04051.wav  \n","  inflating: /content/data/private_test/private_14242.wav  \n","  inflating: /content/data/private_test/private_02420.wav  \n","  inflating: /content/data/private_test/private_12633.wav  \n","  inflating: /content/data/private_test/private_06646.wav  \n","  inflating: /content/data/private_test/private_17993.wav  \n","  inflating: /content/data/private_test/private_09575.wav  \n","  inflating: /content/data/private_test/private_16455.wav  \n","  inflating: /content/data/private_test/private_19766.wav  \n","  inflating: /content/data/private_test/private_01129.wav  \n","  inflating: /content/data/private_test/private_00237.wav  \n","  inflating: /content/data/private_test/private_18478.wav  \n","  inflating: /content/data/private_test/private_07558.wav  \n","  inflating: /content/data/private_test/private_10024.wav  \n","  inflating: /content/data/private_test/private_10742.wav  \n","  inflating: /content/data/private_test/private_01897.wav  \n","  inflating: /content/data/private_test/private_00551.wav  \n","  inflating: /content/data/private_test/private_16333.wav  \n","  inflating: /content/data/private_test/private_19000.wav  \n","  inflating: /content/data/private_test/private_06120.wav  \n","  inflating: /content/data/private_test/private_09213.wav  \n","  inflating: /content/data/private_test/private_12155.wav  \n","  inflating: /content/data/private_test/private_05429.wav  \n","  inflating: /content/data/private_test/private_02346.wav  \n","  inflating: /content/data/private_test/private_03058.wav  \n","  inflating: /content/data/private_test/private_14524.wav  \n","  inflating: /content/data/private_test/private_04737.wav  \n","  inflating: /content/data/private_test/private_08125.wav  \n","  inflating: /content/data/private_test/private_07216.wav  \n","  inflating: /content/data/private_test/private_18336.wav  \n","  inflating: /content/data/private_test/private_00579.wav  \n","  inflating: /content/data/private_test/private_17005.wav  \n","  inflating: /content/data/private_test/private_01667.wav  \n","  inflating: /content/data/private_test/private_19028.wav  \n","  inflating: /content/data/private_test/private_11474.wav  \n","  inflating: /content/data/private_test/private_06108.wav  \n","  inflating: /content/data/private_test/private_05401.wav  \n","  inflating: /content/data/private_test/private_15612.wav  \n","  inflating: /content/data/private_test/private_03070.wav  \n","  inflating: /content/data/private_test/private_13263.wav  \n","  inflating: /content/data/private_test/private_04079.wav  \n","  inflating: /content/data/private_test/private_13505.wav  \n","  inflating: /content/data/private_test/private_03716.wav  \n","  inflating: /content/data/private_test/private_15174.wav  \n","  inflating: /content/data/private_test/private_02408.wav  \n","  inflating: /content/data/private_test/private_05367.wav  \n","  inflating: /content/data/private_test/private_11312.wav  \n","  inflating: /content/data/private_test/private_01101.wav  \n","  inflating: /content/data/private_test/private_18450.wav  \n","  inflating: /content/data/private_test/private_17763.wav  \n","  inflating: /content/data/private_test/private_08643.wav  \n","  inflating: /content/data/private_test/private_19996.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19996.wav  \n","  inflating: /content/data/private_test/private_07570.wav  \n","  inflating: /content/data/private_test/private_11306.wav  \n","  inflating: /content/data/private_test/private_09549.wav  \n","  inflating: /content/data/private_test/private_01115.wav  \n","  inflating: /content/data/private_test/private_16469.wav  \n","  inflating: /content/data/private_test/private_18444.wav  \n","  inflating: /content/data/private_test/private_17777.wav  \n","  inflating: /content/data/private_test/private_19982.wav  \n","  inflating: /content/data/private_test/private_08657.wav  \n","  inflating: /content/data/private_test/private_10018.wav  \n","  inflating: /content/data/private_test/private_07564.wav  \n","  inflating: /content/data/private_test/private_13511.wav  \n","  inflating: /content/data/private_test/private_03702.wav  \n","  inflating: /content/data/private_test/private_15160.wav  \n","  inflating: /content/data/private_test/private_05373.wav  \n","  inflating: /content/data/private_test/private_05415.wav  \n","  inflating: /content/data/private_test/private_12169.wav  \n","  inflating: /content/data/private_test/private_15606.wav  \n","  inflating: /content/data/private_test/private_14518.wav  \n","  inflating: /content/data/private_test/private_03064.wav  \n","  inflating: /content/data/private_test/private_13277.wav  \n","  inflating: /content/data/private_test/private_08131.wav  \n","  inflating: /content/data/private_test/private_07202.wav  \n","  inflating: /content/data/private_test/private_18322.wav  \n","  inflating: /content/data/private_test/private_17011.wav  \n","  inflating: /content/data/private_test/private_01673.wav  \n","  inflating: /content/data/private_test/private_11460.wav  \n","  inflating: /content/data/private_test/private_01698.wav  \n","  inflating: /content/data/private_test/private_01840.wav  \n","  inflating: /content/data/private_test/private_10795.wav  \n","  inflating: /content/data/private_test/private_00586.wav  \n","  inflating: /content/data/private_test/private_15835.wav  \n","  inflating: /content/data/private_test/private_12182.wav  \n","  inflating: /content/data/private_test/private_02391.wav  \n","  inflating: /content/data/private_test/private_04938.wav  \n","  inflating: /content/data/private_test/private_03931.wav  \n","  inflating: /content/data/private_test/private_05398.wav  \n","  inflating: /content/data/private_test/private_04086.wav  \n","  inflating: /content/data/private_test/private_14295.wav  \n","  inflating: /content/data/private_test/private_06849.wav  \n","  inflating: /content/data/private_test/private_19969.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19969.wav  \n","  inflating: /content/data/private_test/private_17944.wav  \n","  inflating: /content/data/private_test/private_06691.wav  \n","  inflating: /content/data/private_test/private_16482.wav  \n","  inflating: /content/data/private_test/private_08864.wav  \n","  inflating: /content/data/private_test/private_17788.wav  \n","  inflating: /content/data/private_test/private_06685.wav  \n","  inflating: /content/data/private_test/private_17950.wav  \n","  inflating: /content/data/private_test/private_16496.wav  \n","  inflating: /content/data/private_test/private_08870.wav  \n","  inflating: /content/data/private_test/private_03925.wav  \n","  inflating: /content/data/private_test/private_04092.wav  \n","  inflating: /content/data/private_test/private_12828.wav  \n","  inflating: /content/data/private_test/private_14281.wav  \n","  inflating: /content/data/private_test/private_13288.wav  \n","  inflating: /content/data/private_test/private_15821.wav  \n","  inflating: /content/data/private_test/private_12196.wav  \n","  inflating: /content/data/private_test/private_02385.wav  \n","  inflating: /content/data/private_test/private_10959.wav  \n","  inflating: /content/data/private_test/private_10781.wav  \n","  inflating: /content/data/private_test/private_01854.wav  \n","  inflating: /content/data/private_test/private_00592.wav  \n","  inflating: /content/data/private_test/private_15809.wav  \n","  inflating: /content/data/private_test/private_04904.wav  \n","  inflating: /content/data/private_test/private_10971.wav  \n","  inflating: /content/data/private_test/private_18493.wav  \n","  inflating: /content/data/private_test/private_06875.wav  \n","  inflating: /content/data/private_test/private_08680.wav  \n","  inflating: /content/data/private_test/private_19955.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19955.wav  \n","  inflating: /content/data/private_test/private_17978.wav  \n","  inflating: /content/data/private_test/private_08858.wav  \n","  inflating: /content/data/private_test/private_12800.wav  \n","  inflating: /content/data/private_test/private_03919.wav  \n","  inflating: /content/data/private_test/private_12814.wav  \n","  inflating: /content/data/private_test/private_18487.wav  \n","  inflating: /content/data/private_test/private_06861.wav  \n","  inflating: /content/data/private_test/private_19941.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19941.wav  \n","  inflating: /content/data/private_test/private_08694.wav  \n","  inflating: /content/data/private_test/private_19799.wav  \n","  inflating: /content/data/private_test/private_10965.wav  \n","  inflating: /content/data/private_test/private_01868.wav  \n","  inflating: /content/data/private_test/private_04910.wav  \n","  inflating: /content/data/private_test/private_05159.wav  \n","  inflating: /content/data/private_test/private_12425.wav  \n","  inflating: /content/data/private_test/private_02636.wav  \n","  inflating: /content/data/private_test/private_14054.wav  \n","  inflating: /content/data/private_test/private_03528.wav  \n","  inflating: /content/data/private_test/private_04247.wav  \n","  inflating: /content/data/private_test/private_10232.wav  \n","  inflating: /content/data/private_test/private_00021.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00021.wav  \n","  inflating: /content/data/private_test/private_19570.wav  \n","  inflating: /content/data/private_test/private_07996.wav  \n","  inflating: /content/data/private_test/private_16643.wav  \n","  inflating: /content/data/private_test/private_09763.wav  \n","  inflating: /content/data/private_test/private_06450.wav  \n","  inflating: /content/data/private_test/private_09005.wav  \n","  inflating: /content/data/private_test/private_06336.wav  \n","  inflating: /content/data/private_test/private_19216.wav  \n","  inflating: /content/data/private_test/private_01459.wav  \n","  inflating: /content/data/private_test/private_16125.wav  \n","  inflating: /content/data/private_test/private_11892.wav  \n","  inflating: /content/data/private_test/private_00747.wav  \n","  inflating: /content/data/private_test/private_18108.wav  \n","  inflating: /content/data/private_test/private_10554.wav  \n","  inflating: /content/data/private_test/private_07028.wav  \n","  inflating: /content/data/private_test/private_04521.wav  \n","  inflating: /content/data/private_test/private_14732.wav  \n","  inflating: /content/data/private_test/private_02150.wav  \n","  inflating: /content/data/private_test/private_12343.wav  \n","  inflating: /content/data/private_test/private_04535.wav  \n","  inflating: /content/data/private_test/private_13049.wav  \n","  inflating: /content/data/private_test/private_14726.wav  \n","  inflating: /content/data/private_test/private_15438.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15438.wav  \n","  inflating: /content/data/private_test/private_02144.wav  \n","  inflating: /content/data/private_test/private_12357.wav  \n","  inflating: /content/data/private_test/private_09011.wav  \n","  inflating: /content/data/private_test/private_06322.wav  \n","  inflating: /content/data/private_test/private_19202.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19202.wav  \n","  inflating: /content/data/private_test/private_16131.wav  \n","  inflating: /content/data/private_test/private_00753.wav  \n","  inflating: /content/data/private_test/private_11886.wav  \n","  inflating: /content/data/private_test/private_10540.wav  \n","  inflating: /content/data/private_test/private_10226.wav  \n","  inflating: /content/data/private_test/private_08469.wav  \n","  inflating: /content/data/private_test/private_00035.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00035.wav  \n","  inflating: /content/data/private_test/private_17549.wav  \n","  inflating: /content/data/private_test/private_19564.wav  \n","  inflating: /content/data/private_test/private_16657.wav  \n","  inflating: /content/data/private_test/private_07982.wav  \n","  inflating: /content/data/private_test/private_09777.wav  \n","  inflating: /content/data/private_test/private_11138.wav  \n","  inflating: /content/data/private_test/private_06444.wav  \n","  inflating: /content/data/private_test/private_12431.wav  \n","  inflating: /content/data/private_test/private_02622.wav  \n","  inflating: /content/data/private_test/private_14040.wav  \n","  inflating: /content/data/private_test/private_04253.wav  \n","  inflating: /content/data/private_test/private_07772.wav  \n","  inflating: /content/data/private_test/private_08441.wav  \n","  inflating: /content/data/private_test/private_17561.wav  \n","  inflating: /content/data/private_test/private_18652.wav  \n","  inflating: /content/data/private_test/private_09987.wav  \n","  inflating: /content/data/private_test/private_01303.wav  \n","  inflating: /content/data/private_test/private_11110.wav  \n","  inflating: /content/data/private_test/private_12419.wav  \n","  inflating: /content/data/private_test/private_05165.wav  \n","  inflating: /content/data/private_test/private_15376.wav  \n","  inflating: /content/data/private_test/private_03514.wav  \n","  inflating: /content/data/private_test/private_14068.wav  \n","  inflating: /content/data/private_test/private_13707.wav  \n","  inflating: /content/data/private_test/private_13061.wav  \n","  inflating: /content/data/private_test/private_03272.wav  \n","  inflating: /content/data/private_test/private_15410.wav  \n","  inflating: /content/data/private_test/private_05603.wav  \n","  inflating: /content/data/private_test/private_11676.wav  \n","  inflating: /content/data/private_test/private_09039.wav  \n","  inflating: /content/data/private_test/private_16119.wav  \n","  inflating: /content/data/private_test/private_01465.wav  \n","  inflating: /content/data/private_test/private_17207.wav  \n","  inflating: /content/data/private_test/private_18134.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18134.wav  \n","  inflating: /content/data/private_test/private_07014.wav  \n","  inflating: /content/data/private_test/private_08327.wav  \n","  inflating: /content/data/private_test/private_10568.wav  \n","  inflating: /content/data/private_test/private_11662.wav  \n","  inflating: /content/data/private_test/private_01471.wav  \n","  inflating: /content/data/private_test/private_17213.wav  \n","  inflating: /content/data/private_test/private_18120.wav  \n","  inflating: /content/data/private_test/private_07000.wav  \n","  inflating: /content/data/private_test/private_08333.wav  \n","  inflating: /content/data/private_test/private_13075.wav  \n","  inflating: /content/data/private_test/private_04509.wav  \n","  inflating: /content/data/private_test/private_03266.wav  \n","  inflating: /content/data/private_test/private_02178.wav  \n","  inflating: /content/data/private_test/private_15404.wav  \n","  inflating: /content/data/private_test/private_05617.wav  \n","  inflating: /content/data/private_test/private_05171.wav  \n","  inflating: /content/data/private_test/private_15362.wav  \n","  inflating: /content/data/private_test/private_03500.wav  \n","  inflating: /content/data/private_test/private_13713.wav  \n","  inflating: /content/data/private_test/private_07766.wav  \n","  inflating: /content/data/private_test/private_08455.wav  \n","  inflating: /content/data/private_test/private_17575.wav  \n","  inflating: /content/data/private_test/private_09993.wav  \n","  inflating: /content/data/private_test/private_18646.wav  \n","  inflating: /content/data/private_test/private_00009.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00009.wav  \n","  inflating: /content/data/private_test/private_01317.wav  \n","  inflating: /content/data/private_test/private_19558.wav  \n","  inflating: /content/data/private_test/private_06478.wav  \n","  inflating: /content/data/private_test/private_11104.wav  \n","  inflating: /content/data/private_test/private_07955.wav  \n","  inflating: /content/data/private_test/private_16680.wav  \n","  inflating: /content/data/private_test/private_18875.wav  \n","  inflating: /content/data/private_test/private_06493.wav  \n","  inflating: /content/data/private_test/private_16858.wav  \n","  inflating: /content/data/private_test/private_09978.wav  \n","  inflating: /content/data/private_test/private_14097.wav  \n","  inflating: /content/data/private_test/private_04284.wav  \n","  inflating: /content/data/private_test/private_15389.wav  \n","  inflating: /content/data/private_test/private_13920.wav  \n","  inflating: /content/data/private_test/private_02193.wav  \n","  inflating: /content/data/private_test/private_14929.wav  \n","  inflating: /content/data/private_test/private_12380.wav  \n","  inflating: /content/data/private_test/private_05824.wav  \n","  inflating: /content/data/private_test/private_11851.wav  \n","  inflating: /content/data/private_test/private_00784.wav  \n","  inflating: /content/data/private_test/private_10597.wav  \n","  inflating: /content/data/private_test/private_11689.wav  \n","  inflating: /content/data/private_test/private_00790.wav  \n","  inflating: /content/data/private_test/private_11845.wav  \n","  inflating: /content/data/private_test/private_10583.wav  \n","  inflating: /content/data/private_test/private_00948.wav  \n","  inflating: /content/data/private_test/private_02187.wav  \n","  inflating: /content/data/private_test/private_12394.wav  \n","  inflating: /content/data/private_test/private_05830.wav  \n","  inflating: /content/data/private_test/private_03299.wav  \n","  inflating: /content/data/private_test/private_14083.wav  \n","  inflating: /content/data/private_test/private_04290.wav  \n","  inflating: /content/data/private_test/private_02839.wav  \n","  inflating: /content/data/private_test/private_13934.wav  \n","  inflating: /content/data/private_test/private_16694.wav  \n","  inflating: /content/data/private_test/private_07941.wav  \n","  inflating: /content/data/private_test/private_18861.wav  \n","  inflating: /content/data/private_test/private_06487.wav  \n","  inflating: /content/data/private_test/private_07799.wav  \n","  inflating: /content/data/private_test/private_02811.wav  \n","  inflating: /content/data/private_test/private_07969.wav  \n","  inflating: /content/data/private_test/private_18849.wav  \n","  inflating: /content/data/private_test/private_16864.wav  \n","  inflating: /content/data/private_test/private_08482.wav  \n","  inflating: /content/data/private_test/private_18691.wav  \n","  inflating: /content/data/private_test/private_09944.wav  \n","  inflating: /content/data/private_test/private_00960.wav  \n","  inflating: /content/data/private_test/private_14915.wav  \n","  inflating: /content/data/private_test/private_05818.wav  \n","  inflating: /content/data/private_test/private_14901.wav  \n","  inflating: /content/data/private_test/private_11879.wav  \n","  inflating: /content/data/private_test/private_00974.wav  \n","  inflating: /content/data/private_test/private_09788.wav  \n","  inflating: /content/data/private_test/private_16870.wav  \n","  inflating: /content/data/private_test/private_08496.wav  \n","  inflating: /content/data/private_test/private_09950.wav  \n","  inflating: /content/data/private_test/private_18685.wav  \n","  inflating: /content/data/private_test/private_02805.wav  \n","  inflating: /content/data/private_test/private_13908.wav  \n","  inflating: /content/data/private_test/private_00975.wav  \n","  inflating: /content/data/private_test/private_11878.wav  \n","  inflating: /content/data/private_test/private_14900.wav  \n","  inflating: /content/data/private_test/private_13909.wav  \n","  inflating: /content/data/private_test/private_02804.wav  \n","  inflating: /content/data/private_test/private_18684.wav  \n","  inflating: /content/data/private_test/private_09951.wav  \n","  inflating: /content/data/private_test/private_08497.wav  \n","  inflating: /content/data/private_test/private_16871.wav  \n","  inflating: /content/data/private_test/private_09789.wav  \n","  inflating: /content/data/private_test/private_09945.wav  \n","  inflating: /content/data/private_test/private_18690.wav  \n","  inflating: /content/data/private_test/private_08483.wav  \n","  inflating: /content/data/private_test/private_16865.wav  \n","  inflating: /content/data/private_test/private_18848.wav  \n","  inflating: /content/data/private_test/private_07968.wav  \n","  inflating: /content/data/private_test/private_02810.wav  \n","  inflating: /content/data/private_test/private_05819.wav  \n","  inflating: /content/data/private_test/private_14914.wav  \n","  inflating: /content/data/private_test/private_00961.wav  \n","  inflating: /content/data/private_test/private_03298.wav  \n","  inflating: /content/data/private_test/private_05831.wav  \n","  inflating: /content/data/private_test/private_12395.wav  \n","  inflating: /content/data/private_test/private_02186.wav  \n","  inflating: /content/data/private_test/private_00949.wav  \n","  inflating: /content/data/private_test/private_10582.wav  \n","  inflating: /content/data/private_test/private_11844.wav  \n","  inflating: /content/data/private_test/private_00791.wav  \n","  inflating: /content/data/private_test/private_07798.wav  \n","  inflating: /content/data/private_test/private_06486.wav  \n","  inflating: /content/data/private_test/private_18860.wav  \n","  inflating: /content/data/private_test/private_07940.wav  \n","  inflating: /content/data/private_test/private_16695.wav  \n","  inflating: /content/data/private_test/private_13935.wav  \n","  inflating: /content/data/private_test/private_02838.wav  \n","  inflating: /content/data/private_test/private_04291.wav  \n","  inflating: /content/data/private_test/private_14082.wav  \n","  inflating: /content/data/private_test/private_13921.wav  \n","  inflating: /content/data/private_test/private_15388.wav  \n","  inflating: /content/data/private_test/private_04285.wav  \n","  inflating: /content/data/private_test/private_14096.wav  \n","  inflating: /content/data/private_test/private_09979.wav  \n","  inflating: /content/data/private_test/private_16859.wav  \n","  inflating: /content/data/private_test/private_06492.wav  \n","  inflating: /content/data/private_test/private_18874.wav  \n","  inflating: /content/data/private_test/private_16681.wav  \n","  inflating: /content/data/private_test/private_07954.wav  \n","  inflating: /content/data/private_test/private_11688.wav  \n","  inflating: /content/data/private_test/private_10596.wav  \n","  inflating: /content/data/private_test/private_00785.wav  \n","  inflating: /content/data/private_test/private_11850.wav  \n","  inflating: /content/data/private_test/private_05825.wav  \n","  inflating: /content/data/private_test/private_12381.wav  \n","  inflating: /content/data/private_test/private_14928.wav  \n","  inflating: /content/data/private_test/private_02192.wav  \n","  inflating: /content/data/private_test/private_05616.wav  \n","  inflating: /content/data/private_test/private_15405.wav  \n","  inflating: /content/data/private_test/private_02179.wav  \n","  inflating: /content/data/private_test/private_03267.wav  \n","  inflating: /content/data/private_test/private_04508.wav  \n","  inflating: /content/data/private_test/private_13074.wav  \n","  inflating: /content/data/private_test/private_08332.wav  \n","  inflating: /content/data/private_test/private_07001.wav  \n","  inflating: /content/data/private_test/private_18121.wav  \n","  inflating: /content/data/private_test/private_17212.wav  \n","  inflating: /content/data/private_test/private_01470.wav  \n","  inflating: /content/data/private_test/private_11663.wav  \n","  inflating: /content/data/private_test/private_11105.wav  \n","  inflating: /content/data/private_test/private_06479.wav  \n","  inflating: /content/data/private_test/private_01316.wav  \n","  inflating: /content/data/private_test/private_19559.wav  \n","  inflating: /content/data/private_test/private_18647.wav  \n","  inflating: /content/data/private_test/private_09992.wav  \n","  inflating: /content/data/private_test/private_00008.wav  \n","  inflating: /content/data/private_test/private_17574.wav  \n","  inflating: /content/data/private_test/private_08454.wav  \n","  inflating: /content/data/private_test/private_07767.wav  \n","  inflating: /content/data/private_test/private_13712.wav  \n","  inflating: /content/data/private_test/private_03501.wav  \n","  inflating: /content/data/private_test/private_15363.wav  \n","  inflating: /content/data/private_test/private_05170.wav  \n","  inflating: /content/data/private_test/private_13706.wav  \n","  inflating: /content/data/private_test/private_14069.wav  \n","  inflating: /content/data/private_test/private_03515.wav  \n","  inflating: /content/data/private_test/private_15377.wav  \n","  inflating: /content/data/private_test/private_05164.wav  \n","  inflating: /content/data/private_test/private_12418.wav  \n","  inflating: /content/data/private_test/private_11111.wav  \n","  inflating: /content/data/private_test/private_01302.wav  \n","  inflating: /content/data/private_test/private_09986.wav  \n","  inflating: /content/data/private_test/private_18653.wav  \n","  inflating: /content/data/private_test/private_17560.wav  \n","  inflating: /content/data/private_test/private_08440.wav  \n","  inflating: /content/data/private_test/private_07773.wav  \n","  inflating: /content/data/private_test/private_08326.wav  \n","  inflating: /content/data/private_test/private_10569.wav  \n","  inflating: /content/data/private_test/private_07015.wav  \n","  inflating: /content/data/private_test/private_18135.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18135.wav  \n","  inflating: /content/data/private_test/private_17206.wav  \n","  inflating: /content/data/private_test/private_01464.wav  \n","  inflating: /content/data/private_test/private_16118.wav  \n","  inflating: /content/data/private_test/private_11677.wav  \n","  inflating: /content/data/private_test/private_09038.wav  \n","  inflating: /content/data/private_test/private_05602.wav  \n","  inflating: /content/data/private_test/private_15411.wav  \n","  inflating: /content/data/private_test/private_03273.wav  \n","  inflating: /content/data/private_test/private_13060.wav  \n","  inflating: /content/data/private_test/private_10541.wav  \n","  inflating: /content/data/private_test/private_11887.wav  \n","  inflating: /content/data/private_test/private_00752.wav  \n","  inflating: /content/data/private_test/private_16130.wav  \n","  inflating: /content/data/private_test/private_19203.wav  \n","  inflating: /content/data/private_test/private_06323.wav  \n","  inflating: /content/data/private_test/private_09010.wav  \n","  inflating: /content/data/private_test/private_12356.wav  \n","  inflating: /content/data/private_test/private_02145.wav  \n","  inflating: /content/data/private_test/private_15439.wav  \n","  inflating: /content/data/private_test/private_14727.wav  \n","  inflating: /content/data/private_test/private_13048.wav  \n","  inflating: /content/data/private_test/private_04534.wav  \n","  inflating: /content/data/private_test/private_04252.wav  \n","  inflating: /content/data/private_test/private_14041.wav  \n","  inflating: /content/data/private_test/private_02623.wav  \n","  inflating: /content/data/private_test/private_12430.wav  \n","  inflating: /content/data/private_test/private_06445.wav  \n","  inflating: /content/data/private_test/private_09776.wav  \n","  inflating: /content/data/private_test/private_11139.wav  \n","  inflating: /content/data/private_test/private_07983.wav  \n","  inflating: /content/data/private_test/private_16656.wav  \n","  inflating: /content/data/private_test/private_19565.wav  \n","  inflating: /content/data/private_test/private_17548.wav  \n","  inflating: /content/data/private_test/private_00034.wav  \n","  inflating: /content/data/private_test/private_10227.wav  \n","  inflating: /content/data/private_test/private_08468.wav  \n","  inflating: /content/data/private_test/private_06451.wav  \n","  inflating: /content/data/private_test/private_09762.wav  \n","  inflating: /content/data/private_test/private_16642.wav  \n","  inflating: /content/data/private_test/private_07997.wav  \n","  inflating: /content/data/private_test/private_19571.wav  \n","  inflating: /content/data/private_test/private_00020.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00020.wav  \n","  inflating: /content/data/private_test/private_10233.wav  \n","  inflating: /content/data/private_test/private_04246.wav  \n","  inflating: /content/data/private_test/private_03529.wav  \n","  inflating: /content/data/private_test/private_14055.wav  \n","  inflating: /content/data/private_test/private_02637.wav  \n","  inflating: /content/data/private_test/private_12424.wav  \n","  inflating: /content/data/private_test/private_05158.wav  \n","  inflating: /content/data/private_test/private_12342.wav  \n","  inflating: /content/data/private_test/private_02151.wav  \n","  inflating: /content/data/private_test/private_14733.wav  \n","  inflating: /content/data/private_test/private_04520.wav  \n","  inflating: /content/data/private_test/private_07029.wav  \n","  inflating: /content/data/private_test/private_10555.wav  \n","  inflating: /content/data/private_test/private_00746.wav  \n","  inflating: /content/data/private_test/private_11893.wav  \n","  inflating: /content/data/private_test/private_18109.wav  \n","  inflating: /content/data/private_test/private_16124.wav  \n","  inflating: /content/data/private_test/private_19217.wav  \n","  inflating: /content/data/private_test/private_01458.wav  \n","  inflating: /content/data/private_test/private_06337.wav  \n","  inflating: /content/data/private_test/private_09004.wav  \n","  inflating: /content/data/private_test/private_19798.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19798.wav  \n","  inflating: /content/data/private_test/private_08695.wav  \n","  inflating: /content/data/private_test/private_19940.wav  \n","  inflating: /content/data/private_test/private_06860.wav  \n","  inflating: /content/data/private_test/private_18486.wav  \n","  inflating: /content/data/private_test/private_12815.wav  \n","  inflating: /content/data/private_test/private_03918.wav  \n","  inflating: /content/data/private_test/private_04911.wav  \n","  inflating: /content/data/private_test/private_01869.wav  \n","  inflating: /content/data/private_test/private_10964.wav  \n","  inflating: /content/data/private_test/private_10970.wav  \n","  inflating: /content/data/private_test/private_04905.wav  \n","  inflating: /content/data/private_test/private_15808.wav  \n","  inflating: /content/data/private_test/private_12801.wav  \n","  inflating: /content/data/private_test/private_08859.wav  \n","  inflating: /content/data/private_test/private_17979.wav  \n","  inflating: /content/data/private_test/private_19954.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19954.wav  \n","  inflating: /content/data/private_test/private_08681.wav  \n","  inflating: /content/data/private_test/private_06874.wav  \n","  inflating: /content/data/private_test/private_18492.wav  \n","  inflating: /content/data/private_test/private_14280.wav  \n","  inflating: /content/data/private_test/private_12829.wav  \n","  inflating: /content/data/private_test/private_04093.wav  \n","  inflating: /content/data/private_test/private_03924.wav  \n","  inflating: /content/data/private_test/private_08871.wav  \n","  inflating: /content/data/private_test/private_16497.wav  \n","  inflating: /content/data/private_test/private_17951.wav  \n","  inflating: /content/data/private_test/private_06684.wav  \n","  inflating: /content/data/private_test/private_17789.wav  \n","  inflating: /content/data/private_test/private_00593.wav  \n","  inflating: /content/data/private_test/private_01855.wav  \n","  inflating: /content/data/private_test/private_10780.wav  \n","  inflating: /content/data/private_test/private_10958.wav  \n","  inflating: /content/data/private_test/private_02384.wav  \n","  inflating: /content/data/private_test/private_12197.wav  \n","  inflating: /content/data/private_test/private_15820.wav  \n","  inflating: /content/data/private_test/private_13289.wav  \n","  inflating: /content/data/private_test/private_04939.wav  \n","  inflating: /content/data/private_test/private_02390.wav  \n","  inflating: /content/data/private_test/private_12183.wav  \n","  inflating: /content/data/private_test/private_15834.wav  \n","  inflating: /content/data/private_test/private_00587.wav  \n","  inflating: /content/data/private_test/private_10794.wav  \n","  inflating: /content/data/private_test/private_01841.wav  \n","  inflating: /content/data/private_test/private_01699.wav  \n","  inflating: /content/data/private_test/private_08865.wav  \n","  inflating: /content/data/private_test/private_16483.wav  \n","  inflating: /content/data/private_test/private_06690.wav  \n","  inflating: /content/data/private_test/private_17945.wav  \n","  inflating: /content/data/private_test/private_19968.wav  \n","  inflating: /content/data/private_test/private_06848.wav  \n","  inflating: /content/data/private_test/private_14294.wav  \n","  inflating: /content/data/private_test/private_04087.wav  \n","  inflating: /content/data/private_test/private_05399.wav  \n","  inflating: /content/data/private_test/private_03930.wav  \n","  inflating: /content/data/private_test/private_05372.wav  \n","  inflating: /content/data/private_test/private_15161.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15161.wav  \n","  inflating: /content/data/private_test/private_03703.wav  \n","  inflating: /content/data/private_test/private_13510.wav  \n","  inflating: /content/data/private_test/private_07565.wav  \n","  inflating: /content/data/private_test/private_08656.wav  \n","  inflating: /content/data/private_test/private_19983.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19983.wav  \n","  inflating: /content/data/private_test/private_10019.wav  \n","  inflating: /content/data/private_test/private_17776.wav  \n","  inflating: /content/data/private_test/private_18445.wav  \n","  inflating: /content/data/private_test/private_16468.wav  \n","  inflating: /content/data/private_test/private_01114.wav  \n","  inflating: /content/data/private_test/private_11307.wav  \n","  inflating: /content/data/private_test/private_09548.wav  \n","  inflating: /content/data/private_test/private_11461.wav  \n","  inflating: /content/data/private_test/private_01672.wav  \n","  inflating: /content/data/private_test/private_17010.wav  \n","  inflating: /content/data/private_test/private_18323.wav  \n","  inflating: /content/data/private_test/private_07203.wav  \n","  inflating: /content/data/private_test/private_08130.wav  \n","  inflating: /content/data/private_test/private_13276.wav  \n","  inflating: /content/data/private_test/private_03065.wav  \n","  inflating: /content/data/private_test/private_14519.wav  \n","  inflating: /content/data/private_test/private_15607.wav  \n","  inflating: /content/data/private_test/private_12168.wav  \n","  inflating: /content/data/private_test/private_05414.wav  \n","  inflating: /content/data/private_test/private_13262.wav  \n","  inflating: /content/data/private_test/private_03071.wav  \n","  inflating: /content/data/private_test/private_15613.wav  \n","  inflating: /content/data/private_test/private_05400.wav  \n","  inflating: /content/data/private_test/private_06109.wav  \n","  inflating: /content/data/private_test/private_11475.wav  \n","  inflating: /content/data/private_test/private_01666.wav  \n","  inflating: /content/data/private_test/private_19029.wav  \n","  inflating: /content/data/private_test/private_17004.wav  \n","  inflating: /content/data/private_test/private_18337.wav  \n","  inflating: /content/data/private_test/private_00578.wav  \n","  inflating: /content/data/private_test/private_07217.wav  \n","  inflating: /content/data/private_test/private_08124.wav  \n","  inflating: /content/data/private_test/private_07571.wav  \n","  inflating: /content/data/private_test/private_19997.wav  \n","  inflating: /content/data/private_test/private_08642.wav  \n","  inflating: /content/data/private_test/private_17762.wav  \n","  inflating: /content/data/private_test/private_18451.wav  \n","  inflating: /content/data/private_test/private_01100.wav  \n","  inflating: /content/data/private_test/private_11313.wav  \n","  inflating: /content/data/private_test/private_05366.wav  \n","  inflating: /content/data/private_test/private_02409.wav  \n","  inflating: /content/data/private_test/private_15175.wav  \n","  inflating: /content/data/private_test/private_03717.wav  \n","  inflating: /content/data/private_test/private_13504.wav  \n","  inflating: /content/data/private_test/private_04078.wav  \n","  inflating: /content/data/private_test/private_10025.wav  \n","  inflating: /content/data/private_test/private_07559.wav  \n","  inflating: /content/data/private_test/private_00236.wav  \n","  inflating: /content/data/private_test/private_18479.wav  \n","  inflating: /content/data/private_test/private_19767.wav  \n","  inflating: /content/data/private_test/private_01128.wav  \n","  inflating: /content/data/private_test/private_16454.wav  \n","  inflating: /content/data/private_test/private_09574.wav  \n","  inflating: /content/data/private_test/private_17992.wav  \n","  inflating: /content/data/private_test/private_06647.wav  \n","  inflating: /content/data/private_test/private_12632.wav  \n","  inflating: /content/data/private_test/private_02421.wav  \n","  inflating: /content/data/private_test/private_14243.wav  \n","  inflating: /content/data/private_test/private_04050.wav  \n","  inflating: /content/data/private_test/private_04736.wav  \n","  inflating: /content/data/private_test/private_14525.wav  \n","  inflating: /content/data/private_test/private_03059.wav  \n","  inflating: /content/data/private_test/private_02347.wav  \n","  inflating: /content/data/private_test/private_05428.wav  \n","  inflating: /content/data/private_test/private_12154.wav  \n","  inflating: /content/data/private_test/private_09212.wav  \n","  inflating: /content/data/private_test/private_06121.wav  \n","  inflating: /content/data/private_test/private_19001.wav  \n","  inflating: /content/data/private_test/private_16332.wav  \n","  inflating: /content/data/private_test/private_00550.wav  \n","  inflating: /content/data/private_test/private_01896.wav  \n","  inflating: /content/data/private_test/private_10743.wav  \n","  inflating: /content/data/private_test/private_09206.wav  \n","  inflating: /content/data/private_test/private_11449.wav  \n","  inflating: /content/data/private_test/private_06135.wav  \n","  inflating: /content/data/private_test/private_19015.wav  \n","  inflating: /content/data/private_test/private_16326.wav  \n","  inflating: /content/data/private_test/private_00544.wav  \n","  inflating: /content/data/private_test/private_17038.wav  \n","  inflating: /content/data/private_test/private_10757.wav  \n","  inflating: /content/data/private_test/private_01882.wav  \n","  inflating: /content/data/private_test/private_08118.wav  \n","  inflating: /content/data/private_test/private_04722.wav  \n","  inflating: /content/data/private_test/private_14531.wav  \n","  inflating: /content/data/private_test/private_02353.wav  \n","  inflating: /content/data/private_test/private_12140.wav  \n","  inflating: /content/data/private_test/private_12626.wav  \n","  inflating: /content/data/private_test/private_15149.wav  \n","  inflating: /content/data/private_test/private_02435.wav  \n","  inflating: /content/data/private_test/private_14257.wav  \n","  inflating: /content/data/private_test/private_04044.wav  \n","  inflating: /content/data/private_test/private_13538.wav  \n","  inflating: /content/data/private_test/private_10031.wav  \n","  inflating: /content/data/private_test/private_00222.wav  \n","  inflating: /content/data/private_test/private_19773.wav  \n","  inflating: /content/data/private_test/private_16440.wav  \n","  inflating: /content/data/private_test/private_09560.wav  \n","  inflating: /content/data/private_test/private_06653.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06653.wav  \n","  inflating: /content/data/private_test/private_17986.wav  \n","  inflating: /content/data/private_test/private_06914.wav  \n","  inflating: /content/data/private_test/private_19834.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19834.wav  \n","  inflating: /content/data/private_test/private_17819.wav  \n","  inflating: /content/data/private_test/private_08939.wav  \n","  inflating: /content/data/private_test/private_12961.wav  \n","  inflating: /content/data/private_test/private_15968.wav  \n","  inflating: /content/data/private_test/private_04865.wav  \n","  inflating: /content/data/private_test/private_10810.wav  \n","  inflating: /content/data/private_test/private_09399.wav  \n","  inflating: /content/data/private_test/private_08087.wav  \n","  inflating: /content/data/private_test/private_18294.wav  \n","  inflating: /content/data/private_test/private_10804.wav  \n","  inflating: /content/data/private_test/private_01909.wav  \n","  inflating: /content/data/private_test/private_08093.wav  \n","  inflating: /content/data/private_test/private_18280.wav  \n","  inflating: /content/data/private_test/private_04871.wav  \n","  inflating: /content/data/private_test/private_03878.wav  \n","  inflating: /content/data/private_test/private_12975.wav  \n","  inflating: /content/data/private_test/private_06900.wav  \n","  inflating: /content/data/private_test/private_19820.wav  \n","  inflating: /content/data/private_test/private_02596.wav  \n","  inflating: /content/data/private_test/private_03850.wav  \n","  inflating: /content/data/private_test/private_12785.wav  \n","  inflating: /content/data/private_test/private_03688.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03688.wav  \n","  inflating: /content/data/private_test/private_00381.wav  \n","  inflating: /content/data/private_test/private_06928.wav  \n","  inflating: /content/data/private_test/private_19808.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19808.wav  \n","  inflating: /content/data/private_test/private_10192.wav  \n","  inflating: /content/data/private_test/private_17825.wav  \n","  inflating: /content/data/private_test/private_08905.wav  \n","  inflating: /content/data/private_test/private_16285.wav  \n","  inflating: /content/data/private_test/private_06096.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06096.wav  \n","  inflating: /content/data/private_test/private_01921.wav  \n","  inflating: /content/data/private_test/private_07388.wav  \n","  inflating: /content/data/private_test/private_14492.wav  \n","  inflating: /content/data/private_test/private_15954.wav  \n","  inflating: /content/data/private_test/private_04681.wav  \n","  inflating: /content/data/private_test/private_04859.wav  \n","  inflating: /content/data/private_test/private_14486.wav  \n","  inflating: /content/data/private_test/private_04695.wav  \n","  inflating: /content/data/private_test/private_15940.wav  \n","  inflating: /content/data/private_test/private_15798.wav  \n","  inflating: /content/data/private_test/private_10838.wav  \n","  inflating: /content/data/private_test/private_16291.wav  \n","  inflating: /content/data/private_test/private_06082.wav  \n","  inflating: /content/data/private_test/private_01935.wav  \n","  inflating: /content/data/private_test/private_00395.wav  \n","  inflating: /content/data/private_test/private_10186.wav  \n","  inflating: /content/data/private_test/private_11298.wav  \n","  inflating: /content/data/private_test/private_17831.wav  \n","  inflating: /content/data/private_test/private_08911.wav  \n","  inflating: /content/data/private_test/private_02582.wav  \n","  inflating: /content/data/private_test/private_12791.wav  \n","  inflating: /content/data/private_test/private_03844.wav  \n","  inflating: /content/data/private_test/private_12949.wav  \n","  inflating: /content/data/private_test/private_13464.wav  \n","  inflating: /content/data/private_test/private_04118.wav  \n","  inflating: /content/data/private_test/private_03677.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03677.wav  \n","  inflating: /content/data/private_test/private_02569.wav  \n","  inflating: /content/data/private_test/private_15015.wav  \n","  inflating: /content/data/private_test/private_05206.wav  \n","  inflating: /content/data/private_test/private_11273.wav  \n","  inflating: /content/data/private_test/private_01060.wav  \n","  inflating: /content/data/private_test/private_17602.wav  \n","  inflating: /content/data/private_test/private_18531.wav  \n","  inflating: /content/data/private_test/private_07411.wav  \n","  inflating: /content/data/private_test/private_08722.wav  \n","  inflating: /content/data/private_test/private_07377.wav  \n","  inflating: /content/data/private_test/private_08044.wav  \n","  inflating: /content/data/private_test/private_17164.wav  \n","  inflating: /content/data/private_test/private_18257.wav  \n","  inflating: /content/data/private_test/private_00418.wav  \n","  inflating: /content/data/private_test/private_01706.wav  \n","  inflating: /content/data/private_test/private_19149.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19149.wav  \n","  inflating: /content/data/private_test/private_06069.wav  \n","  inflating: /content/data/private_test/private_11515.wav  \n","  inflating: /content/data/private_test/private_05560.wav  \n","  inflating: /content/data/private_test/private_15773.wav  \n","  inflating: /content/data/private_test/private_03111.wav  \n","  inflating: /content/data/private_test/private_13302.wav  \n","  inflating: /content/data/private_test/private_12008.wav  \n","  inflating: /content/data/private_test/private_05574.wav  \n","  inflating: /content/data/private_test/private_15767.wav  \n","  inflating: /content/data/private_test/private_03105.wav  \n","  inflating: /content/data/private_test/private_14479.wav  \n","  inflating: /content/data/private_test/private_13316.wav  \n","  inflating: /content/data/private_test/private_07363.wav  \n","  inflating: /content/data/private_test/private_08050.wav  \n","  inflating: /content/data/private_test/private_17170.wav  \n","  inflating: /content/data/private_test/private_18243.wav  \n","  inflating: /content/data/private_test/private_01712.wav  \n","  inflating: /content/data/private_test/private_11501.wav  \n","  inflating: /content/data/private_test/private_11267.wav  \n","  inflating: /content/data/private_test/private_09428.wav  \n","  inflating: /content/data/private_test/private_16508.wav  \n","  inflating: /content/data/private_test/private_01074.wav  \n","  inflating: /content/data/private_test/private_17616.wav  \n","  inflating: /content/data/private_test/private_18525.wav  \n","  inflating: /content/data/private_test/private_07405.wav  \n","  inflating: /content/data/private_test/private_08736.wav  \n","  inflating: /content/data/private_test/private_10179.wav  \n","  inflating: /content/data/private_test/private_13470.wav  \n","  inflating: /content/data/private_test/private_03663.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03663.wav  \n","  inflating: /content/data/private_test/private_15001.wav  \n","  inflating: /content/data/private_test/private_05212.wav  \n","  inflating: /content/data/private_test/private_09400.wav  \n","  inflating: /content/data/private_test/private_06733.wav  \n","  inflating: /content/data/private_test/private_19613.wav  \n","  inflating: /content/data/private_test/private_16520.wav  \n","  inflating: /content/data/private_test/private_00342.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00342.wav  \n","  inflating: /content/data/private_test/private_10151.wav  \n","  inflating: /content/data/private_test/private_04124.wav  \n","  inflating: /content/data/private_test/private_13458.wav  \n","  inflating: /content/data/private_test/private_14337.wav  \n","  inflating: /content/data/private_test/private_15029.wav  \n","  inflating: /content/data/private_test/private_02555.wav  \n","  inflating: /content/data/private_test/private_03893.wav  \n","  inflating: /content/data/private_test/private_12746.wav  \n","  inflating: /content/data/private_test/private_12020.wav  \n","  inflating: /content/data/private_test/private_02233.wav  \n","  inflating: /content/data/private_test/private_14451.wav  \n","  inflating: /content/data/private_test/private_15997.wav  \n","  inflating: /content/data/private_test/private_04642.wav  \n","  inflating: /content/data/private_test/private_10637.wav  \n","  inflating: /content/data/private_test/private_08078.wav  \n","  inflating: /content/data/private_test/private_00424.wav  \n","  inflating: /content/data/private_test/private_17158.wav  \n","  inflating: /content/data/private_test/private_19175.wav  \n","  inflating: /content/data/private_test/private_16246.wav  \n","  inflating: /content/data/private_test/private_09366.wav  \n","  inflating: /content/data/private_test/private_11529.wav  \n","  inflating: /content/data/private_test/private_06055.wav  \n","  inflating: /content/data/private_test/private_10623.wav  \n","  inflating: /content/data/private_test/private_00430.wav  \n","  inflating: /content/data/private_test/private_19161.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19161.wav  \n","  inflating: /content/data/private_test/private_16252.wav  \n","  inflating: /content/data/private_test/private_09372.wav  \n","  inflating: /content/data/private_test/private_06041.wav  \n","  inflating: /content/data/private_test/private_05548.wav  \n","  inflating: /content/data/private_test/private_12034.wav  \n","  inflating: /content/data/private_test/private_02227.wav  \n","  inflating: /content/data/private_test/private_14445.wav  \n","  inflating: /content/data/private_test/private_03139.wav  \n","  inflating: /content/data/private_test/private_04656.wav  \n","  inflating: /content/data/private_test/private_15983.wav  \n","  inflating: /content/data/private_test/private_04130.wav  \n","  inflating: /content/data/private_test/private_14323.wav  \n","  inflating: /content/data/private_test/private_02541.wav  \n","  inflating: /content/data/private_test/private_12752.wav  \n","  inflating: /content/data/private_test/private_03887.wav  \n","  inflating: /content/data/private_test/private_09414.wav  \n","  inflating: /content/data/private_test/private_06727.wav  \n","  inflating: /content/data/private_test/private_19607.wav  \n","  inflating: /content/data/private_test/private_01048.wav  \n","  inflating: /content/data/private_test/private_16534.wav  \n","  inflating: /content/data/private_test/private_00356.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00356.wav  \n","  inflating: /content/data/private_test/private_18519.wav  \n","  inflating: /content/data/private_test/private_10145.wav  \n","  inflating: /content/data/private_test/private_07439.wav  \n","  inflating: /content/data/private_test/private_18096.wav  \n","  inflating: /content/data/private_test/private_08285.wav  \n","  inflating: /content/data/private_test/private_00801.wav  \n","  inflating: /content/data/private_test/private_19388.wav  \n","  inflating: /content/data/private_test/private_14874.wav  \n","  inflating: /content/data/private_test/private_05979.wav  \n","  inflating: /content/data/private_test/private_02970.wav  \n","  inflating: /content/data/private_test/private_07808.wav  \n","  inflating: /content/data/private_test/private_18928.wav  \n","  inflating: /content/data/private_test/private_16905.wav  \n","  inflating: /content/data/private_test/private_09825.wav  \n","  inflating: /content/data/private_test/private_16911.wav  \n","  inflating: /content/data/private_test/private_09831.wav  \n","  inflating: /content/data/private_test/private_02964.wav  \n","  inflating: /content/data/private_test/private_13869.wav  \n","  inflating: /content/data/private_test/private_14860.wav  \n","  inflating: /content/data/private_test/private_11918.wav  \n","  inflating: /content/data/private_test/private_18082.wav  \n","  inflating: /content/data/private_test/private_08291.wav  \n","  inflating: /content/data/private_test/private_00815.wav  \n","  inflating: /content/data/private_test/private_14848.wav  \n","  inflating: /content/data/private_test/private_04483.wav  \n","  inflating: /content/data/private_test/private_05945.wav  \n","  inflating: /content/data/private_test/private_14690.wav  \n","  inflating: /content/data/private_test/private_17399.wav  \n","  inflating: /content/data/private_test/private_11930.wav  \n","  inflating: /content/data/private_test/private_06294.wav  \n","  inflating: /content/data/private_test/private_16087.wav  \n","  inflating: /content/data/private_test/private_07834.wav  \n","  inflating: /content/data/private_test/private_18914.wav  \n","  inflating: /content/data/private_test/private_16939.wav  \n","  inflating: /content/data/private_test/private_10390.wav  \n","  inflating: /content/data/private_test/private_09819.wav  \n","  inflating: /content/data/private_test/private_00183.wav  \n","  inflating: /content/data/private_test/private_13699.wav  \n","  inflating: /content/data/private_test/private_12587.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_12587.wav  \n","  inflating: /content/data/private_test/private_13841.wav  \n","  inflating: /content/data/private_test/private_02794.wav  \n","  inflating: /content/data/private_test/private_02958.wav  \n","  inflating: /content/data/private_test/private_12593.wav  \n","  inflating: /content/data/private_test/private_02780.wav  \n","  inflating: /content/data/private_test/private_13855.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13855.wav  \n","  inflating: /content/data/private_test/private_07820.wav  \n","  inflating: /content/data/private_test/private_01289.wav  \n","  inflating: /content/data/private_test/private_18900.wav  \n","  inflating: /content/data/private_test/private_10384.wav  \n","  inflating: /content/data/private_test/private_00197.wav  \n","  inflating: /content/data/private_test/private_11924.wav  \n","  inflating: /content/data/private_test/private_06280.wav  \n","  inflating: /content/data/private_test/private_00829.wav  \n","  inflating: /content/data/private_test/private_16093.wav  \n","  inflating: /content/data/private_test/private_05789.wav  \n","  inflating: /content/data/private_test/private_04497.wav  \n","  inflating: /content/data/private_test/private_14684.wav  \n","  inflating: /content/data/private_test/private_05951.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_05951.wav  \n","  inflating: /content/data/private_test/private_13100.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13100.wav  \n","  inflating: /content/data/private_test/private_03313.wav  \n","  inflating: /content/data/private_test/private_15571.wav  \n","  inflating: /content/data/private_test/private_05762.wav  \n","  inflating: /content/data/private_test/private_11717.wav  \n","  inflating: /content/data/private_test/private_09158.wav  \n","  inflating: /content/data/private_test/private_01504.wav  \n","  inflating: /content/data/private_test/private_16078.wav  \n","  inflating: /content/data/private_test/private_18055.wav  \n","  inflating: /content/data/private_test/private_17366.wav  \n","  inflating: /content/data/private_test/private_08246.wav  \n","  inflating: /content/data/private_test/private_10409.wav  \n","  inflating: /content/data/private_test/private_07175.wav  \n","  inflating: /content/data/private_test/private_08520.wav  \n","  inflating: /content/data/private_test/private_07613.wav  \n","  inflating: /content/data/private_test/private_18733.wav  \n","  inflating: /content/data/private_test/private_17400.wav  \n","  inflating: /content/data/private_test/private_01262.wav  \n","  inflating: /content/data/private_test/private_11071.wav  \n","  inflating: /content/data/private_test/private_05004.wav  \n","  inflating: /content/data/private_test/private_12578.wav  \n","  inflating: /content/data/private_test/private_15217.wav  \n","  inflating: /content/data/private_test/private_14109.wav  \n","  inflating: /content/data/private_test/private_03475.wav  \n","  inflating: /content/data/private_test/private_13666.wav  \n","  inflating: /content/data/private_test/private_05010.wav  \n","  inflating: /content/data/private_test/private_15203.wav  \n","  inflating: /content/data/private_test/private_03461.wav  \n","  inflating: /content/data/private_test/private_13672.wav  \n","  inflating: /content/data/private_test/private_08534.wav  \n","  inflating: /content/data/private_test/private_07607.wav  \n","  inflating: /content/data/private_test/private_18727.wav  \n","  inflating: /content/data/private_test/private_00168.wav  \n","  inflating: /content/data/private_test/private_17414.wav  \n","  inflating: /content/data/private_test/private_01276.wav  \n","  inflating: /content/data/private_test/private_19439.wav  \n","  inflating: /content/data/private_test/private_11065.wav  \n","  inflating: /content/data/private_test/private_06519.wav  \n","  inflating: /content/data/private_test/private_11703.wav  \n","  inflating: /content/data/private_test/private_01510.wav  \n","  inflating: /content/data/private_test/private_18041.wav  \n","  inflating: /content/data/private_test/private_17372.wav  \n","  inflating: /content/data/private_test/private_08252.wav  \n","  inflating: /content/data/private_test/private_07161.wav  \n","  inflating: /content/data/private_test/private_04468.wav  \n","  inflating: /content/data/private_test/private_13114.wav  \n","  inflating: /content/data/private_test/private_03307.wav  \n","  inflating: /content/data/private_test/private_15565.wav  \n","  inflating: /content/data/private_test/private_02019.wav  \n","  inflating: /content/data/private_test/private_05776.wav  \n","  inflating: /content/data/private_test/private_06257.wav  \n","  inflating: /content/data/private_test/private_09164.wav  \n","  inflating: /content/data/private_test/private_16044.wav  \n","  inflating: /content/data/private_test/private_19377.wav  \n","  inflating: /content/data/private_test/private_01538.wav  \n","  inflating: /content/data/private_test/private_00626.wav  \n","  inflating: /content/data/private_test/private_18069.wav  \n","  inflating: /content/data/private_test/private_07149.wav  \n","  inflating: /content/data/private_test/private_10435.wav  \n","  inflating: /content/data/private_test/private_04440.wav  \n","  inflating: /content/data/private_test/private_05986.wav  \n","  inflating: /content/data/private_test/private_14653.wav  \n","  inflating: /content/data/private_test/private_02031.wav  \n","  inflating: /content/data/private_test/private_12222.wav  \n","  inflating: /content/data/private_test/private_12544.wav  \n","  inflating: /content/data/private_test/private_05038.wav  \n","  inflating: /content/data/private_test/private_13882.wav  \n","  inflating: /content/data/private_test/private_02757.wav  \n","  inflating: /content/data/private_test/private_03449.wav  \n","  inflating: /content/data/private_test/private_14135.wav  \n","  inflating: /content/data/private_test/private_04326.wav  \n","  inflating: /content/data/private_test/private_10353.wav  \n","  inflating: /content/data/private_test/private_00140.wav  \n","  inflating: /content/data/private_test/private_16722.wav  \n","  inflating: /content/data/private_test/private_19411.wav  \n","  inflating: /content/data/private_test/private_06531.wav  \n","  inflating: /content/data/private_test/private_09602.wav  \n","  inflating: /content/data/private_test/private_10347.wav  \n","  inflating: /content/data/private_test/private_08508.wav  \n","  inflating: /content/data/private_test/private_17428.wav  \n","  inflating: /content/data/private_test/private_00154.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00154.wav  \n","  inflating: /content/data/private_test/private_16736.wav  \n","  inflating: /content/data/private_test/private_19405.wav  \n","  inflating: /content/data/private_test/private_06525.wav  \n","  inflating: /content/data/private_test/private_09616.wav  \n","  inflating: /content/data/private_test/private_11059.wav  \n","  inflating: /content/data/private_test/private_12550.wav  \n","  inflating: /content/data/private_test/private_02743.wav  \n","  inflating: /content/data/private_test/private_13896.wav  \n","  inflating: /content/data/private_test/private_14121.wav  \n","  inflating: /content/data/private_test/private_04332.wav  \n","  inflating: /content/data/private_test/private_13128.wav  \n","  inflating: /content/data/private_test/private_04454.wav  \n","  inflating: /content/data/private_test/private_14647.wav  \n","  inflating: /content/data/private_test/private_05992.wav  \n","  inflating: /content/data/private_test/private_02025.wav  \n","  inflating: /content/data/private_test/private_15559.wav  \n","  inflating: /content/data/private_test/private_12236.wav  \n","  inflating: /content/data/private_test/private_06243.wav  \n","  inflating: /content/data/private_test/private_09170.wav  \n","  inflating: /content/data/private_test/private_16050.wav  \n","  inflating: /content/data/private_test/private_19363.wav  \n","  inflating: /content/data/private_test/private_00632.wav  \n","  inflating: /content/data/private_test/private_10421.wav  \n","  inflating: /content/data/private_test/private_06533.wav  \n","  inflating: /content/data/private_test/private_09600.wav  \n","  inflating: /content/data/private_test/private_16720.wav  \n","  inflating: /content/data/private_test/private_19413.wav  \n","  inflating: /content/data/private_test/private_00142.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00142.wav  \n","  inflating: /content/data/private_test/private_10351.wav  \n","  inflating: /content/data/private_test/private_13658.wav  \n","  inflating: /content/data/private_test/private_04324.wav  \n","  inflating: /content/data/private_test/private_14137.wav  \n","  inflating: /content/data/private_test/private_02755.wav  \n","  inflating: /content/data/private_test/private_13880.wav  \n","  inflating: /content/data/private_test/private_15229.wav  \n","  inflating: /content/data/private_test/private_12546.wav  \n","  inflating: /content/data/private_test/private_12220.wav  \n","  inflating: /content/data/private_test/private_14889.wav  \n","  inflating: /content/data/private_test/private_02033.wav  \n","  inflating: /content/data/private_test/private_14651.wav  \n","  inflating: /content/data/private_test/private_05984.wav  \n","  inflating: /content/data/private_test/private_04442.wav  \n","  inflating: /content/data/private_test/private_10437.wav  \n","  inflating: /content/data/private_test/private_08278.wav  \n","  inflating: /content/data/private_test/private_17358.wav  \n","  inflating: /content/data/private_test/private_00624.wav  \n","  inflating: /content/data/private_test/private_16046.wav  \n","  inflating: /content/data/private_test/private_19375.wav  \n","  inflating: /content/data/private_test/private_06255.wav  \n","  inflating: /content/data/private_test/private_09166.wav  \n","  inflating: /content/data/private_test/private_11729.wav  \n","  inflating: /content/data/private_test/private_10423.wav  \n","  inflating: /content/data/private_test/private_00630.wav  \n","  inflating: /content/data/private_test/private_16052.wav  \n","  inflating: /content/data/private_test/private_19361.wav  \n","  inflating: /content/data/private_test/private_06241.wav  \n","  inflating: /content/data/private_test/private_09172.wav  \n","  inflating: /content/data/private_test/private_12234.wav  \n","  inflating: /content/data/private_test/private_05748.wav  \n","  inflating: /content/data/private_test/private_02027.wav  \n","  inflating: /content/data/private_test/private_03339.wav  \n","  inflating: /content/data/private_test/private_05990.wav  \n","  inflating: /content/data/private_test/private_14645.wav  \n","  inflating: /content/data/private_test/private_04456.wav  \n","  inflating: /content/data/private_test/private_02999.wav  \n","  inflating: /content/data/private_test/private_04330.wav  \n","  inflating: /content/data/private_test/private_14123.wav  \n","  inflating: /content/data/private_test/private_13894.wav  \n","  inflating: /content/data/private_test/private_02741.wav  \n","  inflating: /content/data/private_test/private_12552.wav  \n","  inflating: /content/data/private_test/private_06527.wav  \n","  inflating: /content/data/private_test/private_09614.wav  \n","  inflating: /content/data/private_test/private_16734.wav  \n","  inflating: /content/data/private_test/private_19407.wav  \n","  inflating: /content/data/private_test/private_01248.wav  \n","  inflating: /content/data/private_test/private_00156.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00156.wav  \n","  inflating: /content/data/private_test/private_18719.wav  \n","  inflating: /content/data/private_test/private_07639.wav  \n","  inflating: /content/data/private_test/private_10345.wav  \n","  inflating: /content/data/private_test/private_04318.wav  \n","  inflating: /content/data/private_test/private_13664.wav  \n","  inflating: /content/data/private_test/private_03477.wav  \n","  inflating: /content/data/private_test/private_15215.wav  \n","  inflating: /content/data/private_test/private_02769.wav  \n","  inflating: /content/data/private_test/private_05006.wav  \n","  inflating: /content/data/private_test/private_11073.wav  \n","  inflating: /content/data/private_test/private_01260.wav  \n","  inflating: /content/data/private_test/private_18731.wav  \n","  inflating: /content/data/private_test/private_17402.wav  \n","  inflating: /content/data/private_test/private_08522.wav  \n","  inflating: /content/data/private_test/private_07611.wav  \n","  inflating: /content/data/private_test/private_08244.wav  \n","  inflating: /content/data/private_test/private_07177.wav  \n","  inflating: /content/data/private_test/private_18057.wav  \n","  inflating: /content/data/private_test/private_00618.wav  \n","  inflating: /content/data/private_test/private_17364.wav  \n","  inflating: /content/data/private_test/private_01506.wav  \n","  inflating: /content/data/private_test/private_19349.wav  \n","  inflating: /content/data/private_test/private_11715.wav  \n","  inflating: /content/data/private_test/private_06269.wav  \n","  inflating: /content/data/private_test/private_05760.wav  \n","  inflating: /content/data/private_test/private_15573.wav  \n","  inflating: /content/data/private_test/private_03311.wav  \n","  inflating: /content/data/private_test/private_13102.wav  \n","  inflating: /content/data/private_test/private_05774.wav  \n","  inflating: /content/data/private_test/private_12208.wav  \n","  inflating: /content/data/private_test/private_15567.wav  \n","  inflating: /content/data/private_test/private_14679.wav  \n","  inflating: /content/data/private_test/private_03305.wav  \n","  inflating: /content/data/private_test/private_13116.wav  \n","  inflating: /content/data/private_test/private_08250.wav  \n","  inflating: /content/data/private_test/private_07163.wav  \n","  inflating: /content/data/private_test/private_18043.wav  \n","  inflating: /content/data/private_test/private_17370.wav  \n","  inflating: /content/data/private_test/private_01512.wav  \n","  inflating: /content/data/private_test/private_11701.wav  \n","  inflating: /content/data/private_test/private_11067.wav  \n","  inflating: /content/data/private_test/private_09628.wav  \n","  inflating: /content/data/private_test/private_01274.wav  \n","  inflating: /content/data/private_test/private_16708.wav  \n","  inflating: /content/data/private_test/private_18725.wav  \n","  inflating: /content/data/private_test/private_17416.wav  \n","  inflating: /content/data/private_test/private_08536.wav  \n","  inflating: /content/data/private_test/private_10379.wav  \n","  inflating: /content/data/private_test/private_07605.wav  \n","  inflating: /content/data/private_test/private_13670.wav  \n","  inflating: /content/data/private_test/private_03463.wav  \n","  inflating: /content/data/private_test/private_15201.wav  \n","  inflating: /content/data/private_test/private_05012.wav  \n","  inflating: /content/data/private_test/private_02796.wav  \n","  inflating: /content/data/private_test/private_13843.wav  \n","  inflating: /content/data/private_test/private_12585.wav  \n","  inflating: /content/data/private_test/private_03488.wav  \n","  inflating: /content/data/private_test/private_00181.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00181.wav  \n","  inflating: /content/data/private_test/private_10392.wav  \n","  inflating: /content/data/private_test/private_18916.wav  \n","  inflating: /content/data/private_test/private_07836.wav  \n","  inflating: /content/data/private_test/private_16085.wav  \n","  inflating: /content/data/private_test/private_06296.wav  \n","  inflating: /content/data/private_test/private_07188.wav  \n","  inflating: /content/data/private_test/private_11932.wav  \n","  inflating: /content/data/private_test/private_14692.wav  \n","  inflating: /content/data/private_test/private_05947.wav  \n","  inflating: /content/data/private_test/private_04481.wav  \n","  inflating: /content/data/private_test/private_05953.wav  \n","  inflating: /content/data/private_test/private_14686.wav  \n","  inflating: /content/data/private_test/private_04495.wav  \n","  inflating: /content/data/private_test/private_15598.wav  \n","  inflating: /content/data/private_test/private_16091.wav  \n","  inflating: /content/data/private_test/private_06282.wav  \n","  inflating: /content/data/private_test/private_11926.wav  \n","  inflating: /content/data/private_test/private_00195.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00195.wav  \n","  inflating: /content/data/private_test/private_10386.wav  \n","  inflating: /content/data/private_test/private_18902.wav  \n","  inflating: /content/data/private_test/private_11098.wav  \n","  inflating: /content/data/private_test/private_07822.wav  \n","  inflating: /content/data/private_test/private_13857.wav  \n","  inflating: /content/data/private_test/private_02782.wav  \n","  inflating: /content/data/private_test/private_12591.wav  \n","  inflating: /content/data/private_test/private_09827.wav  \n","  inflating: /content/data/private_test/private_16907.wav  \n","  inflating: /content/data/private_test/private_02972.wav  \n","  inflating: /content/data/private_test/private_14876.wav  \n","  inflating: /content/data/private_test/private_00803.wav  \n","  inflating: /content/data/private_test/private_09199.wav  \n","  inflating: /content/data/private_test/private_08287.wav  \n","  inflating: /content/data/private_test/private_18094.wav  \n","  inflating: /content/data/private_test/private_00817.wav  \n","  inflating: /content/data/private_test/private_08293.wav  \n","  inflating: /content/data/private_test/private_18080.wav  \n","  inflating: /content/data/private_test/private_14862.wav  \n","  inflating: /content/data/private_test/private_02966.wav  \n","  inflating: /content/data/private_test/private_09833.wav  \n","  inflating: /content/data/private_test/private_16913.wav  \n","  inflating: /content/data/private_test/private_09364.wav  \n","  inflating: /content/data/private_test/private_06057.wav  \n","  inflating: /content/data/private_test/private_19177.wav  \n","  inflating: /content/data/private_test/private_01738.wav  \n","  inflating: /content/data/private_test/private_16244.wav  \n","  inflating: /content/data/private_test/private_00426.wav  \n","  inflating: /content/data/private_test/private_18269.wav  \n","  inflating: /content/data/private_test/private_10635.wav  \n","  inflating: /content/data/private_test/private_07349.wav  \n","  inflating: /content/data/private_test/private_04640.wav  \n","  inflating: /content/data/private_test/private_15995.wav  \n","  inflating: /content/data/private_test/private_14453.wav  \n","  inflating: /content/data/private_test/private_04898.wav  \n","  inflating: /content/data/private_test/private_02231.wav  \n","  inflating: /content/data/private_test/private_12022.wav  \n","  inflating: /content/data/private_test/private_05238.wav  \n","  inflating: /content/data/private_test/private_12744.wav  \n","  inflating: /content/data/private_test/private_03891.wav  \n","  inflating: /content/data/private_test/private_02557.wav  \n","  inflating: /content/data/private_test/private_14335.wav  \n","  inflating: /content/data/private_test/private_03649.wav  \n","  inflating: /content/data/private_test/private_04126.wav  \n","  inflating: /content/data/private_test/private_10153.wav  \n","  inflating: /content/data/private_test/private_00340.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00340.wav  \n","  inflating: /content/data/private_test/private_19611.wav  \n","  inflating: /content/data/private_test/private_16522.wav  \n","  inflating: /content/data/private_test/private_09402.wav  \n","  inflating: /content/data/private_test/private_06731.wav  \n","  inflating: /content/data/private_test/private_10147.wav  \n","  inflating: /content/data/private_test/private_08708.wav  \n","  inflating: /content/data/private_test/private_00354.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00354.wav  \n","  inflating: /content/data/private_test/private_17628.wav  \n","  inflating: /content/data/private_test/private_19605.wav  \n","  inflating: /content/data/private_test/private_16536.wav  \n","  inflating: /content/data/private_test/private_09416.wav  \n","  inflating: /content/data/private_test/private_11259.wav  \n","  inflating: /content/data/private_test/private_06725.wav  \n","  inflating: /content/data/private_test/private_03885.wav  \n","  inflating: /content/data/private_test/private_12750.wav  \n","  inflating: /content/data/private_test/private_02543.wav  \n","  inflating: /content/data/private_test/private_14321.wav  \n","  inflating: /content/data/private_test/private_12988.wav  \n","  inflating: /content/data/private_test/private_04132.wav  \n","  inflating: /content/data/private_test/private_15981.wav  \n","  inflating: /content/data/private_test/private_04654.wav  \n","  inflating: /content/data/private_test/private_13328.wav  \n","  inflating: /content/data/private_test/private_14447.wav  \n","  inflating: /content/data/private_test/private_15759.wav  \n","  inflating: /content/data/private_test/private_02225.wav  \n","  inflating: /content/data/private_test/private_12036.wav  \n","  inflating: /content/data/private_test/private_09370.wav  \n","  inflating: /content/data/private_test/private_06043.wav  \n","  inflating: /content/data/private_test/private_19163.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19163.wav  \n","  inflating: /content/data/private_test/private_16250.wav  \n","  inflating: /content/data/private_test/private_00432.wav  \n","  inflating: /content/data/private_test/private_10621.wav  \n","  inflating: /content/data/private_test/private_13300.wav  \n","  inflating: /content/data/private_test/private_03113.wav  \n","  inflating: /content/data/private_test/private_15771.wav  \n","  inflating: /content/data/private_test/private_05562.wav  \n","  inflating: /content/data/private_test/private_11517.wav  \n","  inflating: /content/data/private_test/private_09358.wav  \n","  inflating: /content/data/private_test/private_16278.wav  \n","  inflating: /content/data/private_test/private_01704.wav  \n","  inflating: /content/data/private_test/private_17166.wav  \n","  inflating: /content/data/private_test/private_18255.wav  \n","  inflating: /content/data/private_test/private_07375.wav  \n","  inflating: /content/data/private_test/private_08046.wav  \n","  inflating: /content/data/private_test/private_10609.wav  \n","  inflating: /content/data/private_test/private_07413.wav  \n","  inflating: /content/data/private_test/private_08720.wav  \n","  inflating: /content/data/private_test/private_17600.wav  \n","  inflating: /content/data/private_test/private_18533.wav  \n","  inflating: /content/data/private_test/private_01062.wav  \n","  inflating: /content/data/private_test/private_11271.wav  \n","  inflating: /content/data/private_test/private_12778.wav  \n","  inflating: /content/data/private_test/private_05204.wav  \n","  inflating: /content/data/private_test/private_15017.wav  \n","  inflating: /content/data/private_test/private_03675.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03675.wav  \n","  inflating: /content/data/private_test/private_14309.wav  \n","  inflating: /content/data/private_test/private_13466.wav  \n","  inflating: /content/data/private_test/private_05210.wav  \n","  inflating: /content/data/private_test/private_15003.wav  \n","  inflating: /content/data/private_test/private_03661.wav  \n","  inflating: /content/data/private_test/private_13472.wav  \n","  inflating: /content/data/private_test/private_07407.wav  \n","  inflating: /content/data/private_test/private_08734.wav  \n","  inflating: /content/data/private_test/private_17614.wav  \n","  inflating: /content/data/private_test/private_18527.wav  \n","  inflating: /content/data/private_test/private_00368.wav  \n","  inflating: /content/data/private_test/private_01076.wav  \n","  inflating: /content/data/private_test/private_19639.wav  \n","  inflating: /content/data/private_test/private_06719.wav  \n","  inflating: /content/data/private_test/private_11265.wav  \n","  inflating: /content/data/private_test/private_11503.wav  \n","  inflating: /content/data/private_test/private_01710.wav  \n","  inflating: /content/data/private_test/private_17172.wav  \n","  inflating: /content/data/private_test/private_18241.wav  \n","  inflating: /content/data/private_test/private_07361.wav  \n","  inflating: /content/data/private_test/private_08052.wav  \n","  inflating: /content/data/private_test/private_13314.wav  \n","  inflating: /content/data/private_test/private_04668.wav  \n","  inflating: /content/data/private_test/private_03107.wav  \n","  inflating: /content/data/private_test/private_02219.wav  \n","  inflating: /content/data/private_test/private_15765.wav  \n","  inflating: /content/data/private_test/private_05576.wav  \n","  inflating: /content/data/private_test/private_04683.wav  \n","  inflating: /content/data/private_test/private_15956.wav  \n","  inflating: /content/data/private_test/private_14490.wav  \n","  inflating: /content/data/private_test/private_17199.wav  \n","  inflating: /content/data/private_test/private_01923.wav  \n","  inflating: /content/data/private_test/private_06094.wav  \n","  inflating: /content/data/private_test/private_16287.wav  \n","  inflating: /content/data/private_test/private_08907.wav  \n","  inflating: /content/data/private_test/private_17827.wav  \n","  inflating: /content/data/private_test/private_10190.wav  \n","  inflating: /content/data/private_test/private_00383.wav  \n","  inflating: /content/data/private_test/private_13499.wav  \n","  inflating: /content/data/private_test/private_12787.wav  \n","  inflating: /content/data/private_test/private_03852.wav  \n","  inflating: /content/data/private_test/private_02594.wav  \n","  inflating: /content/data/private_test/private_03846.wav  \n","  inflating: /content/data/private_test/private_12793.wav  \n","  inflating: /content/data/private_test/private_02580.wav  \n","  inflating: /content/data/private_test/private_08913.wav  \n","  inflating: /content/data/private_test/private_01089.wav  \n","  inflating: /content/data/private_test/private_17833.wav  \n","  inflating: /content/data/private_test/private_10184.wav  \n","  inflating: /content/data/private_test/private_00397.wav  \n","  inflating: /content/data/private_test/private_01937.wav  \n","  inflating: /content/data/private_test/private_06080.wav  \n","  inflating: /content/data/private_test/private_16293.wav  \n","  inflating: /content/data/private_test/private_05589.wav  \n","  inflating: /content/data/private_test/private_15942.wav  \n","  inflating: /content/data/private_test/private_04697.wav  \n","  inflating: /content/data/private_test/private_14484.wav  \n","  inflating: /content/data/private_test/private_18296.wav  \n","  inflating: /content/data/private_test/private_08085.wav  \n","  inflating: /content/data/private_test/private_10812.wav  \n","  inflating: /content/data/private_test/private_19188.wav  \n","  inflating: /content/data/private_test/private_04867.wav  \n","  inflating: /content/data/private_test/private_12963.wav  \n","  inflating: /content/data/private_test/private_19836.wav  \n","  inflating: /content/data/private_test/private_06916.wav  \n","  inflating: /content/data/private_test/private_19822.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19822.wav  \n","  inflating: /content/data/private_test/private_06902.wav  \n","  inflating: /content/data/private_test/private_12977.wav  \n","  inflating: /content/data/private_test/private_04873.wav  \n","  inflating: /content/data/private_test/private_18282.wav  \n","  inflating: /content/data/private_test/private_08091.wav  \n","  inflating: /content/data/private_test/private_10806.wav  \n","  inflating: /content/data/private_test/private_10741.wav  \n","  inflating: /content/data/private_test/private_01894.wav  \n","  inflating: /content/data/private_test/private_00552.wav  \n","  inflating: /content/data/private_test/private_10999.wav  \n","  inflating: /content/data/private_test/private_19003.wav  \n","  inflating: /content/data/private_test/private_16330.wav  \n","  inflating: /content/data/private_test/private_09210.wav  \n","  inflating: /content/data/private_test/private_06123.wav  \n","  inflating: /content/data/private_test/private_12156.wav  \n","  inflating: /content/data/private_test/private_15639.wav  \n","  inflating: /content/data/private_test/private_02345.wav  \n","  inflating: /content/data/private_test/private_14527.wav  \n","  inflating: /content/data/private_test/private_04734.wav  \n","  inflating: /content/data/private_test/private_13248.wav  \n","  inflating: /content/data/private_test/private_04052.wav  \n","  inflating: /content/data/private_test/private_14241.wav  \n","  inflating: /content/data/private_test/private_02423.wav  \n","  inflating: /content/data/private_test/private_12630.wav  \n","  inflating: /content/data/private_test/private_09576.wav  \n","  inflating: /content/data/private_test/private_11339.wav  \n","  inflating: /content/data/private_test/private_06645.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06645.wav  \n","  inflating: /content/data/private_test/private_17990.wav  \n","  inflating: /content/data/private_test/private_19765.wav  \n","  inflating: /content/data/private_test/private_16456.wav  \n","  inflating: /content/data/private_test/private_00234.wav  \n","  inflating: /content/data/private_test/private_17748.wav  \n","  inflating: /content/data/private_test/private_10027.wav  \n","  inflating: /content/data/private_test/private_08668.wav  \n","  inflating: /content/data/private_test/private_09562.wav  \n","  inflating: /content/data/private_test/private_17984.wav  \n","  inflating: /content/data/private_test/private_06651.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06651.wav  \n","  inflating: /content/data/private_test/private_19771.wav  \n","  inflating: /content/data/private_test/private_16442.wav  \n","  inflating: /content/data/private_test/private_00220.wav  \n","  inflating: /content/data/private_test/private_06889.wav  \n","  inflating: /content/data/private_test/private_10033.wav  \n","  inflating: /content/data/private_test/private_04046.wav  \n","  inflating: /content/data/private_test/private_14255.wav  \n","  inflating: /content/data/private_test/private_03729.wav  \n","  inflating: /content/data/private_test/private_02437.wav  \n","  inflating: /content/data/private_test/private_05358.wav  \n","  inflating: /content/data/private_test/private_12624.wav  \n","  inflating: /content/data/private_test/private_12142.wav  \n","  inflating: /content/data/private_test/private_02351.wav  \n","  inflating: /content/data/private_test/private_14533.wav  \n","  inflating: /content/data/private_test/private_04720.wav  \n","  inflating: /content/data/private_test/private_01880.wav  \n","  inflating: /content/data/private_test/private_10755.wav  \n","  inflating: /content/data/private_test/private_07229.wav  \n","  inflating: /content/data/private_test/private_00546.wav  \n","  inflating: /content/data/private_test/private_18309.wav  \n","  inflating: /content/data/private_test/private_19017.wav  \n","  inflating: /content/data/private_test/private_01658.wav  \n","  inflating: /content/data/private_test/private_16324.wav  \n","  inflating: /content/data/private_test/private_09204.wav  \n","  inflating: /content/data/private_test/private_06137.wav  \n","  inflating: /content/data/private_test/private_05416.wav  \n","  inflating: /content/data/private_test/private_02379.wav  \n","  inflating: /content/data/private_test/private_15605.wav  \n","  inflating: /content/data/private_test/private_03067.wav  \n","  inflating: /content/data/private_test/private_13274.wav  \n","  inflating: /content/data/private_test/private_04708.wav  \n","  inflating: /content/data/private_test/private_07201.wav  \n","  inflating: /content/data/private_test/private_08132.wav  \n","  inflating: /content/data/private_test/private_17012.wav  \n","  inflating: /content/data/private_test/private_18321.wav  \n","  inflating: /content/data/private_test/private_01670.wav  \n","  inflating: /content/data/private_test/private_11463.wav  \n","  inflating: /content/data/private_test/private_06679.wav  \n","  inflating: /content/data/private_test/private_11305.wav  \n","  inflating: /content/data/private_test/private_01116.wav  \n","  inflating: /content/data/private_test/private_19759.wav  \n","  inflating: /content/data/private_test/private_17774.wav  \n","  inflating: /content/data/private_test/private_18447.wav  \n","  inflating: /content/data/private_test/private_00208.wav  \n","  inflating: /content/data/private_test/private_07567.wav  \n","  inflating: /content/data/private_test/private_19981.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19981.wav  \n","  inflating: /content/data/private_test/private_08654.wav  \n","  inflating: /content/data/private_test/private_13512.wav  \n","  inflating: /content/data/private_test/private_03701.wav  \n","  inflating: /content/data/private_test/private_15163.wav  \n","  inflating: /content/data/private_test/private_05370.wav  \n","  inflating: /content/data/private_test/private_13506.wav  \n","  inflating: /content/data/private_test/private_03715.wav  \n","  inflating: /content/data/private_test/private_14269.wav  \n","  inflating: /content/data/private_test/private_15177.wav  \n","  inflating: /content/data/private_test/private_12618.wav  \n","  inflating: /content/data/private_test/private_05364.wav  \n","  inflating: /content/data/private_test/private_11311.wav  \n","  inflating: /content/data/private_test/private_08898.wav  \n","  inflating: /content/data/private_test/private_01102.wav  \n","  inflating: /content/data/private_test/private_17760.wav  \n","  inflating: /content/data/private_test/private_18453.wav  \n","  inflating: /content/data/private_test/private_07573.wav  \n","  inflating: /content/data/private_test/private_08640.wav  \n","  inflating: /content/data/private_test/private_19995.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19995.wav  \n","  inflating: /content/data/private_test/private_07215.wav  \n","  inflating: /content/data/private_test/private_08126.wav  \n","  inflating: /content/data/private_test/private_10769.wav  \n","  inflating: /content/data/private_test/private_17006.wav  \n","  inflating: /content/data/private_test/private_18335.wav  \n","  inflating: /content/data/private_test/private_16318.wav  \n","  inflating: /content/data/private_test/private_01664.wav  \n","  inflating: /content/data/private_test/private_11477.wav  \n","  inflating: /content/data/private_test/private_09238.wav  \n","  inflating: /content/data/private_test/private_05402.wav  \n","  inflating: /content/data/private_test/private_15611.wav  \n","  inflating: /content/data/private_test/private_03073.wav  \n","  inflating: /content/data/private_test/private_13260.wav  \n","  inflating: /content/data/private_test/private_03098.wav  \n","  inflating: /content/data/private_test/private_15822.wav  \n","  inflating: /content/data/private_test/private_12195.wav  \n","  inflating: /content/data/private_test/private_02386.wav  \n","  inflating: /content/data/private_test/private_10782.wav  \n","  inflating: /content/data/private_test/private_01857.wav  \n","  inflating: /content/data/private_test/private_00591.wav  \n","  inflating: /content/data/private_test/private_07598.wav  \n","  inflating: /content/data/private_test/private_06686.wav  \n","  inflating: /content/data/private_test/private_17953.wav  \n","  inflating: /content/data/private_test/private_08873.wav  \n","  inflating: /content/data/private_test/private_16495.wav  \n","  inflating: /content/data/private_test/private_03926.wav  \n","  inflating: /content/data/private_test/private_04091.wav  \n","  inflating: /content/data/private_test/private_14282.wav  \n","  inflating: /content/data/private_test/private_15188.wav  \n","  inflating: /content/data/private_test/private_03932.wav  \n","  inflating: /content/data/private_test/private_04085.wav  \n","  inflating: /content/data/private_test/private_14296.wav  \n","  inflating: /content/data/private_test/private_17947.wav  \n","  inflating: /content/data/private_test/private_06692.wav  \n","  inflating: /content/data/private_test/private_08867.wav  \n","  inflating: /content/data/private_test/private_16481.wav  \n","  inflating: /content/data/private_test/private_11488.wav  \n","  inflating: /content/data/private_test/private_01843.wav  \n","  inflating: /content/data/private_test/private_10796.wav  \n","  inflating: /content/data/private_test/private_00585.wav  \n","  inflating: /content/data/private_test/private_15836.wav  \n","  inflating: /content/data/private_test/private_12181.wav  \n","  inflating: /content/data/private_test/private_02392.wav  \n","  inflating: /content/data/private_test/private_10966.wav  \n","  inflating: /content/data/private_test/private_04913.wav  \n","  inflating: /content/data/private_test/private_12817.wav  \n","  inflating: /content/data/private_test/private_06862.wav  \n","  inflating: /content/data/private_test/private_18484.wav  \n","  inflating: /content/data/private_test/private_19942.wav  \n","  inflating: /content/data/private_test/private_08697.wav  \n","  inflating: /content/data/private_test/private_09589.wav  \n","  inflating: /content/data/private_test/private_06876.wav  \n","  inflating: /content/data/private_test/private_18490.wav  \n","  inflating: /content/data/private_test/private_08683.wav  \n","  inflating: /content/data/private_test/private_19956.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19956.wav  \n","  inflating: /content/data/private_test/private_12803.wav  \n","  inflating: /content/data/private_test/private_04907.wav  \n","  inflating: /content/data/private_test/private_10972.wav  \n","  inflating: /content/data/private_test/private_07759.wav  \n","  inflating: /content/data/private_test/private_10225.wav  \n","  inflating: /content/data/private_test/private_00036.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00036.wav  \n","  inflating: /content/data/private_test/private_18679.wav  \n","  inflating: /content/data/private_test/private_16654.wav  \n","  inflating: /content/data/private_test/private_07981.wav  \n","  inflating: /content/data/private_test/private_19567.wav  \n","  inflating: /content/data/private_test/private_01328.wav  \n","  inflating: /content/data/private_test/private_06447.wav  \n","  inflating: /content/data/private_test/private_09774.wav  \n","  inflating: /content/data/private_test/private_12432.wav  \n","  inflating: /content/data/private_test/private_02621.wav  \n","  inflating: /content/data/private_test/private_14043.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_14043.wav  \n","  inflating: /content/data/private_test/private_04250.wav  \n","  inflating: /content/data/private_test/private_04536.wav  \n","  inflating: /content/data/private_test/private_03259.wav  \n","  inflating: /content/data/private_test/private_14725.wav  \n","  inflating: /content/data/private_test/private_02147.wav  \n","  inflating: /content/data/private_test/private_12354.wav  \n","  inflating: /content/data/private_test/private_05628.wav  \n","  inflating: /content/data/private_test/private_06321.wav  \n","  inflating: /content/data/private_test/private_00988.wav  \n","  inflating: /content/data/private_test/private_09012.wav  \n","  inflating: /content/data/private_test/private_16132.wav  \n","  inflating: /content/data/private_test/private_19201.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19201.wav  \n","  inflating: /content/data/private_test/private_00750.wav  \n","  inflating: /content/data/private_test/private_11885.wav  \n","  inflating: /content/data/private_test/private_10543.wav  \n","  inflating: /content/data/private_test/private_06335.wav  \n","  inflating: /content/data/private_test/private_09006.wav  \n","  inflating: /content/data/private_test/private_11649.wav  \n","  inflating: /content/data/private_test/private_16126.wav  \n","  inflating: /content/data/private_test/private_19215.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19215.wav  \n","  inflating: /content/data/private_test/private_17238.wav  \n","  inflating: /content/data/private_test/private_11891.wav  \n","  inflating: /content/data/private_test/private_00744.wav  \n","  inflating: /content/data/private_test/private_10557.wav  \n","  inflating: /content/data/private_test/private_08318.wav  \n","  inflating: /content/data/private_test/private_04522.wav  \n","  inflating: /content/data/private_test/private_14731.wav  \n","  inflating: /content/data/private_test/private_02153.wav  \n","  inflating: /content/data/private_test/private_12340.wav  \n","  inflating: /content/data/private_test/private_12426.wav  \n","  inflating: /content/data/private_test/private_02635.wav  \n","  inflating: /content/data/private_test/private_15349.wav  \n","  inflating: /content/data/private_test/private_14057.wav  \n","  inflating: /content/data/private_test/private_13738.wav  \n","  inflating: /content/data/private_test/private_04244.wav  \n","  inflating: /content/data/private_test/private_16898.wav  \n","  inflating: /content/data/private_test/private_10231.wav  \n","  inflating: /content/data/private_test/private_00022.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00022.wav  \n","  inflating: /content/data/private_test/private_07995.wav  \n","  inflating: /content/data/private_test/private_16640.wav  \n","  inflating: /content/data/private_test/private_19573.wav  \n","  inflating: /content/data/private_test/private_06453.wav  \n","  inflating: /content/data/private_test/private_09760.wav  \n","  inflating: /content/data/private_test/private_05172.wav  \n","  inflating: /content/data/private_test/private_15361.wav  \n","  inflating: /content/data/private_test/private_03503.wav  \n","  inflating: /content/data/private_test/private_13710.wav  \n","  inflating: /content/data/private_test/private_08456.wav  \n","  inflating: /content/data/private_test/private_10219.wav  \n","  inflating: /content/data/private_test/private_07765.wav  \n","  inflating: /content/data/private_test/private_09990.wav  \n","  inflating: /content/data/private_test/private_18645.wav  \n","  inflating: /content/data/private_test/private_17576.wav  \n","  inflating: /content/data/private_test/private_01314.wav  \n","  inflating: /content/data/private_test/private_16668.wav  \n","  inflating: /content/data/private_test/private_11107.wav  \n","  inflating: /content/data/private_test/private_09748.wav  \n","  inflating: /content/data/private_test/private_11661.wav  \n","  inflating: /content/data/private_test/private_01472.wav  \n","  inflating: /content/data/private_test/private_18123.wav  \n","  inflating: /content/data/private_test/private_17210.wav  \n","  inflating: /content/data/private_test/private_08330.wav  \n","  inflating: /content/data/private_test/private_07003.wav  \n","  inflating: /content/data/private_test/private_13076.wav  \n","  inflating: /content/data/private_test/private_14719.wav  \n","  inflating: /content/data/private_test/private_03265.wav  \n","  inflating: /content/data/private_test/private_15407.wav  \n","  inflating: /content/data/private_test/private_05614.wav  \n","  inflating: /content/data/private_test/private_12368.wav  \n","  inflating: /content/data/private_test/private_13062.wav  \n","  inflating: /content/data/private_test/private_03271.wav  \n","  inflating: /content/data/private_test/private_15413.wav  \n","  inflating: /content/data/private_test/private_05600.wav  \n","  inflating: /content/data/private_test/private_11675.wav  \n","  inflating: /content/data/private_test/private_06309.wav  \n","  inflating: /content/data/private_test/private_01466.wav  \n","  inflating: /content/data/private_test/private_19229.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19229.wav  \n","  inflating: /content/data/private_test/private_18137.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18137.wav  \n","  inflating: /content/data/private_test/private_00778.wav  \n","  inflating: /content/data/private_test/private_17204.wav  \n","  inflating: /content/data/private_test/private_08324.wav  \n","  inflating: /content/data/private_test/private_07017.wav  \n","  inflating: /content/data/private_test/private_08442.wav  \n","  inflating: /content/data/private_test/private_07771.wav  \n","  inflating: /content/data/private_test/private_18651.wav  \n","  inflating: /content/data/private_test/private_09984.wav  \n","  inflating: /content/data/private_test/private_17562.wav  \n","  inflating: /content/data/private_test/private_01300.wav  \n","  inflating: /content/data/private_test/private_18889.wav  \n","  inflating: /content/data/private_test/private_11113.wav  \n","  inflating: /content/data/private_test/private_05166.wav  \n","  inflating: /content/data/private_test/private_15375.wav  \n","  inflating: /content/data/private_test/private_02609.wav  \n","  inflating: /content/data/private_test/private_03517.wav  \n","  inflating: /content/data/private_test/private_04278.wav  \n","  inflating: /content/data/private_test/private_13704.wav  \n","  inflating: /content/data/private_test/private_14080.wav  \n","  inflating: /content/data/private_test/private_04293.wav  \n","  inflating: /content/data/private_test/private_13937.wav  \n","  inflating: /content/data/private_test/private_16697.wav  \n","  inflating: /content/data/private_test/private_07942.wav  \n","  inflating: /content/data/private_test/private_06484.wav  \n","  inflating: /content/data/private_test/private_18862.wav  \n","  inflating: /content/data/private_test/private_17589.wav  \n","  inflating: /content/data/private_test/private_00793.wav  \n","  inflating: /content/data/private_test/private_11846.wav  \n","  inflating: /content/data/private_test/private_10580.wav  \n","  inflating: /content/data/private_test/private_02184.wav  \n","  inflating: /content/data/private_test/private_12397.wav  \n","  inflating: /content/data/private_test/private_13089.wav  \n","  inflating: /content/data/private_test/private_05833.wav  \n","  inflating: /content/data/private_test/private_02190.wav  \n","  inflating: /content/data/private_test/private_12383.wav  \n","  inflating: /content/data/private_test/private_05827.wav  \n","  inflating: /content/data/private_test/private_11852.wav  \n","  inflating: /content/data/private_test/private_00787.wav  \n","  inflating: /content/data/private_test/private_10594.wav  \n","  inflating: /content/data/private_test/private_01499.wav  \n","  inflating: /content/data/private_test/private_07956.wav  \n","  inflating: /content/data/private_test/private_16683.wav  \n","  inflating: /content/data/private_test/private_06490.wav  \n","  inflating: /content/data/private_test/private_18876.wav  \n","  inflating: /content/data/private_test/private_14094.wav  \n","  inflating: /content/data/private_test/private_04287.wav  \n","  inflating: /content/data/private_test/private_05199.wav  \n","  inflating: /content/data/private_test/private_13923.wav  \n","  inflating: /content/data/private_test/private_19598.wav  \n","  inflating: /content/data/private_test/private_08495.wav  \n","  inflating: /content/data/private_test/private_16873.wav  \n","  inflating: /content/data/private_test/private_09953.wav  \n","  inflating: /content/data/private_test/private_18686.wav  \n","  inflating: /content/data/private_test/private_02806.wav  \n","  inflating: /content/data/private_test/private_14902.wav  \n","  inflating: /content/data/private_test/private_00977.wav  \n","  inflating: /content/data/private_test/private_00963.wav  \n","  inflating: /content/data/private_test/private_14916.wav  \n","  inflating: /content/data/private_test/private_02812.wav  \n","  inflating: /content/data/private_test/private_08481.wav  \n","  inflating: /content/data/private_test/private_16867.wav  \n","  inflating: /content/data/private_test/private_18692.wav  \n","  inflating: /content/data/private_test/private_09947.wav  \n","  inflating: /content/data/private_test/private_14917.wav  \n","  inflating: /content/data/private_test/private_00962.wav  \n","  inflating: /content/data/private_test/private_09946.wav  \n","  inflating: /content/data/private_test/private_18693.wav  \n","  inflating: /content/data/private_test/private_16866.wav  \n","  inflating: /content/data/private_test/private_08480.wav  \n","  inflating: /content/data/private_test/private_02813.wav  \n","  inflating: /content/data/private_test/private_02807.wav  \n","  inflating: /content/data/private_test/private_18687.wav  \n","  inflating: /content/data/private_test/private_09952.wav  \n","  inflating: /content/data/private_test/private_16872.wav  \n","  inflating: /content/data/private_test/private_08494.wav  \n","  inflating: /content/data/private_test/private_19599.wav  \n","  inflating: /content/data/private_test/private_00976.wav  \n","  inflating: /content/data/private_test/private_14903.wav  \n","  inflating: /content/data/private_test/private_01498.wav  \n","  inflating: /content/data/private_test/private_10595.wav  \n","  inflating: /content/data/private_test/private_00786.wav  \n","  inflating: /content/data/private_test/private_11853.wav  \n","  inflating: /content/data/private_test/private_05826.wav  \n","  inflating: /content/data/private_test/private_12382.wav  \n","  inflating: /content/data/private_test/private_02191.wav  \n","  inflating: /content/data/private_test/private_13922.wav  \n","  inflating: /content/data/private_test/private_05198.wav  \n","  inflating: /content/data/private_test/private_04286.wav  \n","  inflating: /content/data/private_test/private_14095.wav  \n","  inflating: /content/data/private_test/private_18877.wav  \n","  inflating: /content/data/private_test/private_06491.wav  \n","  inflating: /content/data/private_test/private_16682.wav  \n","  inflating: /content/data/private_test/private_07957.wav  \n","  inflating: /content/data/private_test/private_17588.wav  \n","  inflating: /content/data/private_test/private_18863.wav  \n","  inflating: /content/data/private_test/private_06485.wav  \n","  inflating: /content/data/private_test/private_07943.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_07943.wav  \n","  inflating: /content/data/private_test/private_16696.wav  \n","  inflating: /content/data/private_test/private_13936.wav  \n","  inflating: /content/data/private_test/private_04292.wav  \n","  inflating: /content/data/private_test/private_14081.wav  \n","  inflating: /content/data/private_test/private_05832.wav  \n","  inflating: /content/data/private_test/private_13088.wav  \n","  inflating: /content/data/private_test/private_12396.wav  \n","  inflating: /content/data/private_test/private_02185.wav  \n","  inflating: /content/data/private_test/private_10581.wav  \n","  inflating: /content/data/private_test/private_11847.wav  \n","  inflating: /content/data/private_test/private_00792.wav  \n","  inflating: /content/data/private_test/private_07016.wav  \n","  inflating: /content/data/private_test/private_08325.wav  \n","  inflating: /content/data/private_test/private_17205.wav  \n","  inflating: /content/data/private_test/private_18136.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18136.wav  \n","  inflating: /content/data/private_test/private_00779.wav  \n","  inflating: /content/data/private_test/private_01467.wav  \n","  inflating: /content/data/private_test/private_19228.wav  \n","  inflating: /content/data/private_test/private_06308.wav  \n","  inflating: /content/data/private_test/private_11674.wav  \n","  inflating: /content/data/private_test/private_05601.wav  \n","  inflating: /content/data/private_test/private_15412.wav  \n","  inflating: /content/data/private_test/private_03270.wav  \n","  inflating: /content/data/private_test/private_13063.wav  \n","  inflating: /content/data/private_test/private_13705.wav  \n","  inflating: /content/data/private_test/private_04279.wav  \n","  inflating: /content/data/private_test/private_03516.wav  \n","  inflating: /content/data/private_test/private_02608.wav  \n","  inflating: /content/data/private_test/private_15374.wav  \n","  inflating: /content/data/private_test/private_05167.wav  \n","  inflating: /content/data/private_test/private_18888.wav  \n","  inflating: /content/data/private_test/private_11112.wav  \n","  inflating: /content/data/private_test/private_01301.wav  \n","  inflating: /content/data/private_test/private_17563.wav  \n","  inflating: /content/data/private_test/private_09985.wav  \n","  inflating: /content/data/private_test/private_18650.wav  \n","  inflating: /content/data/private_test/private_07770.wav  \n","  inflating: /content/data/private_test/private_08443.wav  \n","  inflating: /content/data/private_test/private_11106.wav  \n","  inflating: /content/data/private_test/private_09749.wav  \n","  inflating: /content/data/private_test/private_16669.wav  \n","  inflating: /content/data/private_test/private_01315.wav  \n","  inflating: /content/data/private_test/private_17577.wav  \n","  inflating: /content/data/private_test/private_18644.wav  \n","  inflating: /content/data/private_test/private_09991.wav  \n","  inflating: /content/data/private_test/private_07764.wav  \n","  inflating: /content/data/private_test/private_08457.wav  \n","  inflating: /content/data/private_test/private_10218.wav  \n","  inflating: /content/data/private_test/private_13711.wav  \n","  inflating: /content/data/private_test/private_03502.wav  \n","  inflating: /content/data/private_test/private_15360.wav  \n","  inflating: /content/data/private_test/private_05173.wav  \n","  inflating: /content/data/private_test/private_12369.wav  \n","  inflating: /content/data/private_test/private_05615.wav  \n","  inflating: /content/data/private_test/private_15406.wav  \n","  inflating: /content/data/private_test/private_03264.wav  \n","  inflating: /content/data/private_test/private_14718.wav  \n","  inflating: /content/data/private_test/private_13077.wav  \n","  inflating: /content/data/private_test/private_07002.wav  \n","  inflating: /content/data/private_test/private_08331.wav  \n","  inflating: /content/data/private_test/private_17211.wav  \n","  inflating: /content/data/private_test/private_18122.wav  \n","  inflating: /content/data/private_test/private_01473.wav  \n","  inflating: /content/data/private_test/private_11660.wav  \n","  inflating: /content/data/private_test/private_12341.wav  \n","  inflating: /content/data/private_test/private_02152.wav  \n","  inflating: /content/data/private_test/private_14730.wav  \n","  inflating: /content/data/private_test/private_04523.wav  \n","  inflating: /content/data/private_test/private_10556.wav  \n","  inflating: /content/data/private_test/private_08319.wav  \n","  inflating: /content/data/private_test/private_00745.wav  \n","  inflating: /content/data/private_test/private_11890.wav  \n","  inflating: /content/data/private_test/private_17239.wav  \n","  inflating: /content/data/private_test/private_19214.wav  \n","  inflating: /content/data/private_test/private_16127.wav  \n","  inflating: /content/data/private_test/private_09007.wav  \n","  inflating: /content/data/private_test/private_11648.wav  \n","  inflating: /content/data/private_test/private_06334.wav  \n","  inflating: /content/data/private_test/private_09761.wav  \n","  inflating: /content/data/private_test/private_06452.wav  \n","  inflating: /content/data/private_test/private_19572.wav  \n","  inflating: /content/data/private_test/private_16641.wav  \n","  inflating: /content/data/private_test/private_07994.wav  \n","  inflating: /content/data/private_test/private_00023.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00023.wav  \n","  inflating: /content/data/private_test/private_10230.wav  \n","  inflating: /content/data/private_test/private_16899.wav  \n","  inflating: /content/data/private_test/private_04245.wav  \n","  inflating: /content/data/private_test/private_13739.wav  \n","  inflating: /content/data/private_test/private_14056.wav  \n","  inflating: /content/data/private_test/private_15348.wav  \n","  inflating: /content/data/private_test/private_02634.wav  \n","  inflating: /content/data/private_test/private_12427.wav  \n","  inflating: /content/data/private_test/private_04251.wav  \n","  inflating: /content/data/private_test/private_14042.wav  \n","  inflating: /content/data/private_test/private_02620.wav  \n","  inflating: /content/data/private_test/private_12433.wav  \n","  inflating: /content/data/private_test/private_09775.wav  \n","  inflating: /content/data/private_test/private_06446.wav  \n","  inflating: /content/data/private_test/private_19566.wav  \n","  inflating: /content/data/private_test/private_01329.wav  \n","  inflating: /content/data/private_test/private_07980.wav  \n","  inflating: /content/data/private_test/private_16655.wav  \n","  inflating: /content/data/private_test/private_00037.wav  \n","  inflating: /content/data/private_test/private_18678.wav  \n","  inflating: /content/data/private_test/private_10224.wav  \n","  inflating: /content/data/private_test/private_07758.wav  \n","  inflating: /content/data/private_test/private_10542.wav  \n","  inflating: /content/data/private_test/private_11884.wav  \n","  inflating: /content/data/private_test/private_00751.wav  \n","  inflating: /content/data/private_test/private_19200.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19200.wav  \n","  inflating: /content/data/private_test/private_16133.wav  \n","  inflating: /content/data/private_test/private_00989.wav  \n","  inflating: /content/data/private_test/private_09013.wav  \n","  inflating: /content/data/private_test/private_06320.wav  \n","  inflating: /content/data/private_test/private_05629.wav  \n","  inflating: /content/data/private_test/private_12355.wav  \n","  inflating: /content/data/private_test/private_02146.wav  \n","  inflating: /content/data/private_test/private_14724.wav  \n","  inflating: /content/data/private_test/private_03258.wav  \n","  inflating: /content/data/private_test/private_04537.wav  \n","  inflating: /content/data/private_test/private_12802.wav  \n","  inflating: /content/data/private_test/private_19957.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19957.wav  \n","  inflating: /content/data/private_test/private_08682.wav  \n","  inflating: /content/data/private_test/private_18491.wav  \n","  inflating: /content/data/private_test/private_06877.wav  \n","  inflating: /content/data/private_test/private_10973.wav  \n","  inflating: /content/data/private_test/private_04906.wav  \n","  inflating: /content/data/private_test/private_04912.wav  \n","  inflating: /content/data/private_test/private_10967.wav  \n","  inflating: /content/data/private_test/private_09588.wav  \n","  inflating: /content/data/private_test/private_08696.wav  \n","  inflating: /content/data/private_test/private_19943.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19943.wav  \n","  inflating: /content/data/private_test/private_18485.wav  \n","  inflating: /content/data/private_test/private_06863.wav  \n","  inflating: /content/data/private_test/private_12816.wav  \n","  inflating: /content/data/private_test/private_16480.wav  \n","  inflating: /content/data/private_test/private_08866.wav  \n","  inflating: /content/data/private_test/private_06693.wav  \n","  inflating: /content/data/private_test/private_17946.wav  \n","  inflating: /content/data/private_test/private_14297.wav  \n","  inflating: /content/data/private_test/private_04084.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_04084.wav  \n","  inflating: /content/data/private_test/private_03933.wav  \n","  inflating: /content/data/private_test/private_15189.wav  \n","  inflating: /content/data/private_test/private_02393.wav  \n","  inflating: /content/data/private_test/private_12180.wav  \n","  inflating: /content/data/private_test/private_15837.wav  \n","  inflating: /content/data/private_test/private_00584.wav  \n","  inflating: /content/data/private_test/private_10797.wav  \n","  inflating: /content/data/private_test/private_01842.wav  \n","  inflating: /content/data/private_test/private_11489.wav  \n","  inflating: /content/data/private_test/private_00590.wav  \n","  inflating: /content/data/private_test/private_01856.wav  \n","  inflating: /content/data/private_test/private_10783.wav  \n","  inflating: /content/data/private_test/private_02387.wav  \n","  inflating: /content/data/private_test/private_12194.wav  \n","  inflating: /content/data/private_test/private_15823.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15823.wav  \n","  inflating: /content/data/private_test/private_03099.wav  \n","  inflating: /content/data/private_test/private_14283.wav  \n","  inflating: /content/data/private_test/private_04090.wav  \n","  inflating: /content/data/private_test/private_03927.wav  \n","  inflating: /content/data/private_test/private_16494.wav  \n","  inflating: /content/data/private_test/private_08872.wav  \n","  inflating: /content/data/private_test/private_17952.wav  \n","  inflating: /content/data/private_test/private_06687.wav  \n","  inflating: /content/data/private_test/private_07599.wav  \n","  inflating: /content/data/private_test/private_19994.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19994.wav  \n","  inflating: /content/data/private_test/private_08641.wav  \n","  inflating: /content/data/private_test/private_07572.wav  \n","  inflating: /content/data/private_test/private_18452.wav  \n","  inflating: /content/data/private_test/private_17761.wav  \n","  inflating: /content/data/private_test/private_08899.wav  \n","  inflating: /content/data/private_test/private_01103.wav  \n","  inflating: /content/data/private_test/private_11310.wav  \n","  inflating: /content/data/private_test/private_05365.wav  \n","  inflating: /content/data/private_test/private_12619.wav  \n","  inflating: /content/data/private_test/private_15176.wav  \n","  inflating: /content/data/private_test/private_14268.wav  \n","  inflating: /content/data/private_test/private_03714.wav  \n","  inflating: /content/data/private_test/private_13507.wav  \n","  inflating: /content/data/private_test/private_13261.wav  \n","  inflating: /content/data/private_test/private_03072.wav  \n","  inflating: /content/data/private_test/private_15610.wav  \n","  inflating: /content/data/private_test/private_05403.wav  \n","  inflating: /content/data/private_test/private_11476.wav  \n","  inflating: /content/data/private_test/private_09239.wav  \n","  inflating: /content/data/private_test/private_01665.wav  \n","  inflating: /content/data/private_test/private_16319.wav  \n","  inflating: /content/data/private_test/private_18334.wav  \n","  inflating: /content/data/private_test/private_17007.wav  \n","  inflating: /content/data/private_test/private_08127.wav  \n","  inflating: /content/data/private_test/private_10768.wav  \n","  inflating: /content/data/private_test/private_07214.wav  \n","  inflating: /content/data/private_test/private_11462.wav  \n","  inflating: /content/data/private_test/private_01671.wav  \n","  inflating: /content/data/private_test/private_18320.wav  \n","  inflating: /content/data/private_test/private_17013.wav  \n","  inflating: /content/data/private_test/private_08133.wav  \n","  inflating: /content/data/private_test/private_07200.wav  \n","  inflating: /content/data/private_test/private_04709.wav  \n","  inflating: /content/data/private_test/private_13275.wav  \n","  inflating: /content/data/private_test/private_03066.wav  \n","  inflating: /content/data/private_test/private_15604.wav  \n","  inflating: /content/data/private_test/private_02378.wav  \n","  inflating: /content/data/private_test/private_05417.wav  \n","  inflating: /content/data/private_test/private_05371.wav  \n","  inflating: /content/data/private_test/private_15162.wav  \n","  inflating: /content/data/private_test/private_03700.wav  \n","  inflating: /content/data/private_test/private_13513.wav  \n","  inflating: /content/data/private_test/private_08655.wav  \n","  inflating: /content/data/private_test/private_19980.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19980.wav  \n","  inflating: /content/data/private_test/private_07566.wav  \n","  inflating: /content/data/private_test/private_18446.wav  \n","  inflating: /content/data/private_test/private_00209.wav  \n","  inflating: /content/data/private_test/private_17775.wav  \n","  inflating: /content/data/private_test/private_01117.wav  \n","  inflating: /content/data/private_test/private_19758.wav  \n","  inflating: /content/data/private_test/private_11304.wav  \n","  inflating: /content/data/private_test/private_06678.wav  \n","  inflating: /content/data/private_test/private_12625.wav  \n","  inflating: /content/data/private_test/private_05359.wav  \n","  inflating: /content/data/private_test/private_02436.wav  \n","  inflating: /content/data/private_test/private_03728.wav  \n","  inflating: /content/data/private_test/private_14254.wav  \n","  inflating: /content/data/private_test/private_04047.wav  \n","  inflating: /content/data/private_test/private_10032.wav  \n","  inflating: /content/data/private_test/private_06888.wav  \n","  inflating: /content/data/private_test/private_00221.wav  \n","  inflating: /content/data/private_test/private_16443.wav  \n","  inflating: /content/data/private_test/private_19770.wav  \n","  inflating: /content/data/private_test/private_06650.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06650.wav  \n","  inflating: /content/data/private_test/private_17985.wav  \n","  inflating: /content/data/private_test/private_09563.wav  \n","  inflating: /content/data/private_test/private_06136.wav  \n","  inflating: /content/data/private_test/private_09205.wav  \n","  inflating: /content/data/private_test/private_16325.wav  \n","  inflating: /content/data/private_test/private_19016.wav  \n","  inflating: /content/data/private_test/private_01659.wav  \n","  inflating: /content/data/private_test/private_00547.wav  \n","  inflating: /content/data/private_test/private_18308.wav  \n","  inflating: /content/data/private_test/private_07228.wav  \n","  inflating: /content/data/private_test/private_10754.wav  \n","  inflating: /content/data/private_test/private_01881.wav  \n","  inflating: /content/data/private_test/private_04721.wav  \n","  inflating: /content/data/private_test/private_14532.wav  \n","  inflating: /content/data/private_test/private_02350.wav  \n","  inflating: /content/data/private_test/private_12143.wav  \n","  inflating: /content/data/private_test/private_13249.wav  \n","  inflating: /content/data/private_test/private_04735.wav  \n","  inflating: /content/data/private_test/private_14526.wav  \n","  inflating: /content/data/private_test/private_02344.wav  \n","  inflating: /content/data/private_test/private_15638.wav  \n","  inflating: /content/data/private_test/private_12157.wav  \n","  inflating: /content/data/private_test/private_06122.wav  \n","  inflating: /content/data/private_test/private_09211.wav  \n","  inflating: /content/data/private_test/private_16331.wav  \n","  inflating: /content/data/private_test/private_10998.wav  \n","  inflating: /content/data/private_test/private_19002.wav  \n","  inflating: /content/data/private_test/private_00553.wav  \n","  inflating: /content/data/private_test/private_01895.wav  \n","  inflating: /content/data/private_test/private_10740.wav  \n","  inflating: /content/data/private_test/private_10026.wav  \n","  inflating: /content/data/private_test/private_08669.wav  \n","  inflating: /content/data/private_test/private_17749.wav  \n","  inflating: /content/data/private_test/private_00235.wav  \n","  inflating: /content/data/private_test/private_16457.wav  \n","  inflating: /content/data/private_test/private_19764.wav  \n","  inflating: /content/data/private_test/private_17991.wav  \n","  inflating: /content/data/private_test/private_06644.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06644.wav  \n","  inflating: /content/data/private_test/private_09577.wav  \n","  inflating: /content/data/private_test/private_11338.wav  \n","  inflating: /content/data/private_test/private_12631.wav  \n","  inflating: /content/data/private_test/private_02422.wav  \n","  inflating: /content/data/private_test/private_14240.wav  \n","  inflating: /content/data/private_test/private_04053.wav  \n","  inflating: /content/data/private_test/private_12976.wav  \n","  inflating: /content/data/private_test/private_06903.wav  \n","  inflating: /content/data/private_test/private_19823.wav  \n","  inflating: /content/data/private_test/private_10807.wav  \n","  inflating: /content/data/private_test/private_08090.wav  \n","  inflating: /content/data/private_test/private_18283.wav  \n","  inflating: /content/data/private_test/private_04872.wav  \n","  inflating: /content/data/private_test/private_04866.wav  \n","  inflating: /content/data/private_test/private_10813.wav  \n","  inflating: /content/data/private_test/private_19189.wav  \n","  inflating: /content/data/private_test/private_08084.wav  \n","  inflating: /content/data/private_test/private_18297.wav  \n","  inflating: /content/data/private_test/private_06917.wav  \n","  inflating: /content/data/private_test/private_19837.wav  \n","  inflating: /content/data/private_test/private_12962.wav  \n","  inflating: /content/data/private_test/private_00396.wav  \n","  inflating: /content/data/private_test/private_10185.wav  \n","  inflating: /content/data/private_test/private_17832.wav  \n","  inflating: /content/data/private_test/private_08912.wav  \n","  inflating: /content/data/private_test/private_01088.wav  \n","  inflating: /content/data/private_test/private_02581.wav  \n","  inflating: /content/data/private_test/private_12792.wav  \n","  inflating: /content/data/private_test/private_03847.wav  \n","  inflating: /content/data/private_test/private_14485.wav  \n","  inflating: /content/data/private_test/private_04696.wav  \n","  inflating: /content/data/private_test/private_15943.wav  \n","  inflating: /content/data/private_test/private_05588.wav  \n","  inflating: /content/data/private_test/private_16292.wav  \n","  inflating: /content/data/private_test/private_06081.wav  \n","  inflating: /content/data/private_test/private_01936.wav  \n","  inflating: /content/data/private_test/private_16286.wav  \n","  inflating: /content/data/private_test/private_06095.wav  \n","  inflating: /content/data/private_test/private_01922.wav  \n","  inflating: /content/data/private_test/private_17198.wav  \n","  inflating: /content/data/private_test/private_14491.wav  \n","  inflating: /content/data/private_test/private_15957.wav  \n","  inflating: /content/data/private_test/private_04682.wav  \n","  inflating: /content/data/private_test/private_02595.wav  \n","  inflating: /content/data/private_test/private_03853.wav  \n","  inflating: /content/data/private_test/private_12786.wav  \n","  inflating: /content/data/private_test/private_13498.wav  \n","  inflating: /content/data/private_test/private_00382.wav  \n","  inflating: /content/data/private_test/private_10191.wav  \n","  inflating: /content/data/private_test/private_17826.wav  \n","  inflating: /content/data/private_test/private_08906.wav  \n","  inflating: /content/data/private_test/private_11264.wav  \n","  inflating: /content/data/private_test/private_06718.wav  \n","  inflating: /content/data/private_test/private_01077.wav  \n","  inflating: /content/data/private_test/private_19638.wav  \n","  inflating: /content/data/private_test/private_18526.wav  \n","  inflating: /content/data/private_test/private_00369.wav  \n","  inflating: /content/data/private_test/private_17615.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_17615.wav  \n","  inflating: /content/data/private_test/private_08735.wav  \n","  inflating: /content/data/private_test/private_07406.wav  \n","  inflating: /content/data/private_test/private_13473.wav  \n","  inflating: /content/data/private_test/private_03660.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03660.wav  \n","  inflating: /content/data/private_test/private_15002.wav  \n","  inflating: /content/data/private_test/private_05211.wav  \n","  inflating: /content/data/private_test/private_05577.wav  \n","  inflating: /content/data/private_test/private_15764.wav  \n","  inflating: /content/data/private_test/private_02218.wav  \n","  inflating: /content/data/private_test/private_03106.wav  \n","  inflating: /content/data/private_test/private_04669.wav  \n","  inflating: /content/data/private_test/private_13315.wav  \n","  inflating: /content/data/private_test/private_08053.wav  \n","  inflating: /content/data/private_test/private_07360.wav  \n","  inflating: /content/data/private_test/private_18240.wav  \n","  inflating: /content/data/private_test/private_17173.wav  \n","  inflating: /content/data/private_test/private_01711.wav  \n","  inflating: /content/data/private_test/private_11502.wav  \n","  inflating: /content/data/private_test/private_08047.wav  \n","  inflating: /content/data/private_test/private_10608.wav  \n","  inflating: /content/data/private_test/private_07374.wav  \n","  inflating: /content/data/private_test/private_18254.wav  \n","  inflating: /content/data/private_test/private_17167.wav  \n","  inflating: /content/data/private_test/private_01705.wav  \n","  inflating: /content/data/private_test/private_16279.wav  \n","  inflating: /content/data/private_test/private_11516.wav  \n","  inflating: /content/data/private_test/private_09359.wav  \n","  inflating: /content/data/private_test/private_05563.wav  \n","  inflating: /content/data/private_test/private_15770.wav  \n","  inflating: /content/data/private_test/private_03112.wav  \n","  inflating: /content/data/private_test/private_13301.wav  \n","  inflating: /content/data/private_test/private_13467.wav  \n","  inflating: /content/data/private_test/private_14308.wav  \n","  inflating: /content/data/private_test/private_03674.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03674.wav  \n","  inflating: /content/data/private_test/private_15016.wav  \n","  inflating: /content/data/private_test/private_05205.wav  \n","  inflating: /content/data/private_test/private_12779.wav  \n","  inflating: /content/data/private_test/private_11270.wav  \n","  inflating: /content/data/private_test/private_01063.wav  \n","  inflating: /content/data/private_test/private_18532.wav  \n","  inflating: /content/data/private_test/private_17601.wav  \n","  inflating: /content/data/private_test/private_08721.wav  \n","  inflating: /content/data/private_test/private_07412.wav  \n","  inflating: /content/data/private_test/private_04133.wav  \n","  inflating: /content/data/private_test/private_12989.wav  \n","  inflating: /content/data/private_test/private_14320.wav  \n","  inflating: /content/data/private_test/private_02542.wav  \n","  inflating: /content/data/private_test/private_12751.wav  \n","  inflating: /content/data/private_test/private_03884.wav  \n","  inflating: /content/data/private_test/private_06724.wav  \n","  inflating: /content/data/private_test/private_09417.wav  \n","  inflating: /content/data/private_test/private_11258.wav  \n","  inflating: /content/data/private_test/private_16537.wav  \n","  inflating: /content/data/private_test/private_19604.wav  \n","  inflating: /content/data/private_test/private_17629.wav  \n","  inflating: /content/data/private_test/private_00355.wav  \n","  inflating: /content/data/private_test/private_10146.wav  \n","  inflating: /content/data/private_test/private_08709.wav  \n","  inflating: /content/data/private_test/private_10620.wav  \n","  inflating: /content/data/private_test/private_00433.wav  \n","  inflating: /content/data/private_test/private_16251.wav  \n","  inflating: /content/data/private_test/private_19162.wav  \n","  inflating: /content/data/private_test/private_06042.wav  \n","  inflating: /content/data/private_test/private_09371.wav  \n","  inflating: /content/data/private_test/private_12037.wav  \n","  inflating: /content/data/private_test/private_02224.wav  \n","  inflating: /content/data/private_test/private_15758.wav  \n","  inflating: /content/data/private_test/private_14446.wav  \n","  inflating: /content/data/private_test/private_13329.wav  \n","  inflating: /content/data/private_test/private_04655.wav  \n","  inflating: /content/data/private_test/private_15980.wav  \n","  inflating: /content/data/private_test/private_12023.wav  \n","  inflating: /content/data/private_test/private_02230.wav  \n","  inflating: /content/data/private_test/private_04899.wav  \n","  inflating: /content/data/private_test/private_14452.wav  \n","  inflating: /content/data/private_test/private_15994.wav  \n","  inflating: /content/data/private_test/private_04641.wav  \n","  inflating: /content/data/private_test/private_07348.wav  \n","  inflating: /content/data/private_test/private_10634.wav  \n","  inflating: /content/data/private_test/private_00427.wav  \n","  inflating: /content/data/private_test/private_18268.wav  \n","  inflating: /content/data/private_test/private_16245.wav  \n","  inflating: /content/data/private_test/private_19176.wav  \n","  inflating: /content/data/private_test/private_01739.wav  \n","  inflating: /content/data/private_test/private_06056.wav  \n","  inflating: /content/data/private_test/private_09365.wav  \n","  inflating: /content/data/private_test/private_06730.wav  \n","  inflating: /content/data/private_test/private_09403.wav  \n","  inflating: /content/data/private_test/private_16523.wav  \n","  inflating: /content/data/private_test/private_19610.wav  \n","  inflating: /content/data/private_test/private_00341.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00341.wav  \n","  inflating: /content/data/private_test/private_10152.wav  \n","  inflating: /content/data/private_test/private_04127.wav  \n","  inflating: /content/data/private_test/private_03648.wav  \n","  inflating: /content/data/private_test/private_14334.wav  \n","  inflating: /content/data/private_test/private_02556.wav  \n","  inflating: /content/data/private_test/private_03890.wav  \n","  inflating: /content/data/private_test/private_12745.wav  \n","  inflating: /content/data/private_test/private_05239.wav  \n","  inflating: /content/data/private_test/private_14863.wav  \n","  inflating: /content/data/private_test/private_18081.wav  \n","  inflating: /content/data/private_test/private_08292.wav  \n","  inflating: /content/data/private_test/private_00816.wav  \n","  inflating: /content/data/private_test/private_16912.wav  \n","  inflating: /content/data/private_test/private_09832.wav  \n","  inflating: /content/data/private_test/private_02967.wav  \n","  inflating: /content/data/private_test/private_02973.wav  \n","  inflating: /content/data/private_test/private_16906.wav  \n","  inflating: /content/data/private_test/private_09826.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_09826.wav  \n","  inflating: /content/data/private_test/private_18095.wav  \n","  inflating: /content/data/private_test/private_08286.wav  \n","  inflating: /content/data/private_test/private_00802.wav  \n","  inflating: /content/data/private_test/private_09198.wav  \n","  inflating: /content/data/private_test/private_14877.wav  \n","  inflating: /content/data/private_test/private_11927.wav  \n","  inflating: /content/data/private_test/private_06283.wav  \n","  inflating: /content/data/private_test/private_16090.wav  \n","  inflating: /content/data/private_test/private_15599.wav  \n","  inflating: /content/data/private_test/private_04494.wav  \n","  inflating: /content/data/private_test/private_14687.wav  \n","  inflating: /content/data/private_test/private_05952.wav  \n","  inflating: /content/data/private_test/private_12590.wav  \n","  inflating: /content/data/private_test/private_02783.wav  \n","  inflating: /content/data/private_test/private_13856.wav  \n","  inflating: /content/data/private_test/private_07823.wav  \n","  inflating: /content/data/private_test/private_18903.wav  \n","  inflating: /content/data/private_test/private_11099.wav  \n","  inflating: /content/data/private_test/private_10387.wav  \n","  inflating: /content/data/private_test/private_00194.wav  \n","  inflating: /content/data/private_test/private_07837.wav  \n","  inflating: /content/data/private_test/private_18917.wav  \n","  inflating: /content/data/private_test/private_10393.wav  \n","  inflating: /content/data/private_test/private_00180.wav  \n","  inflating: /content/data/private_test/private_03489.wav  \n","  inflating: /content/data/private_test/private_12584.wav  \n","  inflating: /content/data/private_test/private_13842.wav  \n","  inflating: /content/data/private_test/private_02797.wav  \n","  inflating: /content/data/private_test/private_04480.wav  \n","  inflating: /content/data/private_test/private_05946.wav  \n","  inflating: /content/data/private_test/private_14693.wav  \n","  inflating: /content/data/private_test/private_11933.wav  \n","  inflating: /content/data/private_test/private_07189.wav  \n","  inflating: /content/data/private_test/private_06297.wav  \n","  inflating: /content/data/private_test/private_16084.wav  \n","  inflating: /content/data/private_test/private_11700.wav  \n","  inflating: /content/data/private_test/private_01513.wav  \n","  inflating: /content/data/private_test/private_17371.wav  \n","  inflating: /content/data/private_test/private_18042.wav  \n","  inflating: /content/data/private_test/private_07162.wav  \n","  inflating: /content/data/private_test/private_08251.wav  \n","  inflating: /content/data/private_test/private_13117.wav  \n","  inflating: /content/data/private_test/private_03304.wav  \n","  inflating: /content/data/private_test/private_14678.wav  \n","  inflating: /content/data/private_test/private_15566.wav  \n","  inflating: /content/data/private_test/private_12209.wav  \n","  inflating: /content/data/private_test/private_05775.wav  \n","  inflating: /content/data/private_test/private_05013.wav  \n","  inflating: /content/data/private_test/private_15200.wav  \n","  inflating: /content/data/private_test/private_03462.wav  \n","  inflating: /content/data/private_test/private_13671.wav  \n","  inflating: /content/data/private_test/private_07604.wav  \n","  inflating: /content/data/private_test/private_08537.wav  \n","  inflating: /content/data/private_test/private_10378.wav  \n","  inflating: /content/data/private_test/private_17417.wav  \n","  inflating: /content/data/private_test/private_18724.wav  \n","  inflating: /content/data/private_test/private_16709.wav  \n","  inflating: /content/data/private_test/private_01275.wav  \n","  inflating: /content/data/private_test/private_11066.wav  \n","  inflating: /content/data/private_test/private_09629.wav  \n","  inflating: /content/data/private_test/private_07610.wav  \n","  inflating: /content/data/private_test/private_08523.wav  \n","  inflating: /content/data/private_test/private_17403.wav  \n","  inflating: /content/data/private_test/private_18730.wav  \n","  inflating: /content/data/private_test/private_01261.wav  \n","  inflating: /content/data/private_test/private_11072.wav  \n","  inflating: /content/data/private_test/private_05007.wav  \n","  inflating: /content/data/private_test/private_02768.wav  \n","  inflating: /content/data/private_test/private_15214.wav  \n","  inflating: /content/data/private_test/private_03476.wav  \n","  inflating: /content/data/private_test/private_13665.wav  \n","  inflating: /content/data/private_test/private_04319.wav  \n","  inflating: /content/data/private_test/private_13103.wav  \n","  inflating: /content/data/private_test/private_03310.wav  \n","  inflating: /content/data/private_test/private_15572.wav  \n","  inflating: /content/data/private_test/private_05761.wav  \n","  inflating: /content/data/private_test/private_06268.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06268.wav  \n","  inflating: /content/data/private_test/private_11714.wav  \n","  inflating: /content/data/private_test/private_01507.wav  \n","  inflating: /content/data/private_test/private_19348.wav  \n","  inflating: /content/data/private_test/private_17365.wav  \n","  inflating: /content/data/private_test/private_18056.wav  \n","  inflating: /content/data/private_test/private_00619.wav  \n","  inflating: /content/data/private_test/private_07176.wav  \n","  inflating: /content/data/private_test/private_08245.wav  \n","  inflating: /content/data/private_test/private_04457.wav  \n","  inflating: /content/data/private_test/private_14644.wav  \n","  inflating: /content/data/private_test/private_05991.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_05991.wav  \n","  inflating: /content/data/private_test/private_03338.wav  \n","  inflating: /content/data/private_test/private_02026.wav  \n","  inflating: /content/data/private_test/private_05749.wav  \n","  inflating: /content/data/private_test/private_12235.wav  \n","  inflating: /content/data/private_test/private_09173.wav  \n","  inflating: /content/data/private_test/private_06240.wav  \n","  inflating: /content/data/private_test/private_19360.wav  \n","  inflating: /content/data/private_test/private_16053.wav  \n","  inflating: /content/data/private_test/private_00631.wav  \n","  inflating: /content/data/private_test/private_10422.wav  \n","  inflating: /content/data/private_test/private_10344.wav  \n","  inflating: /content/data/private_test/private_07638.wav  \n","  inflating: /content/data/private_test/private_00157.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00157.wav  \n","  inflating: /content/data/private_test/private_18718.wav  \n","  inflating: /content/data/private_test/private_19406.wav  \n","  inflating: /content/data/private_test/private_01249.wav  \n","  inflating: /content/data/private_test/private_16735.wav  \n","  inflating: /content/data/private_test/private_09615.wav  \n","  inflating: /content/data/private_test/private_06526.wav  \n","  inflating: /content/data/private_test/private_12553.wav  \n","  inflating: /content/data/private_test/private_02740.wav  \n","  inflating: /content/data/private_test/private_13895.wav  \n","  inflating: /content/data/private_test/private_14122.wav  \n","  inflating: /content/data/private_test/private_04331.wav  \n","  inflating: /content/data/private_test/private_02998.wav  \n","  inflating: /content/data/private_test/private_12547.wav  \n","  inflating: /content/data/private_test/private_15228.wav  \n","  inflating: /content/data/private_test/private_13881.wav  \n","  inflating: /content/data/private_test/private_02754.wav  \n","  inflating: /content/data/private_test/private_14136.wav  \n","  inflating: /content/data/private_test/private_04325.wav  \n","  inflating: /content/data/private_test/private_13659.wav  \n","  inflating: /content/data/private_test/private_10350.wav  \n","  inflating: /content/data/private_test/private_00143.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00143.wav  \n","  inflating: /content/data/private_test/private_19412.wav  \n","  inflating: /content/data/private_test/private_16721.wav  \n","  inflating: /content/data/private_test/private_09601.wav  \n","  inflating: /content/data/private_test/private_06532.wav  \n","  inflating: /content/data/private_test/private_09167.wav  \n","  inflating: /content/data/private_test/private_11728.wav  \n","  inflating: /content/data/private_test/private_06254.wav  \n","  inflating: /content/data/private_test/private_19374.wav  \n","  inflating: /content/data/private_test/private_16047.wav  \n","  inflating: /content/data/private_test/private_00625.wav  \n","  inflating: /content/data/private_test/private_17359.wav  \n","  inflating: /content/data/private_test/private_10436.wav  \n","  inflating: /content/data/private_test/private_08279.wav  \n","  inflating: /content/data/private_test/private_04443.wav  \n","  inflating: /content/data/private_test/private_05985.wav  \n","  inflating: /content/data/private_test/private_14650.wav  \n","  inflating: /content/data/private_test/private_02032.wav  \n","  inflating: /content/data/private_test/private_14888.wav  \n","  inflating: /content/data/private_test/private_12221.wav  \n","  inflating: /content/data/private_test/private_18052.wav  \n","  inflating: /content/data/private_test/private_17361.wav  \n","  inflating: /content/data/private_test/private_08241.wav  \n","  inflating: /content/data/private_test/private_07172.wav  \n","  inflating: /content/data/private_test/private_11710.wav  \n","  inflating: /content/data/private_test/private_01503.wav  \n","  inflating: /content/data/private_test/private_15576.wav  \n","  inflating: /content/data/private_test/private_05765.wav  \n","  inflating: /content/data/private_test/private_12219.wav  \n","  inflating: /content/data/private_test/private_13107.wav  \n","  inflating: /content/data/private_test/private_14668.wav  \n","  inflating: /content/data/private_test/private_03314.wav  \n","  inflating: /content/data/private_test/private_03472.wav  \n","  inflating: /content/data/private_test/private_13661.wav  \n","  inflating: /content/data/private_test/private_05003.wav  \n","  inflating: /content/data/private_test/private_15210.wav  \n","  inflating: /content/data/private_test/private_01265.wav  \n","  inflating: /content/data/private_test/private_16719.wav  \n","  inflating: /content/data/private_test/private_11076.wav  \n","  inflating: /content/data/private_test/private_09639.wav  \n","  inflating: /content/data/private_test/private_08527.wav  \n","  inflating: /content/data/private_test/private_10368.wav  \n","  inflating: /content/data/private_test/private_07614.wav  \n","  inflating: /content/data/private_test/private_18734.wav  \n","  inflating: /content/data/private_test/private_17407.wav  \n","  inflating: /content/data/private_test/private_01271.wav  \n","  inflating: /content/data/private_test/private_11062.wav  \n","  inflating: /content/data/private_test/private_08533.wav  \n","  inflating: /content/data/private_test/private_07600.wav  \n","  inflating: /content/data/private_test/private_18720.wav  \n","  inflating: /content/data/private_test/private_17413.wav  \n","  inflating: /content/data/private_test/private_03466.wav  \n","  inflating: /content/data/private_test/private_04309.wav  \n","  inflating: /content/data/private_test/private_13675.wav  \n","  inflating: /content/data/private_test/private_05017.wav  \n","  inflating: /content/data/private_test/private_15204.wav  \n","  inflating: /content/data/private_test/private_02778.wav  \n","  inflating: /content/data/private_test/private_15562.wav  \n","  inflating: /content/data/private_test/private_05771.wav  \n","  inflating: /content/data/private_test/private_13113.wav  \n","  inflating: /content/data/private_test/private_03300.wav  \n","  inflating: /content/data/private_test/private_18046.wav  \n","  inflating: /content/data/private_test/private_00609.wav  \n","  inflating: /content/data/private_test/private_17375.wav  \n","  inflating: /content/data/private_test/private_08255.wav  \n","  inflating: /content/data/private_test/private_07166.wav  \n","  inflating: /content/data/private_test/private_11704.wav  \n","  inflating: /content/data/private_test/private_06278.wav  \n","  inflating: /content/data/private_test/private_01517.wav  \n","  inflating: /content/data/private_test/private_19358.wav  \n","  inflating: /content/data/private_test/private_02036.wav  \n","  inflating: /content/data/private_test/private_12225.wav  \n","  inflating: /content/data/private_test/private_05759.wav  \n","  inflating: /content/data/private_test/private_04447.wav  \n","  inflating: /content/data/private_test/private_03328.wav  \n","  inflating: /content/data/private_test/private_14654.wav  \n","  inflating: /content/data/private_test/private_05981.wav  \n","  inflating: /content/data/private_test/private_00621.wav  \n","  inflating: /content/data/private_test/private_10432.wav  \n","  inflating: /content/data/private_test/private_06250.wav  \n","  inflating: /content/data/private_test/private_09163.wav  \n","  inflating: /content/data/private_test/private_16043.wav  \n","  inflating: /content/data/private_test/private_19370.wav  \n","  inflating: /content/data/private_test/private_16725.wav  \n","  inflating: /content/data/private_test/private_19416.wav  \n","  inflating: /content/data/private_test/private_01259.wav  \n","  inflating: /content/data/private_test/private_06536.wav  \n","  inflating: /content/data/private_test/private_09605.wav  \n","  inflating: /content/data/private_test/private_07628.wav  \n","  inflating: /content/data/private_test/private_10354.wav  \n","  inflating: /content/data/private_test/private_00147.wav  \n","  inflating: /content/data/private_test/private_18708.wav  \n","  inflating: /content/data/private_test/private_14132.wav  \n","  inflating: /content/data/private_test/private_02988.wav  \n","  inflating: /content/data/private_test/private_04321.wav  \n","  inflating: /content/data/private_test/private_12543.wav  \n","  inflating: /content/data/private_test/private_02750.wav  \n","  inflating: /content/data/private_test/private_13885.wav  \n","  inflating: /content/data/private_test/private_14126.wav  \n","  inflating: /content/data/private_test/private_13649.wav  \n","  inflating: /content/data/private_test/private_04335.wav  \n","  inflating: /content/data/private_test/private_12557.wav  \n","  inflating: /content/data/private_test/private_13891.wav  \n","  inflating: /content/data/private_test/private_02744.wav  \n","  inflating: /content/data/private_test/private_15238.wav  \n","  inflating: /content/data/private_test/private_16731.wav  \n","  inflating: /content/data/private_test/private_19402.wav  \n","  inflating: /content/data/private_test/private_06522.wav  \n","  inflating: /content/data/private_test/private_09611.wav  \n","  inflating: /content/data/private_test/private_10340.wav  \n","  inflating: /content/data/private_test/private_00153.wav  \n","  inflating: /content/data/private_test/private_17349.wav  \n","  inflating: /content/data/private_test/private_00635.wav  \n","  inflating: /content/data/private_test/private_10426.wav  \n","  inflating: /content/data/private_test/private_08269.wav  \n","  inflating: /content/data/private_test/private_06244.wav  \n","  inflating: /content/data/private_test/private_09177.wav  \n","  inflating: /content/data/private_test/private_11738.wav  \n","  inflating: /content/data/private_test/private_16057.wav  \n","  inflating: /content/data/private_test/private_19364.wav  \n","  inflating: /content/data/private_test/private_02022.wav  \n","  inflating: /content/data/private_test/private_12231.wav  \n","  inflating: /content/data/private_test/private_14898.wav  \n","  inflating: /content/data/private_test/private_04453.wav  \n","  inflating: /content/data/private_test/private_05995.wav  \n","  inflating: /content/data/private_test/private_14640.wav  \n","  inflating: /content/data/private_test/private_14873.wav  \n","  inflating: /content/data/private_test/private_00806.wav  \n","  inflating: /content/data/private_test/private_18091.wav  \n","  inflating: /content/data/private_test/private_08282.wav  \n","  inflating: /content/data/private_test/private_16902.wav  \n","  inflating: /content/data/private_test/private_09822.wav  \n","  inflating: /content/data/private_test/private_02977.wav  \n","  inflating: /content/data/private_test/private_02963.wav  \n","  inflating: /content/data/private_test/private_16916.wav  \n","  inflating: /content/data/private_test/private_09836.wav  \n","  inflating: /content/data/private_test/private_00812.wav  \n","  inflating: /content/data/private_test/private_09188.wav  \n","  inflating: /content/data/private_test/private_18085.wav  \n","  inflating: /content/data/private_test/private_08296.wav  \n","  inflating: /content/data/private_test/private_14867.wav  \n","  inflating: /content/data/private_test/private_06293.wav  \n","  inflating: /content/data/private_test/private_16080.wav  \n","  inflating: /content/data/private_test/private_11937.wav  \n","  inflating: /content/data/private_test/private_04484.wav  \n","  inflating: /content/data/private_test/private_14697.wav  \n","  inflating: /content/data/private_test/private_05942.wav  \n","  inflating: /content/data/private_test/private_15589.wav  \n","  inflating: /content/data/private_test/private_12580.wav  \n","  inflating: /content/data/private_test/private_02793.wav  \n","  inflating: /content/data/private_test/private_13846.wav  \n","  inflating: /content/data/private_test/private_10397.wav  \n","  inflating: /content/data/private_test/private_00184.wav  \n","  inflating: /content/data/private_test/private_07833.wav  \n","  inflating: /content/data/private_test/private_18913.wav  \n","  inflating: /content/data/private_test/private_11089.wav  \n","  inflating: /content/data/private_test/private_10383.wav  \n","  inflating: /content/data/private_test/private_00190.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00190.wav  \n","  inflating: /content/data/private_test/private_07827.wav  \n","  inflating: /content/data/private_test/private_18907.wav  \n","  inflating: /content/data/private_test/private_12594.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_12594.wav  \n","  inflating: /content/data/private_test/private_13852.wav  \n","  inflating: /content/data/private_test/private_02787.wav  \n","  inflating: /content/data/private_test/private_03499.wav  \n","  inflating: /content/data/private_test/private_04490.wav  \n","  inflating: /content/data/private_test/private_05956.wav  \n","  inflating: /content/data/private_test/private_14683.wav  \n","  inflating: /content/data/private_test/private_06287.wav  \n","  inflating: /content/data/private_test/private_16094.wav  \n","  inflating: /content/data/private_test/private_11923.wav  \n","  inflating: /content/data/private_test/private_07199.wav  \n","  inflating: /content/data/private_test/private_17605.wav  \n","  inflating: /content/data/private_test/private_18536.wav  \n","  inflating: /content/data/private_test/private_00379.wav  \n","  inflating: /content/data/private_test/private_07416.wav  \n","  inflating: /content/data/private_test/private_08725.wav  \n","  inflating: /content/data/private_test/private_06708.wav  \n","  inflating: /content/data/private_test/private_11274.wav  \n","  inflating: /content/data/private_test/private_01067.wav  \n","  inflating: /content/data/private_test/private_19628.wav  \n","  inflating: /content/data/private_test/private_15012.wav  \n","  inflating: /content/data/private_test/private_05201.wav  \n","  inflating: /content/data/private_test/private_13463.wav  \n","  inflating: /content/data/private_test/private_03670.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03670.wav  \n","  inflating: /content/data/private_test/private_03116.wav  \n","  inflating: /content/data/private_test/private_13305.wav  \n","  inflating: /content/data/private_test/private_04679.wav  \n","  inflating: /content/data/private_test/private_05567.wav  \n","  inflating: /content/data/private_test/private_02208.wav  \n","  inflating: /content/data/private_test/private_15774.wav  \n","  inflating: /content/data/private_test/private_01701.wav  \n","  inflating: /content/data/private_test/private_11512.wav  \n","  inflating: /content/data/private_test/private_07370.wav  \n","  inflating: /content/data/private_test/private_08043.wav  \n","  inflating: /content/data/private_test/private_17163.wav  \n","  inflating: /content/data/private_test/private_18250.wav  \n","  inflating: /content/data/private_test/private_16269.wav  \n","  inflating: /content/data/private_test/private_01715.wav  \n","  inflating: /content/data/private_test/private_11506.wav  \n","  inflating: /content/data/private_test/private_09349.wav  \n","  inflating: /content/data/private_test/private_07364.wav  \n","  inflating: /content/data/private_test/private_08057.wav  \n","  inflating: /content/data/private_test/private_10618.wav  \n","  inflating: /content/data/private_test/private_17177.wav  \n","  inflating: /content/data/private_test/private_18244.wav  \n","  inflating: /content/data/private_test/private_03102.wav  \n","  inflating: /content/data/private_test/private_13311.wav  \n","  inflating: /content/data/private_test/private_05573.wav  \n","  inflating: /content/data/private_test/private_15760.wav  \n","  inflating: /content/data/private_test/private_15006.wav  \n","  inflating: /content/data/private_test/private_12769.wav  \n","  inflating: /content/data/private_test/private_05215.wav  \n","  inflating: /content/data/private_test/private_13477.wav  \n","  inflating: /content/data/private_test/private_03664.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03664.wav  \n","  inflating: /content/data/private_test/private_14318.wav  \n","  inflating: /content/data/private_test/private_17611.wav  \n","  inflating: /content/data/private_test/private_18522.wav  \n","  inflating: /content/data/private_test/private_07402.wav  \n","  inflating: /content/data/private_test/private_08731.wav  \n","  inflating: /content/data/private_test/private_11260.wav  \n","  inflating: /content/data/private_test/private_01073.wav  \n","  inflating: /content/data/private_test/private_02552.wav  \n","  inflating: /content/data/private_test/private_12741.wav  \n","  inflating: /content/data/private_test/private_03894.wav  \n","  inflating: /content/data/private_test/private_04123.wav  \n","  inflating: /content/data/private_test/private_14330.wav  \n","  inflating: /content/data/private_test/private_12999.wav  \n","  inflating: /content/data/private_test/private_00345.wav  \n","  inflating: /content/data/private_test/private_17639.wav  \n","  inflating: /content/data/private_test/private_10156.wav  \n","  inflating: /content/data/private_test/private_08719.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_08719.wav  \n","  inflating: /content/data/private_test/private_09407.wav  \n","  inflating: /content/data/private_test/private_11248.wav  \n","  inflating: /content/data/private_test/private_06734.wav  \n","  inflating: /content/data/private_test/private_19614.wav  \n","  inflating: /content/data/private_test/private_16527.wav  \n","  inflating: /content/data/private_test/private_19172.wav  \n","  inflating: /content/data/private_test/private_16241.wav  \n","  inflating: /content/data/private_test/private_09361.wav  \n","  inflating: /content/data/private_test/private_06052.wav  \n","  inflating: /content/data/private_test/private_10630.wav  \n","  inflating: /content/data/private_test/private_00423.wav  \n","  inflating: /content/data/private_test/private_14456.wav  \n","  inflating: /content/data/private_test/private_04645.wav  \n","  inflating: /content/data/private_test/private_15990.wav  \n","  inflating: /content/data/private_test/private_13339.wav  \n","  inflating: /content/data/private_test/private_12027.wav  \n","  inflating: /content/data/private_test/private_15748.wav  \n","  inflating: /content/data/private_test/private_02234.wav  \n","  inflating: /content/data/private_test/private_14442.wav  \n","  inflating: /content/data/private_test/private_15984.wav  \n","  inflating: /content/data/private_test/private_04651.wav  \n","  inflating: /content/data/private_test/private_12033.wav  \n","  inflating: /content/data/private_test/private_04889.wav  \n","  inflating: /content/data/private_test/private_02220.wav  \n","  inflating: /content/data/private_test/private_19166.wav  \n","  inflating: /content/data/private_test/private_01729.wav  \n","  inflating: /content/data/private_test/private_16255.wav  \n","  inflating: /content/data/private_test/private_09375.wav  \n","  inflating: /content/data/private_test/private_06046.wav  \n","  inflating: /content/data/private_test/private_10624.wav  \n","  inflating: /content/data/private_test/private_07358.wav  \n","  inflating: /content/data/private_test/private_00437.wav  \n","  inflating: /content/data/private_test/private_18278.wav  \n","  inflating: /content/data/private_test/private_00351.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00351.wav  \n","  inflating: /content/data/private_test/private_10142.wav  \n","  inflating: /content/data/private_test/private_09413.wav  \n","  inflating: /content/data/private_test/private_06720.wav  \n","  inflating: /content/data/private_test/private_19600.wav  \n","  inflating: /content/data/private_test/private_16533.wav  \n","  inflating: /content/data/private_test/private_02546.wav  \n","  inflating: /content/data/private_test/private_05229.wav  \n","  inflating: /content/data/private_test/private_03880.wav  \n","  inflating: /content/data/private_test/private_12755.wav  \n","  inflating: /content/data/private_test/private_04137.wav  \n","  inflating: /content/data/private_test/private_14324.wav  \n","  inflating: /content/data/private_test/private_03658.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03658.wav  \n","  inflating: /content/data/private_test/private_12966.wav  \n","  inflating: /content/data/private_test/private_06913.wav  \n","  inflating: /content/data/private_test/private_19833.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19833.wav  \n","  inflating: /content/data/private_test/private_08080.wav  \n","  inflating: /content/data/private_test/private_18293.wav  \n","  inflating: /content/data/private_test/private_10817.wav  \n","  inflating: /content/data/private_test/private_04862.wav  \n","  inflating: /content/data/private_test/private_04876.wav  \n","  inflating: /content/data/private_test/private_08094.wav  \n","  inflating: /content/data/private_test/private_18287.wav  \n","  inflating: /content/data/private_test/private_10803.wav  \n","  inflating: /content/data/private_test/private_19199.wav  \n","  inflating: /content/data/private_test/private_06907.wav  \n","  inflating: /content/data/private_test/private_19827.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19827.wav  \n","  inflating: /content/data/private_test/private_12972.wav  \n","  inflating: /content/data/private_test/private_17822.wav  \n","  inflating: /content/data/private_test/private_08902.wav  \n","  inflating: /content/data/private_test/private_01098.wav  \n","  inflating: /content/data/private_test/private_00386.wav  \n","  inflating: /content/data/private_test/private_10195.wav  \n","  inflating: /content/data/private_test/private_02591.wav  \n","  inflating: /content/data/private_test/private_12782.wav  \n","  inflating: /content/data/private_test/private_03857.wav  \n","  inflating: /content/data/private_test/private_05598.wav  \n","  inflating: /content/data/private_test/private_14495.wav  \n","  inflating: /content/data/private_test/private_04686.wav  \n","  inflating: /content/data/private_test/private_15953.wav  \n","  inflating: /content/data/private_test/private_01926.wav  \n","  inflating: /content/data/private_test/private_16282.wav  \n","  inflating: /content/data/private_test/private_06091.wav  \n","  inflating: /content/data/private_test/private_01932.wav  \n","  inflating: /content/data/private_test/private_17188.wav  \n","  inflating: /content/data/private_test/private_16296.wav  \n","  inflating: /content/data/private_test/private_06085.wav  \n","  inflating: /content/data/private_test/private_14481.wav  \n","  inflating: /content/data/private_test/private_15947.wav  \n","  inflating: /content/data/private_test/private_04692.wav  \n","  inflating: /content/data/private_test/private_13488.wav  \n","  inflating: /content/data/private_test/private_02585.wav  \n","  inflating: /content/data/private_test/private_03843.wav  \n","  inflating: /content/data/private_test/private_12796.wav  \n","  inflating: /content/data/private_test/private_17836.wav  \n","  inflating: /content/data/private_test/private_08916.wav  \n","  inflating: /content/data/private_test/private_00392.wav  \n","  inflating: /content/data/private_test/private_10181.wav  \n","  inflating: /content/data/private_test/private_08889.wav  \n","  inflating: /content/data/private_test/private_01113.wav  \n","  inflating: /content/data/private_test/private_11300.wav  \n","  inflating: /content/data/private_test/private_07562.wav  \n","  inflating: /content/data/private_test/private_19984.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19984.wav  \n","  inflating: /content/data/private_test/private_08651.wav  \n","  inflating: /content/data/private_test/private_17771.wav  \n","  inflating: /content/data/private_test/private_18442.wav  \n","  inflating: /content/data/private_test/private_03704.wav  \n","  inflating: /content/data/private_test/private_14278.wav  \n","  inflating: /content/data/private_test/private_13517.wav  \n","  inflating: /content/data/private_test/private_12609.wav  \n","  inflating: /content/data/private_test/private_05375.wav  \n","  inflating: /content/data/private_test/private_15166.wav  \n","  inflating: /content/data/private_test/private_15600.wav  \n","  inflating: /content/data/private_test/private_05413.wav  \n","  inflating: /content/data/private_test/private_13271.wav  \n","  inflating: /content/data/private_test/private_03062.wav  \n","  inflating: /content/data/private_test/private_17017.wav  \n","  inflating: /content/data/private_test/private_18324.wav  \n","  inflating: /content/data/private_test/private_07204.wav  \n","  inflating: /content/data/private_test/private_08137.wav  \n","  inflating: /content/data/private_test/private_10778.wav  \n","  inflating: /content/data/private_test/private_11466.wav  \n","  inflating: /content/data/private_test/private_09229.wav  \n","  inflating: /content/data/private_test/private_16309.wav  \n","  inflating: /content/data/private_test/private_01675.wav  \n","  inflating: /content/data/private_test/private_17003.wav  \n","  inflating: /content/data/private_test/private_18330.wav  \n","  inflating: /content/data/private_test/private_07210.wav  \n","  inflating: /content/data/private_test/private_08123.wav  \n","  inflating: /content/data/private_test/private_11472.wav  \n","  inflating: /content/data/private_test/private_01661.wav  \n","  inflating: /content/data/private_test/private_02368.wav  \n","  inflating: /content/data/private_test/private_15614.wav  \n","  inflating: /content/data/private_test/private_05407.wav  \n","  inflating: /content/data/private_test/private_13265.wav  \n","  inflating: /content/data/private_test/private_04719.wav  \n","  inflating: /content/data/private_test/private_03076.wav  \n","  inflating: /content/data/private_test/private_03710.wav  \n","  inflating: /content/data/private_test/private_13503.wav  \n","  inflating: /content/data/private_test/private_05361.wav  \n","  inflating: /content/data/private_test/private_15172.wav  \n","  inflating: /content/data/private_test/private_01107.wav  \n","  inflating: /content/data/private_test/private_19748.wav  \n","  inflating: /content/data/private_test/private_06668.wav  \n","  inflating: /content/data/private_test/private_11314.wav  \n","  inflating: /content/data/private_test/private_07576.wav  \n","  inflating: /content/data/private_test/private_08645.wav  \n","  inflating: /content/data/private_test/private_19990.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19990.wav  \n","  inflating: /content/data/private_test/private_17765.wav  \n","  inflating: /content/data/private_test/private_18456.wav  \n","  inflating: /content/data/private_test/private_00219.wav  \n","  inflating: /content/data/private_test/private_14244.wav  \n","  inflating: /content/data/private_test/private_03738.wav  \n","  inflating: /content/data/private_test/private_04057.wav  \n","  inflating: /content/data/private_test/private_05349.wav  \n","  inflating: /content/data/private_test/private_12635.wav  \n","  inflating: /content/data/private_test/private_02426.wav  \n","  inflating: /content/data/private_test/private_19760.wav  \n","  inflating: /content/data/private_test/private_16453.wav  \n","  inflating: /content/data/private_test/private_09573.wav  \n","  inflating: /content/data/private_test/private_06640.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06640.wav  \n","  inflating: /content/data/private_test/private_17995.wav  \n","  inflating: /content/data/private_test/private_10022.wav  \n","  inflating: /content/data/private_test/private_00231.wav  \n","  inflating: /content/data/private_test/private_06898.wav  \n","  inflating: /content/data/private_test/private_00557.wav  \n","  inflating: /content/data/private_test/private_18318.wav  \n","  inflating: /content/data/private_test/private_10744.wav  \n","  inflating: /content/data/private_test/private_01891.wav  \n","  inflating: /content/data/private_test/private_07238.wav  \n","  inflating: /content/data/private_test/private_09215.wav  \n","  inflating: /content/data/private_test/private_06126.wav  \n","  inflating: /content/data/private_test/private_19006.wav  \n","  inflating: /content/data/private_test/private_01649.wav  \n","  inflating: /content/data/private_test/private_16335.wav  \n","  inflating: /content/data/private_test/private_02340.wav  \n","  inflating: /content/data/private_test/private_12153.wav  \n","  inflating: /content/data/private_test/private_04731.wav  \n","  inflating: /content/data/private_test/private_14522.wav  \n","  inflating: /content/data/private_test/private_15628.wav  \n","  inflating: /content/data/private_test/private_02354.wav  \n","  inflating: /content/data/private_test/private_12147.wav  \n","  inflating: /content/data/private_test/private_04725.wav  \n","  inflating: /content/data/private_test/private_13259.wav  \n","  inflating: /content/data/private_test/private_14536.wav  \n","  inflating: /content/data/private_test/private_00543.wav  \n","  inflating: /content/data/private_test/private_01885.wav  \n","  inflating: /content/data/private_test/private_10750.wav  \n","  inflating: /content/data/private_test/private_09201.wav  \n","  inflating: /content/data/private_test/private_06132.wav  \n","  inflating: /content/data/private_test/private_10988.wav  \n","  inflating: /content/data/private_test/private_19012.wav  \n","  inflating: /content/data/private_test/private_16321.wav  \n","  inflating: /content/data/private_test/private_19774.wav  \n","  inflating: /content/data/private_test/private_16447.wav  \n","  inflating: /content/data/private_test/private_09567.wav  \n","  inflating: /content/data/private_test/private_11328.wav  \n","  inflating: /content/data/private_test/private_17981.wav  \n","  inflating: /content/data/private_test/private_06654.wav  \n","  inflating: /content/data/private_test/private_10036.wav  \n","  inflating: /content/data/private_test/private_08679.wav  \n","  inflating: /content/data/private_test/private_00225.wav  \n","  inflating: /content/data/private_test/private_17759.wav  \n","  inflating: /content/data/private_test/private_14250.wav  \n","  inflating: /content/data/private_test/private_04043.wav  \n","  inflating: /content/data/private_test/private_12621.wav  \n","  inflating: /content/data/private_test/private_02432.wav  \n","  inflating: /content/data/private_test/private_12812.wav  \n","  inflating: /content/data/private_test/private_19947.wav  \n","  inflating: /content/data/private_test/private_08692.wav  \n","  inflating: /content/data/private_test/private_06867.wav  \n","  inflating: /content/data/private_test/private_18481.wav  \n","  inflating: /content/data/private_test/private_10963.wav  \n","  inflating: /content/data/private_test/private_04916.wav  \n","  inflating: /content/data/private_test/private_04902.wav  \n","  inflating: /content/data/private_test/private_10977.wav  \n","  inflating: /content/data/private_test/private_08686.wav  \n","  inflating: /content/data/private_test/private_19953.wav  \n","  inflating: /content/data/private_test/private_06873.wav  \n","  inflating: /content/data/private_test/private_18495.wav  \n","  inflating: /content/data/private_test/private_09598.wav  \n","  inflating: /content/data/private_test/private_12806.wav  \n","  inflating: /content/data/private_test/private_08876.wav  \n","  inflating: /content/data/private_test/private_16490.wav  \n","  inflating: /content/data/private_test/private_06683.wav  \n","  inflating: /content/data/private_test/private_17956.wav  \n","  inflating: /content/data/private_test/private_03923.wav  \n","  inflating: /content/data/private_test/private_15199.wav  \n","  inflating: /content/data/private_test/private_14287.wav  \n","  inflating: /content/data/private_test/private_04094.wav  \n","  inflating: /content/data/private_test/private_15827.wav  \n","  inflating: /content/data/private_test/private_02383.wav  \n","  inflating: /content/data/private_test/private_12190.wav  \n","  inflating: /content/data/private_test/private_11499.wav  \n","  inflating: /content/data/private_test/private_00594.wav  \n","  inflating: /content/data/private_test/private_10787.wav  \n","  inflating: /content/data/private_test/private_01852.wav  \n","  inflating: /content/data/private_test/private_00580.wav  \n","  inflating: /content/data/private_test/private_01846.wav  \n","  inflating: /content/data/private_test/private_10793.wav  \n","  inflating: /content/data/private_test/private_15833.wav  \n","  inflating: /content/data/private_test/private_03089.wav  \n","  inflating: /content/data/private_test/private_02397.wav  \n","  inflating: /content/data/private_test/private_12184.wav  \n","  inflating: /content/data/private_test/private_03937.wav  \n","  inflating: /content/data/private_test/private_14293.wav  \n","  inflating: /content/data/private_test/private_04080.wav  \n","  inflating: /content/data/private_test/private_07589.wav  \n","  inflating: /content/data/private_test/private_08862.wav  \n","  inflating: /content/data/private_test/private_16484.wav  \n","  inflating: /content/data/private_test/private_17942.wav  \n","  inflating: /content/data/private_test/private_06697.wav  \n","  inflating: /content/data/private_test/private_01477.wav  \n","  inflating: /content/data/private_test/private_19238.wav  \n","  inflating: /content/data/private_test/private_11664.wav  \n","  inflating: /content/data/private_test/private_06318.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06318.wav  \n","  inflating: /content/data/private_test/private_08335.wav  \n","  inflating: /content/data/private_test/private_07006.wav  \n","  inflating: /content/data/private_test/private_18126.wav  \n","  inflating: /content/data/private_test/private_00769.wav  \n","  inflating: /content/data/private_test/private_17215.wav  \n","  inflating: /content/data/private_test/private_03260.wav  \n","  inflating: /content/data/private_test/private_13073.wav  \n","  inflating: /content/data/private_test/private_05611.wav  \n","  inflating: /content/data/private_test/private_15402.wav  \n","  inflating: /content/data/private_test/private_15364.wav  \n","  inflating: /content/data/private_test/private_02618.wav  \n","  inflating: /content/data/private_test/private_05177.wav  \n","  inflating: /content/data/private_test/private_04269.wav  \n","  inflating: /content/data/private_test/private_13715.wav  \n","  inflating: /content/data/private_test/private_03506.wav  \n","  inflating: /content/data/private_test/private_09995.wav  \n","  inflating: /content/data/private_test/private_18640.wav  \n","  inflating: /content/data/private_test/private_17573.wav  \n","  inflating: /content/data/private_test/private_08453.wav  \n","  inflating: /content/data/private_test/private_07760.wav  \n","  inflating: /content/data/private_test/private_18898.wav  \n","  inflating: /content/data/private_test/private_11102.wav  \n","  inflating: /content/data/private_test/private_01311.wav  \n","  inflating: /content/data/private_test/private_18654.wav  \n","  inflating: /content/data/private_test/private_09981.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_09981.wav  \n","  inflating: /content/data/private_test/private_17567.wav  \n","  inflating: /content/data/private_test/private_08447.wav  \n","  inflating: /content/data/private_test/private_10208.wav  \n","  inflating: /content/data/private_test/private_07774.wav  \n","  inflating: /content/data/private_test/private_11116.wav  \n","  inflating: /content/data/private_test/private_09759.wav  \n","  inflating: /content/data/private_test/private_01305.wav  \n","  inflating: /content/data/private_test/private_16679.wav  \n","  inflating: /content/data/private_test/private_15370.wav  \n","  inflating: /content/data/private_test/private_05163.wav  \n","  inflating: /content/data/private_test/private_13701.wav  \n","  inflating: /content/data/private_test/private_03512.wav  \n","  inflating: /content/data/private_test/private_14708.wav  \n","  inflating: /content/data/private_test/private_03274.wav  \n","  inflating: /content/data/private_test/private_13067.wav  \n","  inflating: /content/data/private_test/private_05605.wav  \n","  inflating: /content/data/private_test/private_12379.wav  \n","  inflating: /content/data/private_test/private_15416.wav  \n","  inflating: /content/data/private_test/private_01463.wav  \n","  inflating: /content/data/private_test/private_11670.wav  \n","  inflating: /content/data/private_test/private_08321.wav  \n","  inflating: /content/data/private_test/private_07012.wav  \n","  inflating: /content/data/private_test/private_18132.wav  \n","  inflating: /content/data/private_test/private_17201.wav  \n","  inflating: /content/data/private_test/private_14720.wav  \n","  inflating: /content/data/private_test/private_04533.wav  \n","  inflating: /content/data/private_test/private_12351.wav  \n","  inflating: /content/data/private_test/private_02142.wav  \n","  inflating: /content/data/private_test/private_16137.wav  \n","  inflating: /content/data/private_test/private_19204.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19204.wav  \n","  inflating: /content/data/private_test/private_06324.wav  \n","  inflating: /content/data/private_test/private_09017.wav  \n","  inflating: /content/data/private_test/private_11658.wav  \n","  inflating: /content/data/private_test/private_10546.wav  \n","  inflating: /content/data/private_test/private_08309.wav  \n","  inflating: /content/data/private_test/private_17229.wav  \n","  inflating: /content/data/private_test/private_00755.wav  \n","  inflating: /content/data/private_test/private_11880.wav  \n","  inflating: /content/data/private_test/private_00033.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00033.wav  \n","  inflating: /content/data/private_test/private_16889.wav  \n","  inflating: /content/data/private_test/private_10220.wav  \n","  inflating: /content/data/private_test/private_06442.wav  \n","  inflating: /content/data/private_test/private_09771.wav  \n","  inflating: /content/data/private_test/private_16651.wav  \n","  inflating: /content/data/private_test/private_07984.wav  \n","  inflating: /content/data/private_test/private_19562.wav  \n","  inflating: /content/data/private_test/private_02624.wav  \n","  inflating: /content/data/private_test/private_15358.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15358.wav  \n","  inflating: /content/data/private_test/private_12437.wav  \n","  inflating: /content/data/private_test/private_13729.wav  \n","  inflating: /content/data/private_test/private_04255.wav  \n","  inflating: /content/data/private_test/private_14046.wav  \n","  inflating: /content/data/private_test/private_02630.wav  \n","  inflating: /content/data/private_test/private_12423.wav  \n","  inflating: /content/data/private_test/private_04241.wav  \n","  inflating: /content/data/private_test/private_14052.wav  \n","  inflating: /content/data/private_test/private_00027.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00027.wav  \n","  inflating: /content/data/private_test/private_18668.wav  \n","  inflating: /content/data/private_test/private_07748.wav  \n","  inflating: /content/data/private_test/private_10234.wav  \n","  inflating: /content/data/private_test/private_06456.wav  \n","  inflating: /content/data/private_test/private_09765.wav  \n","  inflating: /content/data/private_test/private_07990.wav  \n","  inflating: /content/data/private_test/private_16645.wav  \n","  inflating: /content/data/private_test/private_19576.wav  \n","  inflating: /content/data/private_test/private_01339.wav  \n","  inflating: /content/data/private_test/private_16123.wav  \n","  inflating: /content/data/private_test/private_19210.wav  \n","  inflating: /content/data/private_test/private_06330.wav  \n","  inflating: /content/data/private_test/private_00999.wav  \n","  inflating: /content/data/private_test/private_09003.wav  \n","  inflating: /content/data/private_test/private_10552.wav  \n","  inflating: /content/data/private_test/private_11894.wav  \n","  inflating: /content/data/private_test/private_00741.wav  \n","  inflating: /content/data/private_test/private_03248.wav  \n","  inflating: /content/data/private_test/private_14734.wav  \n","  inflating: /content/data/private_test/private_04527.wav  \n","  inflating: /content/data/private_test/private_12345.wav  \n","  inflating: /content/data/private_test/private_05639.wav  \n","  inflating: /content/data/private_test/private_02156.wav  \n","  inflating: /content/data/private_test/private_14907.wav  \n","  inflating: /content/data/private_test/private_00972.wav  \n","  inflating: /content/data/private_test/private_09956.wav  \n","  inflating: /content/data/private_test/private_18683.wav  \n","  inflating: /content/data/private_test/private_08490.wav  \n","  inflating: /content/data/private_test/private_16876.wav  \n","  inflating: /content/data/private_test/private_02803.wav  \n","  inflating: /content/data/private_test/private_02817.wav  \n","  inflating: /content/data/private_test/private_19589.wav  \n","  inflating: /content/data/private_test/private_18697.wav  \n","  inflating: /content/data/private_test/private_09942.wav  \n","  inflating: /content/data/private_test/private_08484.wav  \n","  inflating: /content/data/private_test/private_16862.wav  \n","  inflating: /content/data/private_test/private_00966.wav  \n","  inflating: /content/data/private_test/private_14913.wav  \n","  inflating: /content/data/private_test/private_10585.wav  \n","  inflating: /content/data/private_test/private_00796.wav  \n","  inflating: /content/data/private_test/private_11843.wav  \n","  inflating: /content/data/private_test/private_01488.wav  \n","  inflating: /content/data/private_test/private_12392.wav  \n","  inflating: /content/data/private_test/private_02181.wav  \n","  inflating: /content/data/private_test/private_05836.wav  \n","  inflating: /content/data/private_test/private_04296.wav  \n","  inflating: /content/data/private_test/private_14085.wav  \n","  inflating: /content/data/private_test/private_13932.wav  \n","  inflating: /content/data/private_test/private_05188.wav  \n","  inflating: /content/data/private_test/private_06481.wav  \n","  inflating: /content/data/private_test/private_18867.wav  \n","  inflating: /content/data/private_test/private_16692.wav  \n","  inflating: /content/data/private_test/private_07947.wav  \n","  inflating: /content/data/private_test/private_06495.wav  \n","  inflating: /content/data/private_test/private_18873.wav  \n","  inflating: /content/data/private_test/private_07953.wav  \n","  inflating: /content/data/private_test/private_16686.wav  \n","  inflating: /content/data/private_test/private_17598.wav  \n","  inflating: /content/data/private_test/private_04282.wav  \n","  inflating: /content/data/private_test/private_14091.wav  \n","  inflating: /content/data/private_test/private_13926.wav  \n","  inflating: /content/data/private_test/private_12386.wav  \n","  inflating: /content/data/private_test/private_02195.wav  \n","  inflating: /content/data/private_test/private_05822.wav  \n","  inflating: /content/data/private_test/private_13098.wav  \n","  inflating: /content/data/private_test/private_10591.wav  \n","  inflating: /content/data/private_test/private_11857.wav  \n","  inflating: /content/data/private_test/private_00782.wav  \n","  inflating: /content/data/private_test/private_13927.wav  \n","  inflating: /content/data/private_test/private_14090.wav  \n","  inflating: /content/data/private_test/private_04283.wav  \n","  inflating: /content/data/private_test/private_17599.wav  \n","  inflating: /content/data/private_test/private_16687.wav  \n","  inflating: /content/data/private_test/private_07952.wav  \n","  inflating: /content/data/private_test/private_18872.wav  \n","  inflating: /content/data/private_test/private_06494.wav  \n","  inflating: /content/data/private_test/private_00783.wav  \n","  inflating: /content/data/private_test/private_11856.wav  \n","  inflating: /content/data/private_test/private_10590.wav  \n","  inflating: /content/data/private_test/private_13099.wav  \n","  inflating: /content/data/private_test/private_05823.wav  \n","  inflating: /content/data/private_test/private_02194.wav  \n","  inflating: /content/data/private_test/private_12387.wav  \n","  inflating: /content/data/private_test/private_05837.wav  \n","  inflating: /content/data/private_test/private_02180.wav  \n","  inflating: /content/data/private_test/private_12393.wav  \n","  inflating: /content/data/private_test/private_01489.wav  \n","  inflating: /content/data/private_test/private_11842.wav  \n","  inflating: /content/data/private_test/private_00797.wav  \n","  inflating: /content/data/private_test/private_10584.wav  \n","  inflating: /content/data/private_test/private_07946.wav  \n","  inflating: /content/data/private_test/private_16693.wav  \n","  inflating: /content/data/private_test/private_18866.wav  \n","  inflating: /content/data/private_test/private_06480.wav  \n","  inflating: /content/data/private_test/private_05189.wav  \n","  inflating: /content/data/private_test/private_13933.wav  \n","  inflating: /content/data/private_test/private_14084.wav  \n","  inflating: /content/data/private_test/private_04297.wav  \n","  inflating: /content/data/private_test/private_16863.wav  \n","  inflating: /content/data/private_test/private_08485.wav  \n","  inflating: /content/data/private_test/private_09943.wav  \n","  inflating: /content/data/private_test/private_18696.wav  \n","  inflating: /content/data/private_test/private_19588.wav  \n","  inflating: /content/data/private_test/private_02816.wav  \n","  inflating: /content/data/private_test/private_14912.wav  \n","  inflating: /content/data/private_test/private_00967.wav  \n","  inflating: /content/data/private_test/private_00973.wav  \n","  inflating: /content/data/private_test/private_14906.wav  \n","  inflating: /content/data/private_test/private_02802.wav  \n","  inflating: /content/data/private_test/private_16877.wav  \n","  inflating: /content/data/private_test/private_08491.wav  \n","  inflating: /content/data/private_test/private_18682.wav  \n","  inflating: /content/data/private_test/private_09957.wav  \n","  inflating: /content/data/private_test/private_19577.wav  \n","  inflating: /content/data/private_test/private_01338.wav  \n","  inflating: /content/data/private_test/private_16644.wav  \n","  inflating: /content/data/private_test/private_07991.wav  \n","  inflating: /content/data/private_test/private_09764.wav  \n","  inflating: /content/data/private_test/private_06457.wav  \n","  inflating: /content/data/private_test/private_10235.wav  \n","  inflating: /content/data/private_test/private_07749.wav  \n","  inflating: /content/data/private_test/private_00026.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00026.wav  \n","  inflating: /content/data/private_test/private_18669.wav  \n","  inflating: /content/data/private_test/private_14053.wav  \n","  inflating: /content/data/private_test/private_04240.wav  \n","  inflating: /content/data/private_test/private_12422.wav  \n","  inflating: /content/data/private_test/private_02631.wav  \n","  inflating: /content/data/private_test/private_02157.wav  \n","  inflating: /content/data/private_test/private_05638.wav  \n","  inflating: /content/data/private_test/private_12344.wav  \n","  inflating: /content/data/private_test/private_04526.wav  \n","  inflating: /content/data/private_test/private_14735.wav  \n","  inflating: /content/data/private_test/private_03249.wav  \n","  inflating: /content/data/private_test/private_00740.wav  \n","  inflating: /content/data/private_test/private_11895.wav  \n","  inflating: /content/data/private_test/private_10553.wav  \n","  inflating: /content/data/private_test/private_00998.wav  \n","  inflating: /content/data/private_test/private_09002.wav  \n","  inflating: /content/data/private_test/private_06331.wav  \n","  inflating: /content/data/private_test/private_19211.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19211.wav  \n","  inflating: /content/data/private_test/private_16122.wav  \n","  inflating: /content/data/private_test/private_11881.wav  \n","  inflating: /content/data/private_test/private_00754.wav  \n","  inflating: /content/data/private_test/private_17228.wav  \n","  inflating: /content/data/private_test/private_10547.wav  \n","  inflating: /content/data/private_test/private_08308.wav  \n","  inflating: /content/data/private_test/private_09016.wav  \n","  inflating: /content/data/private_test/private_11659.wav  \n","  inflating: /content/data/private_test/private_06325.wav  \n","  inflating: /content/data/private_test/private_19205.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19205.wav  \n","  inflating: /content/data/private_test/private_16136.wav  \n","  inflating: /content/data/private_test/private_02143.wav  \n","  inflating: /content/data/private_test/private_12350.wav  \n","  inflating: /content/data/private_test/private_04532.wav  \n","  inflating: /content/data/private_test/private_14721.wav  \n","  inflating: /content/data/private_test/private_14047.wav  \n","  inflating: /content/data/private_test/private_04254.wav  \n","  inflating: /content/data/private_test/private_13728.wav  \n","  inflating: /content/data/private_test/private_12436.wav  \n","  inflating: /content/data/private_test/private_15359.wav  \n","  inflating: /content/data/private_test/private_02625.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_02625.wav  \n","  inflating: /content/data/private_test/private_19563.wav  \n","  inflating: /content/data/private_test/private_07985.wav  \n","  inflating: /content/data/private_test/private_16650.wav  \n","  inflating: /content/data/private_test/private_09770.wav  \n","  inflating: /content/data/private_test/private_06443.wav  \n","  inflating: /content/data/private_test/private_10221.wav  \n","  inflating: /content/data/private_test/private_16888.wav  \n","  inflating: /content/data/private_test/private_00032.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00032.wav  \n","  inflating: /content/data/private_test/private_03513.wav  \n","  inflating: /content/data/private_test/private_13700.wav  \n","  inflating: /content/data/private_test/private_05162.wav  \n","  inflating: /content/data/private_test/private_15371.wav  \n","  inflating: /content/data/private_test/private_16678.wav  \n","  inflating: /content/data/private_test/private_01304.wav  \n","  inflating: /content/data/private_test/private_11117.wav  \n","  inflating: /content/data/private_test/private_09758.wav  \n","  inflating: /content/data/private_test/private_07775.wav  \n","  inflating: /content/data/private_test/private_08446.wav  \n","  inflating: /content/data/private_test/private_10209.wav  \n","  inflating: /content/data/private_test/private_17566.wav  \n","  inflating: /content/data/private_test/private_09980.wav  \n","  inflating: /content/data/private_test/private_18655.wav  \n","  inflating: /content/data/private_test/private_17200.wav  \n","  inflating: /content/data/private_test/private_18133.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18133.wav  \n","  inflating: /content/data/private_test/private_07013.wav  \n","  inflating: /content/data/private_test/private_08320.wav  \n","  inflating: /content/data/private_test/private_11671.wav  \n","  inflating: /content/data/private_test/private_01462.wav  \n","  inflating: /content/data/private_test/private_15417.wav  \n","  inflating: /content/data/private_test/private_12378.wav  \n","  inflating: /content/data/private_test/private_05604.wav  \n","  inflating: /content/data/private_test/private_13066.wav  \n","  inflating: /content/data/private_test/private_03275.wav  \n","  inflating: /content/data/private_test/private_14709.wav  \n","  inflating: /content/data/private_test/private_15403.wav  \n","  inflating: /content/data/private_test/private_05610.wav  \n","  inflating: /content/data/private_test/private_13072.wav  \n","  inflating: /content/data/private_test/private_03261.wav  \n","  inflating: /content/data/private_test/private_17214.wav  \n","  inflating: /content/data/private_test/private_18127.wav  \n","  inflating: /content/data/private_test/private_00768.wav  \n","  inflating: /content/data/private_test/private_07007.wav  \n","  inflating: /content/data/private_test/private_08334.wav  \n","  inflating: /content/data/private_test/private_06319.wav  \n","  inflating: /content/data/private_test/private_11665.wav  \n","  inflating: /content/data/private_test/private_01476.wav  \n","  inflating: /content/data/private_test/private_19239.wav  \n","  inflating: /content/data/private_test/private_01310.wav  \n","  inflating: /content/data/private_test/private_18899.wav  \n","  inflating: /content/data/private_test/private_11103.wav  \n","  inflating: /content/data/private_test/private_07761.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_07761.wav  \n","  inflating: /content/data/private_test/private_08452.wav  \n","  inflating: /content/data/private_test/private_17572.wav  \n","  inflating: /content/data/private_test/private_18641.wav  \n","  inflating: /content/data/private_test/private_09994.wav  \n","  inflating: /content/data/private_test/private_03507.wav  \n","  inflating: /content/data/private_test/private_13714.wav  \n","  inflating: /content/data/private_test/private_04268.wav  \n","  inflating: /content/data/private_test/private_05176.wav  \n","  inflating: /content/data/private_test/private_02619.wav  \n","  inflating: /content/data/private_test/private_15365.wav  \n","  inflating: /content/data/private_test/private_12185.wav  \n","  inflating: /content/data/private_test/private_02396.wav  \n","  inflating: /content/data/private_test/private_03088.wav  \n","  inflating: /content/data/private_test/private_15832.wav  \n","  inflating: /content/data/private_test/private_10792.wav  \n","  inflating: /content/data/private_test/private_01847.wav  \n","  inflating: /content/data/private_test/private_00581.wav  \n","  inflating: /content/data/private_test/private_06696.wav  \n","  inflating: /content/data/private_test/private_17943.wav  \n","  inflating: /content/data/private_test/private_16485.wav  \n","  inflating: /content/data/private_test/private_08863.wav  \n","  inflating: /content/data/private_test/private_07588.wav  \n","  inflating: /content/data/private_test/private_04081.wav  \n","  inflating: /content/data/private_test/private_14292.wav  \n","  inflating: /content/data/private_test/private_03936.wav  \n","  inflating: /content/data/private_test/private_04095.wav  \n","  inflating: /content/data/private_test/private_14286.wav  \n","  inflating: /content/data/private_test/private_15198.wav  \n","  inflating: /content/data/private_test/private_03922.wav  \n","  inflating: /content/data/private_test/private_17957.wav  \n","  inflating: /content/data/private_test/private_06682.wav  \n","  inflating: /content/data/private_test/private_16491.wav  \n","  inflating: /content/data/private_test/private_08877.wav  \n","  inflating: /content/data/private_test/private_01853.wav  \n","  inflating: /content/data/private_test/private_10786.wav  \n","  inflating: /content/data/private_test/private_00595.wav  \n","  inflating: /content/data/private_test/private_11498.wav  \n","  inflating: /content/data/private_test/private_12191.wav  \n","  inflating: /content/data/private_test/private_02382.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_02382.wav  \n","  inflating: /content/data/private_test/private_15826.wav  \n","  inflating: /content/data/private_test/private_10976.wav  \n","  inflating: /content/data/private_test/private_04903.wav  \n","  inflating: /content/data/private_test/private_12807.wav  \n","  inflating: /content/data/private_test/private_09599.wav  \n","  inflating: /content/data/private_test/private_18494.wav  \n","  inflating: /content/data/private_test/private_06872.wav  \n","  inflating: /content/data/private_test/private_19952.wav  \n","  inflating: /content/data/private_test/private_08687.wav  \n","  inflating: /content/data/private_test/private_18480.wav  \n","  inflating: /content/data/private_test/private_06866.wav  \n","  inflating: /content/data/private_test/private_08693.wav  \n","  inflating: /content/data/private_test/private_19946.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19946.wav  \n","  inflating: /content/data/private_test/private_12813.wav  \n","  inflating: /content/data/private_test/private_04917.wav  \n","  inflating: /content/data/private_test/private_10962.wav  \n","  inflating: /content/data/private_test/private_16320.wav  \n","  inflating: /content/data/private_test/private_10989.wav  \n","  inflating: /content/data/private_test/private_19013.wav  \n","  inflating: /content/data/private_test/private_06133.wav  \n","  inflating: /content/data/private_test/private_09200.wav  \n","  inflating: /content/data/private_test/private_10751.wav  \n","  inflating: /content/data/private_test/private_01884.wav  \n","  inflating: /content/data/private_test/private_00542.wav  \n","  inflating: /content/data/private_test/private_14537.wav  \n","  inflating: /content/data/private_test/private_13258.wav  \n","  inflating: /content/data/private_test/private_04724.wav  \n","  inflating: /content/data/private_test/private_12146.wav  \n","  inflating: /content/data/private_test/private_02355.wav  \n","  inflating: /content/data/private_test/private_15629.wav  \n","  inflating: /content/data/private_test/private_02433.wav  \n","  inflating: /content/data/private_test/private_12620.wav  \n","  inflating: /content/data/private_test/private_04042.wav  \n","  inflating: /content/data/private_test/private_14251.wav  \n","  inflating: /content/data/private_test/private_17758.wav  \n","  inflating: /content/data/private_test/private_00224.wav  \n","  inflating: /content/data/private_test/private_10037.wav  \n","  inflating: /content/data/private_test/private_08678.wav  \n","  inflating: /content/data/private_test/private_06655.wav  \n","  inflating: /content/data/private_test/private_17980.wav  \n","  inflating: /content/data/private_test/private_09566.wav  \n","  inflating: /content/data/private_test/private_11329.wav  \n","  inflating: /content/data/private_test/private_16446.wav  \n","  inflating: /content/data/private_test/private_19775.wav  \n","  inflating: /content/data/private_test/private_06899.wav  \n","  inflating: /content/data/private_test/private_00230.wav  \n","  inflating: /content/data/private_test/private_10023.wav  \n","  inflating: /content/data/private_test/private_17994.wav  \n","  inflating: /content/data/private_test/private_06641.wav  \n","  inflating: /content/data/private_test/private_09572.wav  \n","  inflating: /content/data/private_test/private_16452.wav  \n","  inflating: /content/data/private_test/private_19761.wav  \n","  inflating: /content/data/private_test/private_02427.wav  \n","  inflating: /content/data/private_test/private_12634.wav  \n","  inflating: /content/data/private_test/private_05348.wav  \n","  inflating: /content/data/private_test/private_04056.wav  \n","  inflating: /content/data/private_test/private_03739.wav  \n","  inflating: /content/data/private_test/private_14245.wav  \n","  inflating: /content/data/private_test/private_14523.wav  \n","  inflating: /content/data/private_test/private_04730.wav  \n","  inflating: /content/data/private_test/private_12152.wav  \n","  inflating: /content/data/private_test/private_02341.wav  \n","  inflating: /content/data/private_test/private_16334.wav  \n","  inflating: /content/data/private_test/private_19007.wav  \n","  inflating: /content/data/private_test/private_01648.wav  \n","  inflating: /content/data/private_test/private_06127.wav  \n","  inflating: /content/data/private_test/private_09214.wav  \n","  inflating: /content/data/private_test/private_07239.wav  \n","  inflating: /content/data/private_test/private_01890.wav  \n","  inflating: /content/data/private_test/private_10745.wav  \n","  inflating: /content/data/private_test/private_00556.wav  \n","  inflating: /content/data/private_test/private_18319.wav  \n","  inflating: /content/data/private_test/private_03077.wav  \n","  inflating: /content/data/private_test/private_04718.wav  \n","  inflating: /content/data/private_test/private_13264.wav  \n","  inflating: /content/data/private_test/private_05406.wav  \n","  inflating: /content/data/private_test/private_15615.wav  \n","  inflating: /content/data/private_test/private_02369.wav  \n","  inflating: /content/data/private_test/private_01660.wav  \n","  inflating: /content/data/private_test/private_11473.wav  \n","  inflating: /content/data/private_test/private_08122.wav  \n","  inflating: /content/data/private_test/private_07211.wav  \n","  inflating: /content/data/private_test/private_18331.wav  \n","  inflating: /content/data/private_test/private_17002.wav  \n","  inflating: /content/data/private_test/private_18457.wav  \n","  inflating: /content/data/private_test/private_00218.wav  \n","  inflating: /content/data/private_test/private_17764.wav  \n","  inflating: /content/data/private_test/private_19991.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19991.wav  \n","  inflating: /content/data/private_test/private_08644.wav  \n","  inflating: /content/data/private_test/private_07577.wav  \n","  inflating: /content/data/private_test/private_11315.wav  \n","  inflating: /content/data/private_test/private_06669.wav  \n","  inflating: /content/data/private_test/private_01106.wav  \n","  inflating: /content/data/private_test/private_19749.wav  \n","  inflating: /content/data/private_test/private_15173.wav  \n","  inflating: /content/data/private_test/private_05360.wav  \n","  inflating: /content/data/private_test/private_13502.wav  \n","  inflating: /content/data/private_test/private_03711.wav  \n","  inflating: /content/data/private_test/private_15167.wav  \n","  inflating: /content/data/private_test/private_05374.wav  \n","  inflating: /content/data/private_test/private_12608.wav  \n","  inflating: /content/data/private_test/private_13516.wav  \n","  inflating: /content/data/private_test/private_14279.wav  \n","  inflating: /content/data/private_test/private_03705.wav  \n","  inflating: /content/data/private_test/private_18443.wav  \n","  inflating: /content/data/private_test/private_17770.wav  \n","  inflating: /content/data/private_test/private_08650.wav  \n","  inflating: /content/data/private_test/private_19985.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19985.wav  \n","  inflating: /content/data/private_test/private_07563.wav  \n","  inflating: /content/data/private_test/private_11301.wav  \n","  inflating: /content/data/private_test/private_08888.wav  \n","  inflating: /content/data/private_test/private_01112.wav  \n","  inflating: /content/data/private_test/private_01674.wav  \n","  inflating: /content/data/private_test/private_16308.wav  \n","  inflating: /content/data/private_test/private_11467.wav  \n","  inflating: /content/data/private_test/private_09228.wav  \n","  inflating: /content/data/private_test/private_08136.wav  \n","  inflating: /content/data/private_test/private_10779.wav  \n","  inflating: /content/data/private_test/private_07205.wav  \n","  inflating: /content/data/private_test/private_18325.wav  \n","  inflating: /content/data/private_test/private_17016.wav  \n","  inflating: /content/data/private_test/private_03063.wav  \n","  inflating: /content/data/private_test/private_13270.wav  \n","  inflating: /content/data/private_test/private_05412.wav  \n","  inflating: /content/data/private_test/private_15601.wav  \n","  inflating: /content/data/private_test/private_04693.wav  \n","  inflating: /content/data/private_test/private_15946.wav  \n","  inflating: /content/data/private_test/private_14480.wav  \n","  inflating: /content/data/private_test/private_06084.wav  \n","  inflating: /content/data/private_test/private_16297.wav  \n","  inflating: /content/data/private_test/private_17189.wav  \n","  inflating: /content/data/private_test/private_01933.wav  \n","  inflating: /content/data/private_test/private_10180.wav  \n","  inflating: /content/data/private_test/private_00393.wav  \n","  inflating: /content/data/private_test/private_08917.wav  \n","  inflating: /content/data/private_test/private_17837.wav  \n","  inflating: /content/data/private_test/private_12797.wav  \n","  inflating: /content/data/private_test/private_03842.wav  \n","  inflating: /content/data/private_test/private_02584.wav  \n","  inflating: /content/data/private_test/private_13489.wav  \n","  inflating: /content/data/private_test/private_03856.wav  \n","  inflating: /content/data/private_test/private_12783.wav  \n","  inflating: /content/data/private_test/private_02590.wav  \n","  inflating: /content/data/private_test/private_10194.wav  \n","  inflating: /content/data/private_test/private_00387.wav  \n","  inflating: /content/data/private_test/private_08903.wav  \n","  inflating: /content/data/private_test/private_01099.wav  \n","  inflating: /content/data/private_test/private_17823.wav  \n","  inflating: /content/data/private_test/private_06090.wav  \n","  inflating: /content/data/private_test/private_16283.wav  \n","  inflating: /content/data/private_test/private_01927.wav  \n","  inflating: /content/data/private_test/private_15952.wav  \n","  inflating: /content/data/private_test/private_04687.wav  \n","  inflating: /content/data/private_test/private_14494.wav  \n","  inflating: /content/data/private_test/private_05599.wav  \n","  inflating: /content/data/private_test/private_10802.wav  \n","  inflating: /content/data/private_test/private_19198.wav  \n","  inflating: /content/data/private_test/private_18286.wav  \n","  inflating: /content/data/private_test/private_08095.wav  \n","  inflating: /content/data/private_test/private_04877.wav  \n","  inflating: /content/data/private_test/private_12973.wav  \n","  inflating: /content/data/private_test/private_19826.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19826.wav  \n","  inflating: /content/data/private_test/private_06906.wav  \n","  inflating: /content/data/private_test/private_19832.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19832.wav  \n","  inflating: /content/data/private_test/private_06912.wav  \n","  inflating: /content/data/private_test/private_12967.wav  \n","  inflating: /content/data/private_test/private_04863.wav  \n","  inflating: /content/data/private_test/private_10816.wav  \n","  inflating: /content/data/private_test/private_18292.wav  \n","  inflating: /content/data/private_test/private_08081.wav  \n","  inflating: /content/data/private_test/private_00436.wav  \n","  inflating: /content/data/private_test/private_18279.wav  \n","  inflating: /content/data/private_test/private_07359.wav  \n","  inflating: /content/data/private_test/private_10625.wav  \n","  inflating: /content/data/private_test/private_06047.wav  \n","  inflating: /content/data/private_test/private_09374.wav  \n","  inflating: /content/data/private_test/private_16254.wav  \n","  inflating: /content/data/private_test/private_19167.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19167.wav  \n","  inflating: /content/data/private_test/private_01728.wav  \n","  inflating: /content/data/private_test/private_02221.wav  \n","  inflating: /content/data/private_test/private_04888.wav  \n","  inflating: /content/data/private_test/private_12032.wav  \n","  inflating: /content/data/private_test/private_04650.wav  \n","  inflating: /content/data/private_test/private_15985.wav  \n","  inflating: /content/data/private_test/private_14443.wav  \n","  inflating: /content/data/private_test/private_03659.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03659.wav  \n","  inflating: /content/data/private_test/private_14325.wav  \n","  inflating: /content/data/private_test/private_04136.wav  \n","  inflating: /content/data/private_test/private_12754.wav  \n","  inflating: /content/data/private_test/private_03881.wav  \n","  inflating: /content/data/private_test/private_05228.wav  \n","  inflating: /content/data/private_test/private_02547.wav  \n","  inflating: /content/data/private_test/private_16532.wav  \n","  inflating: /content/data/private_test/private_19601.wav  \n","  inflating: /content/data/private_test/private_06721.wav  \n","  inflating: /content/data/private_test/private_09412.wav  \n","  inflating: /content/data/private_test/private_10143.wav  \n","  inflating: /content/data/private_test/private_00350.wav  \n","  inflating: /content/data/private_test/private_16526.wav  \n","  inflating: /content/data/private_test/private_19615.wav  \n","  inflating: /content/data/private_test/private_06735.wav  \n","  inflating: /content/data/private_test/private_09406.wav  \n","  inflating: /content/data/private_test/private_11249.wav  \n","  inflating: /content/data/private_test/private_10157.wav  \n","  inflating: /content/data/private_test/private_08718.wav  \n","  inflating: /content/data/private_test/private_17638.wav  \n","  inflating: /content/data/private_test/private_00344.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00344.wav  \n","  inflating: /content/data/private_test/private_12998.wav  \n","  inflating: /content/data/private_test/private_14331.wav  \n","  inflating: /content/data/private_test/private_04122.wav  \n","  inflating: /content/data/private_test/private_03895.wav  \n","  inflating: /content/data/private_test/private_12740.wav  \n","  inflating: /content/data/private_test/private_02553.wav  \n","  inflating: /content/data/private_test/private_02235.wav  \n","  inflating: /content/data/private_test/private_15749.wav  \n","  inflating: /content/data/private_test/private_12026.wav  \n","  inflating: /content/data/private_test/private_13338.wav  \n","  inflating: /content/data/private_test/private_15991.wav  \n","  inflating: /content/data/private_test/private_04644.wav  \n","  inflating: /content/data/private_test/private_14457.wav  \n","  inflating: /content/data/private_test/private_00422.wav  \n","  inflating: /content/data/private_test/private_10631.wav  \n","  inflating: /content/data/private_test/private_06053.wav  \n","  inflating: /content/data/private_test/private_09360.wav  \n","  inflating: /content/data/private_test/private_16240.wav  \n","  inflating: /content/data/private_test/private_19173.wav  \n","  inflating: /content/data/private_test/private_15761.wav  \n","  inflating: /content/data/private_test/private_05572.wav  \n","  inflating: /content/data/private_test/private_13310.wav  \n","  inflating: /content/data/private_test/private_03103.wav  \n","  inflating: /content/data/private_test/private_18245.wav  \n","  inflating: /content/data/private_test/private_17176.wav  \n","  inflating: /content/data/private_test/private_08056.wav  \n","  inflating: /content/data/private_test/private_10619.wav  \n","  inflating: /content/data/private_test/private_07365.wav  \n","  inflating: /content/data/private_test/private_11507.wav  \n","  inflating: /content/data/private_test/private_09348.wav  \n","  inflating: /content/data/private_test/private_01714.wav  \n","  inflating: /content/data/private_test/private_16268.wav  \n","  inflating: /content/data/private_test/private_01072.wav  \n","  inflating: /content/data/private_test/private_11261.wav  \n","  inflating: /content/data/private_test/private_08730.wav  \n","  inflating: /content/data/private_test/private_07403.wav  \n","  inflating: /content/data/private_test/private_18523.wav  \n","  inflating: /content/data/private_test/private_17610.wav  \n","  inflating: /content/data/private_test/private_14319.wav  \n","  inflating: /content/data/private_test/private_03665.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03665.wav  \n","  inflating: /content/data/private_test/private_13476.wav  \n","  inflating: /content/data/private_test/private_05214.wav  \n","  inflating: /content/data/private_test/private_12768.wav  \n","  inflating: /content/data/private_test/private_15007.wav  \n","  inflating: /content/data/private_test/private_03671.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03671.wav  \n","  inflating: /content/data/private_test/private_13462.wav  \n","  inflating: /content/data/private_test/private_05200.wav  \n","  inflating: /content/data/private_test/private_15013.wav  \n","  inflating: /content/data/private_test/private_01066.wav  \n","  inflating: /content/data/private_test/private_19629.wav  \n","  inflating: /content/data/private_test/private_11275.wav  \n","  inflating: /content/data/private_test/private_06709.wav  \n","  inflating: /content/data/private_test/private_08724.wav  \n","  inflating: /content/data/private_test/private_07417.wav  \n","  inflating: /content/data/private_test/private_18537.wav  \n","  inflating: /content/data/private_test/private_00378.wav  \n","  inflating: /content/data/private_test/private_17604.wav  \n","  inflating: /content/data/private_test/private_18251.wav  \n","  inflating: /content/data/private_test/private_17162.wav  \n","  inflating: /content/data/private_test/private_08042.wav  \n","  inflating: /content/data/private_test/private_07371.wav  \n","  inflating: /content/data/private_test/private_11513.wav  \n","  inflating: /content/data/private_test/private_01700.wav  \n","  inflating: /content/data/private_test/private_15775.wav  \n","  inflating: /content/data/private_test/private_02209.wav  \n","  inflating: /content/data/private_test/private_05566.wav  \n","  inflating: /content/data/private_test/private_04678.wav  \n","  inflating: /content/data/private_test/private_13304.wav  \n","  inflating: /content/data/private_test/private_03117.wav  \n","  inflating: /content/data/private_test/private_03498.wav  \n","  inflating: /content/data/private_test/private_02786.wav  \n","  inflating: /content/data/private_test/private_13853.wav  \n","  inflating: /content/data/private_test/private_12595.wav  \n","  inflating: /content/data/private_test/private_18906.wav  \n","  inflating: /content/data/private_test/private_07826.wav  \n","  inflating: /content/data/private_test/private_00191.wav  \n","  inflating: /content/data/private_test/private_10382.wav  \n","  inflating: /content/data/private_test/private_07198.wav  \n","  inflating: /content/data/private_test/private_11922.wav  \n","  inflating: /content/data/private_test/private_16095.wav  \n","  inflating: /content/data/private_test/private_06286.wav  \n","  inflating: /content/data/private_test/private_14682.wav  \n","  inflating: /content/data/private_test/private_05957.wav  \n","  inflating: /content/data/private_test/private_04491.wav  \n","  inflating: /content/data/private_test/private_15588.wav  \n","  inflating: /content/data/private_test/private_05943.wav  \n","  inflating: /content/data/private_test/private_14696.wav  \n","  inflating: /content/data/private_test/private_04485.wav  \n","  inflating: /content/data/private_test/private_11936.wav  \n","  inflating: /content/data/private_test/private_16081.wav  \n","  inflating: /content/data/private_test/private_06292.wav  \n","  inflating: /content/data/private_test/private_18912.wav  \n","  inflating: /content/data/private_test/private_11088.wav  \n","  inflating: /content/data/private_test/private_07832.wav  \n","  inflating: /content/data/private_test/private_00185.wav  \n","  inflating: /content/data/private_test/private_10396.wav  \n","  inflating: /content/data/private_test/private_13847.wav  \n","  inflating: /content/data/private_test/private_02792.wav  \n","  inflating: /content/data/private_test/private_12581.wav  \n","  inflating: /content/data/private_test/private_09837.wav  \n","  inflating: /content/data/private_test/private_16917.wav  \n","  inflating: /content/data/private_test/private_02962.wav  \n","  inflating: /content/data/private_test/private_14866.wav  \n","  inflating: /content/data/private_test/private_08297.wav  \n","  inflating: /content/data/private_test/private_18084.wav  \n","  inflating: /content/data/private_test/private_00813.wav  \n","  inflating: /content/data/private_test/private_09189.wav  \n","  inflating: /content/data/private_test/private_08283.wav  \n","  inflating: /content/data/private_test/private_18090.wav  \n","  inflating: /content/data/private_test/private_00807.wav  \n","  inflating: /content/data/private_test/private_14872.wav  \n","  inflating: /content/data/private_test/private_02976.wav  \n","  inflating: /content/data/private_test/private_09823.wav  \n","  inflating: /content/data/private_test/private_16903.wav  \n","  inflating: /content/data/private_test/private_00152.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00152.wav  \n","  inflating: /content/data/private_test/private_10341.wav  \n","  inflating: /content/data/private_test/private_09610.wav  \n","  inflating: /content/data/private_test/private_06523.wav  \n","  inflating: /content/data/private_test/private_19403.wav  \n","  inflating: /content/data/private_test/private_16730.wav  \n","  inflating: /content/data/private_test/private_15239.wav  \n","  inflating: /content/data/private_test/private_02745.wav  \n","  inflating: /content/data/private_test/private_13890.wav  \n","  inflating: /content/data/private_test/private_12556.wav  \n","  inflating: /content/data/private_test/private_04334.wav  \n","  inflating: /content/data/private_test/private_13648.wav  \n","  inflating: /content/data/private_test/private_14127.wav  \n","  inflating: /content/data/private_test/private_14641.wav  \n","  inflating: /content/data/private_test/private_05994.wav  \n","  inflating: /content/data/private_test/private_04452.wav  \n","  inflating: /content/data/private_test/private_14899.wav  \n","  inflating: /content/data/private_test/private_12230.wav  \n","  inflating: /content/data/private_test/private_02023.wav  \n","  inflating: /content/data/private_test/private_19365.wav  \n","  inflating: /content/data/private_test/private_16056.wav  \n","  inflating: /content/data/private_test/private_09176.wav  \n","  inflating: /content/data/private_test/private_11739.wav  \n","  inflating: /content/data/private_test/private_06245.wav  \n","  inflating: /content/data/private_test/private_10427.wav  \n","  inflating: /content/data/private_test/private_08268.wav  \n","  inflating: /content/data/private_test/private_00634.wav  \n","  inflating: /content/data/private_test/private_17348.wav  \n","  inflating: /content/data/private_test/private_19371.wav  \n","  inflating: /content/data/private_test/private_16042.wav  \n","  inflating: /content/data/private_test/private_09162.wav  \n","  inflating: /content/data/private_test/private_06251.wav  \n","  inflating: /content/data/private_test/private_10433.wav  \n","  inflating: /content/data/private_test/private_00620.wav  \n","  inflating: /content/data/private_test/private_05980.wav  \n","  inflating: /content/data/private_test/private_14655.wav  \n","  inflating: /content/data/private_test/private_03329.wav  \n","  inflating: /content/data/private_test/private_04446.wav  \n","  inflating: /content/data/private_test/private_05758.wav  \n","  inflating: /content/data/private_test/private_12224.wav  \n","  inflating: /content/data/private_test/private_02037.wav  \n","  inflating: /content/data/private_test/private_13884.wav  \n","  inflating: /content/data/private_test/private_02751.wav  \n","  inflating: /content/data/private_test/private_12542.wav  \n","  inflating: /content/data/private_test/private_04320.wav  \n","  inflating: /content/data/private_test/private_02989.wav  \n","  inflating: /content/data/private_test/private_14133.wav  \n","  inflating: /content/data/private_test/private_00146.wav  \n","  inflating: /content/data/private_test/private_18709.wav  \n","  inflating: /content/data/private_test/private_10355.wav  \n","  inflating: /content/data/private_test/private_07629.wav  \n","  inflating: /content/data/private_test/private_09604.wav  \n","  inflating: /content/data/private_test/private_06537.wav  \n","  inflating: /content/data/private_test/private_19417.wav  \n","  inflating: /content/data/private_test/private_01258.wav  \n","  inflating: /content/data/private_test/private_16724.wav  \n","  inflating: /content/data/private_test/private_02779.wav  \n","  inflating: /content/data/private_test/private_15205.wav  \n","  inflating: /content/data/private_test/private_05016.wav  \n","  inflating: /content/data/private_test/private_13674.wav  \n","  inflating: /content/data/private_test/private_04308.wav  \n","  inflating: /content/data/private_test/private_03467.wav  \n","  inflating: /content/data/private_test/private_17412.wav  \n","  inflating: /content/data/private_test/private_18721.wav  \n","  inflating: /content/data/private_test/private_07601.wav  \n","  inflating: /content/data/private_test/private_08532.wav  \n","  inflating: /content/data/private_test/private_11063.wav  \n","  inflating: /content/data/private_test/private_01270.wav  \n","  inflating: /content/data/private_test/private_01516.wav  \n","  inflating: /content/data/private_test/private_19359.wav  \n","  inflating: /content/data/private_test/private_06279.wav  \n","  inflating: /content/data/private_test/private_11705.wav  \n","  inflating: /content/data/private_test/private_07167.wav  \n","  inflating: /content/data/private_test/private_08254.wav  \n","  inflating: /content/data/private_test/private_17374.wav  \n","  inflating: /content/data/private_test/private_18047.wav  \n","  inflating: /content/data/private_test/private_00608.wav  \n","  inflating: /content/data/private_test/private_03301.wav  \n","  inflating: /content/data/private_test/private_13112.wav  \n","  inflating: /content/data/private_test/private_05770.wav  \n","  inflating: /content/data/private_test/private_15563.wav  \n","  inflating: /content/data/private_test/private_03315.wav  \n","  inflating: /content/data/private_test/private_14669.wav  \n","  inflating: /content/data/private_test/private_13106.wav  \n","  inflating: /content/data/private_test/private_12218.wav  \n","  inflating: /content/data/private_test/private_05764.wav  \n","  inflating: /content/data/private_test/private_15577.wav  \n","  inflating: /content/data/private_test/private_01502.wav  \n","  inflating: /content/data/private_test/private_11711.wav  \n","  inflating: /content/data/private_test/private_07173.wav  \n","  inflating: /content/data/private_test/private_08240.wav  \n","  inflating: /content/data/private_test/private_17360.wav  \n","  inflating: /content/data/private_test/private_18053.wav  \n","  inflating: /content/data/private_test/private_17406.wav  \n","  inflating: /content/data/private_test/private_18735.wav  \n","  inflating: /content/data/private_test/private_07615.wav  \n","  inflating: /content/data/private_test/private_08526.wav  \n","  inflating: /content/data/private_test/private_10369.wav  \n","  inflating: /content/data/private_test/private_11077.wav  \n","  inflating: /content/data/private_test/private_09638.wav  \n","  inflating: /content/data/private_test/private_16718.wav  \n","  inflating: /content/data/private_test/private_01264.wav  \n","  inflating: /content/data/private_test/private_15211.wav  \n","  inflating: /content/data/private_test/private_05002.wav  \n","  inflating: /content/data/private_test/private_13660.wav  \n","  inflating: /content/data/private_test/private_03473.wav  \n","  inflating: /content/data/private_test/private_15561.wav  \n","  inflating: /content/data/private_test/private_05772.wav  \n","  inflating: /content/data/private_test/private_13110.wav  \n","  inflating: /content/data/private_test/private_03303.wav  \n","  inflating: /content/data/private_test/private_17376.wav  \n","  inflating: /content/data/private_test/private_18045.wav  \n","  inflating: /content/data/private_test/private_07165.wav  \n","  inflating: /content/data/private_test/private_08256.wav  \n","  inflating: /content/data/private_test/private_10419.wav  \n","  inflating: /content/data/private_test/private_11707.wav  \n","  inflating: /content/data/private_test/private_09148.wav  \n","  inflating: /content/data/private_test/private_16068.wav  \n","  inflating: /content/data/private_test/private_01514.wav  \n","  inflating: /content/data/private_test/private_01272.wav  \n","  inflating: /content/data/private_test/private_11061.wav  \n","  inflating: /content/data/private_test/private_07603.wav  \n","  inflating: /content/data/private_test/private_08530.wav  \n","  inflating: /content/data/private_test/private_17410.wav  \n","  inflating: /content/data/private_test/private_18723.wav  \n","  inflating: /content/data/private_test/private_03465.wav  \n","  inflating: /content/data/private_test/private_14119.wav  \n","  inflating: /content/data/private_test/private_13676.wav  \n","  inflating: /content/data/private_test/private_12568.wav  \n","  inflating: /content/data/private_test/private_05014.wav  \n","  inflating: /content/data/private_test/private_15207.wav  \n","  inflating: /content/data/private_test/private_03471.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03471.wav  \n","  inflating: /content/data/private_test/private_13662.wav  \n","  inflating: /content/data/private_test/private_05000.wav  \n","  inflating: /content/data/private_test/private_15213.wav  \n","  inflating: /content/data/private_test/private_01266.wav  \n","  inflating: /content/data/private_test/private_19429.wav  \n","  inflating: /content/data/private_test/private_06509.wav  \n","  inflating: /content/data/private_test/private_11075.wav  \n","  inflating: /content/data/private_test/private_07617.wav  \n","  inflating: /content/data/private_test/private_08524.wav  \n","  inflating: /content/data/private_test/private_17404.wav  \n","  inflating: /content/data/private_test/private_18737.wav  \n","  inflating: /content/data/private_test/private_00178.wav  \n","  inflating: /content/data/private_test/private_17362.wav  \n","  inflating: /content/data/private_test/private_18051.wav  \n","  inflating: /content/data/private_test/private_07171.wav  \n","  inflating: /content/data/private_test/private_08242.wav  \n","  inflating: /content/data/private_test/private_11713.wav  \n","  inflating: /content/data/private_test/private_01500.wav  \n","  inflating: /content/data/private_test/private_02009.wav  \n","  inflating: /content/data/private_test/private_15575.wav  \n","  inflating: /content/data/private_test/private_05766.wav  \n","  inflating: /content/data/private_test/private_13104.wav  \n","  inflating: /content/data/private_test/private_04478.wav  \n","  inflating: /content/data/private_test/private_03317.wav  \n","  inflating: /content/data/private_test/private_00636.wav  \n","  inflating: /content/data/private_test/private_18079.wav  \n","  inflating: /content/data/private_test/private_10425.wav  \n","  inflating: /content/data/private_test/private_07159.wav  \n","  inflating: /content/data/private_test/private_09174.wav  \n","  inflating: /content/data/private_test/private_06247.wav  \n","  inflating: /content/data/private_test/private_19367.wav  \n","  inflating: /content/data/private_test/private_01528.wav  \n","  inflating: /content/data/private_test/private_16054.wav  \n","  inflating: /content/data/private_test/private_02021.wav  \n","  inflating: /content/data/private_test/private_12232.wav  \n","  inflating: /content/data/private_test/private_04450.wav  \n","  inflating: /content/data/private_test/private_05996.wav  \n","  inflating: /content/data/private_test/private_14643.wav  \n","  inflating: /content/data/private_test/private_14125.wav  \n","  inflating: /content/data/private_test/private_03459.wav  \n","  inflating: /content/data/private_test/private_04336.wav  \n","  inflating: /content/data/private_test/private_05028.wav  \n","  inflating: /content/data/private_test/private_12554.wav  \n","  inflating: /content/data/private_test/private_13892.wav  \n","  inflating: /content/data/private_test/private_02747.wav  \n","  inflating: /content/data/private_test/private_19401.wav  \n","  inflating: /content/data/private_test/private_16732.wav  \n","  inflating: /content/data/private_test/private_09612.wav  \n","  inflating: /content/data/private_test/private_06521.wav  \n","  inflating: /content/data/private_test/private_10343.wav  \n","  inflating: /content/data/private_test/private_00150.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00150.wav  \n","  inflating: /content/data/private_test/private_19415.wav  \n","  inflating: /content/data/private_test/private_16726.wav  \n","  inflating: /content/data/private_test/private_09606.wav  \n","  inflating: /content/data/private_test/private_11049.wav  \n","  inflating: /content/data/private_test/private_06535.wav  \n","  inflating: /content/data/private_test/private_10357.wav  \n","  inflating: /content/data/private_test/private_08518.wav  \n","  inflating: /content/data/private_test/private_00144.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00144.wav  \n","  inflating: /content/data/private_test/private_17438.wav  \n","  inflating: /content/data/private_test/private_14131.wav  \n","  inflating: /content/data/private_test/private_04322.wav  \n","  inflating: /content/data/private_test/private_12540.wav  \n","  inflating: /content/data/private_test/private_02753.wav  \n","  inflating: /content/data/private_test/private_13886.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13886.wav  \n","  inflating: /content/data/private_test/private_15549.wav  \n","  inflating: /content/data/private_test/private_02035.wav  \n","  inflating: /content/data/private_test/private_12226.wav  \n","  inflating: /content/data/private_test/private_04444.wav  \n","  inflating: /content/data/private_test/private_13138.wav  \n","  inflating: /content/data/private_test/private_14657.wav  \n","  inflating: /content/data/private_test/private_05982.wav  \n","  inflating: /content/data/private_test/private_00622.wav  \n","  inflating: /content/data/private_test/private_10431.wav  \n","  inflating: /content/data/private_test/private_09160.wav  \n","  inflating: /content/data/private_test/private_06253.wav  \n","  inflating: /content/data/private_test/private_19373.wav  \n","  inflating: /content/data/private_test/private_16040.wav  \n","  inflating: /content/data/private_test/private_00811.wav  \n","  inflating: /content/data/private_test/private_19398.wav  \n","  inflating: /content/data/private_test/private_18086.wav  \n","  inflating: /content/data/private_test/private_08295.wav  \n","  inflating: /content/data/private_test/private_05969.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_05969.wav  \n","  inflating: /content/data/private_test/private_14864.wav  \n","  inflating: /content/data/private_test/private_02960.wav  \n","  inflating: /content/data/private_test/private_16915.wav  \n","  inflating: /content/data/private_test/private_09835.wav  \n","  inflating: /content/data/private_test/private_07818.wav  \n","  inflating: /content/data/private_test/private_18938.wav  \n","  inflating: /content/data/private_test/private_16901.wav  \n","  inflating: /content/data/private_test/private_09821.wav  \n","  inflating: /content/data/private_test/private_13879.wav  \n","  inflating: /content/data/private_test/private_02974.wav  \n","  inflating: /content/data/private_test/private_14870.wav  \n","  inflating: /content/data/private_test/private_00805.wav  \n","  inflating: /content/data/private_test/private_11908.wav  \n","  inflating: /content/data/private_test/private_18092.wav  \n","  inflating: /content/data/private_test/private_08281.wav  \n","  inflating: /content/data/private_test/private_04493.wav  \n","  inflating: /content/data/private_test/private_05955.wav  \n","  inflating: /content/data/private_test/private_14680.wav  \n","  inflating: /content/data/private_test/private_14858.wav  \n","  inflating: /content/data/private_test/private_06284.wav  \n","  inflating: /content/data/private_test/private_16097.wav  \n","  inflating: /content/data/private_test/private_11920.wav  \n","  inflating: /content/data/private_test/private_17389.wav  \n","  inflating: /content/data/private_test/private_10380.wav  \n","  inflating: /content/data/private_test/private_16929.wav  \n","  inflating: /content/data/private_test/private_09809.wav  \n","  inflating: /content/data/private_test/private_00193.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00193.wav  \n","  inflating: /content/data/private_test/private_07824.wav  \n","  inflating: /content/data/private_test/private_18904.wav  \n","  inflating: /content/data/private_test/private_12597.wav  \n","  inflating: /content/data/private_test/private_13851.wav  \n","  inflating: /content/data/private_test/private_02784.wav  \n","  inflating: /content/data/private_test/private_13689.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13689.wav  \n","  inflating: /content/data/private_test/private_12583.wav  \n","  inflating: /content/data/private_test/private_02790.wav  \n","  inflating: /content/data/private_test/private_13845.wav  \n","  inflating: /content/data/private_test/private_02948.wav  \n","  inflating: /content/data/private_test/private_10394.wav  \n","  inflating: /content/data/private_test/private_00187.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00187.wav  \n","  inflating: /content/data/private_test/private_01299.wav  \n","  inflating: /content/data/private_test/private_07830.wav  \n","  inflating: /content/data/private_test/private_18910.wav  \n","  inflating: /content/data/private_test/private_00839.wav  \n","  inflating: /content/data/private_test/private_06290.wav  \n","  inflating: /content/data/private_test/private_16083.wav  \n","  inflating: /content/data/private_test/private_11934.wav  \n","  inflating: /content/data/private_test/private_04487.wav  \n","  inflating: /content/data/private_test/private_14694.wav  \n","  inflating: /content/data/private_test/private_05941.wav  \n","  inflating: /content/data/private_test/private_05799.wav  \n","  inflating: /content/data/private_test/private_15005.wav  \n","  inflating: /content/data/private_test/private_02579.wav  \n","  inflating: /content/data/private_test/private_05216.wav  \n","  inflating: /content/data/private_test/private_04108.wav  \n","  inflating: /content/data/private_test/private_13474.wav  \n","  inflating: /content/data/private_test/private_03667.wav  \n","  inflating: /content/data/private_test/private_18521.wav  \n","  inflating: /content/data/private_test/private_17612.wav  \n","  inflating: /content/data/private_test/private_08732.wav  \n","  inflating: /content/data/private_test/private_07401.wav  \n","  inflating: /content/data/private_test/private_11263.wav  \n","  inflating: /content/data/private_test/private_01070.wav  \n","  inflating: /content/data/private_test/private_01716.wav  \n","  inflating: /content/data/private_test/private_19159.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19159.wav  \n","  inflating: /content/data/private_test/private_11505.wav  \n","  inflating: /content/data/private_test/private_06079.wav  \n","  inflating: /content/data/private_test/private_08054.wav  \n","  inflating: /content/data/private_test/private_07367.wav  \n","  inflating: /content/data/private_test/private_18247.wav  \n","  inflating: /content/data/private_test/private_00408.wav  \n","  inflating: /content/data/private_test/private_17174.wav  \n","  inflating: /content/data/private_test/private_03101.wav  \n","  inflating: /content/data/private_test/private_13312.wav  \n","  inflating: /content/data/private_test/private_05570.wav  \n","  inflating: /content/data/private_test/private_15763.wav  \n","  inflating: /content/data/private_test/private_14469.wav  \n","  inflating: /content/data/private_test/private_03115.wav  \n","  inflating: /content/data/private_test/private_13306.wav  \n","  inflating: /content/data/private_test/private_05564.wav  \n","  inflating: /content/data/private_test/private_12018.wav  \n","  inflating: /content/data/private_test/private_15777.wav  \n","  inflating: /content/data/private_test/private_01702.wav  \n","  inflating: /content/data/private_test/private_11511.wav  \n","  inflating: /content/data/private_test/private_08040.wav  \n","  inflating: /content/data/private_test/private_07373.wav  \n","  inflating: /content/data/private_test/private_18253.wav  \n","  inflating: /content/data/private_test/private_17160.wav  \n","  inflating: /content/data/private_test/private_18535.wav  \n","  inflating: /content/data/private_test/private_17606.wav  \n","  inflating: /content/data/private_test/private_08726.wav  \n","  inflating: /content/data/private_test/private_10169.wav  \n","  inflating: /content/data/private_test/private_07415.wav  \n","  inflating: /content/data/private_test/private_11277.wav  \n","  inflating: /content/data/private_test/private_09438.wav  \n","  inflating: /content/data/private_test/private_01064.wav  \n","  inflating: /content/data/private_test/private_16518.wav  \n","  inflating: /content/data/private_test/private_15011.wav  \n","  inflating: /content/data/private_test/private_05202.wav  \n","  inflating: /content/data/private_test/private_13460.wav  \n","  inflating: /content/data/private_test/private_03673.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03673.wav  \n","  inflating: /content/data/private_test/private_00352.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00352.wav  \n","  inflating: /content/data/private_test/private_10141.wav  \n","  inflating: /content/data/private_test/private_06723.wav  \n","  inflating: /content/data/private_test/private_09410.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_09410.wav  \n","  inflating: /content/data/private_test/private_16530.wav  \n","  inflating: /content/data/private_test/private_19603.wav  \n","  inflating: /content/data/private_test/private_02545.wav  \n","  inflating: /content/data/private_test/private_15039.wav  \n","  inflating: /content/data/private_test/private_03883.wav  \n","  inflating: /content/data/private_test/private_12756.wav  \n","  inflating: /content/data/private_test/private_13448.wav  \n","  inflating: /content/data/private_test/private_04134.wav  \n","  inflating: /content/data/private_test/private_14327.wav  \n","  inflating: /content/data/private_test/private_14441.wav  \n","  inflating: /content/data/private_test/private_15987.wav  \n","  inflating: /content/data/private_test/private_04652.wav  \n","  inflating: /content/data/private_test/private_12030.wav  \n","  inflating: /content/data/private_test/private_02223.wav  \n","  inflating: /content/data/private_test/private_16256.wav  \n","  inflating: /content/data/private_test/private_19165.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19165.wav  \n","  inflating: /content/data/private_test/private_06045.wav  \n","  inflating: /content/data/private_test/private_09376.wav  \n","  inflating: /content/data/private_test/private_11539.wav  \n","  inflating: /content/data/private_test/private_10627.wav  \n","  inflating: /content/data/private_test/private_08068.wav  \n","  inflating: /content/data/private_test/private_17148.wav  \n","  inflating: /content/data/private_test/private_00434.wav  \n","  inflating: /content/data/private_test/private_16242.wav  \n","  inflating: /content/data/private_test/private_19171.wav  \n","  inflating: /content/data/private_test/private_06051.wav  \n","  inflating: /content/data/private_test/private_09362.wav  \n","  inflating: /content/data/private_test/private_10633.wav  \n","  inflating: /content/data/private_test/private_00420.wav  \n","  inflating: /content/data/private_test/private_03129.wav  \n","  inflating: /content/data/private_test/private_14455.wav  \n","  inflating: /content/data/private_test/private_04646.wav  \n","  inflating: /content/data/private_test/private_15993.wav  \n","  inflating: /content/data/private_test/private_12024.wav  \n","  inflating: /content/data/private_test/private_05558.wav  \n","  inflating: /content/data/private_test/private_02237.wav  \n","  inflating: /content/data/private_test/private_02551.wav  \n","  inflating: /content/data/private_test/private_12742.wav  \n","  inflating: /content/data/private_test/private_03897.wav  \n","  inflating: /content/data/private_test/private_04120.wav  \n","  inflating: /content/data/private_test/private_14333.wav  \n","  inflating: /content/data/private_test/private_00346.wav  \n","  inflating: /content/data/private_test/private_18509.wav  \n","  inflating: /content/data/private_test/private_07429.wav  \n","  inflating: /content/data/private_test/private_10155.wav  \n","  inflating: /content/data/private_test/private_06737.wav  \n","  inflating: /content/data/private_test/private_09404.wav  \n","  inflating: /content/data/private_test/private_16524.wav  \n","  inflating: /content/data/private_test/private_19617.wav  \n","  inflating: /content/data/private_test/private_01058.wav  \n","  inflating: /content/data/private_test/private_17809.wav  \n","  inflating: /content/data/private_test/private_08929.wav  \n","  inflating: /content/data/private_test/private_06904.wav  \n","  inflating: /content/data/private_test/private_19824.wav  \n","  inflating: /content/data/private_test/private_12971.wav  \n","  inflating: /content/data/private_test/private_04875.wav  \n","  inflating: /content/data/private_test/private_15978.wav  \n","  inflating: /content/data/private_test/private_08097.wav  \n","  inflating: /content/data/private_test/private_18284.wav  \n","  inflating: /content/data/private_test/private_10800.wav  \n","  inflating: /content/data/private_test/private_09389.wav  \n","  inflating: /content/data/private_test/private_01919.wav  \n","  inflating: /content/data/private_test/private_08083.wav  \n","  inflating: /content/data/private_test/private_18290.wav  \n","  inflating: /content/data/private_test/private_10814.wav  \n","  inflating: /content/data/private_test/private_04861.wav  \n","  inflating: /content/data/private_test/private_12965.wav  \n","  inflating: /content/data/private_test/private_03868.wav  \n","  inflating: /content/data/private_test/private_06910.wav  \n","  inflating: /content/data/private_test/private_19830.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19830.wav  \n","  inflating: /content/data/private_test/private_03698.wav  \n","  inflating: /content/data/private_test/private_02586.wav  \n","  inflating: /content/data/private_test/private_03840.wav  \n","  inflating: /content/data/private_test/private_12795.wav  \n","  inflating: /content/data/private_test/private_17835.wav  \n","  inflating: /content/data/private_test/private_08915.wav  \n","  inflating: /content/data/private_test/private_06938.wav  \n","  inflating: /content/data/private_test/private_00391.wav  \n","  inflating: /content/data/private_test/private_19818.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19818.wav  \n","  inflating: /content/data/private_test/private_10182.wav  \n","  inflating: /content/data/private_test/private_07398.wav  \n","  inflating: /content/data/private_test/private_01931.wav  \n","  inflating: /content/data/private_test/private_16295.wav  \n","  inflating: /content/data/private_test/private_06086.wav  \n","  inflating: /content/data/private_test/private_04849.wav  \n","  inflating: /content/data/private_test/private_14482.wav  \n","  inflating: /content/data/private_test/private_15944.wav  \n","  inflating: /content/data/private_test/private_04691.wav  \n","  inflating: /content/data/private_test/private_15788.wav  \n","  inflating: /content/data/private_test/private_14496.wav  \n","  inflating: /content/data/private_test/private_04685.wav  \n","  inflating: /content/data/private_test/private_15950.wav  \n","  inflating: /content/data/private_test/private_01925.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_01925.wav  \n","  inflating: /content/data/private_test/private_16281.wav  \n","  inflating: /content/data/private_test/private_10828.wav  \n","  inflating: /content/data/private_test/private_06092.wav  \n","  inflating: /content/data/private_test/private_17821.wav  \n","  inflating: /content/data/private_test/private_11288.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_11288.wav  \n","  inflating: /content/data/private_test/private_08901.wav  \n","  inflating: /content/data/private_test/private_00385.wav  \n","  inflating: /content/data/private_test/private_10196.wav  \n","  inflating: /content/data/private_test/private_12959.wav  \n","  inflating: /content/data/private_test/private_02592.wav  \n","  inflating: /content/data/private_test/private_12781.wav  \n","  inflating: /content/data/private_test/private_03854.wav  \n","  inflating: /content/data/private_test/private_03713.wav  \n","  inflating: /content/data/private_test/private_13500.wav  \n","  inflating: /content/data/private_test/private_05362.wav  \n","  inflating: /content/data/private_test/private_15171.wav  \n","  inflating: /content/data/private_test/private_01104.wav  \n","  inflating: /content/data/private_test/private_16478.wav  \n","  inflating: /content/data/private_test/private_11317.wav  \n","  inflating: /content/data/private_test/private_09558.wav  \n","  inflating: /content/data/private_test/private_08646.wav  \n","  inflating: /content/data/private_test/private_19993.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19993.wav  \n","  inflating: /content/data/private_test/private_10009.wav  \n","  inflating: /content/data/private_test/private_07575.wav  \n","  inflating: /content/data/private_test/private_18455.wav  \n","  inflating: /content/data/private_test/private_17766.wav  \n","  inflating: /content/data/private_test/private_18333.wav  \n","  inflating: /content/data/private_test/private_17000.wav  \n","  inflating: /content/data/private_test/private_08120.wav  \n","  inflating: /content/data/private_test/private_07213.wav  \n","  inflating: /content/data/private_test/private_11471.wav  \n","  inflating: /content/data/private_test/private_01662.wav  \n","  inflating: /content/data/private_test/private_15617.wav  \n","  inflating: /content/data/private_test/private_05404.wav  \n","  inflating: /content/data/private_test/private_12178.wav  \n","  inflating: /content/data/private_test/private_13266.wav  \n","  inflating: /content/data/private_test/private_14509.wav  \n","  inflating: /content/data/private_test/private_03075.wav  \n","  inflating: /content/data/private_test/private_15603.wav  \n","  inflating: /content/data/private_test/private_05410.wav  \n","  inflating: /content/data/private_test/private_13272.wav  \n","  inflating: /content/data/private_test/private_03061.wav  \n","  inflating: /content/data/private_test/private_18327.wav  \n","  inflating: /content/data/private_test/private_00568.wav  \n","  inflating: /content/data/private_test/private_17014.wav  \n","  inflating: /content/data/private_test/private_08134.wav  \n","  inflating: /content/data/private_test/private_07207.wav  \n","  inflating: /content/data/private_test/private_11465.wav  \n","  inflating: /content/data/private_test/private_06119.wav  \n","  inflating: /content/data/private_test/private_01676.wav  \n","  inflating: /content/data/private_test/private_19039.wav  \n","  inflating: /content/data/private_test/private_01110.wav  \n","  inflating: /content/data/private_test/private_11303.wav  \n","  inflating: /content/data/private_test/private_19987.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19987.wav  \n","  inflating: /content/data/private_test/private_08652.wav  \n","  inflating: /content/data/private_test/private_07561.wav  \n","  inflating: /content/data/private_test/private_18441.wav  \n","  inflating: /content/data/private_test/private_17772.wav  \n","  inflating: /content/data/private_test/private_03707.wav  \n","  inflating: /content/data/private_test/private_04068.wav  \n","  inflating: /content/data/private_test/private_13514.wav  \n","  inflating: /content/data/private_test/private_05376.wav  \n","  inflating: /content/data/private_test/private_15165.wav  \n","  inflating: /content/data/private_test/private_02419.wav  \n","  inflating: /content/data/private_test/private_16444.wav  \n","  inflating: /content/data/private_test/private_19777.wav  \n","  inflating: /content/data/private_test/private_01138.wav  \n","  inflating: /content/data/private_test/private_17982.wav  \n","  inflating: /content/data/private_test/private_06657.wav  \n","  inflating: /content/data/private_test/private_09564.wav  \n","  inflating: /content/data/private_test/private_07549.wav  \n","  inflating: /content/data/private_test/private_10035.wav  \n","  inflating: /content/data/private_test/private_00226.wav  \n","  inflating: /content/data/private_test/private_18469.wav  \n","  inflating: /content/data/private_test/private_14253.wav  \n","  inflating: /content/data/private_test/private_04040.wav  \n","  inflating: /content/data/private_test/private_12622.wav  \n","  inflating: /content/data/private_test/private_02431.wav  \n","  inflating: /content/data/private_test/private_02357.wav  \n","  inflating: /content/data/private_test/private_12144.wav  \n","  inflating: /content/data/private_test/private_05438.wav  \n","  inflating: /content/data/private_test/private_04726.wav  \n","  inflating: /content/data/private_test/private_03049.wav  \n","  inflating: /content/data/private_test/private_14535.wav  \n","  inflating: /content/data/private_test/private_00540.wav  \n","  inflating: /content/data/private_test/private_01886.wav  \n","  inflating: /content/data/private_test/private_10753.wav  \n","  inflating: /content/data/private_test/private_06131.wav  \n","  inflating: /content/data/private_test/private_09202.wav  \n","  inflating: /content/data/private_test/private_16322.wav  \n","  inflating: /content/data/private_test/private_19011.wav  \n","  inflating: /content/data/private_test/private_17028.wav  \n","  inflating: /content/data/private_test/private_00554.wav  \n","  inflating: /content/data/private_test/private_10747.wav  \n","  inflating: /content/data/private_test/private_01892.wav  \n","  inflating: /content/data/private_test/private_08108.wav  \n","  inflating: /content/data/private_test/private_06125.wav  \n","  inflating: /content/data/private_test/private_09216.wav  \n","  inflating: /content/data/private_test/private_11459.wav  \n","  inflating: /content/data/private_test/private_16336.wav  \n","  inflating: /content/data/private_test/private_19005.wav  \n","  inflating: /content/data/private_test/private_02343.wav  \n","  inflating: /content/data/private_test/private_12150.wav  \n","  inflating: /content/data/private_test/private_04732.wav  \n","  inflating: /content/data/private_test/private_14521.wav  \n","  inflating: /content/data/private_test/private_14247.wav  \n","  inflating: /content/data/private_test/private_13528.wav  \n","  inflating: /content/data/private_test/private_04054.wav  \n","  inflating: /content/data/private_test/private_12636.wav  \n","  inflating: /content/data/private_test/private_02425.wav  \n","  inflating: /content/data/private_test/private_15159.wav  \n","  inflating: /content/data/private_test/private_16450.wav  \n","  inflating: /content/data/private_test/private_19763.wav  \n","  inflating: /content/data/private_test/private_06643.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06643.wav  \n","  inflating: /content/data/private_test/private_17996.wav  \n","  inflating: /content/data/private_test/private_09570.wav  \n","  inflating: /content/data/private_test/private_10021.wav  \n","  inflating: /content/data/private_test/private_00232.wav  \n","  inflating: /content/data/private_test/private_08685.wav  \n","  inflating: /content/data/private_test/private_19950.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19950.wav  \n","  inflating: /content/data/private_test/private_18496.wav  \n","  inflating: /content/data/private_test/private_06870.wav  \n","  inflating: /content/data/private_test/private_19788.wav  \n","  inflating: /content/data/private_test/private_03908.wav  \n","  inflating: /content/data/private_test/private_12805.wav  \n","  inflating: /content/data/private_test/private_04901.wav  \n","  inflating: /content/data/private_test/private_10974.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_10974.wav  \n","  inflating: /content/data/private_test/private_01879.wav  \n","  inflating: /content/data/private_test/private_10960.wav  \n","  inflating: /content/data/private_test/private_15818.wav  \n","  inflating: /content/data/private_test/private_04915.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_04915.wav  \n","  inflating: /content/data/private_test/private_12811.wav  \n","  inflating: /content/data/private_test/private_19944.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19944.wav  \n","  inflating: /content/data/private_test/private_08691.wav  \n","  inflating: /content/data/private_test/private_18482.wav  \n","  inflating: /content/data/private_test/private_06864.wav  \n","  inflating: /content/data/private_test/private_08849.wav  \n","  inflating: /content/data/private_test/private_17969.wav  \n","  inflating: /content/data/private_test/private_03934.wav  \n","  inflating: /content/data/private_test/private_12839.wav  \n","  inflating: /content/data/private_test/private_14290.wav  \n","  inflating: /content/data/private_test/private_04083.wav  \n","  inflating: /content/data/private_test/private_17799.wav  \n","  inflating: /content/data/private_test/private_16487.wav  \n","  inflating: /content/data/private_test/private_08861.wav  \n","  inflating: /content/data/private_test/private_17941.wav  \n","  inflating: /content/data/private_test/private_06694.wav  \n","  inflating: /content/data/private_test/private_10948.wav  \n","  inflating: /content/data/private_test/private_00583.wav  \n","  inflating: /content/data/private_test/private_01845.wav  \n","  inflating: /content/data/private_test/private_10790.wav  \n","  inflating: /content/data/private_test/private_13299.wav  \n","  inflating: /content/data/private_test/private_15830.wav  \n","  inflating: /content/data/private_test/private_02394.wav  \n","  inflating: /content/data/private_test/private_12187.wav  \n","  inflating: /content/data/private_test/private_15824.wav  \n","  inflating: /content/data/private_test/private_02380.wav  \n","  inflating: /content/data/private_test/private_04929.wav  \n","  inflating: /content/data/private_test/private_12193.wav  \n","  inflating: /content/data/private_test/private_01689.wav  \n","  inflating: /content/data/private_test/private_00597.wav  \n","  inflating: /content/data/private_test/private_10784.wav  \n","  inflating: /content/data/private_test/private_01851.wav  \n","  inflating: /content/data/private_test/private_19978.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19978.wav  \n","  inflating: /content/data/private_test/private_06858.wav  \n","  inflating: /content/data/private_test/private_16493.wav  \n","  inflating: /content/data/private_test/private_08875.wav  \n","  inflating: /content/data/private_test/private_06680.wav  \n","  inflating: /content/data/private_test/private_17955.wav  \n","  inflating: /content/data/private_test/private_03920.wav  \n","  inflating: /content/data/private_test/private_05389.wav  \n","  inflating: /content/data/private_test/private_14284.wav  \n","  inflating: /content/data/private_test/private_04097.wav  \n","  inflating: /content/data/private_test/private_03277.wav  \n","  inflating: /content/data/private_test/private_13064.wav  \n","  inflating: /content/data/private_test/private_04518.wav  \n","  inflating: /content/data/private_test/private_05606.wav  \n","  inflating: /content/data/private_test/private_02169.wav  \n","  inflating: /content/data/private_test/private_15415.wav  \n","  inflating: /content/data/private_test/private_01460.wav  \n","  inflating: /content/data/private_test/private_11673.wav  \n","  inflating: /content/data/private_test/private_07011.wav  \n","  inflating: /content/data/private_test/private_08322.wav  \n","  inflating: /content/data/private_test/private_17202.wav  \n","  inflating: /content/data/private_test/private_18131.wav  \n","  inflating: /content/data/private_test/private_17564.wav  \n","  inflating: /content/data/private_test/private_18657.wav  \n","  inflating: /content/data/private_test/private_09982.wav  \n","  inflating: /content/data/private_test/private_00018.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00018.wav  \n","  inflating: /content/data/private_test/private_07777.wav  \n","  inflating: /content/data/private_test/private_08444.wav  \n","  inflating: /content/data/private_test/private_06469.wav  \n","  inflating: /content/data/private_test/private_11115.wav  \n","  inflating: /content/data/private_test/private_01306.wav  \n","  inflating: /content/data/private_test/private_19549.wav  \n","  inflating: /content/data/private_test/private_15373.wav  \n","  inflating: /content/data/private_test/private_05160.wav  \n","  inflating: /content/data/private_test/private_13702.wav  \n","  inflating: /content/data/private_test/private_03511.wav  \n","  inflating: /content/data/private_test/private_15367.wav  \n","  inflating: /content/data/private_test/private_12408.wav  \n","  inflating: /content/data/private_test/private_05174.wav  \n","  inflating: /content/data/private_test/private_13716.wav  \n","  inflating: /content/data/private_test/private_03505.wav  \n","  inflating: /content/data/private_test/private_14079.wav  \n","  inflating: /content/data/private_test/private_17570.wav  \n","  inflating: /content/data/private_test/private_09996.wav  \n","  inflating: /content/data/private_test/private_18643.wav  \n","  inflating: /content/data/private_test/private_07763.wav  \n","  inflating: /content/data/private_test/private_08450.wav  \n","  inflating: /content/data/private_test/private_11101.wav  \n","  inflating: /content/data/private_test/private_01312.wav  \n","  inflating: /content/data/private_test/private_16108.wav  \n","  inflating: /content/data/private_test/private_01474.wav  \n","  inflating: /content/data/private_test/private_11667.wav  \n","  inflating: /content/data/private_test/private_09028.wav  \n","  inflating: /content/data/private_test/private_07005.wav  \n","  inflating: /content/data/private_test/private_08336.wav  \n","  inflating: /content/data/private_test/private_10579.wav  \n","  inflating: /content/data/private_test/private_17216.wav  \n","  inflating: /content/data/private_test/private_18125.wav  \n","  inflating: /content/data/private_test/private_03263.wav  \n","  inflating: /content/data/private_test/private_13070.wav  \n","  inflating: /content/data/private_test/private_05612.wav  \n","  inflating: /content/data/private_test/private_15401.wav  \n","  inflating: /content/data/private_test/private_19213.wav  \n","  inflating: /content/data/private_test/private_16120.wav  \n","  inflating: /content/data/private_test/private_09000.wav  \n","  inflating: /content/data/private_test/private_06333.wav  \n","  inflating: /content/data/private_test/private_10551.wav  \n","  inflating: /content/data/private_test/private_11897.wav  \n","  inflating: /content/data/private_test/private_00742.wav  \n","  inflating: /content/data/private_test/private_14737.wav  \n","  inflating: /content/data/private_test/private_04524.wav  \n","  inflating: /content/data/private_test/private_13058.wav  \n","  inflating: /content/data/private_test/private_12346.wav  \n","  inflating: /content/data/private_test/private_15429.wav  \n","  inflating: /content/data/private_test/private_02155.wav  \n","  inflating: /content/data/private_test/private_02633.wav  \n","  inflating: /content/data/private_test/private_12420.wav  \n","  inflating: /content/data/private_test/private_04242.wav  \n","  inflating: /content/data/private_test/private_14051.wav  \n","  inflating: /content/data/private_test/private_00024.wav  \n","  inflating: /content/data/private_test/private_17558.wav  \n","  inflating: /content/data/private_test/private_10237.wav  \n","  inflating: /content/data/private_test/private_08478.wav  \n","  inflating: /content/data/private_test/private_09766.wav  \n","  inflating: /content/data/private_test/private_11129.wav  \n","  inflating: /content/data/private_test/private_06455.wav  \n","  inflating: /content/data/private_test/private_19575.wav  \n","  inflating: /content/data/private_test/private_07993.wav  \n","  inflating: /content/data/private_test/private_16646.wav  \n","  inflating: /content/data/private_test/private_00030.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00030.wav  \n","  inflating: /content/data/private_test/private_10223.wav  \n","  inflating: /content/data/private_test/private_09772.wav  \n","  inflating: /content/data/private_test/private_06441.wav  \n","  inflating: /content/data/private_test/private_19561.wav  \n","  inflating: /content/data/private_test/private_16652.wav  \n","  inflating: /content/data/private_test/private_07987.wav  \n","  inflating: /content/data/private_test/private_02627.wav  \n","  inflating: /content/data/private_test/private_05148.wav  \n","  inflating: /content/data/private_test/private_12434.wav  \n","  inflating: /content/data/private_test/private_04256.wav  \n","  inflating: /content/data/private_test/private_14045.wav  \n","  inflating: /content/data/private_test/private_03539.wav  \n","  inflating: /content/data/private_test/private_14723.wav  \n","  inflating: /content/data/private_test/private_04530.wav  \n","  inflating: /content/data/private_test/private_12352.wav  \n","  inflating: /content/data/private_test/private_02141.wav  \n","  inflating: /content/data/private_test/private_19207.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19207.wav  \n","  inflating: /content/data/private_test/private_01448.wav  \n","  inflating: /content/data/private_test/private_16134.wav  \n","  inflating: /content/data/private_test/private_09014.wav  \n","  inflating: /content/data/private_test/private_06327.wav  \n","  inflating: /content/data/private_test/private_10545.wav  \n","  inflating: /content/data/private_test/private_07039.wav  \n","  inflating: /content/data/private_test/private_00756.wav  \n","  inflating: /content/data/private_test/private_11883.wav  \n","  inflating: /content/data/private_test/private_18119.wav  \n","  inflating: /content/data/private_test/private_11868.wav  \n","  inflating: /content/data/private_test/private_00965.wav  \n","  inflating: /content/data/private_test/private_14910.wav  \n","  inflating: /content/data/private_test/private_02814.wav  \n","  inflating: /content/data/private_test/private_13919.wav  \n","  inflating: /content/data/private_test/private_09799.wav  \n","  inflating: /content/data/private_test/private_18694.wav  \n","  inflating: /content/data/private_test/private_09941.wav  \n","  inflating: /content/data/private_test/private_16861.wav  \n","  inflating: /content/data/private_test/private_08487.wav  \n","  inflating: /content/data/private_test/private_18858.wav  \n","  inflating: /content/data/private_test/private_07978.wav  \n","  inflating: /content/data/private_test/private_09955.wav  \n","  inflating: /content/data/private_test/private_18680.wav  \n","  inflating: /content/data/private_test/private_16875.wav  \n","  inflating: /content/data/private_test/private_08493.wav  \n","  inflating: /content/data/private_test/private_02800.wav  \n","  inflating: /content/data/private_test/private_14904.wav  \n","  inflating: /content/data/private_test/private_05809.wav  \n","  inflating: /content/data/private_test/private_00971.wav  \n","  inflating: /content/data/private_test/private_12385.wav  \n","  inflating: /content/data/private_test/private_02196.wav  \n","  inflating: /content/data/private_test/private_05821.wav  \n","  inflating: /content/data/private_test/private_03288.wav  \n","  inflating: /content/data/private_test/private_10592.wav  \n","  inflating: /content/data/private_test/private_11854.wav  \n","  inflating: /content/data/private_test/private_00781.wav  \n","  inflating: /content/data/private_test/private_00959.wav  \n","  inflating: /content/data/private_test/private_18870.wav  \n","  inflating: /content/data/private_test/private_06496.wav  \n","  inflating: /content/data/private_test/private_07950.wav  \n","  inflating: /content/data/private_test/private_16685.wav  \n","  inflating: /content/data/private_test/private_07788.wav  \n","  inflating: /content/data/private_test/private_04281.wav  \n","  inflating: /content/data/private_test/private_02828.wav  \n","  inflating: /content/data/private_test/private_14092.wav  \n","  inflating: /content/data/private_test/private_13925.wav  \n","  inflating: /content/data/private_test/private_04295.wav  \n","  inflating: /content/data/private_test/private_14086.wav  \n","  inflating: /content/data/private_test/private_15398.wav  \n","  inflating: /content/data/private_test/private_13931.wav  \n","  inflating: /content/data/private_test/private_18864.wav  \n","  inflating: /content/data/private_test/private_06482.wav  \n","  inflating: /content/data/private_test/private_16691.wav  \n","  inflating: /content/data/private_test/private_07944.wav  \n","  inflating: /content/data/private_test/private_09969.wav  \n","  inflating: /content/data/private_test/private_16849.wav  \n","  inflating: /content/data/private_test/private_10586.wav  \n","  inflating: /content/data/private_test/private_00795.wav  \n","  inflating: /content/data/private_test/private_11840.wav  \n","  inflating: /content/data/private_test/private_11698.wav  \n","  inflating: /content/data/private_test/private_14938.wav  \n","  inflating: /content/data/private_test/private_12391.wav  \n","  inflating: /content/data/private_test/private_02182.wav  \n","  inflating: /content/data/private_test/private_05835.wav  \n","  inflating: /content/data/private_test/private_16848.wav  \n","  inflating: /content/data/private_test/private_09968.wav  \n","  inflating: /content/data/private_test/private_07945.wav  \n","  inflating: /content/data/private_test/private_16690.wav  \n","  inflating: /content/data/private_test/private_06483.wav  \n","  inflating: /content/data/private_test/private_18865.wav  \n","  inflating: /content/data/private_test/private_13930.wav  \n","  inflating: /content/data/private_test/private_15399.wav  \n","  inflating: /content/data/private_test/private_14087.wav  \n","  inflating: /content/data/private_test/private_04294.wav  \n","  inflating: /content/data/private_test/private_05834.wav  \n","  inflating: /content/data/private_test/private_02183.wav  \n","  inflating: /content/data/private_test/private_12390.wav  \n","  inflating: /content/data/private_test/private_14939.wav  \n","  inflating: /content/data/private_test/private_11699.wav  \n","  inflating: /content/data/private_test/private_11841.wav  \n","  inflating: /content/data/private_test/private_00794.wav  \n","  inflating: /content/data/private_test/private_10587.wav  \n","  inflating: /content/data/private_test/private_00958.wav  \n","  inflating: /content/data/private_test/private_00780.wav  \n","  inflating: /content/data/private_test/private_11855.wav  \n","  inflating: /content/data/private_test/private_10593.wav  \n","  inflating: /content/data/private_test/private_03289.wav  \n","  inflating: /content/data/private_test/private_05820.wav  \n","  inflating: /content/data/private_test/private_02197.wav  \n","  inflating: /content/data/private_test/private_12384.wav  \n","  inflating: /content/data/private_test/private_13924.wav  \n","  inflating: /content/data/private_test/private_14093.wav  \n","  inflating: /content/data/private_test/private_02829.wav  \n","  inflating: /content/data/private_test/private_04280.wav  \n","  inflating: /content/data/private_test/private_07789.wav  \n","  inflating: /content/data/private_test/private_16684.wav  \n","  inflating: /content/data/private_test/private_07951.wav  \n","  inflating: /content/data/private_test/private_06497.wav  \n","  inflating: /content/data/private_test/private_18871.wav  \n","  inflating: /content/data/private_test/private_02801.wav  \n","  inflating: /content/data/private_test/private_08492.wav  \n","  inflating: /content/data/private_test/private_16874.wav  \n","  inflating: /content/data/private_test/private_18681.wav  \n","  inflating: /content/data/private_test/private_09954.wav  \n","  inflating: /content/data/private_test/private_07979.wav  \n","  inflating: /content/data/private_test/private_18859.wav  \n","  inflating: /content/data/private_test/private_00970.wav  \n","  inflating: /content/data/private_test/private_05808.wav  \n","  inflating: /content/data/private_test/private_14905.wav  \n","  inflating: /content/data/private_test/private_14911.wav  \n","  inflating: /content/data/private_test/private_00964.wav  \n","  inflating: /content/data/private_test/private_11869.wav  \n","  inflating: /content/data/private_test/private_08486.wav  \n","  inflating: /content/data/private_test/private_16860.wav  \n","  inflating: /content/data/private_test/private_09940.wav  \n","  inflating: /content/data/private_test/private_18695.wav  \n","  inflating: /content/data/private_test/private_09798.wav  \n","  inflating: /content/data/private_test/private_13918.wav  \n","  inflating: /content/data/private_test/private_02815.wav  \n","  inflating: /content/data/private_test/private_03538.wav  \n","  inflating: /content/data/private_test/private_14044.wav  \n","  inflating: /content/data/private_test/private_04257.wav  \n","  inflating: /content/data/private_test/private_12435.wav  \n","  inflating: /content/data/private_test/private_05149.wav  \n","  inflating: /content/data/private_test/private_02626.wav  \n","  inflating: /content/data/private_test/private_07986.wav  \n","  inflating: /content/data/private_test/private_16653.wav  \n","  inflating: /content/data/private_test/private_19560.wav  \n","  inflating: /content/data/private_test/private_06440.wav  \n","  inflating: /content/data/private_test/private_09773.wav  \n","  inflating: /content/data/private_test/private_10222.wav  \n","  inflating: /content/data/private_test/private_00031.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00031.wav  \n","  inflating: /content/data/private_test/private_11882.wav  \n","  inflating: /content/data/private_test/private_00757.wav  \n","  inflating: /content/data/private_test/private_18118.wav  \n","  inflating: /content/data/private_test/private_07038.wav  \n","  inflating: /content/data/private_test/private_10544.wav  \n","  inflating: /content/data/private_test/private_06326.wav  \n","  inflating: /content/data/private_test/private_09015.wav  \n","  inflating: /content/data/private_test/private_16135.wav  \n","  inflating: /content/data/private_test/private_19206.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19206.wav  \n","  inflating: /content/data/private_test/private_01449.wav  \n","  inflating: /content/data/private_test/private_02140.wav  \n","  inflating: /content/data/private_test/private_12353.wav  \n","  inflating: /content/data/private_test/private_04531.wav  \n","  inflating: /content/data/private_test/private_14722.wav  \n","  inflating: /content/data/private_test/private_02154.wav  \n","  inflating: /content/data/private_test/private_15428.wav  \n","  inflating: /content/data/private_test/private_12347.wav  \n","  inflating: /content/data/private_test/private_13059.wav  \n","  inflating: /content/data/private_test/private_04525.wav  \n","  inflating: /content/data/private_test/private_14736.wav  \n","  inflating: /content/data/private_test/private_00743.wav  \n","  inflating: /content/data/private_test/private_11896.wav  \n","  inflating: /content/data/private_test/private_10550.wav  \n","  inflating: /content/data/private_test/private_06332.wav  \n","  inflating: /content/data/private_test/private_09001.wav  \n","  inflating: /content/data/private_test/private_16121.wav  \n","  inflating: /content/data/private_test/private_19212.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19212.wav  \n","  inflating: /content/data/private_test/private_16647.wav  \n","  inflating: /content/data/private_test/private_07992.wav  \n","  inflating: /content/data/private_test/private_19574.wav  \n","  inflating: /content/data/private_test/private_06454.wav  \n","  inflating: /content/data/private_test/private_09767.wav  \n","  inflating: /content/data/private_test/private_11128.wav  \n","  inflating: /content/data/private_test/private_10236.wav  \n","  inflating: /content/data/private_test/private_08479.wav  \n","  inflating: /content/data/private_test/private_17559.wav  \n","  inflating: /content/data/private_test/private_00025.wav  \n","  inflating: /content/data/private_test/private_14050.wav  \n","  inflating: /content/data/private_test/private_04243.wav  \n","  inflating: /content/data/private_test/private_12421.wav  \n","  inflating: /content/data/private_test/private_02632.wav  \n","  inflating: /content/data/private_test/private_01313.wav  \n","  inflating: /content/data/private_test/private_11100.wav  \n","  inflating: /content/data/private_test/private_08451.wav  \n","  inflating: /content/data/private_test/private_07762.wav  \n","  inflating: /content/data/private_test/private_18642.wav  \n","  inflating: /content/data/private_test/private_09997.wav  \n","  inflating: /content/data/private_test/private_17571.wav  \n","  inflating: /content/data/private_test/private_14078.wav  \n","  inflating: /content/data/private_test/private_03504.wav  \n","  inflating: /content/data/private_test/private_13717.wav  \n","  inflating: /content/data/private_test/private_05175.wav  \n","  inflating: /content/data/private_test/private_12409.wav  \n","  inflating: /content/data/private_test/private_15366.wav  \n","  inflating: /content/data/private_test/private_15400.wav  \n","  inflating: /content/data/private_test/private_05613.wav  \n","  inflating: /content/data/private_test/private_13071.wav  \n","  inflating: /content/data/private_test/private_03262.wav  \n","  inflating: /content/data/private_test/private_18124.wav  \n","  inflating: /content/data/private_test/private_17217.wav  \n","  inflating: /content/data/private_test/private_08337.wav  \n","  inflating: /content/data/private_test/private_10578.wav  \n","  inflating: /content/data/private_test/private_07004.wav  \n","  inflating: /content/data/private_test/private_11666.wav  \n","  inflating: /content/data/private_test/private_09029.wav  \n","  inflating: /content/data/private_test/private_01475.wav  \n","  inflating: /content/data/private_test/private_16109.wav  \n","  inflating: /content/data/private_test/private_18130.wav  \n","  inflating: /content/data/private_test/private_17203.wav  \n","  inflating: /content/data/private_test/private_08323.wav  \n","  inflating: /content/data/private_test/private_07010.wav  \n","  inflating: /content/data/private_test/private_11672.wav  \n","  inflating: /content/data/private_test/private_01461.wav  \n","  inflating: /content/data/private_test/private_15414.wav  \n","  inflating: /content/data/private_test/private_02168.wav  \n","  inflating: /content/data/private_test/private_05607.wav  \n","  inflating: /content/data/private_test/private_04519.wav  \n","  inflating: /content/data/private_test/private_13065.wav  \n","  inflating: /content/data/private_test/private_03276.wav  \n","  inflating: /content/data/private_test/private_03510.wav  \n","  inflating: /content/data/private_test/private_13703.wav  \n","  inflating: /content/data/private_test/private_05161.wav  \n","  inflating: /content/data/private_test/private_15372.wav  \n","  inflating: /content/data/private_test/private_01307.wav  \n","  inflating: /content/data/private_test/private_19548.wav  \n","  inflating: /content/data/private_test/private_11114.wav  \n","  inflating: /content/data/private_test/private_06468.wav  \n","  inflating: /content/data/private_test/private_08445.wav  \n","  inflating: /content/data/private_test/private_07776.wav  \n","  inflating: /content/data/private_test/private_09983.wav  \n","  inflating: /content/data/private_test/private_18656.wav  \n","  inflating: /content/data/private_test/private_00019.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00019.wav  \n","  inflating: /content/data/private_test/private_17565.wav  \n","  inflating: /content/data/private_test/private_01850.wav  \n","  inflating: /content/data/private_test/private_10785.wav  \n","  inflating: /content/data/private_test/private_00596.wav  \n","  inflating: /content/data/private_test/private_01688.wav  \n","  inflating: /content/data/private_test/private_12192.wav  \n","  inflating: /content/data/private_test/private_04928.wav  \n","  inflating: /content/data/private_test/private_02381.wav  \n","  inflating: /content/data/private_test/private_15825.wav  \n","  inflating: /content/data/private_test/private_04096.wav  \n","  inflating: /content/data/private_test/private_14285.wav  \n","  inflating: /content/data/private_test/private_05388.wav  \n","  inflating: /content/data/private_test/private_03921.wav  \n","  inflating: /content/data/private_test/private_17954.wav  \n","  inflating: /content/data/private_test/private_06681.wav  \n","  inflating: /content/data/private_test/private_08874.wav  \n","  inflating: /content/data/private_test/private_16492.wav  \n","  inflating: /content/data/private_test/private_06859.wav  \n","  inflating: /content/data/private_test/private_19979.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19979.wav  \n","  inflating: /content/data/private_test/private_06695.wav  \n","  inflating: /content/data/private_test/private_17940.wav  \n","  inflating: /content/data/private_test/private_08860.wav  \n","  inflating: /content/data/private_test/private_16486.wav  \n","  inflating: /content/data/private_test/private_17798.wav  \n","  inflating: /content/data/private_test/private_04082.wav  \n","  inflating: /content/data/private_test/private_14291.wav  \n","  inflating: /content/data/private_test/private_12838.wav  \n","  inflating: /content/data/private_test/private_03935.wav  \n","  inflating: /content/data/private_test/private_12186.wav  \n","  inflating: /content/data/private_test/private_02395.wav  \n","  inflating: /content/data/private_test/private_15831.wav  \n","  inflating: /content/data/private_test/private_13298.wav  \n","  inflating: /content/data/private_test/private_10791.wav  \n","  inflating: /content/data/private_test/private_01844.wav  \n","  inflating: /content/data/private_test/private_00582.wav  \n","  inflating: /content/data/private_test/private_10949.wav  \n","  inflating: /content/data/private_test/private_04914.wav  \n","  inflating: /content/data/private_test/private_15819.wav  \n","  inflating: /content/data/private_test/private_10961.wav  \n","  inflating: /content/data/private_test/private_17968.wav  \n","  inflating: /content/data/private_test/private_08848.wav  \n","  inflating: /content/data/private_test/private_06865.wav  \n","  inflating: /content/data/private_test/private_18483.wav  \n","  inflating: /content/data/private_test/private_08690.wav  \n","  inflating: /content/data/private_test/private_19945.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19945.wav  \n","  inflating: /content/data/private_test/private_12810.wav  \n","  inflating: /content/data/private_test/private_12804.wav  \n","  inflating: /content/data/private_test/private_03909.wav  \n","  inflating: /content/data/private_test/private_19789.wav  \n","  inflating: /content/data/private_test/private_06871.wav  \n","  inflating: /content/data/private_test/private_18497.wav  \n","  inflating: /content/data/private_test/private_19951.wav  \n","  inflating: /content/data/private_test/private_08684.wav  \n","  inflating: /content/data/private_test/private_01878.wav  \n","  inflating: /content/data/private_test/private_10975.wav  \n","  inflating: /content/data/private_test/private_04900.wav  \n","  inflating: /content/data/private_test/private_14520.wav  \n","  inflating: /content/data/private_test/private_04733.wav  \n","  inflating: /content/data/private_test/private_12151.wav  \n","  inflating: /content/data/private_test/private_02342.wav  \n","  inflating: /content/data/private_test/private_19004.wav  \n","  inflating: /content/data/private_test/private_16337.wav  \n","  inflating: /content/data/private_test/private_09217.wav  \n","  inflating: /content/data/private_test/private_11458.wav  \n","  inflating: /content/data/private_test/private_06124.wav  \n","  inflating: /content/data/private_test/private_01893.wav  \n","  inflating: /content/data/private_test/private_10746.wav  \n","  inflating: /content/data/private_test/private_08109.wav  \n","  inflating: /content/data/private_test/private_00555.wav  \n","  inflating: /content/data/private_test/private_17029.wav  \n","  inflating: /content/data/private_test/private_00233.wav  \n","  inflating: /content/data/private_test/private_10020.wav  \n","  inflating: /content/data/private_test/private_09571.wav  \n","  inflating: /content/data/private_test/private_17997.wav  \n","  inflating: /content/data/private_test/private_06642.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06642.wav  \n","  inflating: /content/data/private_test/private_19762.wav  \n","  inflating: /content/data/private_test/private_16451.wav  \n","  inflating: /content/data/private_test/private_15158.wav  \n","  inflating: /content/data/private_test/private_02424.wav  \n","  inflating: /content/data/private_test/private_12637.wav  \n","  inflating: /content/data/private_test/private_04055.wav  \n","  inflating: /content/data/private_test/private_13529.wav  \n","  inflating: /content/data/private_test/private_14246.wav  \n","  inflating: /content/data/private_test/private_02430.wav  \n","  inflating: /content/data/private_test/private_12623.wav  \n","  inflating: /content/data/private_test/private_04041.wav  \n","  inflating: /content/data/private_test/private_14252.wav  \n","  inflating: /content/data/private_test/private_00227.wav  \n","  inflating: /content/data/private_test/private_18468.wav  \n","  inflating: /content/data/private_test/private_10034.wav  \n","  inflating: /content/data/private_test/private_07548.wav  \n","  inflating: /content/data/private_test/private_09565.wav  \n","  inflating: /content/data/private_test/private_06656.wav  \n","  inflating: /content/data/private_test/private_17983.wav  \n","  inflating: /content/data/private_test/private_19776.wav  \n","  inflating: /content/data/private_test/private_01139.wav  \n","  inflating: /content/data/private_test/private_16445.wav  \n","  inflating: /content/data/private_test/private_19010.wav  \n","  inflating: /content/data/private_test/private_16323.wav  \n","  inflating: /content/data/private_test/private_09203.wav  \n","  inflating: /content/data/private_test/private_06130.wav  \n","  inflating: /content/data/private_test/private_10752.wav  \n","  inflating: /content/data/private_test/private_01887.wav  \n","  inflating: /content/data/private_test/private_00541.wav  \n","  inflating: /content/data/private_test/private_14534.wav  \n","  inflating: /content/data/private_test/private_03048.wav  \n","  inflating: /content/data/private_test/private_04727.wav  \n","  inflating: /content/data/private_test/private_05439.wav  \n","  inflating: /content/data/private_test/private_12145.wav  \n","  inflating: /content/data/private_test/private_02356.wav  \n","  inflating: /content/data/private_test/private_01677.wav  \n","  inflating: /content/data/private_test/private_19038.wav  \n","  inflating: /content/data/private_test/private_06118.wav  \n","  inflating: /content/data/private_test/private_11464.wav  \n","  inflating: /content/data/private_test/private_07206.wav  \n","  inflating: /content/data/private_test/private_08135.wav  \n","  inflating: /content/data/private_test/private_17015.wav  \n","  inflating: /content/data/private_test/private_18326.wav  \n","  inflating: /content/data/private_test/private_00569.wav  \n","  inflating: /content/data/private_test/private_03060.wav  \n","  inflating: /content/data/private_test/private_13273.wav  \n","  inflating: /content/data/private_test/private_05411.wav  \n","  inflating: /content/data/private_test/private_15602.wav  \n","  inflating: /content/data/private_test/private_02418.wav  \n","  inflating: /content/data/private_test/private_15164.wav  \n","  inflating: /content/data/private_test/private_05377.wav  \n","  inflating: /content/data/private_test/private_13515.wav  \n","  inflating: /content/data/private_test/private_04069.wav  \n","  inflating: /content/data/private_test/private_03706.wav  \n","  inflating: /content/data/private_test/private_17773.wav  \n","  inflating: /content/data/private_test/private_18440.wav  \n","  inflating: /content/data/private_test/private_07560.wav  \n","  inflating: /content/data/private_test/private_08653.wav  \n","  inflating: /content/data/private_test/private_19986.wav  \n","  inflating: /content/data/private_test/private_11302.wav  \n","  inflating: /content/data/private_test/private_01111.wav  \n","  inflating: /content/data/private_test/private_17767.wav  \n","  inflating: /content/data/private_test/private_18454.wav  \n","  inflating: /content/data/private_test/private_07574.wav  \n","  inflating: /content/data/private_test/private_19992.wav  \n","  inflating: /content/data/private_test/private_08647.wav  \n","  inflating: /content/data/private_test/private_10008.wav  \n","  inflating: /content/data/private_test/private_11316.wav  \n","  inflating: /content/data/private_test/private_09559.wav  \n","  inflating: /content/data/private_test/private_16479.wav  \n","  inflating: /content/data/private_test/private_01105.wav  \n","  inflating: /content/data/private_test/private_15170.wav  \n","  inflating: /content/data/private_test/private_05363.wav  \n","  inflating: /content/data/private_test/private_13501.wav  \n","  inflating: /content/data/private_test/private_03712.wav  \n","  inflating: /content/data/private_test/private_03074.wav  \n","  inflating: /content/data/private_test/private_14508.wav  \n","  inflating: /content/data/private_test/private_13267.wav  \n","  inflating: /content/data/private_test/private_12179.wav  \n","  inflating: /content/data/private_test/private_05405.wav  \n","  inflating: /content/data/private_test/private_15616.wav  \n","  inflating: /content/data/private_test/private_01663.wav  \n","  inflating: /content/data/private_test/private_11470.wav  \n","  inflating: /content/data/private_test/private_07212.wav  \n","  inflating: /content/data/private_test/private_08121.wav  \n","  inflating: /content/data/private_test/private_17001.wav  \n","  inflating: /content/data/private_test/private_18332.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18332.wav  \n","  inflating: /content/data/private_test/private_06093.wav  \n","  inflating: /content/data/private_test/private_10829.wav  \n","  inflating: /content/data/private_test/private_16280.wav  \n","  inflating: /content/data/private_test/private_01924.wav  \n","  inflating: /content/data/private_test/private_15951.wav  \n","  inflating: /content/data/private_test/private_04684.wav  \n","  inflating: /content/data/private_test/private_14497.wav  \n","  inflating: /content/data/private_test/private_15789.wav  \n","  inflating: /content/data/private_test/private_03855.wav  \n","  inflating: /content/data/private_test/private_12780.wav  \n","  inflating: /content/data/private_test/private_02593.wav  \n","  inflating: /content/data/private_test/private_12958.wav  \n","  inflating: /content/data/private_test/private_10197.wav  \n","  inflating: /content/data/private_test/private_00384.wav  \n","  inflating: /content/data/private_test/private_08900.wav  \n","  inflating: /content/data/private_test/private_11289.wav  \n","  inflating: /content/data/private_test/private_17820.wav  \n","  inflating: /content/data/private_test/private_19819.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19819.wav  \n","  inflating: /content/data/private_test/private_10183.wav  \n","  inflating: /content/data/private_test/private_00390.wav  \n","  inflating: /content/data/private_test/private_06939.wav  \n","  inflating: /content/data/private_test/private_08914.wav  \n","  inflating: /content/data/private_test/private_17834.wav  \n","  inflating: /content/data/private_test/private_12794.wav  \n","  inflating: /content/data/private_test/private_03841.wav  \n","  inflating: /content/data/private_test/private_02587.wav  \n","  inflating: /content/data/private_test/private_03699.wav  \n","  inflating: /content/data/private_test/private_04690.wav  \n","  inflating: /content/data/private_test/private_15945.wav  \n","  inflating: /content/data/private_test/private_14483.wav  \n","  inflating: /content/data/private_test/private_04848.wav  \n","  inflating: /content/data/private_test/private_06087.wav  \n","  inflating: /content/data/private_test/private_16294.wav  \n","  inflating: /content/data/private_test/private_01930.wav  \n","  inflating: /content/data/private_test/private_07399.wav  \n","  inflating: /content/data/private_test/private_04860.wav  \n","  inflating: /content/data/private_test/private_10815.wav  \n","  inflating: /content/data/private_test/private_18291.wav  \n","  inflating: /content/data/private_test/private_01918.wav  \n","  inflating: /content/data/private_test/private_08082.wav  \n","  inflating: /content/data/private_test/private_19831.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19831.wav  \n","  inflating: /content/data/private_test/private_06911.wav  \n","  inflating: /content/data/private_test/private_03869.wav  \n","  inflating: /content/data/private_test/private_12964.wav  \n","  inflating: /content/data/private_test/private_12970.wav  \n","  inflating: /content/data/private_test/private_19825.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19825.wav  \n","  inflating: /content/data/private_test/private_06905.wav  \n","  inflating: /content/data/private_test/private_08928.wav  \n","  inflating: /content/data/private_test/private_17808.wav  \n","  inflating: /content/data/private_test/private_09388.wav  \n","  inflating: /content/data/private_test/private_10801.wav  \n","  inflating: /content/data/private_test/private_18285.wav  \n","  inflating: /content/data/private_test/private_08096.wav  \n","  inflating: /content/data/private_test/private_15979.wav  \n","  inflating: /content/data/private_test/private_04874.wav  \n","  inflating: /content/data/private_test/private_02236.wav  \n","  inflating: /content/data/private_test/private_05559.wav  \n","  inflating: /content/data/private_test/private_12025.wav  \n","  inflating: /content/data/private_test/private_15992.wav  \n","  inflating: /content/data/private_test/private_04647.wav  \n","  inflating: /content/data/private_test/private_14454.wav  \n","  inflating: /content/data/private_test/private_03128.wav  \n","  inflating: /content/data/private_test/private_00421.wav  \n","  inflating: /content/data/private_test/private_10632.wav  \n","  inflating: /content/data/private_test/private_09363.wav  \n","  inflating: /content/data/private_test/private_06050.wav  \n","  inflating: /content/data/private_test/private_19170.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19170.wav  \n","  inflating: /content/data/private_test/private_16243.wav  \n","  inflating: /content/data/private_test/private_19616.wav  \n","  inflating: /content/data/private_test/private_01059.wav  \n","  inflating: /content/data/private_test/private_16525.wav  \n","  inflating: /content/data/private_test/private_09405.wav  \n","  inflating: /content/data/private_test/private_06736.wav  \n","  inflating: /content/data/private_test/private_10154.wav  \n","  inflating: /content/data/private_test/private_07428.wav  \n","  inflating: /content/data/private_test/private_00347.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00347.wav  \n","  inflating: /content/data/private_test/private_18508.wav  \n","  inflating: /content/data/private_test/private_14332.wav  \n","  inflating: /content/data/private_test/private_04121.wav  \n","  inflating: /content/data/private_test/private_03896.wav  \n","  inflating: /content/data/private_test/private_12743.wav  \n","  inflating: /content/data/private_test/private_02550.wav  \n","  inflating: /content/data/private_test/private_14326.wav  \n","  inflating: /content/data/private_test/private_04135.wav  \n","  inflating: /content/data/private_test/private_13449.wav  \n","  inflating: /content/data/private_test/private_12757.wav  \n","  inflating: /content/data/private_test/private_03882.wav  \n","  inflating: /content/data/private_test/private_15038.wav  \n","  inflating: /content/data/private_test/private_02544.wav  \n","  inflating: /content/data/private_test/private_19602.wav  \n","  inflating: /content/data/private_test/private_16531.wav  \n","  inflating: /content/data/private_test/private_09411.wav  \n","  inflating: /content/data/private_test/private_06722.wav  \n","  inflating: /content/data/private_test/private_10140.wav  \n","  inflating: /content/data/private_test/private_00353.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00353.wav  \n","  inflating: /content/data/private_test/private_00435.wav  \n","  inflating: /content/data/private_test/private_17149.wav  \n","  inflating: /content/data/private_test/private_10626.wav  \n","  inflating: /content/data/private_test/private_08069.wav  \n","  inflating: /content/data/private_test/private_09377.wav  \n","  inflating: /content/data/private_test/private_11538.wav  \n","  inflating: /content/data/private_test/private_06044.wav  \n","  inflating: /content/data/private_test/private_19164.wav  \n","  inflating: /content/data/private_test/private_16257.wav  \n","  inflating: /content/data/private_test/private_02222.wav  \n","  inflating: /content/data/private_test/private_12031.wav  \n","  inflating: /content/data/private_test/private_04653.wav  \n","  inflating: /content/data/private_test/private_15986.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15986.wav  \n","  inflating: /content/data/private_test/private_14440.wav  \n","  inflating: /content/data/private_test/private_17161.wav  \n","  inflating: /content/data/private_test/private_18252.wav  \n","  inflating: /content/data/private_test/private_07372.wav  \n","  inflating: /content/data/private_test/private_08041.wav  \n","  inflating: /content/data/private_test/private_11510.wav  \n","  inflating: /content/data/private_test/private_01703.wav  \n","  inflating: /content/data/private_test/private_15776.wav  \n","  inflating: /content/data/private_test/private_12019.wav  \n","  inflating: /content/data/private_test/private_05565.wav  \n","  inflating: /content/data/private_test/private_13307.wav  \n","  inflating: /content/data/private_test/private_03114.wav  \n","  inflating: /content/data/private_test/private_14468.wav  \n","  inflating: /content/data/private_test/private_03672.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03672.wav  \n","  inflating: /content/data/private_test/private_13461.wav  \n","  inflating: /content/data/private_test/private_05203.wav  \n","  inflating: /content/data/private_test/private_15010.wav  \n","  inflating: /content/data/private_test/private_16519.wav  \n","  inflating: /content/data/private_test/private_01065.wav  \n","  inflating: /content/data/private_test/private_11276.wav  \n","  inflating: /content/data/private_test/private_09439.wav  \n","  inflating: /content/data/private_test/private_07414.wav  \n","  inflating: /content/data/private_test/private_08727.wav  \n","  inflating: /content/data/private_test/private_10168.wav  \n","  inflating: /content/data/private_test/private_17607.wav  \n","  inflating: /content/data/private_test/private_18534.wav  \n","  inflating: /content/data/private_test/private_01071.wav  \n","  inflating: /content/data/private_test/private_11262.wav  \n","  inflating: /content/data/private_test/private_07400.wav  \n","  inflating: /content/data/private_test/private_08733.wav  \n","  inflating: /content/data/private_test/private_17613.wav  \n","  inflating: /content/data/private_test/private_18520.wav  \n","  inflating: /content/data/private_test/private_03666.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03666.wav  \n","  inflating: /content/data/private_test/private_13475.wav  \n","  inflating: /content/data/private_test/private_04109.wav  \n","  inflating: /content/data/private_test/private_05217.wav  \n","  inflating: /content/data/private_test/private_02578.wav  \n","  inflating: /content/data/private_test/private_15004.wav  \n","  inflating: /content/data/private_test/private_15762.wav  \n","  inflating: /content/data/private_test/private_05571.wav  \n","  inflating: /content/data/private_test/private_13313.wav  \n","  inflating: /content/data/private_test/private_03100.wav  \n","  inflating: /content/data/private_test/private_17175.wav  \n","  inflating: /content/data/private_test/private_18246.wav  \n","  inflating: /content/data/private_test/private_00409.wav  \n","  inflating: /content/data/private_test/private_07366.wav  \n","  inflating: /content/data/private_test/private_08055.wav  \n","  inflating: /content/data/private_test/private_06078.wav  \n","  inflating: /content/data/private_test/private_11504.wav  \n","  inflating: /content/data/private_test/private_01717.wav  \n","  inflating: /content/data/private_test/private_19158.wav  \n","  inflating: /content/data/private_test/private_18911.wav  \n","  inflating: /content/data/private_test/private_07831.wav  \n","  inflating: /content/data/private_test/private_01298.wav  \n","  inflating: /content/data/private_test/private_00186.wav  \n","  inflating: /content/data/private_test/private_10395.wav  \n","  inflating: /content/data/private_test/private_02949.wav  \n","  inflating: /content/data/private_test/private_13844.wav  \n","  inflating: /content/data/private_test/private_02791.wav  \n","  inflating: /content/data/private_test/private_12582.wav  \n","  inflating: /content/data/private_test/private_05798.wav  \n","  inflating: /content/data/private_test/private_05940.wav  \n","  inflating: /content/data/private_test/private_14695.wav  \n","  inflating: /content/data/private_test/private_04486.wav  \n","  inflating: /content/data/private_test/private_11935.wav  \n","  inflating: /content/data/private_test/private_16082.wav  \n","  inflating: /content/data/private_test/private_06291.wav  \n","  inflating: /content/data/private_test/private_00838.wav  \n","  inflating: /content/data/private_test/private_17388.wav  \n","  inflating: /content/data/private_test/private_11921.wav  \n","  inflating: /content/data/private_test/private_16096.wav  \n","  inflating: /content/data/private_test/private_06285.wav  \n","  inflating: /content/data/private_test/private_14859.wav  \n","  inflating: /content/data/private_test/private_14681.wav  \n","  inflating: /content/data/private_test/private_05954.wav  \n","  inflating: /content/data/private_test/private_04492.wav  \n","  inflating: /content/data/private_test/private_13688.wav  \n","  inflating: /content/data/private_test/private_02785.wav  \n","  inflating: /content/data/private_test/private_13850.wav  \n","  inflating: /content/data/private_test/private_12596.wav  \n","  inflating: /content/data/private_test/private_18905.wav  \n","  inflating: /content/data/private_test/private_07825.wav  \n","  inflating: /content/data/private_test/private_09808.wav  \n","  inflating: /content/data/private_test/private_00192.wav  \n","  inflating: /content/data/private_test/private_16928.wav  \n","  inflating: /content/data/private_test/private_10381.wav  \n","  inflating: /content/data/private_test/private_02975.wav  \n","  inflating: /content/data/private_test/private_13878.wav  \n","  inflating: /content/data/private_test/private_09820.wav  \n","  inflating: /content/data/private_test/private_16900.wav  \n","  inflating: /content/data/private_test/private_08280.wav  \n","  inflating: /content/data/private_test/private_11909.wav  \n","  inflating: /content/data/private_test/private_18093.wav  \n","  inflating: /content/data/private_test/private_00804.wav  \n","  inflating: /content/data/private_test/private_14871.wav  \n","  inflating: /content/data/private_test/private_14865.wav  \n","  inflating: /content/data/private_test/private_05968.wav  \n","  inflating: /content/data/private_test/private_08294.wav  \n","  inflating: /content/data/private_test/private_18087.wav  \n","  inflating: /content/data/private_test/private_19399.wav  \n","  inflating: /content/data/private_test/private_00810.wav  \n","  inflating: /content/data/private_test/private_18939.wav  \n","  inflating: /content/data/private_test/private_07819.wav  \n","  inflating: /content/data/private_test/private_09834.wav  \n","  inflating: /content/data/private_test/private_16914.wav  \n","  inflating: /content/data/private_test/private_02961.wav  \n","  inflating: /content/data/private_test/private_13887.wav  \n","  inflating: /content/data/private_test/private_02752.wav  \n","  inflating: /content/data/private_test/private_12541.wav  \n","  inflating: /content/data/private_test/private_04323.wav  \n","  inflating: /content/data/private_test/private_14130.wav  \n","  inflating: /content/data/private_test/private_17439.wav  \n","  inflating: /content/data/private_test/private_00145.wav  \n","  inflating: /content/data/private_test/private_10356.wav  \n","  inflating: /content/data/private_test/private_08519.wav  \n","  inflating: /content/data/private_test/private_06534.wav  \n","  inflating: /content/data/private_test/private_09607.wav  \n","  inflating: /content/data/private_test/private_11048.wav  \n","  inflating: /content/data/private_test/private_16727.wav  \n","  inflating: /content/data/private_test/private_19414.wav  \n","  inflating: /content/data/private_test/private_16041.wav  \n","  inflating: /content/data/private_test/private_19372.wav  \n","  inflating: /content/data/private_test/private_06252.wav  \n","  inflating: /content/data/private_test/private_09161.wav  \n","  inflating: /content/data/private_test/private_10430.wav  \n","  inflating: /content/data/private_test/private_00623.wav  \n","  inflating: /content/data/private_test/private_05983.wav  \n","  inflating: /content/data/private_test/private_14656.wav  \n","  inflating: /content/data/private_test/private_13139.wav  \n","  inflating: /content/data/private_test/private_04445.wav  \n","  inflating: /content/data/private_test/private_12227.wav  \n","  inflating: /content/data/private_test/private_02034.wav  \n","  inflating: /content/data/private_test/private_15548.wav  \n","  inflating: /content/data/private_test/private_14642.wav  \n","  inflating: /content/data/private_test/private_05997.wav  \n","  inflating: /content/data/private_test/private_04451.wav  \n","  inflating: /content/data/private_test/private_12233.wav  \n","  inflating: /content/data/private_test/private_02020.wav  \n","  inflating: /content/data/private_test/private_16055.wav  \n","  inflating: /content/data/private_test/private_19366.wav  \n","  inflating: /content/data/private_test/private_01529.wav  \n","  inflating: /content/data/private_test/private_06246.wav  \n","  inflating: /content/data/private_test/private_09175.wav  \n","  inflating: /content/data/private_test/private_07158.wav  \n","  inflating: /content/data/private_test/private_10424.wav  \n","  inflating: /content/data/private_test/private_00637.wav  \n","  inflating: /content/data/private_test/private_18078.wav  \n","  inflating: /content/data/private_test/private_00151.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00151.wav  \n","  inflating: /content/data/private_test/private_10342.wav  \n","  inflating: /content/data/private_test/private_06520.wav  \n","  inflating: /content/data/private_test/private_09613.wav  \n","  inflating: /content/data/private_test/private_16733.wav  \n","  inflating: /content/data/private_test/private_19400.wav  \n","  inflating: /content/data/private_test/private_02746.wav  \n","  inflating: /content/data/private_test/private_13893.wav  \n","  inflating: /content/data/private_test/private_12555.wav  \n","  inflating: /content/data/private_test/private_05029.wav  \n","  inflating: /content/data/private_test/private_04337.wav  \n","  inflating: /content/data/private_test/private_03458.wav  \n","  inflating: /content/data/private_test/private_14124.wav  \n","  inflating: /content/data/private_test/private_18736.wav  \n","  inflating: /content/data/private_test/private_00179.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00179.wav  \n","  inflating: /content/data/private_test/private_17405.wav  \n","  inflating: /content/data/private_test/private_08525.wav  \n","  inflating: /content/data/private_test/private_07616.wav  \n","  inflating: /content/data/private_test/private_11074.wav  \n","  inflating: /content/data/private_test/private_06508.wav  \n","  inflating: /content/data/private_test/private_01267.wav  \n","  inflating: /content/data/private_test/private_19428.wav  \n","  inflating: /content/data/private_test/private_15212.wav  \n","  inflating: /content/data/private_test/private_05001.wav  \n","  inflating: /content/data/private_test/private_13663.wav  \n","  inflating: /content/data/private_test/private_03470.wav  \n","  inflating: /content/data/private_test/private_03316.wav  \n","  inflating: /content/data/private_test/private_04479.wav  \n","  inflating: /content/data/private_test/private_13105.wav  \n","  inflating: /content/data/private_test/private_05767.wav  \n","  inflating: /content/data/private_test/private_15574.wav  \n","  inflating: /content/data/private_test/private_02008.wav  \n","  inflating: /content/data/private_test/private_01501.wav  \n","  inflating: /content/data/private_test/private_11712.wav  \n","  inflating: /content/data/private_test/private_08243.wav  \n","  inflating: /content/data/private_test/private_07170.wav  \n","  inflating: /content/data/private_test/private_18050.wav  \n","  inflating: /content/data/private_test/private_17363.wav  \n","  inflating: /content/data/private_test/private_01515.wav  \n","  inflating: /content/data/private_test/private_16069.wav  \n","  inflating: /content/data/private_test/private_11706.wav  \n","  inflating: /content/data/private_test/private_09149.wav  \n","  inflating: /content/data/private_test/private_08257.wav  \n","  inflating: /content/data/private_test/private_10418.wav  \n","  inflating: /content/data/private_test/private_07164.wav  \n","  inflating: /content/data/private_test/private_18044.wav  \n","  inflating: /content/data/private_test/private_17377.wav  \n","  inflating: /content/data/private_test/private_03302.wav  \n","  inflating: /content/data/private_test/private_13111.wav  \n","  inflating: /content/data/private_test/private_05773.wav  \n","  inflating: /content/data/private_test/private_15560.wav  \n","  inflating: /content/data/private_test/private_15206.wav  \n","  inflating: /content/data/private_test/private_05015.wav  \n","  inflating: /content/data/private_test/private_12569.wav  \n","  inflating: /content/data/private_test/private_13677.wav  \n","  inflating: /content/data/private_test/private_14118.wav  \n","  inflating: /content/data/private_test/private_03464.wav  \n","  inflating: /content/data/private_test/private_18722.wav  \n","  inflating: /content/data/private_test/private_17411.wav  \n","  inflating: /content/data/private_test/private_08531.wav  \n","  inflating: /content/data/private_test/private_07602.wav  \n","  inflating: /content/data/private_test/private_11060.wav  \n","  inflating: /content/data/private_test/private_01273.wav  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQlG4lBgsPV2","executionInfo":{"status":"ok","timestamp":1623286824120,"user_tz":-480,"elapsed":20169,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"671836d4-c4c6-4fde-8f68-d7f0b4262d21"},"source":["# !wget https://zenodo.org/record/3987831/files/Cnn14_DecisionLevelAtt_mAP%3D0.425.pth?download=1 -O /content/Cnn14_DecisionLevelAtt_mAP0.425.pth\n","!wget https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1 -O /content/Cnn14_mAP_0.431.pth"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-06-10 01:00:03--  https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1\n","Resolving zenodo.org (zenodo.org)... 137.138.76.77\n","Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 327428481 (312M) [application/octet-stream]\n","Saving to: ‘/content/Cnn14_mAP_0.431.pth’\n","\n","/content/Cnn14_mAP_ 100%[===================>] 312.26M  18.9MB/s    in 18s     \n","\n","2021-06-10 01:00:23 (17.0 MB/s) - ‘/content/Cnn14_mAP_0.431.pth’ saved [327428481/327428481]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87sXCPsZGWNK","executionInfo":{"status":"ok","timestamp":1623286835361,"user_tz":-480,"elapsed":11253,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e0573b49-479a-452f-bf79-89f513984d61"},"source":["!pip install -U catalyst\n","!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n","!pip install audiomentations"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting catalyst\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/9b/cfcf2d617c11ab871e21abc10181c17ece5f2fd75436d33394e7b0f147e1/catalyst-21.5-py2.py3-none-any.whl (518kB)\n","\u001b[K     |████████████████████████████████| 522kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (4.41.1)\n","Collecting PyYAML>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 17.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.8.1+cu101)\n","Requirement already satisfied, skipping upgrade: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.19.5)\n","Collecting tensorboardX>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 33.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->catalyst) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=2.1.0->catalyst) (3.12.4)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX>=2.1.0->catalyst) (57.0.0)\n","Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX>=2.1.0->catalyst) (1.15.0)\n","Installing collected packages: PyYAML, tensorboardX, catalyst\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.4.1 catalyst-21.5 tensorboardX-2.2\n","Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n","  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-sh8dnkni\n","  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-sh8dnkni\n","Building wheels for collected packages: warmup-scheduler\n","  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-cp37-none-any.whl size=3880 sha256=6d58816c766201609f4e2bb1acab4134edeba0082dee4ec1b7c5d3774b6567c2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-k1f9uqu3/wheels/b7/24/83/d30234cc013cff538805b14df916e79091f7cf9ee2c5bf3a64\n","Successfully built warmup-scheduler\n","Installing collected packages: warmup-scheduler\n","Successfully installed warmup-scheduler-0.3.2\n","Collecting audiomentations\n","  Downloading https://files.pythonhosted.org/packages/fb/e1/3078fe444be2a100d804ee1296115367c27fa1dfa6298bf4155f77345822/audiomentations-0.16.0-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.19.5)\n","Requirement already satisfied: librosa<=0.8.0,>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.0)\n","Requirement already satisfied: scipy<1.6.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.2.2)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.51.2)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.22.2.post1)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (4.4.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (1.0.1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (2.1.9)\n","Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.10.3.post1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (1.3.0)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<=0.8.0,>=0.6.1->audiomentations) (1.15.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations) (57.0.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.14.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.23.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.4.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.24.3)\n","Installing collected packages: audiomentations\n","Successfully installed audiomentations-0.16.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:31.262304Z","iopub.status.busy":"2020-08-14T10:26:31.261641Z","iopub.status.idle":"2020-08-14T10:26:43.679533Z","shell.execute_reply":"2020-08-14T10:26:43.678430Z"},"papermill":{"duration":12.43994,"end_time":"2020-08-14T10:26:43.679682","exception":false,"start_time":"2020-08-14T10:26:31.239742","status":"completed"},"tags":[],"id":"IEEebe8MEIAX"},"source":["import cv2\n","import audioread\n","import logging\n","import os\n","import random\n","import time\n","import warnings\n","\n","from tqdm import tqdm\n","\n","import librosa\n","import librosa.display as display\n","import numpy as np\n","import pandas as pd\n","import soundfile as sf\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","\n","from contextlib import contextmanager\n","from IPython.display import Audio\n","from pathlib import Path\n","from typing import Optional, List\n","\n","from catalyst.dl import SupervisedRunner, CallbackOrder, Callback, CheckpointCallback  # ,State \n","from fastprogress import progress_bar\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score, average_precision_score, roc_auc_score\n","\n","from warmup_scheduler import GradualWarmupScheduler\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tTXrngte18w"},"source":["from catalyst import dl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-08-14T10:26:43.721167Z","iopub.status.busy":"2020-08-14T10:26:43.720157Z","iopub.status.idle":"2020-08-14T10:26:43.726429Z","shell.execute_reply":"2020-08-14T10:26:43.725812Z"},"papermill":{"duration":0.033698,"end_time":"2020-08-14T10:26:43.726552","exception":false,"start_time":"2020-08-14T10:26:43.692854","status":"completed"},"tags":[],"id":"Hiklv0e0EIAY"},"source":["def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","    \n","    \n","def get_logger(out_file=None):\n","    logger = logging.getLogger()\n","    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n","    logger.handlers = []\n","    logger.setLevel(logging.INFO)\n","\n","    handler = logging.StreamHandler()\n","    handler.setFormatter(formatter)\n","    handler.setLevel(logging.INFO)\n","    logger.addHandler(handler)\n","\n","    if out_file is not None:\n","        fh = logging.FileHandler(out_file)\n","        fh.setFormatter(formatter)\n","        fh.setLevel(logging.INFO)\n","        logger.addHandler(fh)\n","    logger.info(\"logger set up\")\n","    return logger\n","    \n","    \n","@contextmanager\n","def timer(name: str, logger: Optional[logging.Logger] = None):\n","    t0 = time.time()\n","    msg = f\"[{name}] start\"\n","    if logger is None:\n","        print(msg)\n","    else:\n","        logger.info(msg)\n","    yield\n","\n","    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n","    if logger is None:\n","        print(msg)\n","    else:\n","        logger.info(msg)\n","    \n","    \n","set_seed(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-08-14T10:26:43.757897Z","iopub.status.busy":"2020-08-14T10:26:43.757128Z","iopub.status.idle":"2020-08-14T10:26:43.759607Z","shell.execute_reply":"2020-08-14T10:26:43.760169Z"},"papermill":{"duration":0.021548,"end_time":"2020-08-14T10:26:43.760287","exception":false,"start_time":"2020-08-14T10:26:43.738739","status":"completed"},"tags":[],"id":"-p-NMx2QEIAZ"},"source":["INPUT_ROOT = \"/content/\" \n","RAW_DATA = INPUT_ROOT + \"data/\"\n","TRAIN_AUDIO_DIR = RAW_DATA + \"train/\"\n","# TRAIN_RESAMPLED_AUDIO_DIRS = [\n","#   INPUT_ROOT / \"birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5)\n","# ]\n","TEST_AUDIO_DIR = RAW_DATA + \"public_test/\"\n","# TEST2_AUDIO_DIR = RAW_DATA + \"private_test/\"\n","\n","!mv /content/data/private_test/*.wav /content/data/public_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:43.794375Z","iopub.status.busy":"2020-08-14T10:26:43.793466Z","iopub.status.idle":"2020-08-14T10:26:44.076854Z","shell.execute_reply":"2020-08-14T10:26:44.075869Z"},"papermill":{"duration":0.30389,"end_time":"2020-08-14T10:26:44.076977","exception":false,"start_time":"2020-08-14T10:26:43.773087","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"7axhB8DBEIAa","executionInfo":{"status":"ok","timestamp":1623286844819,"user_tz":-480,"elapsed":973,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"04aefd87-e574-4f94-a1c2-e0581514eb4d"},"source":["import pandas as pd\n","\n","train = pd.read_csv(RAW_DATA + \"meta_train.csv\")\n","train['Filename'] = train['Filename'] + '.wav'\n","print(train.head())\n","test = pd.read_csv(\"/gdrive/My Drive/Colab Notebooks/esun_tbrain/dog_sound/data/sample_submission.csv\")\n","test['Filename'] = test['Filename'] + '.wav'\n","print(test.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["          Filename  Label   Remark\n","0  train_00001.wav      0  Barking\n","1  train_00002.wav      0  Barking\n","2  train_00003.wav      0  Barking\n","3  train_00004.wav      0  Barking\n","4  train_00005.wav      0  Barking\n","           Filename  Barking  Howling  Crying  COSmoke  GlassBreaking  Other\n","0  public_00001.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","1  public_00002.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","2  public_00003.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","3  public_00004.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","4  public_00005.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012238,"end_time":"2020-08-14T10:26:44.102097","exception":false,"start_time":"2020-08-14T10:26:44.089859","status":"completed"},"tags":[],"id":"KLsj27zUEIAa"},"source":["### torchlibrosa\n","\n","\n","In PANNs, `torchlibrosa`, a PyTorch based implementation are used to replace some of the `librosa`'s functions. Here I use some functions of `torchlibrosa`.\n","\n","Ref: https://github.com/qiuqiangkong/torchlibrosa"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:44.186427Z","iopub.status.busy":"2020-08-14T10:26:44.160452Z","iopub.status.idle":"2020-08-14T10:26:44.202387Z","shell.execute_reply":"2020-08-14T10:26:44.201812Z"},"papermill":{"duration":0.062799,"end_time":"2020-08-14T10:26:44.202497","exception":false,"start_time":"2020-08-14T10:26:44.139698","status":"completed"},"tags":[],"id":"_69sQ63eEIAb"},"source":["class DFTBase(nn.Module):\n","    def __init__(self):\n","        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n","        super(DFTBase, self).__init__()\n","\n","    def dft_matrix(self, n):\n","        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n","        omega = np.exp(-2 * np.pi * 1j / n)\n","        W = np.power(omega, x * y)\n","        return W\n","\n","    def idft_matrix(self, n):\n","        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n","        omega = np.exp(2 * np.pi * 1j / n)\n","        W = np.power(omega, x * y)\n","        return W\n","    \n","    \n","class STFT(DFTBase):\n","    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n","        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n","        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n","        of librosa.core.stft\n","        \"\"\"\n","        super(STFT, self).__init__()\n","\n","        assert pad_mode in ['constant', 'reflect']\n","\n","        self.n_fft = n_fft\n","        self.center = center\n","        self.pad_mode = pad_mode\n","\n","        # By default, use the entire frame\n","        if win_length is None:\n","            win_length = n_fft\n","\n","        # Set the default hop, if it's not already specified\n","        if hop_length is None:\n","            hop_length = int(win_length // 4)\n","\n","        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n","\n","        # Pad the window out to n_fft size\n","        fft_window = librosa.util.pad_center(fft_window, n_fft)\n","\n","        # DFT & IDFT matrix\n","        self.W = self.dft_matrix(n_fft)\n","\n","        out_channels = n_fft // 2 + 1\n","\n","        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n","            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n","            groups=1, bias=False)\n","\n","        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n","            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n","            groups=1, bias=False)\n","\n","        self.conv_real.weight.data = torch.Tensor(\n","            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n","        # (n_fft // 2 + 1, 1, n_fft)\n","\n","        self.conv_imag.weight.data = torch.Tensor(\n","            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n","        # (n_fft // 2 + 1, 1, n_fft)\n","\n","        if freeze_parameters:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, data_length)\n","        Returns:\n","          real: (batch_size, n_fft // 2 + 1, time_steps)\n","          imag: (batch_size, n_fft // 2 + 1, time_steps)\n","        \"\"\"\n","\n","        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n","\n","        if self.center:\n","            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n","\n","        real = self.conv_real(x)\n","        imag = self.conv_imag(x)\n","        # (batch_size, n_fft // 2 + 1, time_steps)\n","\n","        real = real[:, None, :, :].transpose(2, 3)\n","        imag = imag[:, None, :, :].transpose(2, 3)\n","        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n","\n","        return real, imag\n","    \n","    \n","class Spectrogram(nn.Module):\n","    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n","        window='hann', center=True, pad_mode='reflect', power=2.0, \n","        freeze_parameters=True):\n","        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n","        Conv1d. The function has the same output of librosa.core.stft\n","        \"\"\"\n","        super(Spectrogram, self).__init__()\n","\n","        self.power = power\n","\n","        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n","            win_length=win_length, window=window, center=center, \n","            pad_mode=pad_mode, freeze_parameters=True)\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n","        Returns:\n","          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n","        \"\"\"\n","\n","        (real, imag) = self.stft.forward(input)\n","        # (batch_size, n_fft // 2 + 1, time_steps)\n","\n","        spectrogram = real ** 2 + imag ** 2\n","\n","        if self.power == 2.0:\n","            pass\n","        else:\n","            spectrogram = spectrogram ** (power / 2.0)\n","\n","        return spectrogram\n","\n","    \n","class LogmelFilterBank(nn.Module):\n","    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n","        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n","        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n","        the pytorch implementation of as librosa.filters.mel \n","        \"\"\"\n","        super(LogmelFilterBank, self).__init__()\n","\n","        self.is_log = is_log\n","        self.ref = ref\n","        self.amin = amin\n","        self.top_db = top_db\n","\n","        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n","            fmin=fmin, fmax=fmax).T\n","        # (n_fft // 2 + 1, mel_bins)\n","\n","        self.melW = nn.Parameter(torch.Tensor(self.melW))\n","\n","        if freeze_parameters:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, channels, time_steps)\n","        \n","        Output: (batch_size, time_steps, mel_bins)\n","        \"\"\"\n","\n","        # Mel spectrogram\n","        mel_spectrogram = torch.matmul(input, self.melW)\n","\n","        # Logmel spectrogram\n","        if self.is_log:\n","            output = self.power_to_db(mel_spectrogram)\n","        else:\n","            output = mel_spectrogram\n","\n","        return output\n","\n","\n","    def power_to_db(self, input):\n","        \"\"\"Power to db, this function is the pytorch implementation of \n","        librosa.core.power_to_lb\n","        \"\"\"\n","        ref_value = self.ref\n","        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n","        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n","\n","        if self.top_db is not None:\n","            if self.top_db < 0:\n","                raise ParameterError('top_db must be non-negative')\n","            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n","\n","        return log_spec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:44.246248Z","iopub.status.busy":"2020-08-14T10:26:44.240679Z","iopub.status.idle":"2020-08-14T10:26:44.249107Z","shell.execute_reply":"2020-08-14T10:26:44.248561Z"},"papermill":{"duration":0.03404,"end_time":"2020-08-14T10:26:44.249211","exception":false,"start_time":"2020-08-14T10:26:44.215171","status":"completed"},"tags":[],"id":"OCjDEDLVEIAd"},"source":["class DropStripes(nn.Module):\n","    def __init__(self, dim, drop_width, stripes_num):\n","        \"\"\"Drop stripes. \n","        Args:\n","          dim: int, dimension along which to drop\n","          drop_width: int, maximum width of stripes to drop\n","          stripes_num: int, how many stripes to drop\n","        \"\"\"\n","        super(DropStripes, self).__init__()\n","\n","        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n","\n","        self.dim = dim\n","        self.drop_width = drop_width\n","        self.stripes_num = stripes_num\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n","\n","        assert input.ndimension() == 4\n","\n","        if self.training is False:\n","            return input\n","\n","        else:\n","            batch_size = input.shape[0]\n","            total_width = input.shape[self.dim]\n","\n","            for n in range(batch_size):\n","                self.transform_slice(input[n], total_width)\n","\n","            return input\n","\n","\n","    def transform_slice(self, e, total_width):\n","        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n","\n","        for _ in range(self.stripes_num):\n","            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n","            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n","\n","            if self.dim == 2:\n","                e[:, bgn : bgn + distance, :] = 0\n","            elif self.dim == 3:\n","                e[:, :, bgn : bgn + distance] = 0\n","\n","\n","class SpecAugmentation(nn.Module):\n","    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n","        freq_stripes_num):\n","        \"\"\"Spec augmetation. \n","        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n","        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n","        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n","        Args:\n","          time_drop_width: int\n","          time_stripes_num: int\n","          freq_drop_width: int\n","          freq_stripes_num: int\n","        \"\"\"\n","\n","        super(SpecAugmentation, self).__init__()\n","\n","        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n","            stripes_num=time_stripes_num)\n","\n","        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n","            stripes_num=freq_stripes_num)\n","\n","    def forward(self, input):\n","        x = self.time_dropper(input)\n","        x = self.freq_dropper(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012766,"end_time":"2020-08-14T10:26:44.276141","exception":false,"start_time":"2020-08-14T10:26:44.263375","status":"completed"},"tags":[],"id":"6i-L7_hMEIAe"},"source":["### audioset_tagging_cnn\n","\n","I also use `Cnn14_DecisionLevelAtt` model from [PANNs models](https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/pytorch/models.py), which is a SED model."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012344,"end_time":"2020-08-14T10:26:44.325825","exception":false,"start_time":"2020-08-14T10:26:44.313481","status":"completed"},"tags":[],"id":"QTxn9Q0WEIAf"},"source":["### Building blocks"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:44.384942Z","iopub.status.busy":"2020-08-14T10:26:44.363459Z","iopub.status.idle":"2020-08-14T10:26:44.388830Z","shell.execute_reply":"2020-08-14T10:26:44.388352Z"},"papermill":{"duration":0.051099,"end_time":"2020-08-14T10:26:44.388925","exception":false,"start_time":"2020-08-14T10:26:44.337826","status":"completed"},"tags":[],"id":"AkrKwIXWEIAf"},"source":["def init_layer(layer):\n","    nn.init.xavier_uniform_(layer.weight)\n","\n","    if hasattr(layer, \"bias\"):\n","        if layer.bias is not None:\n","            layer.bias.data.fill_(0.)\n","\n","\n","def init_bn(bn):\n","    bn.bias.data.fill_(0.)\n","    bn.weight.data.fill_(1.0)\n","\n","\n","def interpolate(x: torch.Tensor, ratio: int):\n","    \"\"\"Interpolate data in time domain. This is used to compensate the\n","    resolution reduction in downsampling of a CNN.\n","\n","    Args:\n","      x: (batch_size, time_steps, classes_num)\n","      ratio: int, ratio to interpolate\n","    Returns:\n","      upsampled: (batch_size, time_steps * ratio, classes_num)\n","    \"\"\"\n","    (batch_size, time_steps, classes_num) = x.shape\n","    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n","    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n","    return upsampled\n","\n","\n","def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n","    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n","    is the same as the value of the last frame.\n","    Args:\n","      framewise_output: (batch_size, frames_num, classes_num)\n","      frames_num: int, number of frames to pad\n","    Outputs:\n","      output: (batch_size, frames_num, classes_num)\n","    \"\"\"\n","    pad = framewise_output[:, -1:, :].repeat(\n","        1, frames_num - framewise_output.shape[1], 1)\n","    \"\"\"tensor for padding\"\"\"\n","\n","    output = torch.cat((framewise_output, pad), dim=1)\n","    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n","\n","    return output\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=(3, 3),\n","            stride=(1, 1),\n","            padding=(1, 1),\n","            bias=False)\n","\n","        self.conv2 = nn.Conv2d(\n","            in_channels=out_channels,\n","            out_channels=out_channels,\n","            kernel_size=(3, 3),\n","            stride=(1, 1),\n","            padding=(1, 1),\n","            bias=False)\n","\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        init_layer(self.conv1)\n","        init_layer(self.conv2)\n","        init_bn(self.bn1)\n","        init_bn(self.bn2)\n","\n","    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n","\n","        x = input\n","        x = F.relu_(self.bn1(self.conv1(x)))\n","        x = F.relu_(self.bn2(self.conv2(x)))\n","        if pool_type == 'max':\n","            x = F.max_pool2d(x, kernel_size=pool_size)\n","        elif pool_type == 'avg':\n","            x = F.avg_pool2d(x, kernel_size=pool_size)\n","        elif pool_type == 'avg+max':\n","            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n","            x2 = F.max_pool2d(x, kernel_size=pool_size)\n","            x = x1 + x2\n","        else:\n","            raise Exception('Incorrect argument!')\n","\n","        return x\n","\n","\n","class AttBlock(nn.Module):\n","    def __init__(self,\n","                 in_features: int,\n","                 out_features: int,\n","                 activation=\"linear\",\n","                 temperature=1.0):\n","        super().__init__()\n","\n","        self.activation = activation\n","        self.temperature = temperature\n","        self.att = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","        self.cla = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","\n","        self.bn_att = nn.BatchNorm1d(out_features)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_layer(self.att)\n","        init_layer(self.cla)\n","        init_bn(self.bn_att)\n","\n","    def forward(self, x):\n","        # x: (n_samples, n_in, n_time)\n","        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n","        cla = self.nonlinear_transform(self.cla(x))\n","        x = torch.sum(norm_att * cla, dim=2)\n","        return x, norm_att, cla\n","\n","    def nonlinear_transform(self, x):\n","        if self.activation == 'linear':\n","            return x\n","        elif self.activation == 'sigmoid':\n","            return torch.sigmoid(x)\n","        elif self.activation == 'softmax':\n","            return nn.LogSoftmax(dim=-1)(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EN6gM0upq2_a"},"source":["def do_mixup(x, mixup_lambda):\n","    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n","    (1, 3, 5, ...).\n","    Args:\n","      x: (batch_size * 2, ...)\n","      mixup_lambda: (batch_size * 2,)\n","    Returns:\n","      out: (batch_size, ...)\n","    \"\"\"\n","    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n","        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n","    return out\n","\n","\n","class Mixup(object):\n","    def __init__(self, mixup_alpha, random_seed=1234):\n","        \"\"\"Mixup coefficient generator.\n","        \"\"\"\n","        self.mixup_alpha = mixup_alpha\n","        self.random_state = np.random.RandomState(random_seed)\n","\n","    def get_lambda(self, batch_size):\n","        \"\"\"Get mixup random coefficients.\n","        Args:\n","          batch_size: int\n","        Returns:\n","          mixup_lambdas: (batch_size,)\n","        \"\"\"\n","        mixup_lambdas = []\n","        for n in range(0, batch_size, 2):\n","            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n","            mixup_lambdas.append(lam)\n","            mixup_lambdas.append(1. - lam)\n","\n","        return np.array(mixup_lambdas)\n","\n","class Cnn14(nn.Module):\n","    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n","        fmax, classes_num):\n","        \n","        super(Cnn14, self).__init__()\n","\n","        window = 'hann'\n","        center = True\n","        pad_mode = 'reflect'\n","        ref = 1.0\n","        amin = 1e-10\n","        top_db = None\n","\n","        # Spectrogram extractor\n","        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n","            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n","            freeze_parameters=True)\n","\n","        # Logmel feature extractor\n","        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n","            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n","            freeze_parameters=True)\n","\n","        # Spec augmenter\n","        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n","            freq_drop_width=8, freq_stripes_num=2)\n","\n","        self.bn0 = nn.BatchNorm2d(64)\n","\n","        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n","        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n","        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n","        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n","        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n","        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n","\n","        self.fc1 = nn.Linear(2048, 2048, bias=True)\n","        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n","        \n","        self.init_weight()\n","\n","    def init_weight(self):\n","        init_bn(self.bn0)\n","        init_layer(self.fc1)\n","        init_layer(self.fc_audioset)\n"," \n","    def forward(self, input, mixup_lambda=None):\n","        \"\"\"\n","        Input: (batch_size, data_length)\"\"\"\n","\n","        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n","        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n","\n","        x = x.transpose(1, 3)\n","        x = self.bn0(x)\n","        x = x.transpose(1, 3)\n","        \n","        if self.training:\n","            x = self.spec_augmenter(x)\n","\n","        # Mixup on spectrogram\n","        if self.training and mixup_lambda is not None:\n","            x = do_mixup(x, mixup_lambda)\n","\n","        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = torch.mean(x, dim=3)\n","        \n","        (x1, _) = torch.max(x, dim=2)\n","        x2 = torch.mean(x, dim=2)\n","        x = x1 + x2\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = F.relu_(self.fc1(x))\n","        embedding = F.dropout(x, p=0.5, training=self.training)\n","        # clipwise_output = torch.sigmoid(self.fc_audioset(x))\n","        clipwise_output = self.fc_audioset(x)\n","        \n","        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n","\n","        return output_dict\n","\n","class Transfer_Cnn14(nn.Module):\n","    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n","        fmax, classes_num, freeze_base):\n","        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n","        \"\"\"\n","        super(Transfer_Cnn14, self).__init__()\n","        audioset_classes_num = 527\n","        \n","        self.base = Cnn14(sample_rate, window_size, hop_size, mel_bins, fmin, \n","            fmax, audioset_classes_num)\n","\n","        # Transfer to another task layer\n","        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n","\n","        if freeze_base:\n","            # Freeze AudioSet pretrained layers\n","            for param in self.base.parameters():\n","                param.requires_grad = False\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_layer(self.fc_transfer)\n","\n","    def load_from_pretrain(self, pretrained_checkpoint_path):\n","        checkpoint = torch.load(pretrained_checkpoint_path)\n","        self.base.load_state_dict(checkpoint['model'])\n","\n","    def forward(self, input, mixup_lambda=None):\n","        \"\"\"Input: (batch_size, data_length)\n","        \"\"\"\n","        output_dict = self.base(input, mixup_lambda)\n","        embedding = output_dict['embedding']\n","\n","        clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n","        output_dict['clipwise_output'] = clipwise_output\n"," \n","        return output_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012155,"end_time":"2020-08-14T10:26:44.478670","exception":false,"start_time":"2020-08-14T10:26:44.466515","status":"completed"},"tags":[],"id":"tUGg3XUpEIAg"},"source":["What is good in PANNs models is that they accept raw audio clip as input. Let's put a chunk into the CNN feature extractor of the model above."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:44.509078Z","iopub.status.busy":"2020-08-14T10:26:44.508213Z","iopub.status.idle":"2020-08-14T10:26:44.612574Z","shell.execute_reply":"2020-08-14T10:26:44.613080Z"},"papermill":{"duration":0.122259,"end_time":"2020-08-14T10:26:44.613225","exception":false,"start_time":"2020-08-14T10:26:44.490966","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"Pxmp98pcEIAg","executionInfo":{"status":"ok","timestamp":1623286845881,"user_tz":-480,"elapsed":17,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"9e46ea14-df61-4f7d-a37e-5355302df0b4"},"source":["path_train_data = '/content/data/train'\n","path_test_data = '/content/data/public_test'\n","path_meta = '/content/data/meta_train.csv'\n","path_submission = 'data/sample_submission.csv'\n","CLASSES = 6\n","SR = 8000  # 8000\n","sample_duration = 40000\n","hop_length = 256 # making it 128 in size\n","fmin = 0\n","fmax = 4000  # fmax <= sampling_rate\n","n_mels = 64  # 128\n","n_fft = 1024 # n_mels * 20\n","padmode = 'reflect'\n","res_type = 'kaiser_best'  # 'kaiser_fast', 'kaiser_best\n","\n","# (n+1)*n_fft-hop=40000\n","\n","train_ratio = 0.9\n","val_ratio = 1 - train_ratio\n","\n","warmup_epo = 20\n","cosine_epo = 180\n","n_epochs = warmup_epo + cosine_epo\n","warmup_factor = 10\n","batch_size = 128\n","LR = 5e-4\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(device)\n","\n","kernel_type = 'starter'  # experiment_name\n","use_fold = 0  # no use"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:44.961633Z","iopub.status.busy":"2020-08-14T10:26:44.960605Z","iopub.status.idle":"2020-08-14T10:26:46.429944Z","shell.execute_reply":"2020-08-14T10:26:46.431123Z"},"papermill":{"duration":1.508482,"end_time":"2020-08-14T10:26:46.431338","exception":false,"start_time":"2020-08-14T10:26:44.922856","status":"completed"},"tags":[],"id":"CAFhRJpDEIAj"},"source":["model_config = {\n","    \"sample_rate\": SR,\n","    \"window_size\": n_fft,\n","    \"hop_size\": hop_length,\n","    \"mel_bins\": n_mels,\n","    \"fmin\": fmin,\n","    \"fmax\": fmax,\n","    \"classes_num\": CLASSES,\n","    \"freeze_base\": False\n","}\n","\n","# model = PANNsCNN14Att(**model_config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031231,"end_time":"2020-08-14T10:26:48.347603","exception":false,"start_time":"2020-08-14T10:26:48.316372","status":"completed"},"tags":[],"id":"oYSXOnCNEIAl"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:48.540845Z","iopub.status.busy":"2020-08-14T10:26:48.540238Z","iopub.status.idle":"2020-08-14T10:26:48.544594Z","shell.execute_reply":"2020-08-14T10:26:48.544102Z"},"papermill":{"duration":0.050901,"end_time":"2020-08-14T10:26:48.544697","exception":false,"start_time":"2020-08-14T10:26:48.493796","status":"completed"},"tags":[],"id":"qByzHHHlEIAl"},"source":["PERIOD = 5\n","\n","class PANNsDataset(data.Dataset):\n","    def __init__(\n","            self,\n","            file_list: List[List[str]],\n","            waveform_transforms=None,\n","            training=True):\n","        self.file_list = file_list  # list of list: [file_path, code]\n","        self.waveform_transforms = waveform_transforms\n","        self.training = training\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx: int):\n","        if self.training:\n","            wav_path, code = self.file_list[idx]\n","\n","            y, sr = sf.read(TRAIN_AUDIO_DIR + wav_path)\n","\n","            if self.waveform_transforms:\n","                # y = self.waveform_transforms(y)\n","                y = self.waveform_transforms(samples=y, sample_rate=8000)\n","            \n","            # else:\n","\n","            len_y = len(y)\n","            effective_length = sr * PERIOD\n","            if len_y < effective_length:\n","                new_y = np.zeros(effective_length, dtype=y.dtype)\n","                start = np.random.randint(effective_length - len_y)\n","                new_y[start:start + len_y] = y\n","                y = new_y.astype(np.float32)\n","            elif len_y > effective_length:\n","                start = np.random.randint(len_y - effective_length)\n","                y = y[start:start + effective_length].astype(np.float32)\n","            else:\n","                y = y.astype(np.float32)\n","\n","            labels = np.zeros(CLASSES, dtype=\"f\")\n","            labels[code] = 1\n","\n","            return {\"waveform\": y, \"targets\": labels}\n","        else:\n","            wav_path = self.file_list[idx]\n","\n","            y, sr = sf.read(TEST_AUDIO_DIR + wav_path)\n","\n","            len_y = len(y)\n","            effective_length = sr * PERIOD\n","            if len_y < effective_length:\n","                new_y = np.zeros(effective_length, dtype=y.dtype)\n","                start = np.random.randint(effective_length - len_y)\n","                new_y[start:start + len_y] = y\n","                y = new_y.astype(np.float32)\n","            elif len_y > effective_length:\n","                start = np.random.randint(len_y - effective_length)\n","                y = y[start:start + effective_length].astype(np.float32)\n","            else:\n","                y = y.astype(np.float32)\n","\n","            return {\"waveform\": y}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031604,"end_time":"2020-08-14T10:26:48.609308","exception":false,"start_time":"2020-08-14T10:26:48.577704","status":"completed"},"tags":[],"id":"LFJXP9EDEIAm"},"source":["### Criterion"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:48.692139Z","iopub.status.busy":"2020-08-14T10:26:48.691461Z","iopub.status.idle":"2020-08-14T10:26:48.695848Z","shell.execute_reply":"2020-08-14T10:26:48.695373Z"},"papermill":{"duration":0.054385,"end_time":"2020-08-14T10:26:48.695948","exception":false,"start_time":"2020-08-14T10:26:48.641563","status":"completed"},"tags":[],"id":"GeL5KP6fEIAm"},"source":["class PANNsLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # self.bce = nn.BCELoss()\n","        # self.bce = nn.BCEWithLogitsLoss()\n","        self.bce = nn.NLLLoss()\n","\n","    def forward(self, input, target):\n","        input_ = input[\"clipwise_output\"]\n","        input_ = torch.where(torch.isnan(input_),\n","                             torch.zeros_like(input_),\n","                             input_)\n","        input_ = torch.where(torch.isinf(input_),\n","                             torch.zeros_like(input_),\n","                             input_)\n","\n","        # target = target.float()\n","\n","        return self.bce(input_, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCiEcsOCcIs2"},"source":["class CutMixCrossEntropyLoss(nn.Module):\n","    def __init__(self, size_average=True):\n","        super().__init__()\n","        self.size_average = size_average\n","\n","    def forward(self, input, target):\n","        if len(target.size()) == 1:\n","            target = F.one_hot(target, num_classes=input.size(-1))\n","            target = target.float().cuda()\n","        return cross_entropy(input, target, self.size_average)\n","\n","\n","def cross_entropy(input, target, size_average=True):\n","    \"\"\" Cross entropy that accepts soft targets\n","    Args:\n","         pred: predictions for neural network\n","         targets: targets, can be soft\n","         size_average: if false, sum is returned instead of mean\n","    Examples::\n","        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n","        input = torch.autograd.Variable(out, requires_grad=True)\n","        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n","        target = torch.autograd.Variable(y1)\n","        loss = cross_entropy(input, target)\n","        loss.backward()\n","    \"\"\"\n","    # logsoftmax = torch.nn.LogSoftmax(dim=1)\n","    # if size_average:\n","    #     return torch.mean(torch.sum(-target * logsoftmax(input), dim=1))\n","    # else:\n","    #     return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))\n","    \n","    if size_average:\n","        return torch.mean(torch.sum(-target * (input), dim=1))\n","    else:\n","        return torch.sum(torch.sum(-target * (input), dim=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.032181,"end_time":"2020-08-14T10:26:48.760978","exception":false,"start_time":"2020-08-14T10:26:48.728797","status":"completed"},"tags":[],"id":"qrUskYXWEIAm"},"source":["### Callbacks"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:48.854976Z","iopub.status.busy":"2020-08-14T10:26:48.849889Z","iopub.status.idle":"2020-08-14T10:26:48.858082Z","shell.execute_reply":"2020-08-14T10:26:48.857544Z"},"papermill":{"duration":0.065321,"end_time":"2020-08-14T10:26:48.858189","exception":false,"start_time":"2020-08-14T10:26:48.792868","status":"completed"},"tags":[],"id":"1TB3whHKEIAm"},"source":["# class F1Callback(Callback):\n","#     def __init__(self,\n","#                  input_key: str = \"targets\",\n","#                  output_key: str = \"logits\",\n","#                  model_output_key: str = \"clipwise_output\",\n","#                  prefix: str = \"f1\"):\n","#         super().__init__(CallbackOrder.Metric)\n","\n","#         self.input_key = input_key\n","#         self.output_key = output_key\n","#         self.model_output_key = model_output_key\n","#         self.prefix = prefix\n","\n","#     def on_loader_start(self, state: State):\n","#         self.prediction: List[np.ndarray] = []\n","#         self.target: List[np.ndarray] = []\n","\n","#     def on_batch_end(self, state: State):\n","#         targ = state.input[self.input_key].detach().cpu().numpy()\n","#         out = state.output[self.output_key]\n","\n","#         clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n","\n","#         self.prediction.append(clipwise_output)\n","#         self.target.append(targ)\n","\n","#         y_pred = clipwise_output.argmax(axis=1)\n","#         y_true = targ.argmax(axis=1)\n","\n","#         score = f1_score(y_true, y_pred, average=\"macro\")\n","#         state.batch_metrics[self.prefix] = score\n","\n","#     def on_loader_end(self, state: State):\n","#         y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n","#         y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n","#         score = f1_score(y_true, y_pred, average=\"macro\")\n","#         state.loader_metrics[self.prefix] = score\n","#         if state.is_valid_loader:\n","#             state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n","#                                 self.prefix] = score\n","#         else:\n","#             state.epoch_metrics[\"train_epoch_\" + self.prefix] = score\n","\n","# class AUCCallback(Callback):\n","#     def __init__(self,\n","#                  input_key: str = \"targets\",\n","#                  output_key: str = \"logits\",\n","#                  model_output_key: str = \"clipwise_output\",\n","#                  prefix: str = \"auc\"):\n","#         super().__init__(CallbackOrder.Metric)\n","\n","#         self.input_key = input_key\n","#         self.output_key = output_key\n","#         self.model_output_key = model_output_key\n","#         self.prefix = prefix\n","\n","#     def on_loader_start(self, runner):\n","#         self.prediction: List[np.ndarray] = []\n","#         self.target: List[np.ndarray] = []\n","\n","#     def on_batch_end(self, runner):\n","#         targ = runner.input[self.input_key].detach().cpu().numpy()\n","#         out = runner.output[self.output_key]\n","\n","#         clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n","\n","#         self.prediction.append(clipwise_output)\n","#         self.target.append(targ)\n","\n","#         y_pred = clipwise_output.argmax(axis=1)\n","#         y_true = targ.argmax(axis=1)\n","\n","#         score = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","#         runner.batch_metrics[self.prefix] = score\n","\n","#     def on_loader_end(self, runner):\n","#         y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n","#         y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n","#         score = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","#         runner.loader_metrics[self.prefix] = score\n","#         if runner.is_valid_loader:\n","#             runner.epoch_metrics[runner.valid_loader + \"_epoch_\" +\n","#                                 self.prefix] = score\n","#         else:\n","#             runner.epoch_metrics[\"train_epoch_\" + self.prefix] = score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.033622,"end_time":"2020-08-14T10:26:48.924696","exception":false,"start_time":"2020-08-14T10:26:48.891074","status":"completed"},"tags":[],"id":"JPjbSmM_EIAm"},"source":["### Train\n","\n","Some code are taken from https://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast .\n","Thanks @ttahara!"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:49.001106Z","iopub.status.busy":"2020-08-14T10:26:49.000144Z","iopub.status.idle":"2020-08-14T10:26:49.708541Z","shell.execute_reply":"2020-08-14T10:26:49.707629Z"},"papermill":{"duration":0.749533,"end_time":"2020-08-14T10:26:49.708664","exception":false,"start_time":"2020-08-14T10:26:48.959131","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"YkG-3zh-EIAn","executionInfo":{"status":"ok","timestamp":1623286845885,"user_tz":-480,"elapsed":17,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"caadbcf4-84cb-4d54-ee12-f288a797a0e3"},"source":["print(train.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1200, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:49.806182Z","iopub.status.busy":"2020-08-14T10:26:49.804681Z","iopub.status.idle":"2020-08-14T10:26:49.875178Z","shell.execute_reply":"2020-08-14T10:26:49.875766Z"},"papermill":{"duration":0.133113,"end_time":"2020-08-14T10:26:49.875942","exception":false,"start_time":"2020-08-14T10:26:49.742829","status":"completed"},"tags":[],"id":"f5XhzpdSEIAn"},"source":["skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","train[\"fold\"] = -1\n","for fold_id, (train_index, val_index) in enumerate(skf.split(train, train['Label'])):\n","    train.iloc[val_index, -1] = fold_id"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:49.960106Z","iopub.status.busy":"2020-08-14T10:26:49.959257Z","iopub.status.idle":"2020-08-14T10:26:50.006856Z","shell.execute_reply":"2020-08-14T10:26:50.007827Z"},"papermill":{"duration":0.094264,"end_time":"2020-08-14T10:26:50.007988","exception":false,"start_time":"2020-08-14T10:26:49.913724","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"TTptf0tlEIAn","executionInfo":{"status":"ok","timestamp":1623286845886,"user_tz":-480,"elapsed":13,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"0a54121d-1fd6-49e9-d0b7-894859a19ab2"},"source":["train_file_list = train.query(\"fold != @use_fold\")[[\"Filename\", \"Label\"]].values.tolist()\n","val_file_list = train.query(\"fold == @use_fold\")[[\"Filename\", \"Label\"]].values.tolist()\n","\n","print(\"[fold {}] train: {}, val: {}\".format(use_fold, len(train_file_list), len(val_file_list)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[fold 0] train: 960, val: 240\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vk2mW0UFOwAj"},"source":["# Fix Warmup Bug\n","class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:50.093491Z","iopub.status.busy":"2020-08-14T10:26:50.092046Z","iopub.status.idle":"2020-08-14T10:26:59.767467Z","shell.execute_reply":"2020-08-14T10:26:59.766328Z"},"papermill":{"duration":9.724143,"end_time":"2020-08-14T10:26:59.767606","exception":false,"start_time":"2020-08-14T10:26:50.043463","status":"completed"},"tags":[],"id":"pzdMf1ghEIAn"},"source":["from audiomentations import Compose, AddGaussianNoise, Gain, TimeStretch, PitchShift, Shift, TimeMask, AddGaussianSNR\n","\n","train_transform = Compose([\n","    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),\n","    # AddGaussianSNR(p=0.3),\n","    Gain(min_gain_in_db=-12, max_gain_in_db=12, p=0.3),\n","    # PitchShift(min_semitones=-4, max_semitones=4, p=0.3),\n","    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.3),\n","    Shift(min_fraction=-0.5, max_fraction=0.5, rollover=False, p=0.3),\n","    # TimeMask(min_band_part=0.0, max_band_part=0.2, p=0.5),\n","    # FrequencyMask(min_frequency_band=0.0, max_frequency_band=0.2, p=0.5)\n","])\n","\n","# loaders\n","loaders = {\n","    \"train\": data.DataLoader(PANNsDataset(train_file_list, None), \n","                             batch_size=batch_size, \n","                             shuffle=True, \n","                             num_workers=2, \n","                             pin_memory=True, \n","                             drop_last=True),\n","    \"valid\": data.DataLoader(PANNsDataset(val_file_list, None), \n","                             batch_size=batch_size, \n","                             shuffle=False,\n","                             num_workers=2,\n","                             pin_memory=True,\n","                             drop_last=False)\n","}\n","\n","train_loader = data.DataLoader(PANNsDataset(train_file_list, train_transform), \n","                             batch_size=batch_size, \n","                             shuffle=True, \n","                             num_workers=2, \n","                             pin_memory=True, \n","                             drop_last=False)\n","val_loader = data.DataLoader(PANNsDataset(val_file_list, None), \n","                             batch_size=batch_size, \n","                             shuffle=False,\n","                             num_workers=2,\n","                             pin_memory=True,\n","                             drop_last=False)\n","\n","# model\n","# model_config[\"classes_num\"] = 527  # for load weights only \n","# model = PANNsCNN14Att(**model_config)\n","# weights = torch.load(INPUT_ROOT + \"Cnn14_DecisionLevelAtt_mAP0.425.pth\")\n","# # Fixed in V3\n","# model.load_state_dict(weights[\"model\"])\n","# model.att_block = AttBlock(2048, CLASSES, activation='softmax')\n","# model.att_block.init_weights()\n","# model.to(device)\n","\n","# TODO\n","model = Transfer_Cnn14(**model_config)\n","model.load_from_pretrain(INPUT_ROOT + \"Cnn14_mAP_0.431.pth\")\n","model.to(device)\n","\n","# Optimizer\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","\n","# Scheduler\n","# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n","scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs-warmup_epo)\n","scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=warmup_factor, total_epoch=warmup_epo, after_scheduler=scheduler_cosine)\n","\n","# Loss\n","# criterion = PANNsLoss().to(device)\n","criterion = CutMixCrossEntropyLoss().to(device)\n","\n","# callbacks\n","# callbacks = [\n","#     AUCCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"auc\"),\n","#     # CheckpointCallback(save_n_best=0)\n","#     # F1Callback(input_key=\"targets\", output_key=\"logits\", prefix=\"f1\"),\n","#     # mAPCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"mAP\"),\n","    \n","# ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34y6eVBSd77i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623288953709,"user_tz":-480,"elapsed":2046967,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"5802294a-9bce-4d89-9466-02343368f006"},"source":["def train_epoch(loader, optimizer):\n","    model.train()\n","    train_loss = []\n","    LOGITS = []\n","    PREDS = []\n","    TARGETS = []\n","\n","    bar = tqdm(loader)\n","    for d in bar:\n","        data, target = d['waveform'], d['targets']\n","        optimizer.zero_grad()\n","        scheduler.step()\n","        \n","        data, target = data.to(device), target.to(device)\n","        \n","        # mixup_augmenter = Mixup(mixup_alpha=1.)\n","        # mixup_lambda = mixup_augmenter.get_lambda(batch_size=len(data))\n","        # mixup_lambda = torch.tensor(mixup_lambda, dtype=torch.float32).to(device)\n","        # target_mix = do_mixup(target, mixup_lambda)\n","       \n","        logits = model(data, mixup_lambda=None)\n","        # print(logits['framewise_output'].shape)    # shape (batch_size, frame_size, classes)\n","        # print(logits['clipwise_output'].shape)    # shape (batch_size, classes)\n","        logits_clip = logits['clipwise_output']\n","\n","        # loss = criterion(logits, target)\n","        ### hard cross entropy\n","        loss = criterion(logits_clip, (target==1).nonzero(as_tuple=True)[1].to(torch.long))\n","        ### soft cross entropy\n","        # loss = criterion(logits_clip, target_mix)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_np = loss.detach().cpu().numpy()\n","        train_loss.append(loss_np)\n","        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n","        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n","\n","        with torch.no_grad():\n","            # pred = logits_clip.argmax(dim=1).detach()\n","            logits_org = model(data)\n","            LOGITS.append(logits_org['clipwise_output'])\n","            PREDS.append(logits_org['clipwise_output'])\n","\n","            # to one hot\n","            # target = F.one_hot(target, num_classes=CLASSES)\n","            TARGETS.append(target)\n","\n","    PREDS = torch.cat(PREDS).detach().cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","    auc = roc_auc_score(TARGETS, PREDS, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","    return train_loss, auc\n","\n","\n","def val_epoch(loader, get_output=False):\n","\n","    model.eval()\n","    val_loss = []\n","    LOGITS = []\n","    PREDS = []\n","    TARGETS = []\n","\n","    with torch.no_grad():\n","        for d in tqdm(loader):\n","            data, target = d['waveform'], d['targets']\n","            data, target = data.to(device), target.to(device)\n","            logits = model(data)\n","            logits_clip = logits['clipwise_output']\n","\n","            # loss = criterion(logits, target)\n","            loss = criterion(logits_clip, (target==1).nonzero(as_tuple=True)[1].to(torch.long))\n","\n","            pred = logits_clip.argmax(dim=1).detach()\n","            LOGITS.append(logits_clip)\n","            PREDS.append(logits_clip)\n","            \n","            # to one hot\n","            # target = F.one_hot(target, num_classes=CLASSES)\n","            TARGETS.append(target)\n","\n","            val_loss.append(loss.detach().cpu().numpy())\n","        val_loss = np.mean(val_loss)\n","\n","    LOGITS = torch.cat(LOGITS).cpu().numpy()\n","    PREDS = torch.cat(PREDS).detach().cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","    auc = roc_auc_score(TARGETS, PREDS, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","\n","    if get_output:\n","        return LOGITS\n","    else:\n","        return val_loss, auc\n","\n","auc_max = 0\n","best_file = f'{kernel_type}_best_fold{use_fold}.pth'\n","for epoch in range(1, n_epochs+1):\n","    print(time.ctime(), 'Epoch:', epoch)\n","    scheduler.step(epoch-1)\n","\n","    train_loss, tr_auc = train_epoch(train_loader, optimizer)\n","    val_loss, val_auc = val_epoch(val_loader)\n","\n","    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, train auc: {(tr_auc):.5f}, val loss: {np.mean(val_loss):.5f}, val auc: {(val_auc):.5f}'\n","    print(content)\n","    with open(f'log_{kernel_type}.txt', 'a') as appender:\n","        appender.write(content + '\\n')\n","\n","    if val_auc > auc_max:\n","        print('score2 ({:.6f} --> {:.6f}).  Saving model ...'.format(auc_max, val_auc))\n","        torch.save(model.state_dict(), best_file)\n","        auc_max = val_auc\n","\n","torch.save(model.state_dict(), os.path.join(f'{kernel_type}_final_fold{use_fold}.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:01:46 2021 Epoch: 1\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.85442, smth: 1.85442:   0%|          | 0/8 [00:08<?, ?it/s]\u001b[A\n","loss: 1.85442, smth: 1.85442:  12%|█▎        | 1/8 [00:08<00:58,  8.30s/it]\u001b[A\n","loss: 1.81677, smth: 1.83559:  12%|█▎        | 1/8 [00:08<00:58,  8.30s/it]\u001b[A\n","loss: 1.81677, smth: 1.83559:  25%|██▌       | 2/8 [00:08<00:36,  6.00s/it]\u001b[A\n","loss: 1.85474, smth: 1.84198:  25%|██▌       | 2/8 [00:09<00:36,  6.00s/it]\u001b[A\n","loss: 1.85474, smth: 1.84198:  38%|███▊      | 3/8 [00:09<00:22,  4.42s/it]\u001b[A\n","loss: 1.72455, smth: 1.81262:  38%|███▊      | 3/8 [00:10<00:22,  4.42s/it]\u001b[A\n","loss: 1.72455, smth: 1.81262:  50%|█████     | 4/8 [00:10<00:13,  3.31s/it]\u001b[A\n","loss: 1.73398, smth: 1.79689:  50%|█████     | 4/8 [00:10<00:13,  3.31s/it]\u001b[A\n","loss: 1.73398, smth: 1.79689:  62%|██████▎   | 5/8 [00:11<00:07,  2.52s/it]\u001b[A\n","loss: 1.67622, smth: 1.77678:  62%|██████▎   | 5/8 [00:11<00:07,  2.52s/it]\u001b[A\n","loss: 1.67622, smth: 1.77678:  75%|███████▌  | 6/8 [00:11<00:03,  1.98s/it]\u001b[A\n","loss: 1.69309, smth: 1.76482:  75%|███████▌  | 6/8 [00:12<00:03,  1.98s/it]\u001b[A\n","loss: 1.69309, smth: 1.76482:  88%|████████▊ | 7/8 [00:12<00:01,  1.58s/it]\u001b[A\n","loss: 1.45127, smth: 1.72563:  88%|████████▊ | 7/8 [00:17<00:01,  1.58s/it]\u001b[A\n","loss: 1.45127, smth: 1.72563: 100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  1.17it/s]\u001b[A\n","100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:07 2021 Epoch 1, lr: 0.0023000, train loss: 1.72563, train auc: 0.61963, val loss: 1.00917, val auc: 0.93760\n","score2 (0.000000 --> 0.937604).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:08 2021 Epoch: 2\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.38608, smth: 1.38608:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 1.38608, smth: 1.38608:  12%|█▎        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 1.15968, smth: 1.27288:  12%|█▎        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 1.15968, smth: 1.27288:  25%|██▌       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 1.10371, smth: 1.21649:  25%|██▌       | 2/8 [00:05<00:14,  2.40s/it]\u001b[A\n","loss: 1.10371, smth: 1.21649:  38%|███▊      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 1.13177, smth: 1.19531:  38%|███▊      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 1.13177, smth: 1.19531:  50%|█████     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 1.06556, smth: 1.16936:  50%|█████     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 1.06556, smth: 1.16936:  62%|██████▎   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.96249, smth: 1.13488:  62%|██████▎   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.96249, smth: 1.13488:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 1.11051, smth: 1.13140:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 1.11051, smth: 1.13140:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 1.17383, smth: 1.13670:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 1.17383, smth: 1.13670: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:19 2021 Epoch 2, lr: 0.0025250, train loss: 1.13670, train auc: 0.86957, val loss: 0.80580, val auc: 0.94940\n","score2 (0.937604 --> 0.949396).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:20 2021 Epoch: 3\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.15091, smth: 1.15091:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.15091, smth: 1.15091:  12%|█▎        | 1/8 [00:02<00:16,  2.41s/it]\u001b[A\n","loss: 0.92139, smth: 1.03615:  12%|█▎        | 1/8 [00:03<00:16,  2.41s/it]\u001b[A\n","loss: 0.92139, smth: 1.03615:  25%|██▌       | 2/8 [00:03<00:11,  1.91s/it]\u001b[A\n","loss: 0.84774, smth: 0.97335:  25%|██▌       | 2/8 [00:04<00:11,  1.91s/it]\u001b[A\n","loss: 0.84774, smth: 0.97335:  38%|███▊      | 3/8 [00:04<00:08,  1.64s/it]\u001b[A\n","loss: 1.00178, smth: 0.98046:  38%|███▊      | 3/8 [00:05<00:08,  1.64s/it]\u001b[A\n","loss: 1.00178, smth: 0.98046:  50%|█████     | 4/8 [00:05<00:05,  1.44s/it]\u001b[A\n","loss: 0.81792, smth: 0.94795:  50%|█████     | 4/8 [00:06<00:05,  1.44s/it]\u001b[A\n","loss: 0.81792, smth: 0.94795:  62%|██████▎   | 5/8 [00:06<00:04,  1.36s/it]\u001b[A\n","loss: 0.72244, smth: 0.91036:  62%|██████▎   | 5/8 [00:07<00:04,  1.36s/it]\u001b[A\n","loss: 0.72244, smth: 0.91036:  75%|███████▌  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 1.01908, smth: 0.92590:  75%|███████▌  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 1.01908, smth: 0.92590:  88%|████████▊ | 7/8 [00:07<00:01,  1.09s/it]\u001b[A\n","loss: 0.85947, smth: 0.91759:  88%|████████▊ | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n","loss: 0.85947, smth: 0.91759: 100%|██████████| 8/8 [00:08<00:00,  1.06s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.25it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:29 2021 Epoch 3, lr: 0.0027500, train loss: 0.91759, train auc: 0.93619, val loss: 0.63942, val auc: 0.96346\n","score2 (0.949396 --> 0.963458).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:30 2021 Epoch: 4\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.83254, smth: 0.83254:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.83254, smth: 0.83254:  12%|█▎        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.80906, smth: 0.82080:  12%|█▎        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.80906, smth: 0.82080:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.83037, smth: 0.82399:  25%|██▌       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.83037, smth: 0.82399:  38%|███▊      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.66744, smth: 0.78485:  38%|███▊      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.66744, smth: 0.78485:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.62307, smth: 0.75250:  50%|█████     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.62307, smth: 0.75250:  62%|██████▎   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.51512, smth: 0.71293:  62%|██████▎   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.51512, smth: 0.71293:  75%|███████▌  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.55345, smth: 0.69015:  75%|███████▌  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.55345, smth: 0.69015:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.50414, smth: 0.66690:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.50414, smth: 0.66690: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:40 2021 Epoch 4, lr: 0.0029750, train loss: 0.66690, train auc: 0.96245, val loss: 0.77613, val auc: 0.95485\n","Thu Jun 10 01:02:40 2021 Epoch: 5\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48700, smth: 0.48700:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.48700, smth: 0.48700:  12%|█▎        | 1/8 [00:03<00:22,  3.21s/it]\u001b[A\n","loss: 0.43448, smth: 0.46074:  12%|█▎        | 1/8 [00:04<00:22,  3.21s/it]\u001b[A\n","loss: 0.43448, smth: 0.46074:  25%|██▌       | 2/8 [00:04<00:15,  2.51s/it]\u001b[A\n","loss: 0.68850, smth: 0.53666:  25%|██▌       | 2/8 [00:05<00:15,  2.51s/it]\u001b[A\n","loss: 0.68850, smth: 0.53666:  38%|███▊      | 3/8 [00:05<00:10,  2.17s/it]\u001b[A\n","loss: 0.80894, smth: 0.60473:  38%|███▊      | 3/8 [00:06<00:10,  2.17s/it]\u001b[A\n","loss: 0.80894, smth: 0.60473:  50%|█████     | 4/8 [00:06<00:07,  1.85s/it]\u001b[A\n","loss: 0.49556, smth: 0.58290:  50%|█████     | 4/8 [00:07<00:07,  1.85s/it]\u001b[A\n","loss: 0.49556, smth: 0.58290:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.54938, smth: 0.57731:  62%|██████▎   | 5/8 [00:08<00:04,  1.52s/it]\u001b[A\n","loss: 0.54938, smth: 0.57731:  75%|███████▌  | 6/8 [00:08<00:02,  1.47s/it]\u001b[A\n","loss: 0.76756, smth: 0.60449:  75%|███████▌  | 6/8 [00:09<00:02,  1.47s/it]\u001b[A\n","loss: 0.76756, smth: 0.60449:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.57820, smth: 0.60120:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.57820, smth: 0.60120: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  4.03it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:50 2021 Epoch 5, lr: 0.0032000, train loss: 0.60120, train auc: 0.96984, val loss: 0.61952, val auc: 0.97215\n","score2 (0.963458 --> 0.972146).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:52 2021 Epoch: 6\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59530, smth: 0.59530:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59530, smth: 0.59530:  12%|█▎        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.46294, smth: 0.52912:  12%|█▎        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.46294, smth: 0.52912:  25%|██▌       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.61850, smth: 0.55891:  25%|██▌       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.61850, smth: 0.55891:  38%|███▊      | 3/8 [00:04<00:09,  1.95s/it]\u001b[A\n","loss: 0.44162, smth: 0.52959:  38%|███▊      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.44162, smth: 0.52959:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.48667, smth: 0.52100:  50%|█████     | 4/8 [00:06<00:06,  1.60s/it]\u001b[A\n","loss: 0.48667, smth: 0.52100:  62%|██████▎   | 5/8 [00:06<00:04,  1.45s/it]\u001b[A\n","loss: 0.72897, smth: 0.55566:  62%|██████▎   | 5/8 [00:07<00:04,  1.45s/it]\u001b[A\n","loss: 0.72897, smth: 0.55566:  75%|███████▌  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.51500, smth: 0.54986:  75%|███████▌  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.51500, smth: 0.54986:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.33096, smth: 0.52249:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.33096, smth: 0.52249: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.75it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:02 2021 Epoch 6, lr: 0.0034250, train loss: 0.52249, train auc: 0.96954, val loss: 0.49305, val auc: 0.97660\n","score2 (0.972146 --> 0.976604).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:03 2021 Epoch: 7\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.46693, smth: 0.46693:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.46693, smth: 0.46693:  12%|█▎        | 1/8 [00:02<00:17,  2.46s/it]\u001b[A\n","loss: 0.62007, smth: 0.54350:  12%|█▎        | 1/8 [00:03<00:17,  2.46s/it]\u001b[A\n","loss: 0.62007, smth: 0.54350:  25%|██▌       | 2/8 [00:03<00:11,  1.95s/it]\u001b[A\n","loss: 0.86328, smth: 0.65009:  25%|██▌       | 2/8 [00:04<00:11,  1.95s/it]\u001b[A\n","loss: 0.86328, smth: 0.65009:  38%|███▊      | 3/8 [00:04<00:08,  1.79s/it]\u001b[A\n","loss: 0.67372, smth: 0.65600:  38%|███▊      | 3/8 [00:05<00:08,  1.79s/it]\u001b[A\n","loss: 0.67372, smth: 0.65600:  50%|█████     | 4/8 [00:05<00:05,  1.47s/it]\u001b[A\n","loss: 0.55663, smth: 0.63613:  50%|█████     | 4/8 [00:07<00:05,  1.47s/it]\u001b[A\n","loss: 0.55663, smth: 0.63613:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.40313, smth: 0.59729:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.40313, smth: 0.59729:  75%|███████▌  | 6/8 [00:07<00:02,  1.33s/it]\u001b[A\n","loss: 0.65392, smth: 0.60538:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.65392, smth: 0.60538:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.33880, smth: 0.57206:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.33880, smth: 0.57206: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.90it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:13 2021 Epoch 7, lr: 0.0036500, train loss: 0.57206, train auc: 0.96694, val loss: 0.77462, val auc: 0.96185\n","Thu Jun 10 01:03:13 2021 Epoch: 8\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.57412, smth: 0.57412:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.57412, smth: 0.57412:  12%|█▎        | 1/8 [00:03<00:24,  3.55s/it]\u001b[A\n","loss: 0.45798, smth: 0.51605:  12%|█▎        | 1/8 [00:04<00:24,  3.55s/it]\u001b[A\n","loss: 0.45798, smth: 0.51605:  25%|██▌       | 2/8 [00:04<00:16,  2.70s/it]\u001b[A\n","loss: 0.50549, smth: 0.51253:  25%|██▌       | 2/8 [00:05<00:16,  2.70s/it]\u001b[A\n","loss: 0.50549, smth: 0.51253:  38%|███▊      | 3/8 [00:05<00:11,  2.37s/it]\u001b[A\n","loss: 0.54082, smth: 0.51960:  38%|███▊      | 3/8 [00:06<00:11,  2.37s/it]\u001b[A\n","loss: 0.54082, smth: 0.51960:  50%|█████     | 4/8 [00:06<00:07,  1.92s/it]\u001b[A\n","loss: 0.58616, smth: 0.53291:  50%|█████     | 4/8 [00:07<00:07,  1.92s/it]\u001b[A\n","loss: 0.58616, smth: 0.53291:  62%|██████▎   | 5/8 [00:07<00:05,  1.69s/it]\u001b[A\n","loss: 0.41711, smth: 0.51361:  62%|██████▎   | 5/8 [00:08<00:05,  1.69s/it]\u001b[A\n","loss: 0.41711, smth: 0.51361:  75%|███████▌  | 6/8 [00:08<00:02,  1.47s/it]\u001b[A\n","loss: 0.51410, smth: 0.51368:  75%|███████▌  | 6/8 [00:10<00:02,  1.47s/it]\u001b[A\n","loss: 0.51410, smth: 0.51368:  88%|████████▊ | 7/8 [00:10<00:01,  1.41s/it]\u001b[A\n","loss: 0.83069, smth: 0.55331:  88%|████████▊ | 7/8 [00:10<00:01,  1.41s/it]\u001b[A\n","loss: 0.83069, smth: 0.55331: 100%|██████████| 8/8 [00:10<00:00,  1.32s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.36it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  4.08it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:24 2021 Epoch 8, lr: 0.0038750, train loss: 0.55331, train auc: 0.97342, val loss: 0.99307, val auc: 0.96292\n","Thu Jun 10 01:03:24 2021 Epoch: 9\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59457, smth: 0.59457:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59457, smth: 0.59457:  12%|█▎        | 1/8 [00:02<00:19,  2.76s/it]\u001b[A\n","loss: 0.36774, smth: 0.48116:  12%|█▎        | 1/8 [00:03<00:19,  2.76s/it]\u001b[A\n","loss: 0.36774, smth: 0.48116:  25%|██▌       | 2/8 [00:03<00:12,  2.16s/it]\u001b[A\n","loss: 0.43057, smth: 0.46430:  25%|██▌       | 2/8 [00:05<00:12,  2.16s/it]\u001b[A\n","loss: 0.43057, smth: 0.46430:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.58863, smth: 0.49538:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.58863, smth: 0.49538:  50%|█████     | 4/8 [00:06<00:06,  1.65s/it]\u001b[A\n","loss: 0.78695, smth: 0.55369:  50%|█████     | 4/8 [00:07<00:06,  1.65s/it]\u001b[A\n","loss: 0.78695, smth: 0.55369:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.72497, smth: 0.58224:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.72497, smth: 0.58224:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.71348, smth: 0.60099:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.71348, smth: 0.60099:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.51424, smth: 0.59014:  88%|████████▊ | 7/8 [00:09<00:01,  1.10s/it]\u001b[A\n","loss: 0.51424, smth: 0.59014: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.98it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:34 2021 Epoch 9, lr: 0.0041000, train loss: 0.59014, train auc: 0.96939, val loss: 0.68119, val auc: 0.97322\n","Thu Jun 10 01:03:34 2021 Epoch: 10\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.69259, smth: 0.69259:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.69259, smth: 0.69259:  12%|█▎        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.90948, smth: 0.80104:  12%|█▎        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.90948, smth: 0.80104:  25%|██▌       | 2/8 [00:04<00:15,  2.51s/it]\u001b[A\n","loss: 0.45597, smth: 0.68601:  25%|██▌       | 2/8 [00:05<00:15,  2.51s/it]\u001b[A\n","loss: 0.45597, smth: 0.68601:  38%|███▊      | 3/8 [00:05<00:10,  2.19s/it]\u001b[A\n","loss: 0.44144, smth: 0.62487:  38%|███▊      | 3/8 [00:06<00:10,  2.19s/it]\u001b[A\n","loss: 0.44144, smth: 0.62487:  50%|█████     | 4/8 [00:06<00:06,  1.75s/it]\u001b[A\n","loss: 1.03797, smth: 0.70749:  50%|█████     | 4/8 [00:07<00:06,  1.75s/it]\u001b[A\n","loss: 1.03797, smth: 0.70749:  62%|██████▎   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.67638, smth: 0.70230:  62%|██████▎   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.67638, smth: 0.70230:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.39480, smth: 0.65837:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.39480, smth: 0.65837:  88%|████████▊ | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.66543, smth: 0.65926:  88%|████████▊ | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.66543, smth: 0.65926: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.30it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  4.02it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:44 2021 Epoch 10, lr: 0.0043250, train loss: 0.65926, train auc: 0.97140, val loss: 0.66924, val auc: 0.97214\n","Thu Jun 10 01:03:44 2021 Epoch: 11\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.53183, smth: 0.53183:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.53183, smth: 0.53183:  12%|█▎        | 1/8 [00:02<00:18,  2.63s/it]\u001b[A\n","loss: 0.81047, smth: 0.67115:  12%|█▎        | 1/8 [00:03<00:18,  2.63s/it]\u001b[A\n","loss: 0.81047, smth: 0.67115:  25%|██▌       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.61005, smth: 0.65079:  25%|██▌       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.61005, smth: 0.65079:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.82511, smth: 0.69437:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.82511, smth: 0.69437:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.47226, smth: 0.64995:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.47226, smth: 0.64995:  62%|██████▎   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.63443, smth: 0.64736:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.63443, smth: 0.64736:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.45816, smth: 0.62033:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.45816, smth: 0.62033:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.43810, smth: 0.59755:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.43810, smth: 0.59755: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.24it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.99it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:53 2021 Epoch 11, lr: 0.0045500, train loss: 0.59755, train auc: 0.97359, val loss: 0.77577, val auc: 0.97439\n","Thu Jun 10 01:03:53 2021 Epoch: 12\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.61797, smth: 0.61797:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.61797, smth: 0.61797:  12%|█▎        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.59983, smth: 0.60890:  12%|█▎        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.59983, smth: 0.60890:  25%|██▌       | 2/8 [00:03<00:13,  2.27s/it]\u001b[A\n","loss: 0.53437, smth: 0.58406:  25%|██▌       | 2/8 [00:04<00:13,  2.27s/it]\u001b[A\n","loss: 0.53437, smth: 0.58406:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.54804, smth: 0.57505:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.54804, smth: 0.57505:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.94870, smth: 0.64978:  50%|█████     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.94870, smth: 0.64978:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.87158, smth: 0.68675:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.87158, smth: 0.68675:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.63518, smth: 0.67938:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.63518, smth: 0.67938:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.92282, smth: 0.70981:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.92282, smth: 0.70981: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  4.00it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:03 2021 Epoch 12, lr: 0.0047750, train loss: 0.70981, train auc: 0.96498, val loss: 1.03174, val auc: 0.96960\n","Thu Jun 10 01:04:03 2021 Epoch: 13\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.62070, smth: 0.62070:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.62070, smth: 0.62070:  12%|█▎        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 1.06041, smth: 0.84056:  12%|█▎        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 1.06041, smth: 0.84056:  25%|██▌       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.57550, smth: 0.75220:  25%|██▌       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.57550, smth: 0.75220:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.85053, smth: 0.77679:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.85053, smth: 0.77679:  50%|█████     | 4/8 [00:05<00:05,  1.49s/it]\u001b[A\n","loss: 0.80302, smth: 0.78203:  50%|█████     | 4/8 [00:06<00:05,  1.49s/it]\u001b[A\n","loss: 0.80302, smth: 0.78203:  62%|██████▎   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.64461, smth: 0.75913:  62%|██████▎   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.64461, smth: 0.75913:  75%|███████▌  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 1.05283, smth: 0.80109:  75%|███████▌  | 6/8 [00:08<00:02,  1.22s/it]\u001b[A\n","loss: 1.05283, smth: 0.80109:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.25037, smth: 0.73225:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.25037, smth: 0.73225: 100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.22it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.96it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:12 2021 Epoch 13, lr: 0.0050000, train loss: 0.73225, train auc: 0.95104, val loss: 8.15957, val auc: 0.79714\n","Thu Jun 10 01:04:12 2021 Epoch: 14\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.64485, smth: 0.64485:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.64485, smth: 0.64485:  12%|█▎        | 1/8 [00:03<00:20,  3.00s/it]\u001b[A\n","loss: 0.48115, smth: 0.56300:  12%|█▎        | 1/8 [00:03<00:20,  3.00s/it]\u001b[A\n","loss: 0.48115, smth: 0.56300:  25%|██▌       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.62838, smth: 0.58479:  25%|██▌       | 2/8 [00:05<00:14,  2.34s/it]\u001b[A\n","loss: 0.62838, smth: 0.58479:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.69743, smth: 0.61295:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.69743, smth: 0.61295:  50%|█████     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 1.00829, smth: 0.69202:  50%|█████     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 1.00829, smth: 0.69202:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.92567, smth: 0.73096:  62%|██████▎   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.92567, smth: 0.73096:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.69690, smth: 0.72610:  75%|███████▌  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.69690, smth: 0.72610:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n","\n","loss: 0.68355, smth: 0.72078:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.68355, smth: 0.72078: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.19it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.96it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:23 2021 Epoch 14, lr: 0.0050000, train loss: 0.72078, train auc: 0.95693, val loss: 5.91686, val auc: 0.86220\n","Thu Jun 10 01:04:23 2021 Epoch: 15\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.65834, smth: 0.65834:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.65834, smth: 0.65834:  12%|█▎        | 1/8 [00:02<00:19,  2.80s/it]\u001b[A\n","loss: 0.57534, smth: 0.61684:  12%|█▎        | 1/8 [00:03<00:19,  2.80s/it]\u001b[A\n","loss: 0.57534, smth: 0.61684:  25%|██▌       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.78426, smth: 0.67265:  25%|██▌       | 2/8 [00:04<00:13,  2.18s/it]\u001b[A\n","loss: 0.78426, smth: 0.67265:  38%|███▊      | 3/8 [00:04<00:09,  1.96s/it]\u001b[A\n","loss: 0.88895, smth: 0.72672:  38%|███▊      | 3/8 [00:05<00:09,  1.96s/it]\u001b[A\n","loss: 0.88895, smth: 0.72672:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.80375, smth: 0.74213:  50%|█████     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.80375, smth: 0.74213:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.64549, smth: 0.72602:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.64549, smth: 0.72602:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.59957, smth: 0.70796:  75%|███████▌  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.59957, smth: 0.70796:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.71804, smth: 0.70922:  88%|████████▊ | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.71804, smth: 0.70922: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.21it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.97it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:32 2021 Epoch 15, lr: 0.0049985, train loss: 0.70922, train auc: 0.96025, val loss: 5.74044, val auc: 0.86356\n","Thu Jun 10 01:04:32 2021 Epoch: 16\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41030, smth: 0.41030:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41030, smth: 0.41030:  12%|█▎        | 1/8 [00:02<00:19,  2.72s/it]\u001b[A\n","loss: 0.58773, smth: 0.49902:  12%|█▎        | 1/8 [00:03<00:19,  2.72s/it]\u001b[A\n","loss: 0.58773, smth: 0.49902:  25%|██▌       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 1.00416, smth: 0.66740:  25%|██▌       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 1.00416, smth: 0.66740:  38%|███▊      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 1.41490, smth: 0.85427:  38%|███▊      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 1.41490, smth: 0.85427:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.77959, smth: 0.83934:  50%|█████     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.77959, smth: 0.83934:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.99615, smth: 0.86547:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.99615, smth: 0.86547:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 1.01265, smth: 0.88650:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 1.01265, smth: 0.88650:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 1.40597, smth: 0.95143:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 1.40597, smth: 0.95143: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.32it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.98it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:43 2021 Epoch 16, lr: 0.0049966, train loss: 0.95143, train auc: 0.95334, val loss: 1.50027, val auc: 0.94986\n","Thu Jun 10 01:04:43 2021 Epoch: 17\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.05981, smth: 1.05981:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.05981, smth: 1.05981:  12%|█▎        | 1/8 [00:02<00:20,  2.86s/it]\u001b[A\n","loss: 0.84214, smth: 0.95098:  12%|█▎        | 1/8 [00:03<00:20,  2.86s/it]\u001b[A\n","loss: 0.84214, smth: 0.95098:  25%|██▌       | 2/8 [00:03<00:13,  2.24s/it]\u001b[A\n","loss: 0.74591, smth: 0.88262:  25%|██▌       | 2/8 [00:05<00:13,  2.24s/it]\u001b[A\n","loss: 0.74591, smth: 0.88262:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.58753, smth: 0.80885:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.58753, smth: 0.80885:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.72751, smth: 0.79258:  50%|█████     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.72751, smth: 0.79258:  62%|██████▎   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.60803, smth: 0.76182:  62%|██████▎   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.60803, smth: 0.76182:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.87757, smth: 0.77836:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.87757, smth: 0.77836:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.88345, smth: 0.79149:  88%|████████▊ | 7/8 [00:09<00:01,  1.13s/it]\u001b[A\n","loss: 0.88345, smth: 0.79149: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.82it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:52 2021 Epoch 17, lr: 0.0049939, train loss: 0.79149, train auc: 0.96404, val loss: 0.96668, val auc: 0.94975\n","Thu Jun 10 01:04:52 2021 Epoch: 18\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.70969, smth: 0.70969:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.70969, smth: 0.70969:  12%|█▎        | 1/8 [00:03<00:21,  3.14s/it]\u001b[A\n","loss: 0.66427, smth: 0.68698:  12%|█▎        | 1/8 [00:03<00:21,  3.14s/it]\u001b[A\n","loss: 0.66427, smth: 0.68698:  25%|██▌       | 2/8 [00:03<00:14,  2.41s/it]\u001b[A\n","loss: 1.05702, smth: 0.81033:  25%|██▌       | 2/8 [00:05<00:14,  2.41s/it]\u001b[A\n","loss: 1.05702, smth: 0.81033:  38%|███▊      | 3/8 [00:05<00:11,  2.20s/it]\u001b[A\n","loss: 0.93764, smth: 0.84216:  38%|███▊      | 3/8 [00:06<00:11,  2.20s/it]\u001b[A\n","loss: 0.93764, smth: 0.84216:  50%|█████     | 4/8 [00:06<00:07,  1.77s/it]\u001b[A\n","loss: 0.69837, smth: 0.81340:  50%|█████     | 4/8 [00:07<00:07,  1.77s/it]\u001b[A\n","loss: 0.69837, smth: 0.81340:  62%|██████▎   | 5/8 [00:07<00:05,  1.67s/it]\u001b[A\n","loss: 0.61742, smth: 0.78074:  62%|██████▎   | 5/8 [00:08<00:05,  1.67s/it]\u001b[A\n","loss: 0.61742, smth: 0.78074:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.90524, smth: 0.79852:  75%|███████▌  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.90524, smth: 0.79852:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.93680, smth: 0.81581:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.93680, smth: 0.81581: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:03 2021 Epoch 18, lr: 0.0049905, train loss: 0.81581, train auc: 0.95060, val loss: 9.92424, val auc: 0.86247\n","Thu Jun 10 01:05:03 2021 Epoch: 19\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.01120, smth: 1.01120:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.01120, smth: 1.01120:  12%|█▎        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.62598, smth: 0.81859:  12%|█▎        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.62598, smth: 0.81859:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.81636, smth: 0.81785:  25%|██▌       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.81636, smth: 0.81785:  38%|███▊      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.69231, smth: 0.78646:  38%|███▊      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.69231, smth: 0.78646:  50%|█████     | 4/8 [00:05<00:06,  1.54s/it]\u001b[A\n","loss: 0.72103, smth: 0.77337:  50%|█████     | 4/8 [00:07<00:06,  1.54s/it]\u001b[A\n","loss: 0.72103, smth: 0.77337:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 1.13018, smth: 0.83284:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 1.13018, smth: 0.83284:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 1.00076, smth: 0.85683:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 1.00076, smth: 0.85683:  88%|████████▊ | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 1.15204, smth: 0.89373:  88%|████████▊ | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 1.15204, smth: 0.89373: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.96it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:12 2021 Epoch 19, lr: 0.0049863, train loss: 0.89373, train auc: 0.95589, val loss: 1.05282, val auc: 0.93817\n","Thu Jun 10 01:05:12 2021 Epoch: 20\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.79186, smth: 0.79186:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.79186, smth: 0.79186:  12%|█▎        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.57217, smth: 0.68202:  12%|█▎        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.57217, smth: 0.68202:  25%|██▌       | 2/8 [00:03<00:13,  2.17s/it]\u001b[A\n","loss: 0.69258, smth: 0.68554:  25%|██▌       | 2/8 [00:05<00:13,  2.17s/it]\u001b[A\n","loss: 0.69258, smth: 0.68554:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.72626, smth: 0.69572:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.72626, smth: 0.69572:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.64613, smth: 0.68580:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.64613, smth: 0.68580:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.54329, smth: 0.66205:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.54329, smth: 0.66205:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.96462, smth: 0.70527:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.96462, smth: 0.70527:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.68279, smth: 0.70246:  88%|████████▊ | 7/8 [00:09<00:01,  1.11s/it]\u001b[A\n","loss: 0.68279, smth: 0.70246: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:22 2021 Epoch 20, lr: 0.0049814, train loss: 0.70246, train auc: 0.95123, val loss: 0.64336, val auc: 0.96596\n","Thu Jun 10 01:05:22 2021 Epoch: 21\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59275, smth: 0.59275:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59275, smth: 0.59275:  12%|█▎        | 1/8 [00:02<00:17,  2.51s/it]\u001b[A\n","loss: 0.51678, smth: 0.55477:  12%|█▎        | 1/8 [00:03<00:17,  2.51s/it]\u001b[A\n","loss: 0.51678, smth: 0.55477:  25%|██▌       | 2/8 [00:03<00:11,  1.97s/it]\u001b[A\n","loss: 0.62369, smth: 0.57774:  25%|██▌       | 2/8 [00:04<00:11,  1.97s/it]\u001b[A\n","loss: 0.62369, smth: 0.57774:  38%|███▊      | 3/8 [00:04<00:08,  1.74s/it]\u001b[A\n","loss: 0.94090, smth: 0.66853:  38%|███▊      | 3/8 [00:05<00:08,  1.74s/it]\u001b[A\n","loss: 0.94090, smth: 0.66853:  50%|█████     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.45736, smth: 0.62630:  50%|█████     | 4/8 [00:06<00:05,  1.43s/it]\u001b[A\n","loss: 0.45736, smth: 0.62630:  62%|██████▎   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.62714, smth: 0.62644:  62%|██████▎   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.62714, smth: 0.62644:  75%|███████▌  | 6/8 [00:07<00:02,  1.19s/it]\u001b[A\n","loss: 0.69770, smth: 0.63662:  75%|███████▌  | 6/8 [00:08<00:02,  1.19s/it]\u001b[A\n","loss: 0.69770, smth: 0.63662:  88%|████████▊ | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 2.65275, smth: 0.88863:  88%|████████▊ | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 2.65275, smth: 0.88863: 100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.25it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.94it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:31 2021 Epoch 21, lr: 0.0049757, train loss: 0.88863, train auc: 0.95313, val loss: 1.86420, val auc: 0.94827\n","Thu Jun 10 01:05:31 2021 Epoch: 22\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.40282, smth: 1.40282:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.40282, smth: 1.40282:  12%|█▎        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.77409, smth: 1.08846:  12%|█▎        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.77409, smth: 1.08846:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 1.08674, smth: 1.08788:  25%|██▌       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 1.08674, smth: 1.08788:  38%|███▊      | 3/8 [00:04<00:08,  1.74s/it]\u001b[A\n","loss: 2.28033, smth: 1.38600:  38%|███▊      | 3/8 [00:05<00:08,  1.74s/it]\u001b[A\n","loss: 2.28033, smth: 1.38600:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 1.84629, smth: 1.47806:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 1.84629, smth: 1.47806:  62%|██████▎   | 5/8 [00:06<00:04,  1.34s/it]\u001b[A\n","loss: 1.06994, smth: 1.41004:  62%|██████▎   | 5/8 [00:07<00:04,  1.34s/it]\u001b[A\n","loss: 1.06994, smth: 1.41004:  75%|███████▌  | 6/8 [00:07<00:02,  1.42s/it]\u001b[A\n","loss: 0.91058, smth: 1.33869:  75%|███████▌  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.91058, smth: 1.33869:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 1.84471, smth: 1.40194:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 1.84471, smth: 1.40194: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.87it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:41 2021 Epoch 22, lr: 0.0049692, train loss: 1.40194, train auc: 0.91230, val loss: 11.83332, val auc: 0.79329\n","Thu Jun 10 01:05:41 2021 Epoch: 23\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.47198, smth: 1.47198:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.47198, smth: 1.47198:  12%|█▎        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 1.09394, smth: 1.28296:  12%|█▎        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 1.09394, smth: 1.28296:  25%|██▌       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 1.07596, smth: 1.21396:  25%|██▌       | 2/8 [00:05<00:13,  2.26s/it]\u001b[A\n","loss: 1.07596, smth: 1.21396:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 1.04525, smth: 1.17178:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 1.04525, smth: 1.17178:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 1.21603, smth: 1.18063:  50%|█████     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 1.21603, smth: 1.18063:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.90728, smth: 1.13507:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.90728, smth: 1.13507:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.76042, smth: 1.08155:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.76042, smth: 1.08155:  88%|████████▊ | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 1.02207, smth: 1.07412:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 1.02207, smth: 1.07412: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.29it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  4.01it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:51 2021 Epoch 23, lr: 0.0049620, train loss: 1.07412, train auc: 0.93030, val loss: 0.89682, val auc: 0.93692\n","Thu Jun 10 01:05:51 2021 Epoch: 24\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.98188, smth: 0.98188:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.98188, smth: 0.98188:  12%|█▎        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 0.74438, smth: 0.86313:  12%|█▎        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 0.74438, smth: 0.86313:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.82990, smth: 0.85205:  25%|██▌       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.82990, smth: 0.85205:  38%|███▊      | 3/8 [00:04<00:09,  1.81s/it]\u001b[A\n","loss: 0.83544, smth: 0.84790:  38%|███▊      | 3/8 [00:05<00:09,  1.81s/it]\u001b[A\n","loss: 0.83544, smth: 0.84790:  50%|█████     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.96941, smth: 0.87220:  50%|█████     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.96941, smth: 0.87220:  62%|██████▎   | 5/8 [00:06<00:04,  1.41s/it]\u001b[A\n","loss: 1.02980, smth: 0.89847:  62%|██████▎   | 5/8 [00:07<00:04,  1.41s/it]\u001b[A\n","loss: 1.02980, smth: 0.89847:  75%|███████▌  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.64395, smth: 0.86211:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.64395, smth: 0.86211:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.88485, smth: 0.86495:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.88485, smth: 0.86495: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.24it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:00 2021 Epoch 24, lr: 0.0049541, train loss: 0.86495, train auc: 0.92997, val loss: 0.59958, val auc: 0.96698\n","Thu Jun 10 01:06:00 2021 Epoch: 25\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.56553, smth: 0.56553:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.56553, smth: 0.56553:  12%|█▎        | 1/8 [00:02<00:19,  2.77s/it]\u001b[A\n","loss: 0.72634, smth: 0.64594:  12%|█▎        | 1/8 [00:03<00:19,  2.77s/it]\u001b[A\n","loss: 0.72634, smth: 0.64594:  25%|██▌       | 2/8 [00:03<00:12,  2.17s/it]\u001b[A\n","loss: 0.72172, smth: 0.67120:  25%|██▌       | 2/8 [00:05<00:12,  2.17s/it]\u001b[A\n","loss: 0.72172, smth: 0.67120:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.72308, smth: 0.68417:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.72308, smth: 0.68417:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.97751, smth: 0.74284:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.97751, smth: 0.74284:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.65783, smth: 0.72867:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.65783, smth: 0.72867:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.65356, smth: 0.71794:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.65356, smth: 0.71794:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.78194, smth: 0.72594:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.78194, smth: 0.72594: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:10 2021 Epoch 25, lr: 0.0049454, train loss: 0.72594, train auc: 0.95243, val loss: 0.52566, val auc: 0.97652\n","Thu Jun 10 01:06:10 2021 Epoch: 26\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.66352, smth: 0.66352:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.66352, smth: 0.66352:  12%|█▎        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 1.27738, smth: 0.97045:  12%|█▎        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 1.27738, smth: 0.97045:  25%|██▌       | 2/8 [00:03<00:12,  2.13s/it]\u001b[A\n","loss: 0.68798, smth: 0.87629:  25%|██▌       | 2/8 [00:04<00:12,  2.13s/it]\u001b[A\n","loss: 0.68798, smth: 0.87629:  38%|███▊      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.67430, smth: 0.82580:  38%|███▊      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.67430, smth: 0.82580:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.75840, smth: 0.81232:  50%|█████     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.75840, smth: 0.81232:  62%|██████▎   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.73878, smth: 0.80006:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.73878, smth: 0.80006:  75%|███████▌  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.69042, smth: 0.78440:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.69042, smth: 0.78440:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.64476, smth: 0.76694:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.64476, smth: 0.76694: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.23it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.95it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:20 2021 Epoch 26, lr: 0.0049359, train loss: 0.76694, train auc: 0.94970, val loss: 0.45816, val auc: 0.97648\n","Thu Jun 10 01:06:20 2021 Epoch: 27\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48637, smth: 0.48637:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.48637, smth: 0.48637:  12%|█▎        | 1/8 [00:02<00:19,  2.71s/it]\u001b[A\n","loss: 2.61476, smth: 1.55056:  12%|█▎        | 1/8 [00:03<00:19,  2.71s/it]\u001b[A\n","loss: 2.61476, smth: 1.55056:  25%|██▌       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.56730, smth: 1.22281:  25%|██▌       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.56730, smth: 1.22281:  38%|███▊      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.60030, smth: 1.06718:  38%|███▊      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.60030, smth: 1.06718:  50%|█████     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 1.02708, smth: 1.05916:  50%|█████     | 4/8 [00:06<00:06,  1.57s/it]\u001b[A\n","loss: 1.02708, smth: 1.05916:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.73976, smth: 1.00593:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.73976, smth: 1.00593:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 1.11915, smth: 1.02210:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 1.11915, smth: 1.02210:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.93001, smth: 1.01059:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.93001, smth: 1.01059: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:30 2021 Epoch 27, lr: 0.0049257, train loss: 1.01059, train auc: 0.94613, val loss: 1.29746, val auc: 0.93975\n","Thu Jun 10 01:06:30 2021 Epoch: 28\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.83990, smth: 0.83990:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.83990, smth: 0.83990:  12%|█▎        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 0.64743, smth: 0.74366:  12%|█▎        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 0.64743, smth: 0.74366:  25%|██▌       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.58769, smth: 0.69167:  25%|██▌       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.58769, smth: 0.69167:  38%|███▊      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.88267, smth: 0.73942:  38%|███▊      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.88267, smth: 0.73942:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.80287, smth: 0.75211:  50%|█████     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.80287, smth: 0.75211:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.86701, smth: 0.77126:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.86701, smth: 0.77126:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.67263, smth: 0.75717:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.67263, smth: 0.75717:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 1.04107, smth: 0.79266:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 1.04107, smth: 0.79266: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.92it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:40 2021 Epoch 28, lr: 0.0049148, train loss: 0.79266, train auc: 0.94695, val loss: 0.61770, val auc: 0.96598\n","Thu Jun 10 01:06:40 2021 Epoch: 29\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.96258, smth: 0.96258:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.96258, smth: 0.96258:  12%|█▎        | 1/8 [00:02<00:17,  2.56s/it]\u001b[A\n","loss: 0.84412, smth: 0.90335:  12%|█▎        | 1/8 [00:03<00:17,  2.56s/it]\u001b[A\n","loss: 0.84412, smth: 0.90335:  25%|██▌       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.88251, smth: 0.89640:  25%|██▌       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.88251, smth: 0.89640:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.67355, smth: 0.84069:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.67355, smth: 0.84069:  50%|█████     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.84851, smth: 0.84225:  50%|█████     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.84851, smth: 0.84225:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.96418, smth: 0.86258:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.96418, smth: 0.86258:  75%|███████▌  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 1.08302, smth: 0.89407:  75%|███████▌  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 1.08302, smth: 0.89407:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.61592, smth: 0.85930:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.61592, smth: 0.85930: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.26it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:50 2021 Epoch 29, lr: 0.0049032, train loss: 0.85930, train auc: 0.94340, val loss: 0.41609, val auc: 0.98237\n","score2 (0.976604 --> 0.982375).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:51 2021 Epoch: 30\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.77179, smth: 0.77179:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.77179, smth: 0.77179:  12%|█▎        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.78742, smth: 0.77961:  12%|█▎        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.78742, smth: 0.77961:  25%|██▌       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.75158, smth: 0.77026:  25%|██▌       | 2/8 [00:05<00:13,  2.22s/it]\u001b[A\n","loss: 0.75158, smth: 0.77026:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.66800, smth: 0.74470:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.66800, smth: 0.74470:  50%|█████     | 4/8 [00:06<00:06,  1.66s/it]\u001b[A\n","loss: 0.82789, smth: 0.76134:  50%|█████     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.82789, smth: 0.76134:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.75598, smth: 0.76044:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.75598, smth: 0.76044:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.48049, smth: 0.72045:  75%|███████▌  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.48049, smth: 0.72045:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.55699, smth: 0.70002:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.55699, smth: 0.70002: 100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.85it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.60it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:01 2021 Epoch 30, lr: 0.0048908, train loss: 0.70002, train auc: 0.95395, val loss: 0.58365, val auc: 0.97433\n","Thu Jun 10 01:07:01 2021 Epoch: 31\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.43496, smth: 0.43496:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.43496, smth: 0.43496:  12%|█▎        | 1/8 [00:03<00:23,  3.38s/it]\u001b[A\n","loss: 0.53036, smth: 0.48266:  12%|█▎        | 1/8 [00:04<00:23,  3.38s/it]\u001b[A\n","loss: 0.53036, smth: 0.48266:  25%|██▌       | 2/8 [00:04<00:15,  2.60s/it]\u001b[A\n","loss: 0.62310, smth: 0.52947:  25%|██▌       | 2/8 [00:05<00:15,  2.60s/it]\u001b[A\n","loss: 0.62310, smth: 0.52947:  38%|███▊      | 3/8 [00:05<00:11,  2.33s/it]\u001b[A\n","loss: 0.62320, smth: 0.55290:  38%|███▊      | 3/8 [00:06<00:11,  2.33s/it]\u001b[A\n","loss: 0.62320, smth: 0.55290:  50%|█████     | 4/8 [00:06<00:07,  1.86s/it]\u001b[A\n","loss: 0.64727, smth: 0.57178:  50%|█████     | 4/8 [00:08<00:07,  1.86s/it]\u001b[A\n","loss: 0.64727, smth: 0.57178:  62%|██████▎   | 5/8 [00:08<00:05,  1.74s/it]\u001b[A\n","loss: 0.60657, smth: 0.57758:  62%|██████▎   | 5/8 [00:08<00:05,  1.74s/it]\u001b[A\n","loss: 0.60657, smth: 0.57758:  75%|███████▌  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.56161, smth: 0.57530:  75%|███████▌  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.56161, smth: 0.57530:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.70261, smth: 0.59121:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.70261, smth: 0.59121: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.86it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:12 2021 Epoch 31, lr: 0.0048776, train loss: 0.59121, train auc: 0.96249, val loss: 0.41474, val auc: 0.98365\n","score2 (0.982375 --> 0.983646).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:13 2021 Epoch: 32\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.51781, smth: 0.51781:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.51781, smth: 0.51781:  12%|█▎        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.64156, smth: 0.57969:  12%|█▎        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.64156, smth: 0.57969:  25%|██▌       | 2/8 [00:03<00:14,  2.41s/it]\u001b[A\n","loss: 0.78209, smth: 0.64715:  25%|██▌       | 2/8 [00:05<00:14,  2.41s/it]\u001b[A\n","loss: 0.78209, smth: 0.64715:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.72508, smth: 0.66663:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.72508, smth: 0.66663:  50%|█████     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.59966, smth: 0.65324:  50%|█████     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.59966, smth: 0.65324:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.48271, smth: 0.62482:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.48271, smth: 0.62482:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.49350, smth: 0.60606:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.49350, smth: 0.60606:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.44333, smth: 0.58572:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.44333, smth: 0.58572: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.72it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:23 2021 Epoch 32, lr: 0.0048638, train loss: 0.58572, train auc: 0.96093, val loss: 0.50449, val auc: 0.98377\n","score2 (0.983646 --> 0.983771).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:24 2021 Epoch: 33\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.73825, smth: 0.73825:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.73825, smth: 0.73825:  12%|█▎        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.62407, smth: 0.68116:  12%|█▎        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.62407, smth: 0.68116:  25%|██▌       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.48113, smth: 0.61448:  25%|██▌       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.48113, smth: 0.61448:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.46714, smth: 0.57765:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.46714, smth: 0.57765:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.49527, smth: 0.56117:  50%|█████     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.49527, smth: 0.56117:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.57784, smth: 0.56395:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.57784, smth: 0.56395:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.53299, smth: 0.55953:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.53299, smth: 0.55953:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.72356, smth: 0.58003:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.72356, smth: 0.58003: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.88it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.60it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:35 2021 Epoch 33, lr: 0.0048492, train loss: 0.58003, train auc: 0.96542, val loss: 0.40897, val auc: 0.98317\n","Thu Jun 10 01:07:35 2021 Epoch: 34\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41279, smth: 0.41279:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41279, smth: 0.41279:  12%|█▎        | 1/8 [00:03<00:21,  3.09s/it]\u001b[A\n","loss: 0.59800, smth: 0.50540:  12%|█▎        | 1/8 [00:03<00:21,  3.09s/it]\u001b[A\n","loss: 0.59800, smth: 0.50540:  25%|██▌       | 2/8 [00:03<00:14,  2.39s/it]\u001b[A\n","loss: 0.60925, smth: 0.54002:  25%|██▌       | 2/8 [00:05<00:14,  2.39s/it]\u001b[A\n","loss: 0.60925, smth: 0.54002:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.48601, smth: 0.52652:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.48601, smth: 0.52652:  50%|█████     | 4/8 [00:06<00:06,  1.72s/it]\u001b[A\n","loss: 0.48438, smth: 0.51809:  50%|█████     | 4/8 [00:07<00:06,  1.72s/it]\u001b[A\n","loss: 0.48438, smth: 0.51809:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.53409, smth: 0.52076:  62%|██████▎   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.53409, smth: 0.52076:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.43698, smth: 0.50879:  75%|███████▌  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.43698, smth: 0.50879:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.59570, smth: 0.51965:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.59570, smth: 0.51965: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:45 2021 Epoch 34, lr: 0.0048340, train loss: 0.51965, train auc: 0.97087, val loss: 0.39675, val auc: 0.98621\n","score2 (0.983771 --> 0.986208).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:46 2021 Epoch: 35\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59830, smth: 0.59830:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59830, smth: 0.59830:  12%|█▎        | 1/8 [00:02<00:17,  2.43s/it]\u001b[A\n","loss: 0.51440, smth: 0.55635:  12%|█▎        | 1/8 [00:03<00:17,  2.43s/it]\u001b[A\n","loss: 0.51440, smth: 0.55635:  25%|██▌       | 2/8 [00:03<00:11,  1.93s/it]\u001b[A\n","loss: 0.50610, smth: 0.53960:  25%|██▌       | 2/8 [00:04<00:11,  1.93s/it]\u001b[A\n","loss: 0.50610, smth: 0.53960:  38%|███▊      | 3/8 [00:04<00:08,  1.72s/it]\u001b[A\n","loss: 0.41026, smth: 0.50726:  38%|███▊      | 3/8 [00:05<00:08,  1.72s/it]\u001b[A\n","loss: 0.41026, smth: 0.50726:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.63563, smth: 0.53294:  50%|█████     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.63563, smth: 0.53294:  62%|██████▎   | 5/8 [00:06<00:04,  1.35s/it]\u001b[A\n","loss: 0.54791, smth: 0.53543:  62%|██████▎   | 5/8 [00:07<00:04,  1.35s/it]\u001b[A\n","loss: 0.54791, smth: 0.53543:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.50802, smth: 0.53152:  75%|███████▌  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.50802, smth: 0.53152:  88%|████████▊ | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.63494, smth: 0.54444:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.63494, smth: 0.54444: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.92it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:56 2021 Epoch 35, lr: 0.0048180, train loss: 0.54444, train auc: 0.96835, val loss: 0.30754, val auc: 0.98740\n","score2 (0.986208 --> 0.987396).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:57 2021 Epoch: 36\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48584, smth: 0.48584:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.48584, smth: 0.48584:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.48177, smth: 0.48380:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.48177, smth: 0.48380:  25%|██▌       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.59893, smth: 0.52218:  25%|██▌       | 2/8 [00:05<00:14,  2.34s/it]\u001b[A\n","loss: 0.59893, smth: 0.52218:  38%|███▊      | 3/8 [00:05<00:10,  2.12s/it]\u001b[A\n","loss: 0.48122, smth: 0.51194:  38%|███▊      | 3/8 [00:06<00:10,  2.12s/it]\u001b[A\n","loss: 0.48122, smth: 0.51194:  50%|█████     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.71943, smth: 0.55344:  50%|█████     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.71943, smth: 0.55344:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.42839, smth: 0.53260:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.42839, smth: 0.53260:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.62710, smth: 0.54610:  75%|███████▌  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.62710, smth: 0.54610:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.43553, smth: 0.53228:  88%|████████▊ | 7/8 [00:09<00:01,  1.11s/it]\u001b[A\n","loss: 0.43553, smth: 0.53228: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.80it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:07 2021 Epoch 36, lr: 0.0048013, train loss: 0.53228, train auc: 0.97550, val loss: 0.34623, val auc: 0.98560\n","Thu Jun 10 01:08:07 2021 Epoch: 37\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.33461, smth: 0.33461:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.33461, smth: 0.33461:  12%|█▎        | 1/8 [00:03<00:26,  3.85s/it]\u001b[A\n","loss: 0.52457, smth: 0.42959:  12%|█▎        | 1/8 [00:04<00:26,  3.85s/it]\u001b[A\n","loss: 0.52457, smth: 0.42959:  25%|██▌       | 2/8 [00:04<00:17,  2.90s/it]\u001b[A\n","loss: 0.46537, smth: 0.44152:  25%|██▌       | 2/8 [00:05<00:17,  2.90s/it]\u001b[A\n","loss: 0.46537, smth: 0.44152:  38%|███▊      | 3/8 [00:05<00:12,  2.41s/it]\u001b[A\n","loss: 0.50294, smth: 0.45687:  38%|███▊      | 3/8 [00:06<00:12,  2.41s/it]\u001b[A\n","loss: 0.50294, smth: 0.45687:  50%|█████     | 4/8 [00:06<00:07,  1.91s/it]\u001b[A\n","loss: 0.52020, smth: 0.46954:  50%|█████     | 4/8 [00:08<00:07,  1.91s/it]\u001b[A\n","loss: 0.52020, smth: 0.46954:  62%|██████▎   | 5/8 [00:08<00:05,  1.81s/it]\u001b[A\n","loss: 0.50431, smth: 0.47533:  62%|██████▎   | 5/8 [00:08<00:05,  1.81s/it]\u001b[A\n","loss: 0.50431, smth: 0.47533:  75%|███████▌  | 6/8 [00:08<00:02,  1.48s/it]\u001b[A\n","loss: 0.47179, smth: 0.47483:  75%|███████▌  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.47179, smth: 0.47483:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.40252, smth: 0.46579:  88%|████████▊ | 7/8 [00:10<00:01,  1.30s/it]\u001b[A\n","loss: 0.40252, smth: 0.46579: 100%|██████████| 8/8 [00:10<00:00,  1.28s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.95it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:18 2021 Epoch 37, lr: 0.0047839, train loss: 0.46579, train auc: 0.97810, val loss: 0.30624, val auc: 0.98813\n","score2 (0.987396 --> 0.988125).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:19 2021 Epoch: 38\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.30732, smth: 0.30732:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.30732, smth: 0.30732:  12%|█▎        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.66945, smth: 0.48838:  12%|█▎        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.66945, smth: 0.48838:  25%|██▌       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.46780, smth: 0.48152:  25%|██▌       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.46780, smth: 0.48152:  38%|███▊      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.48148, smth: 0.48151:  38%|███▊      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.48148, smth: 0.48151:  50%|█████     | 4/8 [00:05<00:05,  1.45s/it]\u001b[A\n","loss: 0.43065, smth: 0.47134:  50%|█████     | 4/8 [00:06<00:05,  1.45s/it]\u001b[A\n","loss: 0.43065, smth: 0.47134:  62%|██████▎   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.41751, smth: 0.46237:  62%|██████▎   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.41751, smth: 0.46237:  75%|███████▌  | 6/8 [00:07<00:02,  1.42s/it]\u001b[A\n","loss: 0.42570, smth: 0.45713:  75%|███████▌  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.42570, smth: 0.45713:  88%|████████▊ | 7/8 [00:08<00:01,  1.31s/it]\u001b[A\n","loss: 0.45270, smth: 0.45657:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.45270, smth: 0.45657: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.83it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:29 2021 Epoch 38, lr: 0.0047658, train loss: 0.45657, train auc: 0.97865, val loss: 0.30934, val auc: 0.98954\n","score2 (0.988125 --> 0.989542).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:31 2021 Epoch: 39\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.42306, smth: 0.42306:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.42306, smth: 0.42306:  12%|█▎        | 1/8 [00:03<00:23,  3.36s/it]\u001b[A\n","loss: 0.40848, smth: 0.41577:  12%|█▎        | 1/8 [00:04<00:23,  3.36s/it]\u001b[A\n","loss: 0.40848, smth: 0.41577:  25%|██▌       | 2/8 [00:04<00:15,  2.58s/it]\u001b[A\n","loss: 0.35620, smth: 0.39592:  25%|██▌       | 2/8 [00:05<00:15,  2.58s/it]\u001b[A\n","loss: 0.35620, smth: 0.39592:  38%|███▊      | 3/8 [00:05<00:11,  2.23s/it]\u001b[A\n","loss: 0.36169, smth: 0.38736:  38%|███▊      | 3/8 [00:06<00:11,  2.23s/it]\u001b[A\n","loss: 0.36169, smth: 0.38736:  50%|█████     | 4/8 [00:06<00:07,  1.78s/it]\u001b[A\n","loss: 0.38276, smth: 0.38644:  50%|█████     | 4/8 [00:07<00:07,  1.78s/it]\u001b[A\n","loss: 0.38276, smth: 0.38644:  62%|██████▎   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.34750, smth: 0.37995:  62%|██████▎   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.34750, smth: 0.37995:  75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.43022, smth: 0.38713:  75%|███████▌  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.43022, smth: 0.38713:  88%|████████▊ | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 1.10696, smth: 0.47711:  88%|████████▊ | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 1.10696, smth: 0.47711: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:41 2021 Epoch 39, lr: 0.0047470, train loss: 0.47711, train auc: 0.98044, val loss: 0.29876, val auc: 0.98854\n","Thu Jun 10 01:08:41 2021 Epoch: 40\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.61816, smth: 0.61816:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.61816, smth: 0.61816:  12%|█▎        | 1/8 [00:03<00:26,  3.76s/it]\u001b[A\n","loss: 0.41083, smth: 0.51449:  12%|█▎        | 1/8 [00:04<00:26,  3.76s/it]\u001b[A\n","loss: 0.41083, smth: 0.51449:  25%|██▌       | 2/8 [00:04<00:17,  2.87s/it]\u001b[A\n","loss: 0.46014, smth: 0.49638:  25%|██▌       | 2/8 [00:06<00:17,  2.87s/it]\u001b[A\n","loss: 0.46014, smth: 0.49638:  38%|███▊      | 3/8 [00:06<00:12,  2.49s/it]\u001b[A\n","loss: 0.53825, smth: 0.50685:  38%|███▊      | 3/8 [00:06<00:12,  2.49s/it]\u001b[A\n","loss: 0.53825, smth: 0.50685:  50%|█████     | 4/8 [00:06<00:07,  1.97s/it]\u001b[A\n","loss: 0.37709, smth: 0.48089:  50%|█████     | 4/8 [00:08<00:07,  1.97s/it]\u001b[A\n","loss: 0.37709, smth: 0.48089:  62%|██████▎   | 5/8 [00:08<00:05,  1.82s/it]\u001b[A\n","loss: 0.57779, smth: 0.49704:  62%|██████▎   | 5/8 [00:09<00:05,  1.82s/it]\u001b[A\n","loss: 0.57779, smth: 0.49704:  75%|███████▌  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.37372, smth: 0.47943:  75%|███████▌  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.37372, smth: 0.47943:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.68510, smth: 0.50513:  88%|████████▊ | 7/8 [00:10<00:01,  1.26s/it]\u001b[A\n","loss: 0.68510, smth: 0.50513: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:52 2021 Epoch 40, lr: 0.0047275, train loss: 0.50513, train auc: 0.97793, val loss: 0.27275, val auc: 0.99033\n","score2 (0.989542 --> 0.990333).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:53 2021 Epoch: 41\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.44453, smth: 0.44453:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.44453, smth: 0.44453:  12%|█▎        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.38781, smth: 0.41617:  12%|█▎        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.38781, smth: 0.41617:  25%|██▌       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.47249, smth: 0.43494:  25%|██▌       | 2/8 [00:05<00:14,  2.34s/it]\u001b[A\n","loss: 0.47249, smth: 0.43494:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.39773, smth: 0.42564:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.39773, smth: 0.42564:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.41126, smth: 0.42276:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.41126, smth: 0.42276:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.38674, smth: 0.41676:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.38674, smth: 0.41676:  75%|███████▌  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.55520, smth: 0.43654:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.55520, smth: 0.43654:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.43261, smth: 0.43605:  88%|████████▊ | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 0.43261, smth: 0.43605: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.89it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.56it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:03 2021 Epoch 41, lr: 0.0047074, train loss: 0.43605, train auc: 0.98161, val loss: 0.33325, val auc: 0.98592\n","Thu Jun 10 01:09:03 2021 Epoch: 42\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.44554, smth: 0.44554:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.44554, smth: 0.44554:  12%|█▎        | 1/8 [00:03<00:23,  3.41s/it]\u001b[A\n","loss: 0.44482, smth: 0.44518:  12%|█▎        | 1/8 [00:04<00:23,  3.41s/it]\u001b[A\n","loss: 0.44482, smth: 0.44518:  25%|██▌       | 2/8 [00:04<00:16,  2.67s/it]\u001b[A\n","loss: 0.40943, smth: 0.43326:  25%|██▌       | 2/8 [00:05<00:16,  2.67s/it]\u001b[A\n","loss: 0.40943, smth: 0.43326:  38%|███▊      | 3/8 [00:05<00:11,  2.31s/it]\u001b[A\n","loss: 0.29198, smth: 0.39794:  38%|███▊      | 3/8 [00:06<00:11,  2.31s/it]\u001b[A\n","loss: 0.29198, smth: 0.39794:  50%|█████     | 4/8 [00:06<00:07,  1.86s/it]\u001b[A\n","loss: 0.44847, smth: 0.40805:  50%|█████     | 4/8 [00:07<00:07,  1.86s/it]\u001b[A\n","loss: 0.44847, smth: 0.40805:  62%|██████▎   | 5/8 [00:07<00:05,  1.68s/it]\u001b[A\n","loss: 0.48083, smth: 0.42018:  62%|██████▎   | 5/8 [00:08<00:05,  1.68s/it]\u001b[A\n","loss: 0.48083, smth: 0.42018:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.37020, smth: 0.41304:  75%|███████▌  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.37020, smth: 0.41304:  88%|████████▊ | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.42597, smth: 0.41466:  88%|████████▊ | 7/8 [00:10<00:01,  1.37s/it]\u001b[A\n","loss: 0.42597, smth: 0.41466: 100%|██████████| 8/8 [00:10<00:00,  1.31s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.86it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:14 2021 Epoch 42, lr: 0.0046865, train loss: 0.41466, train auc: 0.97929, val loss: 0.36345, val auc: 0.98785\n","Thu Jun 10 01:09:14 2021 Epoch: 43\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.52100, smth: 0.52100:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.52100, smth: 0.52100:  12%|█▎        | 1/8 [00:02<00:20,  2.98s/it]\u001b[A\n","loss: 0.34568, smth: 0.43334:  12%|█▎        | 1/8 [00:03<00:20,  2.98s/it]\u001b[A\n","loss: 0.34568, smth: 0.43334:  25%|██▌       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.32810, smth: 0.39826:  25%|██▌       | 2/8 [00:04<00:13,  2.32s/it]\u001b[A\n","loss: 0.32810, smth: 0.39826:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.48845, smth: 0.42081:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.48845, smth: 0.42081:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.54971, smth: 0.44659:  50%|█████     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.54971, smth: 0.44659:  62%|██████▎   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.44420, smth: 0.44619:  62%|██████▎   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.44420, smth: 0.44619:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.35903, smth: 0.43374:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.35903, smth: 0.43374:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.59107, smth: 0.45340:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.59107, smth: 0.45340: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.02it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:24 2021 Epoch 43, lr: 0.0046651, train loss: 0.45340, train auc: 0.98025, val loss: 0.24884, val auc: 0.99142\n","score2 (0.990333 --> 0.991417).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:26 2021 Epoch: 44\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48248, smth: 0.48248:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.48248, smth: 0.48248:  12%|█▎        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.28369, smth: 0.38309:  12%|█▎        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.28369, smth: 0.38309:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.40463, smth: 0.39027:  25%|██▌       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.40463, smth: 0.39027:  38%|███▊      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.38106, smth: 0.38797:  38%|███▊      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.38106, smth: 0.38797:  50%|█████     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.38660, smth: 0.38769:  50%|█████     | 4/8 [00:07<00:06,  1.58s/it]\u001b[A\n","loss: 0.38660, smth: 0.38769:  62%|██████▎   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.48363, smth: 0.40368:  62%|██████▎   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.48363, smth: 0.40368:  75%|███████▌  | 6/8 [00:07<00:02,  1.33s/it]\u001b[A\n","loss: 0.50081, smth: 0.41756:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.50081, smth: 0.41756:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.63085, smth: 0.44422:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.63085, smth: 0.44422: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.87it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.56it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:36 2021 Epoch 44, lr: 0.0046429, train loss: 0.44422, train auc: 0.98068, val loss: 0.27114, val auc: 0.98948\n","Thu Jun 10 01:09:36 2021 Epoch: 45\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.30057, smth: 0.30057:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.30057, smth: 0.30057:  12%|█▎        | 1/8 [00:03<00:25,  3.70s/it]\u001b[A\n","loss: 0.42453, smth: 0.36255:  12%|█▎        | 1/8 [00:04<00:25,  3.70s/it]\u001b[A\n","loss: 0.42453, smth: 0.36255:  25%|██▌       | 2/8 [00:04<00:16,  2.83s/it]\u001b[A\n","loss: 0.37875, smth: 0.36795:  25%|██▌       | 2/8 [00:05<00:16,  2.83s/it]\u001b[A\n","loss: 0.37875, smth: 0.36795:  38%|███▊      | 3/8 [00:05<00:12,  2.41s/it]\u001b[A\n","loss: 0.56304, smth: 0.41673:  38%|███▊      | 3/8 [00:06<00:12,  2.41s/it]\u001b[A\n","loss: 0.56304, smth: 0.41673:  50%|█████     | 4/8 [00:06<00:07,  1.94s/it]\u001b[A\n","loss: 0.34387, smth: 0.40215:  50%|█████     | 4/8 [00:08<00:07,  1.94s/it]\u001b[A\n","loss: 0.34387, smth: 0.40215:  62%|██████▎   | 5/8 [00:08<00:05,  1.75s/it]\u001b[A\n","loss: 0.48432, smth: 0.41585:  62%|██████▎   | 5/8 [00:08<00:05,  1.75s/it]\u001b[A\n","loss: 0.48432, smth: 0.41585:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.56274, smth: 0.43683:  75%|███████▌  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.56274, smth: 0.43683:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.39902, smth: 0.43211:  88%|████████▊ | 7/8 [00:10<00:01,  1.30s/it]\u001b[A\n","loss: 0.39902, smth: 0.43211: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:47 2021 Epoch 45, lr: 0.0046201, train loss: 0.43211, train auc: 0.98563, val loss: 0.31331, val auc: 0.98635\n","Thu Jun 10 01:09:47 2021 Epoch: 46\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41725, smth: 0.41725:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41725, smth: 0.41725:  12%|█▎        | 1/8 [00:02<00:20,  2.99s/it]\u001b[A\n","loss: 0.36070, smth: 0.38897:  12%|█▎        | 1/8 [00:03<00:20,  2.99s/it]\u001b[A\n","loss: 0.36070, smth: 0.38897:  25%|██▌       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.44601, smth: 0.40798:  25%|██▌       | 2/8 [00:04<00:14,  2.34s/it]\u001b[A\n","loss: 0.44601, smth: 0.40798:  38%|███▊      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.40476, smth: 0.40718:  38%|███▊      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.40476, smth: 0.40718:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.38935, smth: 0.40361:  50%|█████     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.38935, smth: 0.40361:  62%|██████▎   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.39384, smth: 0.40198:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.39384, smth: 0.40198:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.30870, smth: 0.38866:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.30870, smth: 0.38866:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.48405, smth: 0.40058:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.48405, smth: 0.40058: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.91it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:56 2021 Epoch 46, lr: 0.0045967, train loss: 0.40058, train auc: 0.98223, val loss: 0.39283, val auc: 0.98227\n","Thu Jun 10 01:09:56 2021 Epoch: 47\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.37610, smth: 0.37610:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.37610, smth: 0.37610:  12%|█▎        | 1/8 [00:02<00:20,  2.86s/it]\u001b[A\n","loss: 0.32197, smth: 0.34903:  12%|█▎        | 1/8 [00:03<00:20,  2.86s/it]\u001b[A\n","loss: 0.32197, smth: 0.34903:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.35649, smth: 0.35152:  25%|██▌       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.35649, smth: 0.35152:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.52965, smth: 0.39605:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.52965, smth: 0.39605:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.50587, smth: 0.41801:  50%|█████     | 4/8 [00:07<00:06,  1.61s/it]\u001b[A\n","loss: 0.50587, smth: 0.41801:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.38997, smth: 0.41334:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.38997, smth: 0.41334:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.44859, smth: 0.41838:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.44859, smth: 0.41838:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.57761, smth: 0.43828:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.57761, smth: 0.43828: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:06 2021 Epoch 47, lr: 0.0045726, train loss: 0.43828, train auc: 0.97940, val loss: 0.61005, val auc: 0.98397\n","Thu Jun 10 01:10:06 2021 Epoch: 48\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.42707, smth: 0.42707:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.42707, smth: 0.42707:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.31022, smth: 0.36865:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.31022, smth: 0.36865:  25%|██▌       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.45934, smth: 0.39888:  25%|██▌       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.45934, smth: 0.39888:  38%|███▊      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.44391, smth: 0.41014:  38%|███▊      | 3/8 [00:06<00:09,  1.91s/it]\u001b[A\n","loss: 0.44391, smth: 0.41014:  50%|█████     | 4/8 [00:06<00:07,  1.82s/it]\u001b[A\n","loss: 0.42391, smth: 0.41289:  50%|█████     | 4/8 [00:07<00:07,  1.82s/it]\u001b[A\n","loss: 0.42391, smth: 0.41289:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.42511, smth: 0.41493:  62%|██████▎   | 5/8 [00:08<00:04,  1.50s/it]\u001b[A\n","loss: 0.42511, smth: 0.41493:  75%|███████▌  | 6/8 [00:08<00:03,  1.57s/it]\u001b[A\n","loss: 0.47938, smth: 0.42414:  75%|███████▌  | 6/8 [00:09<00:03,  1.57s/it]\u001b[A\n","loss: 0.47938, smth: 0.42414:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.69519, smth: 0.45802:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.69519, smth: 0.45802: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:17 2021 Epoch 48, lr: 0.0045479, train loss: 0.45802, train auc: 0.98478, val loss: 0.41478, val auc: 0.98371\n","Thu Jun 10 01:10:17 2021 Epoch: 49\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41023, smth: 0.41023:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41023, smth: 0.41023:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.28452, smth: 0.34738:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.28452, smth: 0.34738:  25%|██▌       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.62837, smth: 0.44104:  25%|██▌       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.62837, smth: 0.44104:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.48653, smth: 0.45241:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.48653, smth: 0.45241:  50%|█████     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.39521, smth: 0.44097:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.39521, smth: 0.44097:  62%|██████▎   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.47725, smth: 0.44702:  62%|██████▎   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.47725, smth: 0.44702:  75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.43763, smth: 0.44568:  75%|███████▌  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.43763, smth: 0.44568:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.44915, smth: 0.44611:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.44915, smth: 0.44611: 100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:27 2021 Epoch 49, lr: 0.0045225, train loss: 0.44611, train auc: 0.98579, val loss: 0.32182, val auc: 0.98827\n","Thu Jun 10 01:10:27 2021 Epoch: 50\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35260, smth: 0.35260:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35260, smth: 0.35260:  12%|█▎        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.46032, smth: 0.40646:  12%|█▎        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.46032, smth: 0.40646:  25%|██▌       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.36312, smth: 0.39202:  25%|██▌       | 2/8 [00:05<00:13,  2.26s/it]\u001b[A\n","loss: 0.36312, smth: 0.39202:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.31467, smth: 0.37268:  38%|███▊      | 3/8 [00:06<00:10,  2.11s/it]\u001b[A\n","loss: 0.31467, smth: 0.37268:  50%|█████     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.70601, smth: 0.43934:  50%|█████     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.70601, smth: 0.43934:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.48598, smth: 0.44712:  62%|██████▎   | 5/8 [00:08<00:04,  1.57s/it]\u001b[A\n","loss: 0.48598, smth: 0.44712:  75%|███████▌  | 6/8 [00:08<00:02,  1.45s/it]\u001b[A\n","loss: 0.32002, smth: 0.42896:  75%|███████▌  | 6/8 [00:09<00:02,  1.45s/it]\u001b[A\n","loss: 0.32002, smth: 0.42896:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 1.08231, smth: 0.51063:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 1.08231, smth: 0.51063: 100%|██████████| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:38 2021 Epoch 50, lr: 0.0044966, train loss: 0.51063, train auc: 0.98685, val loss: 0.47187, val auc: 0.98069\n","Thu Jun 10 01:10:38 2021 Epoch: 51\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41488, smth: 0.41488:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41488, smth: 0.41488:  12%|█▎        | 1/8 [00:02<00:17,  2.57s/it]\u001b[A\n","loss: 0.43223, smth: 0.42355:  12%|█▎        | 1/8 [00:03<00:17,  2.57s/it]\u001b[A\n","loss: 0.43223, smth: 0.42355:  25%|██▌       | 2/8 [00:03<00:12,  2.02s/it]\u001b[A\n","loss: 0.45458, smth: 0.43390:  25%|██▌       | 2/8 [00:04<00:12,  2.02s/it]\u001b[A\n","loss: 0.45458, smth: 0.43390:  38%|███▊      | 3/8 [00:04<00:09,  1.80s/it]\u001b[A\n","loss: 0.47550, smth: 0.44430:  38%|███▊      | 3/8 [00:05<00:09,  1.80s/it]\u001b[A\n","loss: 0.47550, smth: 0.44430:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.75071, smth: 0.50558:  50%|█████     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.75071, smth: 0.50558:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.43639, smth: 0.49405:  62%|██████▎   | 5/8 [00:08<00:04,  1.55s/it]\u001b[A\n","loss: 0.43639, smth: 0.49405:  75%|███████▌  | 6/8 [00:08<00:02,  1.46s/it]\u001b[A\n","loss: 1.16308, smth: 0.58963:  75%|███████▌  | 6/8 [00:09<00:02,  1.46s/it]\u001b[A\n","loss: 1.16308, smth: 0.58963:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.35417, smth: 0.56019:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.35417, smth: 0.56019: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.83it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:48 2021 Epoch 51, lr: 0.0044700, train loss: 0.56019, train auc: 0.97159, val loss: 0.46546, val auc: 0.97663\n","Thu Jun 10 01:10:48 2021 Epoch: 52\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.08114, smth: 1.08114:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 1.08114, smth: 1.08114:  12%|█▎        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.79922, smth: 0.94018:  12%|█▎        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.79922, smth: 0.94018:  25%|██▌       | 2/8 [00:04<00:14,  2.50s/it]\u001b[A\n","loss: 0.82479, smth: 0.90172:  25%|██▌       | 2/8 [00:05<00:14,  2.50s/it]\u001b[A\n","loss: 0.82479, smth: 0.90172:  38%|███▊      | 3/8 [00:05<00:11,  2.30s/it]\u001b[A\n","loss: 0.70118, smth: 0.85158:  38%|███▊      | 3/8 [00:06<00:11,  2.30s/it]\u001b[A\n","loss: 0.70118, smth: 0.85158:  50%|█████     | 4/8 [00:06<00:07,  1.85s/it]\u001b[A\n","loss: 0.60968, smth: 0.80320:  50%|█████     | 4/8 [00:08<00:07,  1.85s/it]\u001b[A\n","loss: 0.60968, smth: 0.80320:  62%|██████▎   | 5/8 [00:08<00:05,  1.87s/it]\u001b[A\n","loss: 0.37916, smth: 0.73253:  62%|██████▎   | 5/8 [00:09<00:05,  1.87s/it]\u001b[A\n","loss: 0.37916, smth: 0.73253:  75%|███████▌  | 6/8 [00:09<00:03,  1.51s/it]\u001b[A\n","loss: 0.59672, smth: 0.71313:  75%|███████▌  | 6/8 [00:09<00:03,  1.51s/it]\u001b[A\n","loss: 0.59672, smth: 0.71313:  88%|████████▊ | 7/8 [00:10<00:01,  1.28s/it]\u001b[A\n","loss: 0.59224, smth: 0.69802:  88%|████████▊ | 7/8 [00:10<00:01,  1.28s/it]\u001b[A\n","loss: 0.59224, smth: 0.69802: 100%|██████████| 8/8 [00:10<00:00,  1.31s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.88it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:59 2021 Epoch 52, lr: 0.0044429, train loss: 0.69802, train auc: 0.97311, val loss: 1.17827, val auc: 0.96225\n","Thu Jun 10 01:10:59 2021 Epoch: 53\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35566, smth: 0.35566:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35566, smth: 0.35566:  12%|█▎        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.56477, smth: 0.46021:  12%|█▎        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.56477, smth: 0.46021:  25%|██▌       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.42288, smth: 0.44777:  25%|██▌       | 2/8 [00:04<00:13,  2.30s/it]\u001b[A\n","loss: 0.42288, smth: 0.44777:  38%|███▊      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.45602, smth: 0.44983:  38%|███▊      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.45602, smth: 0.44983:  50%|█████     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.43656, smth: 0.44718:  50%|█████     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.43656, smth: 0.44718:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.54249, smth: 0.46306:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.54249, smth: 0.46306:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.51072, smth: 0.46987:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.51072, smth: 0.46987:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.55465, smth: 0.48047:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.55465, smth: 0.48047: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:09 2021 Epoch 53, lr: 0.0044151, train loss: 0.48047, train auc: 0.97955, val loss: 0.36064, val auc: 0.98842\n","Thu Jun 10 01:11:09 2021 Epoch: 54\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35618, smth: 0.35618:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35618, smth: 0.35618:  12%|█▎        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.35401, smth: 0.35509:  12%|█▎        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.35401, smth: 0.35509:  25%|██▌       | 2/8 [00:03<00:14,  2.37s/it]\u001b[A\n","loss: 0.74772, smth: 0.48597:  25%|██▌       | 2/8 [00:05<00:14,  2.37s/it]\u001b[A\n","loss: 0.74772, smth: 0.48597:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.42305, smth: 0.47024:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.42305, smth: 0.47024:  50%|█████     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.44703, smth: 0.46560:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.44703, smth: 0.46560:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.44817, smth: 0.46269:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.44817, smth: 0.46269:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.40239, smth: 0.45408:  75%|███████▌  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.40239, smth: 0.45408:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.33413, smth: 0.43908:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.33413, smth: 0.43908: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:19 2021 Epoch 54, lr: 0.0043868, train loss: 0.43908, train auc: 0.97883, val loss: 0.30868, val auc: 0.98852\n","Thu Jun 10 01:11:19 2021 Epoch: 55\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.45369, smth: 0.45369:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.45369, smth: 0.45369:  12%|█▎        | 1/8 [00:02<00:20,  2.87s/it]\u001b[A\n","loss: 0.34131, smth: 0.39750:  12%|█▎        | 1/8 [00:03<00:20,  2.87s/it]\u001b[A\n","loss: 0.34131, smth: 0.39750:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.29795, smth: 0.36432:  25%|██▌       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.29795, smth: 0.36432:  38%|███▊      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.35227, smth: 0.36131:  38%|███▊      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.35227, smth: 0.36131:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.27857, smth: 0.34476:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.27857, smth: 0.34476:  62%|██████▎   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.49207, smth: 0.36931:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.49207, smth: 0.36931:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.54158, smth: 0.39392:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.54158, smth: 0.39392:  88%|████████▊ | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.57412, smth: 0.41645:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.57412, smth: 0.41645: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.23it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.90it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:29 2021 Epoch 55, lr: 0.0043579, train loss: 0.41645, train auc: 0.97957, val loss: 0.63003, val auc: 0.98146\n","Thu Jun 10 01:11:29 2021 Epoch: 56\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.34370, smth: 0.34370:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.34370, smth: 0.34370:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.38052, smth: 0.36211:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.38052, smth: 0.36211:  25%|██▌       | 2/8 [00:03<00:14,  2.48s/it]\u001b[A\n","loss: 0.34465, smth: 0.35629:  25%|██▌       | 2/8 [00:05<00:14,  2.48s/it]\u001b[A\n","loss: 0.34465, smth: 0.35629:  38%|███▊      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.38465, smth: 0.36338:  38%|███▊      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.38465, smth: 0.36338:  50%|█████     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.38750, smth: 0.36820:  50%|█████     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.38750, smth: 0.36820:  62%|██████▎   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.42048, smth: 0.37692:  62%|██████▎   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.42048, smth: 0.37692:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.38534, smth: 0.37812:  75%|███████▌  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.38534, smth: 0.37812:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.41672, smth: 0.38294:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.41672, smth: 0.38294: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.86it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:39 2021 Epoch 56, lr: 0.0043284, train loss: 0.38294, train auc: 0.98471, val loss: 0.77396, val auc: 0.98202\n","Thu Jun 10 01:11:39 2021 Epoch: 57\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.36262, smth: 0.36262:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.36262, smth: 0.36262:  12%|█▎        | 1/8 [00:02<00:20,  2.97s/it]\u001b[A\n","loss: 0.28325, smth: 0.32293:  12%|█▎        | 1/8 [00:03<00:20,  2.97s/it]\u001b[A\n","loss: 0.28325, smth: 0.32293:  25%|██▌       | 2/8 [00:03<00:13,  2.31s/it]\u001b[A\n","loss: 0.39231, smth: 0.34606:  25%|██▌       | 2/8 [00:05<00:13,  2.31s/it]\u001b[A\n","loss: 0.39231, smth: 0.34606:  38%|███▊      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.43354, smth: 0.36793:  38%|███▊      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.43354, smth: 0.36793:  50%|█████     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.38575, smth: 0.37149:  50%|█████     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.38575, smth: 0.37149:  62%|██████▎   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.41630, smth: 0.37896:  62%|██████▎   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.41630, smth: 0.37896:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.37985, smth: 0.37909:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.37985, smth: 0.37909:  88%|████████▊ | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.47972, smth: 0.39167:  88%|████████▊ | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.47972, smth: 0.39167: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.88it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:49 2021 Epoch 57, lr: 0.0042983, train loss: 0.39167, train auc: 0.98404, val loss: 0.54262, val auc: 0.98141\n","Thu Jun 10 01:11:49 2021 Epoch: 58\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32854, smth: 0.32854:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32854, smth: 0.32854:  12%|█▎        | 1/8 [00:02<00:20,  2.93s/it]\u001b[A\n","loss: 0.39828, smth: 0.36341:  12%|█▎        | 1/8 [00:03<00:20,  2.93s/it]\u001b[A\n","loss: 0.39828, smth: 0.36341:  25%|██▌       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.27161, smth: 0.33281:  25%|██▌       | 2/8 [00:04<00:13,  2.30s/it]\u001b[A\n","loss: 0.27161, smth: 0.33281:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.37382, smth: 0.34306:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.37382, smth: 0.34306:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.40443, smth: 0.35534:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.40443, smth: 0.35534:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.62073, smth: 0.39957:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.62073, smth: 0.39957:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.34839, smth: 0.39226:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.34839, smth: 0.39226:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.38244, smth: 0.39103:  88%|████████▊ | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 0.38244, smth: 0.39103: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.87it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:59 2021 Epoch 58, lr: 0.0042678, train loss: 0.39103, train auc: 0.98464, val loss: 0.42086, val auc: 0.98311\n","Thu Jun 10 01:11:59 2021 Epoch: 59\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35786, smth: 0.35786:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35786, smth: 0.35786:  12%|█▎        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.38031, smth: 0.36909:  12%|█▎        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.38031, smth: 0.36909:  25%|██▌       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.35052, smth: 0.36290:  25%|██▌       | 2/8 [00:04<00:12,  2.15s/it]\u001b[A\n","loss: 0.35052, smth: 0.36290:  38%|███▊      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.32333, smth: 0.35301:  38%|███▊      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.32333, smth: 0.35301:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.49073, smth: 0.38055:  50%|█████     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.49073, smth: 0.38055:  62%|██████▎   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.45310, smth: 0.39264:  62%|██████▎   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.45310, smth: 0.39264:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.33517, smth: 0.38443:  75%|███████▌  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.33517, smth: 0.38443:  88%|████████▊ | 7/8 [00:09<00:01,  1.38s/it]\u001b[A\n","loss: 0.40925, smth: 0.38753:  88%|████████▊ | 7/8 [00:09<00:01,  1.38s/it]\u001b[A\n","loss: 0.40925, smth: 0.38753: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:10 2021 Epoch 59, lr: 0.0042366, train loss: 0.38753, train auc: 0.98353, val loss: 0.48069, val auc: 0.98381\n","Thu Jun 10 01:12:10 2021 Epoch: 60\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.33926, smth: 0.33926:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.33926, smth: 0.33926:  12%|█▎        | 1/8 [00:02<00:20,  2.87s/it]\u001b[A\n","loss: 0.37131, smth: 0.35528:  12%|█▎        | 1/8 [00:03<00:20,  2.87s/it]\u001b[A\n","loss: 0.37131, smth: 0.35528:  25%|██▌       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.39996, smth: 0.37017:  25%|██▌       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.39996, smth: 0.37017:  38%|███▊      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.43791, smth: 0.38711:  38%|███▊      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.43791, smth: 0.38711:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.33771, smth: 0.37723:  50%|█████     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.33771, smth: 0.37723:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.30336, smth: 0.36492:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.30336, smth: 0.36492:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.42329, smth: 0.37326:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.42329, smth: 0.37326:  88%|████████▊ | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.63144, smth: 0.40553:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.63144, smth: 0.40553: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:20 2021 Epoch 60, lr: 0.0042050, train loss: 0.40553, train auc: 0.97838, val loss: 0.32776, val auc: 0.98821\n","Thu Jun 10 01:12:20 2021 Epoch: 61\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.58762, smth: 0.58762:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.58762, smth: 0.58762:  12%|█▎        | 1/8 [00:02<00:20,  2.94s/it]\u001b[A\n","loss: 0.48206, smth: 0.53484:  12%|█▎        | 1/8 [00:03<00:20,  2.94s/it]\u001b[A\n","loss: 0.48206, smth: 0.53484:  25%|██▌       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.50358, smth: 0.52442:  25%|██▌       | 2/8 [00:05<00:13,  2.30s/it]\u001b[A\n","loss: 0.50358, smth: 0.52442:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.38509, smth: 0.48959:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.38509, smth: 0.48959:  50%|█████     | 4/8 [00:05<00:06,  1.66s/it]\u001b[A\n","loss: 0.37829, smth: 0.46733:  50%|█████     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.37829, smth: 0.46733:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.61461, smth: 0.49188:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.61461, smth: 0.49188:  75%|███████▌  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.39839, smth: 0.47852:  75%|███████▌  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.39839, smth: 0.47852:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.44957, smth: 0.47490:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.44957, smth: 0.47490: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:30 2021 Epoch 61, lr: 0.0041728, train loss: 0.47490, train auc: 0.98307, val loss: 0.36773, val auc: 0.98679\n","Thu Jun 10 01:12:30 2021 Epoch: 62\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31787, smth: 0.31787:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31787, smth: 0.31787:  12%|█▎        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.55577, smth: 0.43682:  12%|█▎        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.55577, smth: 0.43682:  25%|██▌       | 2/8 [00:03<00:14,  2.38s/it]\u001b[A\n","loss: 0.40543, smth: 0.42635:  25%|██▌       | 2/8 [00:05<00:14,  2.38s/it]\u001b[A\n","loss: 0.40543, smth: 0.42635:  38%|███▊      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.39612, smth: 0.41879:  38%|███▊      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.39612, smth: 0.41879:  50%|█████     | 4/8 [00:06<00:06,  1.72s/it]\u001b[A\n","loss: 0.45569, smth: 0.42617:  50%|█████     | 4/8 [00:07<00:06,  1.72s/it]\u001b[A\n","loss: 0.45569, smth: 0.42617:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.29656, smth: 0.40457:  62%|██████▎   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.29656, smth: 0.40457:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.44004, smth: 0.40964:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.44004, smth: 0.40964:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.43396, smth: 0.41268:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.43396, smth: 0.41268: 100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:40 2021 Epoch 62, lr: 0.0041401, train loss: 0.41268, train auc: 0.98585, val loss: 0.25020, val auc: 0.99023\n","Thu Jun 10 01:12:40 2021 Epoch: 63\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.38927, smth: 0.38927:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.38927, smth: 0.38927:  12%|█▎        | 1/8 [00:03<00:21,  3.10s/it]\u001b[A\n","loss: 0.22422, smth: 0.30675:  12%|█▎        | 1/8 [00:03<00:21,  3.10s/it]\u001b[A\n","loss: 0.22422, smth: 0.30675:  25%|██▌       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.43279, smth: 0.34876:  25%|██▌       | 2/8 [00:05<00:14,  2.40s/it]\u001b[A\n","loss: 0.43279, smth: 0.34876:  38%|███▊      | 3/8 [00:05<00:10,  2.16s/it]\u001b[A\n","loss: 0.31866, smth: 0.34124:  38%|███▊      | 3/8 [00:06<00:10,  2.16s/it]\u001b[A\n","loss: 0.31866, smth: 0.34124:  50%|█████     | 4/8 [00:06<00:06,  1.74s/it]\u001b[A\n","loss: 0.50533, smth: 0.37406:  50%|█████     | 4/8 [00:07<00:06,  1.74s/it]\u001b[A\n","loss: 0.50533, smth: 0.37406:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.37552, smth: 0.37430:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.37552, smth: 0.37430:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.55740, smth: 0.40046:  75%|███████▌  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.55740, smth: 0.40046:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.27109, smth: 0.38429:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.27109, smth: 0.38429: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.82it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:50 2021 Epoch 63, lr: 0.0041070, train loss: 0.38429, train auc: 0.98857, val loss: 0.31024, val auc: 0.98815\n","Thu Jun 10 01:12:50 2021 Epoch: 64\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29140, smth: 0.29140:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29140, smth: 0.29140:  12%|█▎        | 1/8 [00:02<00:20,  2.93s/it]\u001b[A\n","loss: 0.29271, smth: 0.29206:  12%|█▎        | 1/8 [00:03<00:20,  2.93s/it]\u001b[A\n","loss: 0.29271, smth: 0.29206:  25%|██▌       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.54786, smth: 0.37732:  25%|██▌       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.54786, smth: 0.37732:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.43329, smth: 0.39132:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.43329, smth: 0.39132:  50%|█████     | 4/8 [00:05<00:06,  1.66s/it]\u001b[A\n","loss: 0.40352, smth: 0.39376:  50%|█████     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.40352, smth: 0.39376:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.34358, smth: 0.38539:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.34358, smth: 0.38539:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.41097, smth: 0.38905:  75%|███████▌  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.41097, smth: 0.38905:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.60652, smth: 0.41623:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.60652, smth: 0.41623: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.89it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:00 2021 Epoch 64, lr: 0.0040733, train loss: 0.41623, train auc: 0.98596, val loss: 0.26519, val auc: 0.99027\n","Thu Jun 10 01:13:00 2021 Epoch: 65\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.38637, smth: 0.38637:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.38637, smth: 0.38637:  12%|█▎        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.51473, smth: 0.45055:  12%|█▎        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.51473, smth: 0.45055:  25%|██▌       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.23113, smth: 0.37741:  25%|██▌       | 2/8 [00:04<00:12,  2.07s/it]\u001b[A\n","loss: 0.23113, smth: 0.37741:  38%|███▊      | 3/8 [00:04<00:08,  1.80s/it]\u001b[A\n","loss: 0.40215, smth: 0.38360:  38%|███▊      | 3/8 [00:05<00:08,  1.80s/it]\u001b[A\n","loss: 0.40215, smth: 0.38360:  50%|█████     | 4/8 [00:05<00:06,  1.65s/it]\u001b[A\n","loss: 0.27604, smth: 0.36209:  50%|█████     | 4/8 [00:06<00:06,  1.65s/it]\u001b[A\n","loss: 0.27604, smth: 0.36209:  62%|██████▎   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.40562, smth: 0.36934:  62%|██████▎   | 5/8 [00:08<00:04,  1.39s/it]\u001b[A\n","loss: 0.40562, smth: 0.36934:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.38972, smth: 0.37225:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.38972, smth: 0.37225:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.25996, smth: 0.35822:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.25996, smth: 0.35822: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.85it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:10 2021 Epoch 65, lr: 0.0040392, train loss: 0.35822, train auc: 0.98708, val loss: 0.33476, val auc: 0.98923\n","Thu Jun 10 01:13:10 2021 Epoch: 66\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.42147, smth: 0.42147:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.42147, smth: 0.42147:  12%|█▎        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.30152, smth: 0.36150:  12%|█▎        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.30152, smth: 0.36150:  25%|██▌       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.20899, smth: 0.31066:  25%|██▌       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.20899, smth: 0.31066:  38%|███▊      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.20342, smth: 0.28385:  38%|███▊      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.20342, smth: 0.28385:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.28310, smth: 0.28370:  50%|█████     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.28310, smth: 0.28370:  62%|██████▎   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.53975, smth: 0.32637:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.53975, smth: 0.32637:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.51716, smth: 0.35363:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.51716, smth: 0.35363:  88%|████████▊ | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.35432, smth: 0.35372:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.35432, smth: 0.35372: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:20 2021 Epoch 66, lr: 0.0040045, train loss: 0.35372, train auc: 0.98562, val loss: 0.33359, val auc: 0.98802\n","Thu Jun 10 01:13:20 2021 Epoch: 67\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32693, smth: 0.32693:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32693, smth: 0.32693:  12%|█▎        | 1/8 [00:02<00:19,  2.83s/it]\u001b[A\n","loss: 0.45221, smth: 0.38957:  12%|█▎        | 1/8 [00:03<00:19,  2.83s/it]\u001b[A\n","loss: 0.45221, smth: 0.38957:  25%|██▌       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.31738, smth: 0.36551:  25%|██▌       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.31738, smth: 0.36551:  38%|███▊      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.39472, smth: 0.37281:  38%|███▊      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.39472, smth: 0.37281:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.27039, smth: 0.35233:  50%|█████     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.27039, smth: 0.35233:  62%|██████▎   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.32161, smth: 0.34721:  62%|██████▎   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.32161, smth: 0.34721:  75%|███████▌  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.32059, smth: 0.34340:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.32059, smth: 0.34340:  88%|████████▊ | 7/8 [00:08<00:01,  1.24s/it]\u001b[A\n","loss: 0.39724, smth: 0.35013:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.39724, smth: 0.35013: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.19it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.90it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:30 2021 Epoch 67, lr: 0.0039695, train loss: 0.35013, train auc: 0.98743, val loss: 0.34121, val auc: 0.98830\n","Thu Jun 10 01:13:30 2021 Epoch: 68\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35040, smth: 0.35040:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35040, smth: 0.35040:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.23769, smth: 0.29404:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.23769, smth: 0.29404:  25%|██▌       | 2/8 [00:03<00:13,  2.33s/it]\u001b[A\n","loss: 0.38234, smth: 0.32348:  25%|██▌       | 2/8 [00:04<00:13,  2.33s/it]\u001b[A\n","loss: 0.38234, smth: 0.32348:  38%|███▊      | 3/8 [00:04<00:09,  2.00s/it]\u001b[A\n","loss: 0.29261, smth: 0.31576:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.29261, smth: 0.31576:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.23325, smth: 0.29926:  50%|█████     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.23325, smth: 0.29926:  62%|██████▎   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.38526, smth: 0.31359:  62%|██████▎   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.38526, smth: 0.31359:  75%|███████▌  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.29838, smth: 0.31142:  75%|███████▌  | 6/8 [00:09<00:02,  1.41s/it]\u001b[A\n","loss: 0.29838, smth: 0.31142:  88%|████████▊ | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.24716, smth: 0.30339:  88%|████████▊ | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.24716, smth: 0.30339: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:41 2021 Epoch 68, lr: 0.0039339, train loss: 0.30339, train auc: 0.99137, val loss: 0.41275, val auc: 0.98483\n","Thu Jun 10 01:13:41 2021 Epoch: 69\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.38381, smth: 0.38381:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.38381, smth: 0.38381:  12%|█▎        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.31847, smth: 0.35114:  12%|█▎        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.31847, smth: 0.35114:  25%|██▌       | 2/8 [00:03<00:14,  2.47s/it]\u001b[A\n","loss: 0.35914, smth: 0.35381:  25%|██▌       | 2/8 [00:05<00:14,  2.47s/it]\u001b[A\n","loss: 0.35914, smth: 0.35381:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.28823, smth: 0.33741:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.28823, smth: 0.33741:  50%|█████     | 4/8 [00:05<00:06,  1.71s/it]\u001b[A\n","loss: 0.31162, smth: 0.33225:  50%|█████     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.31162, smth: 0.33225:  62%|██████▎   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.31219, smth: 0.32891:  62%|██████▎   | 5/8 [00:08<00:04,  1.61s/it]\u001b[A\n","loss: 0.31219, smth: 0.32891:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.25189, smth: 0.31791:  75%|███████▌  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.25189, smth: 0.31791:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.24397, smth: 0.30867:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.24397, smth: 0.30867: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:51 2021 Epoch 69, lr: 0.0038980, train loss: 0.30867, train auc: 0.98983, val loss: 0.36368, val auc: 0.98815\n","Thu Jun 10 01:13:51 2021 Epoch: 70\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26413, smth: 0.26413:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26413, smth: 0.26413:  12%|█▎        | 1/8 [00:03<00:21,  3.05s/it]\u001b[A\n","loss: 0.35278, smth: 0.30846:  12%|█▎        | 1/8 [00:03<00:21,  3.05s/it]\u001b[A\n","loss: 0.35278, smth: 0.30846:  25%|██▌       | 2/8 [00:03<00:14,  2.36s/it]\u001b[A\n","loss: 0.28114, smth: 0.29935:  25%|██▌       | 2/8 [00:05<00:14,  2.36s/it]\u001b[A\n","loss: 0.28114, smth: 0.29935:  38%|███▊      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 0.39077, smth: 0.32220:  38%|███▊      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 0.39077, smth: 0.32220:  50%|█████     | 4/8 [00:06<00:06,  1.75s/it]\u001b[A\n","loss: 0.31098, smth: 0.31996:  50%|█████     | 4/8 [00:07<00:06,  1.75s/it]\u001b[A\n","loss: 0.31098, smth: 0.31996:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.30100, smth: 0.31680:  62%|██████▎   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.30100, smth: 0.31680:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.25725, smth: 0.30829:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.25725, smth: 0.30829:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.26630, smth: 0.30304:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.26630, smth: 0.30304: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:01 2021 Epoch 70, lr: 0.0038616, train loss: 0.30304, train auc: 0.99229, val loss: 0.31160, val auc: 0.98948\n","Thu Jun 10 01:14:01 2021 Epoch: 71\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.37746, smth: 0.37746:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.37746, smth: 0.37746:  12%|█▎        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.27732, smth: 0.32739:  12%|█▎        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.27732, smth: 0.32739:  25%|██▌       | 2/8 [00:03<00:13,  2.28s/it]\u001b[A\n","loss: 0.24409, smth: 0.29963:  25%|██▌       | 2/8 [00:05<00:13,  2.28s/it]\u001b[A\n","loss: 0.24409, smth: 0.29963:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.30925, smth: 0.30203:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.30925, smth: 0.30203:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.49795, smth: 0.34121:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.49795, smth: 0.34121:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.29435, smth: 0.33340:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.29435, smth: 0.33340:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37816, smth: 0.33980:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37816, smth: 0.33980:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.29144, smth: 0.33375:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.29144, smth: 0.33375: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:11 2021 Epoch 71, lr: 0.0038248, train loss: 0.33375, train auc: 0.99154, val loss: 0.33253, val auc: 0.98935\n","Thu Jun 10 01:14:11 2021 Epoch: 72\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.34061, smth: 0.34061:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.34061, smth: 0.34061:  12%|█▎        | 1/8 [00:02<00:15,  2.25s/it]\u001b[A\n","loss: 0.34800, smth: 0.34430:  12%|█▎        | 1/8 [00:02<00:15,  2.25s/it]\u001b[A\n","loss: 0.34800, smth: 0.34430:  25%|██▌       | 2/8 [00:02<00:10,  1.80s/it]\u001b[A\n","loss: 0.33895, smth: 0.34252:  25%|██▌       | 2/8 [00:04<00:10,  1.80s/it]\u001b[A\n","loss: 0.33895, smth: 0.34252:  38%|███▊      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.35556, smth: 0.34578:  38%|███▊      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.35556, smth: 0.34578:  50%|█████     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.38217, smth: 0.35306:  50%|█████     | 4/8 [00:06<00:05,  1.43s/it]\u001b[A\n","loss: 0.38217, smth: 0.35306:  62%|██████▎   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.22246, smth: 0.33129:  62%|██████▎   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.22246, smth: 0.33129:  75%|███████▌  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.22781, smth: 0.31651:  75%|███████▌  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.22781, smth: 0.31651:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.57944, smth: 0.34937:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.57944, smth: 0.34937: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:21 2021 Epoch 72, lr: 0.0037876, train loss: 0.34937, train auc: 0.98839, val loss: 0.34760, val auc: 0.98815\n","Thu Jun 10 01:14:21 2021 Epoch: 73\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28011, smth: 0.28011:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.28011, smth: 0.28011:  12%|█▎        | 1/8 [00:02<00:17,  2.47s/it]\u001b[A\n","loss: 0.30987, smth: 0.29499:  12%|█▎        | 1/8 [00:03<00:17,  2.47s/it]\u001b[A\n","loss: 0.30987, smth: 0.29499:  25%|██▌       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.23332, smth: 0.27443:  25%|██▌       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.23332, smth: 0.27443:  38%|███▊      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.30956, smth: 0.28321:  38%|███▊      | 3/8 [00:06<00:09,  1.82s/it]\u001b[A\n","loss: 0.30956, smth: 0.28321:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.32713, smth: 0.29200:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.32713, smth: 0.29200:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.30099, smth: 0.29350:  62%|██████▎   | 5/8 [00:08<00:04,  1.47s/it]\u001b[A\n","loss: 0.30099, smth: 0.29350:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.28628, smth: 0.29247:  75%|███████▌  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.28628, smth: 0.29247:  88%|████████▊ | 7/8 [00:09<00:01,  1.38s/it]\u001b[A\n","loss: 0.35742, smth: 0.30059:  88%|████████▊ | 7/8 [00:10<00:01,  1.38s/it]\u001b[A\n","loss: 0.35742, smth: 0.30059: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.61it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:32 2021 Epoch 73, lr: 0.0037500, train loss: 0.30059, train auc: 0.98977, val loss: 0.46137, val auc: 0.98320\n","Thu Jun 10 01:14:32 2021 Epoch: 74\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.27149, smth: 0.27149:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.27149, smth: 0.27149:  12%|█▎        | 1/8 [00:02<00:19,  2.79s/it]\u001b[A\n","loss: 0.25071, smth: 0.26110:  12%|█▎        | 1/8 [00:03<00:19,  2.79s/it]\u001b[A\n","loss: 0.25071, smth: 0.26110:  25%|██▌       | 2/8 [00:03<00:12,  2.16s/it]\u001b[A\n","loss: 0.26708, smth: 0.26309:  25%|██▌       | 2/8 [00:04<00:12,  2.16s/it]\u001b[A\n","loss: 0.26708, smth: 0.26309:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.28246, smth: 0.26793:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.28246, smth: 0.26793:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.36044, smth: 0.28643:  50%|█████     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.36044, smth: 0.28643:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.20114, smth: 0.27222:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.20114, smth: 0.27222:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.21448, smth: 0.26397:  75%|███████▌  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.21448, smth: 0.26397:  88%|████████▊ | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.50536, smth: 0.29414:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.50536, smth: 0.29414: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:42 2021 Epoch 74, lr: 0.0037120, train loss: 0.29414, train auc: 0.99306, val loss: 0.42634, val auc: 0.98384\n","Thu Jun 10 01:14:42 2021 Epoch: 75\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.43287, smth: 0.43287:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.43287, smth: 0.43287:  12%|█▎        | 1/8 [00:02<00:18,  2.69s/it]\u001b[A\n","loss: 0.27501, smth: 0.35394:  12%|█▎        | 1/8 [00:03<00:18,  2.69s/it]\u001b[A\n","loss: 0.27501, smth: 0.35394:  25%|██▌       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.34316, smth: 0.35035:  25%|██▌       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.34316, smth: 0.35035:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.33205, smth: 0.34578:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.33205, smth: 0.34578:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.28351, smth: 0.33332:  50%|█████     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.28351, smth: 0.33332:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.38134, smth: 0.34133:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.38134, smth: 0.34133:  75%|███████▌  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.26063, smth: 0.32980:  75%|███████▌  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.26063, smth: 0.32980:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.44396, smth: 0.34407:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.44396, smth: 0.34407: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:52 2021 Epoch 75, lr: 0.0036737, train loss: 0.34407, train auc: 0.99131, val loss: 0.44194, val auc: 0.98610\n","Thu Jun 10 01:14:52 2021 Epoch: 76\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29371, smth: 0.29371:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29371, smth: 0.29371:  12%|█▎        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.40221, smth: 0.34796:  12%|█▎        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.40221, smth: 0.34796:  25%|██▌       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.26166, smth: 0.31919:  25%|██▌       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.26166, smth: 0.31919:  38%|███▊      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.26966, smth: 0.30681:  38%|███▊      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.26966, smth: 0.30681:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.29332, smth: 0.30411:  50%|█████     | 4/8 [00:07<00:06,  1.59s/it]\u001b[A\n","loss: 0.29332, smth: 0.30411:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.44793, smth: 0.32808:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.44793, smth: 0.32808:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.29597, smth: 0.32350:  75%|███████▌  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.29597, smth: 0.32350:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.37687, smth: 0.33017:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.37687, smth: 0.33017: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:02 2021 Epoch 76, lr: 0.0036350, train loss: 0.33017, train auc: 0.98872, val loss: 0.52347, val auc: 0.98264\n","Thu Jun 10 01:15:02 2021 Epoch: 77\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.53455, smth: 0.53455:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.53455, smth: 0.53455:  12%|█▎        | 1/8 [00:02<00:18,  2.58s/it]\u001b[A\n","loss: 0.43580, smth: 0.48518:  12%|█▎        | 1/8 [00:03<00:18,  2.58s/it]\u001b[A\n","loss: 0.43580, smth: 0.48518:  25%|██▌       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.22926, smth: 0.39987:  25%|██▌       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.22926, smth: 0.39987:  38%|███▊      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.31479, smth: 0.37860:  38%|███▊      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.31479, smth: 0.37860:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.33688, smth: 0.37026:  50%|█████     | 4/8 [00:07<00:06,  1.53s/it]\u001b[A\n","loss: 0.33688, smth: 0.37026:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.42202, smth: 0.37888:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.42202, smth: 0.37888:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.35410, smth: 0.37534:  75%|███████▌  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.35410, smth: 0.37534:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.30371, smth: 0.36639:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.30371, smth: 0.36639: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:12 2021 Epoch 77, lr: 0.0035959, train loss: 0.36639, train auc: 0.98808, val loss: 0.61472, val auc: 0.98158\n","Thu Jun 10 01:15:12 2021 Epoch: 78\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29130, smth: 0.29130:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29130, smth: 0.29130:  12%|█▎        | 1/8 [00:02<00:18,  2.66s/it]\u001b[A\n","loss: 0.48550, smth: 0.38840:  12%|█▎        | 1/8 [00:03<00:18,  2.66s/it]\u001b[A\n","loss: 0.48550, smth: 0.38840:  25%|██▌       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.32698, smth: 0.36793:  25%|██▌       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.32698, smth: 0.36793:  38%|███▊      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.41750, smth: 0.38032:  38%|███▊      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.41750, smth: 0.38032:  50%|█████     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.22379, smth: 0.34901:  50%|█████     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A\n","loss: 0.22379, smth: 0.34901:  62%|██████▎   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.35587, smth: 0.35016:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.35587, smth: 0.35016:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.27561, smth: 0.33951:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.27561, smth: 0.33951:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.42737, smth: 0.35049:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.42737, smth: 0.35049: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:22 2021 Epoch 78, lr: 0.0035565, train loss: 0.35049, train auc: 0.99002, val loss: 0.31348, val auc: 0.98881\n","Thu Jun 10 01:15:22 2021 Epoch: 79\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31862, smth: 0.31862:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31862, smth: 0.31862:  12%|█▎        | 1/8 [00:02<00:19,  2.80s/it]\u001b[A\n","loss: 0.28818, smth: 0.30340:  12%|█▎        | 1/8 [00:03<00:19,  2.80s/it]\u001b[A\n","loss: 0.28818, smth: 0.30340:  25%|██▌       | 2/8 [00:03<00:13,  2.17s/it]\u001b[A\n","loss: 0.22643, smth: 0.27774:  25%|██▌       | 2/8 [00:04<00:13,  2.17s/it]\u001b[A\n","loss: 0.22643, smth: 0.27774:  38%|███▊      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.41944, smth: 0.31317:  38%|███▊      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.41944, smth: 0.31317:  50%|█████     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.49187, smth: 0.34891:  50%|█████     | 4/8 [00:07<00:06,  1.57s/it]\u001b[A\n","loss: 0.49187, smth: 0.34891:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.36792, smth: 0.35208:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.36792, smth: 0.35208:  75%|███████▌  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.35941, smth: 0.35312:  75%|███████▌  | 6/8 [00:09<00:02,  1.30s/it]\u001b[A\n","loss: 0.35941, smth: 0.35312:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.53464, smth: 0.37581:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.53464, smth: 0.37581: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:32 2021 Epoch 79, lr: 0.0035168, train loss: 0.37581, train auc: 0.99080, val loss: 0.32500, val auc: 0.98848\n","Thu Jun 10 01:15:32 2021 Epoch: 80\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.33413, smth: 0.33413:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.33413, smth: 0.33413:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.35492, smth: 0.34453:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.35492, smth: 0.34453:  25%|██▌       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.35412, smth: 0.34772:  25%|██▌       | 2/8 [00:05<00:13,  2.22s/it]\u001b[A\n","loss: 0.35412, smth: 0.34772:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.27602, smth: 0.32980:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.27602, smth: 0.32980:  50%|█████     | 4/8 [00:06<00:06,  1.66s/it]\u001b[A\n","loss: 0.26942, smth: 0.31772:  50%|█████     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.26942, smth: 0.31772:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.30309, smth: 0.31528:  62%|██████▎   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.30309, smth: 0.31528:  75%|███████▌  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.24586, smth: 0.30537:  75%|███████▌  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.24586, smth: 0.30537:  88%|████████▊ | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.41226, smth: 0.31873:  88%|████████▊ | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.41226, smth: 0.31873: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:43 2021 Epoch 80, lr: 0.0034768, train loss: 0.31873, train auc: 0.98943, val loss: 0.44224, val auc: 0.98552\n","Thu Jun 10 01:15:43 2021 Epoch: 81\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.37552, smth: 0.37552:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.37552, smth: 0.37552:  12%|█▎        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.28602, smth: 0.33077:  12%|█▎        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.28602, smth: 0.33077:  25%|██▌       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.32894, smth: 0.33016:  25%|██▌       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.32894, smth: 0.33016:  38%|███▊      | 3/8 [00:05<00:10,  2.16s/it]\u001b[A\n","loss: 0.28884, smth: 0.31983:  38%|███▊      | 3/8 [00:06<00:10,  2.16s/it]\u001b[A\n","loss: 0.28884, smth: 0.31983:  50%|█████     | 4/8 [00:06<00:07,  1.75s/it]\u001b[A\n","loss: 0.36707, smth: 0.32928:  50%|█████     | 4/8 [00:07<00:07,  1.75s/it]\u001b[A\n","loss: 0.36707, smth: 0.32928:  62%|██████▎   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.23285, smth: 0.31321:  62%|██████▎   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.23285, smth: 0.31321:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.20439, smth: 0.29766:  75%|███████▌  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.20439, smth: 0.29766:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.17448, smth: 0.28226:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.17448, smth: 0.28226: 100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:53 2021 Epoch 81, lr: 0.0034365, train loss: 0.28226, train auc: 0.98964, val loss: 0.35889, val auc: 0.98781\n","Thu Jun 10 01:15:53 2021 Epoch: 82\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29103, smth: 0.29103:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.29103, smth: 0.29103:  12%|█▎        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.36249, smth: 0.32676:  12%|█▎        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.36249, smth: 0.32676:  25%|██▌       | 2/8 [00:04<00:15,  2.50s/it]\u001b[A\n","loss: 0.30334, smth: 0.31895:  25%|██▌       | 2/8 [00:05<00:15,  2.50s/it]\u001b[A\n","loss: 0.30334, smth: 0.31895:  38%|███▊      | 3/8 [00:05<00:10,  2.16s/it]\u001b[A\n","loss: 0.25322, smth: 0.30252:  38%|███▊      | 3/8 [00:06<00:10,  2.16s/it]\u001b[A\n","loss: 0.25322, smth: 0.30252:  50%|█████     | 4/8 [00:06<00:06,  1.74s/it]\u001b[A\n","loss: 0.24961, smth: 0.29194:  50%|█████     | 4/8 [00:07<00:06,  1.74s/it]\u001b[A\n","loss: 0.24961, smth: 0.29194:  62%|██████▎   | 5/8 [00:07<00:05,  1.68s/it]\u001b[A\n","loss: 0.33063, smth: 0.29839:  62%|██████▎   | 5/8 [00:08<00:05,  1.68s/it]\u001b[A\n","loss: 0.33063, smth: 0.29839:  75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.28402, smth: 0.29633:  75%|███████▌  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.28402, smth: 0.29633:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.44466, smth: 0.31488:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.44466, smth: 0.31488: 100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.21it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.88it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:03 2021 Epoch 82, lr: 0.0033959, train loss: 0.31488, train auc: 0.98898, val loss: 0.36431, val auc: 0.98727\n","Thu Jun 10 01:16:03 2021 Epoch: 83\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21920, smth: 0.21920:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21920, smth: 0.21920:  12%|█▎        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.39845, smth: 0.30882:  12%|█▎        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.39845, smth: 0.30882:  25%|██▌       | 2/8 [00:03<00:13,  2.17s/it]\u001b[A\n","loss: 0.30090, smth: 0.30618:  25%|██▌       | 2/8 [00:04<00:13,  2.17s/it]\u001b[A\n","loss: 0.30090, smth: 0.30618:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.25973, smth: 0.29457:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.25973, smth: 0.29457:  50%|█████     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.25628, smth: 0.28691:  50%|█████     | 4/8 [00:07<00:06,  1.51s/it]\u001b[A\n","loss: 0.25628, smth: 0.28691:  62%|██████▎   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.29579, smth: 0.28839:  62%|██████▎   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.29579, smth: 0.28839:  75%|███████▌  | 6/8 [00:08<00:02,  1.40s/it]\u001b[A\n","loss: 0.30673, smth: 0.29101:  75%|███████▌  | 6/8 [00:09<00:02,  1.40s/it]\u001b[A\n","loss: 0.30673, smth: 0.29101:  88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.27585, smth: 0.28912:  88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.27585, smth: 0.28912: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.83it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:14 2021 Epoch 83, lr: 0.0033551, train loss: 0.28912, train auc: 0.98953, val loss: 0.37633, val auc: 0.98600\n","Thu Jun 10 01:16:14 2021 Epoch: 84\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32016, smth: 0.32016:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.32016, smth: 0.32016:  12%|█▎        | 1/8 [00:03<00:23,  3.38s/it]\u001b[A\n","loss: 0.18367, smth: 0.25191:  12%|█▎        | 1/8 [00:04<00:23,  3.38s/it]\u001b[A\n","loss: 0.18367, smth: 0.25191:  25%|██▌       | 2/8 [00:04<00:15,  2.61s/it]\u001b[A\n","loss: 0.22327, smth: 0.24237:  25%|██▌       | 2/8 [00:05<00:15,  2.61s/it]\u001b[A\n","loss: 0.22327, smth: 0.24237:  38%|███▊      | 3/8 [00:05<00:10,  2.19s/it]\u001b[A\n","loss: 0.27176, smth: 0.24972:  38%|███▊      | 3/8 [00:06<00:10,  2.19s/it]\u001b[A\n","loss: 0.27176, smth: 0.24972:  50%|█████     | 4/8 [00:06<00:07,  1.76s/it]\u001b[A\n","loss: 0.25313, smth: 0.25040:  50%|█████     | 4/8 [00:07<00:07,  1.76s/it]\u001b[A\n","loss: 0.25313, smth: 0.25040:  62%|██████▎   | 5/8 [00:07<00:05,  1.73s/it]\u001b[A\n","loss: 0.32461, smth: 0.26277:  62%|██████▎   | 5/8 [00:08<00:05,  1.73s/it]\u001b[A\n","loss: 0.32461, smth: 0.26277:  75%|███████▌  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.37197, smth: 0.27837:  75%|███████▌  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.37197, smth: 0.27837:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.41121, smth: 0.29497:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.41121, smth: 0.29497: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:24 2021 Epoch 84, lr: 0.0033139, train loss: 0.29497, train auc: 0.99252, val loss: 0.43774, val auc: 0.98476\n","Thu Jun 10 01:16:24 2021 Epoch: 85\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.27250, smth: 0.27250:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.27250, smth: 0.27250:  12%|█▎        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.33062, smth: 0.30156:  12%|█▎        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.33062, smth: 0.30156:  25%|██▌       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.22325, smth: 0.27546:  25%|██▌       | 2/8 [00:05<00:13,  2.30s/it]\u001b[A\n","loss: 0.22325, smth: 0.27546:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.29528, smth: 0.28041:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.29528, smth: 0.28041:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.28557, smth: 0.28144:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.28557, smth: 0.28144:  62%|██████▎   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.90058, smth: 0.38463:  62%|██████▎   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.90058, smth: 0.38463:  75%|███████▌  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.22061, smth: 0.36120:  75%|███████▌  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.22061, smth: 0.36120:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.42338, smth: 0.36897:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.42338, smth: 0.36897: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:35 2021 Epoch 85, lr: 0.0032725, train loss: 0.36897, train auc: 0.99061, val loss: 0.46371, val auc: 0.98377\n","Thu Jun 10 01:16:35 2021 Epoch: 86\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.20548, smth: 0.20548:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.20548, smth: 0.20548:  12%|█▎        | 1/8 [00:02<00:20,  2.97s/it]\u001b[A\n","loss: 0.40021, smth: 0.30285:  12%|█▎        | 1/8 [00:03<00:20,  2.97s/it]\u001b[A\n","loss: 0.40021, smth: 0.30285:  25%|██▌       | 2/8 [00:03<00:13,  2.33s/it]\u001b[A\n","loss: 0.41511, smth: 0.34027:  25%|██▌       | 2/8 [00:05<00:13,  2.33s/it]\u001b[A\n","loss: 0.41511, smth: 0.34027:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.30897, smth: 0.33244:  38%|███▊      | 3/8 [00:06<00:10,  2.11s/it]\u001b[A\n","loss: 0.30897, smth: 0.33244:  50%|█████     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.32566, smth: 0.33109:  50%|█████     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.32566, smth: 0.33109:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.33775, smth: 0.33220:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.33775, smth: 0.33220:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.29837, smth: 0.32736:  75%|███████▌  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.29837, smth: 0.32736:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.54015, smth: 0.35396:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.54015, smth: 0.35396: 100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:45 2021 Epoch 86, lr: 0.0032309, train loss: 0.35396, train auc: 0.98764, val loss: 0.34002, val auc: 0.98673\n","Thu Jun 10 01:16:45 2021 Epoch: 87\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31966, smth: 0.31966:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31966, smth: 0.31966:  12%|█▎        | 1/8 [00:02<00:18,  2.70s/it]\u001b[A\n","loss: 0.49758, smth: 0.40862:  12%|█▎        | 1/8 [00:03<00:18,  2.70s/it]\u001b[A\n","loss: 0.49758, smth: 0.40862:  25%|██▌       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.42661, smth: 0.41461:  25%|██▌       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.42661, smth: 0.41461:  38%|███▊      | 3/8 [00:04<00:09,  1.84s/it]\u001b[A\n","loss: 0.35562, smth: 0.39987:  38%|███▊      | 3/8 [00:05<00:09,  1.84s/it]\u001b[A\n","loss: 0.35562, smth: 0.39987:  50%|█████     | 4/8 [00:05<00:06,  1.50s/it]\u001b[A\n","loss: 0.34685, smth: 0.38926:  50%|█████     | 4/8 [00:06<00:06,  1.50s/it]\u001b[A\n","loss: 0.34685, smth: 0.38926:  62%|██████▎   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.44754, smth: 0.39898:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.44754, smth: 0.39898:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.40574, smth: 0.39994:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.40574, smth: 0.39994:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.30199, smth: 0.38770:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.30199, smth: 0.38770: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:55 2021 Epoch 87, lr: 0.0031891, train loss: 0.38770, train auc: 0.98297, val loss: 0.44487, val auc: 0.98340\n","Thu Jun 10 01:16:55 2021 Epoch: 88\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.36232, smth: 0.36232:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.36232, smth: 0.36232:  12%|█▎        | 1/8 [00:02<00:20,  2.86s/it]\u001b[A\n","loss: 0.35418, smth: 0.35825:  12%|█▎        | 1/8 [00:03<00:20,  2.86s/it]\u001b[A\n","loss: 0.35418, smth: 0.35825:  25%|██▌       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.26022, smth: 0.32557:  25%|██▌       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.26022, smth: 0.32557:  38%|███▊      | 3/8 [00:04<00:09,  1.97s/it]\u001b[A\n","loss: 0.35425, smth: 0.33274:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.35425, smth: 0.33274:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.45363, smth: 0.35692:  50%|█████     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.45363, smth: 0.35692:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.51508, smth: 0.38328:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.51508, smth: 0.38328:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.32114, smth: 0.37440:  75%|███████▌  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.32114, smth: 0.37440:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.39343, smth: 0.37678:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.39343, smth: 0.37678: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.89it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:05 2021 Epoch 88, lr: 0.0031470, train loss: 0.37678, train auc: 0.98493, val loss: 0.38338, val auc: 0.98529\n","Thu Jun 10 01:17:05 2021 Epoch: 89\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26417, smth: 0.26417:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26417, smth: 0.26417:  12%|█▎        | 1/8 [00:02<00:20,  2.93s/it]\u001b[A\n","loss: 0.36304, smth: 0.31361:  12%|█▎        | 1/8 [00:03<00:20,  2.93s/it]\u001b[A\n","loss: 0.36304, smth: 0.31361:  25%|██▌       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.47045, smth: 0.36589:  25%|██▌       | 2/8 [00:04<00:13,  2.29s/it]\u001b[A\n","loss: 0.47045, smth: 0.36589:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32201, smth: 0.35492:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32201, smth: 0.35492:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.36350, smth: 0.35664:  50%|█████     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 0.36350, smth: 0.35664:  62%|██████▎   | 5/8 [00:06<00:04,  1.44s/it]\u001b[A\n","loss: 0.31581, smth: 0.34983:  62%|██████▎   | 5/8 [00:08<00:04,  1.44s/it]\u001b[A\n","loss: 0.31581, smth: 0.34983:  75%|███████▌  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.24503, smth: 0.33486:  75%|███████▌  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.24503, smth: 0.33486:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.32469, smth: 0.33359:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.32469, smth: 0.33359: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:15 2021 Epoch 89, lr: 0.0031048, train loss: 0.33359, train auc: 0.99094, val loss: 0.44288, val auc: 0.98394\n","Thu Jun 10 01:17:15 2021 Epoch: 90\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18949, smth: 0.18949:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18949, smth: 0.18949:  12%|█▎        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.31447, smth: 0.25198:  12%|█▎        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.31447, smth: 0.25198:  25%|██▌       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.34252, smth: 0.28216:  25%|██▌       | 2/8 [00:05<00:12,  2.07s/it]\u001b[A\n","loss: 0.34252, smth: 0.28216:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.34623, smth: 0.29818:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.34623, smth: 0.29818:  50%|█████     | 4/8 [00:06<00:06,  1.63s/it]\u001b[A\n","loss: 0.33756, smth: 0.30605:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.33756, smth: 0.30605:  62%|██████▎   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.32944, smth: 0.30995:  62%|██████▎   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.32944, smth: 0.30995:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.27152, smth: 0.30446:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.27152, smth: 0.30446:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.18043, smth: 0.28896:  88%|████████▊ | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 0.18043, smth: 0.28896: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:25 2021 Epoch 90, lr: 0.0030624, train loss: 0.28896, train auc: 0.99185, val loss: 0.38933, val auc: 0.98596\n","Thu Jun 10 01:17:25 2021 Epoch: 91\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35741, smth: 0.35741:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35741, smth: 0.35741:  12%|█▎        | 1/8 [00:02<00:19,  2.83s/it]\u001b[A\n","loss: 0.40308, smth: 0.38025:  12%|█▎        | 1/8 [00:03<00:19,  2.83s/it]\u001b[A\n","loss: 0.40308, smth: 0.38025:  25%|██▌       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.29537, smth: 0.35195:  25%|██▌       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.29537, smth: 0.35195:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.25016, smth: 0.32651:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.25016, smth: 0.32651:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.24197, smth: 0.30960:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.24197, smth: 0.30960:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.28166, smth: 0.30494:  62%|██████▎   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.28166, smth: 0.30494:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.31095, smth: 0.30580:  75%|███████▌  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.31095, smth: 0.30580:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26206, smth: 0.30033:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26206, smth: 0.30033: 100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:35 2021 Epoch 91, lr: 0.0030198, train loss: 0.30033, train auc: 0.99085, val loss: 0.42208, val auc: 0.98433\n","Thu Jun 10 01:17:35 2021 Epoch: 92\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19387, smth: 0.19387:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19387, smth: 0.19387:  12%|█▎        | 1/8 [00:02<00:19,  2.72s/it]\u001b[A\n","loss: 0.31380, smth: 0.25384:  12%|█▎        | 1/8 [00:03<00:19,  2.72s/it]\u001b[A\n","loss: 0.31380, smth: 0.25384:  25%|██▌       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.22517, smth: 0.24428:  25%|██▌       | 2/8 [00:04<00:12,  2.15s/it]\u001b[A\n","loss: 0.22517, smth: 0.24428:  38%|███▊      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.25499, smth: 0.24696:  38%|███▊      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.25499, smth: 0.24696:  50%|█████     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.30050, smth: 0.25767:  50%|█████     | 4/8 [00:07<00:06,  1.57s/it]\u001b[A\n","loss: 0.30050, smth: 0.25767:  62%|██████▎   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.23727, smth: 0.25427:  62%|██████▎   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.23727, smth: 0.25427:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.33067, smth: 0.26518:  75%|███████▌  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.33067, smth: 0.26518:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.30580, smth: 0.27026:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.30580, smth: 0.27026: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.00it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:45 2021 Epoch 92, lr: 0.0029770, train loss: 0.27026, train auc: 0.99110, val loss: 0.44196, val auc: 0.98221\n","Thu Jun 10 01:17:45 2021 Epoch: 93\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31093, smth: 0.31093:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.31093, smth: 0.31093:  12%|█▎        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.23565, smth: 0.27329:  12%|█▎        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.23565, smth: 0.27329:  25%|██▌       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.18704, smth: 0.24454:  25%|██▌       | 2/8 [00:05<00:14,  2.40s/it]\u001b[A\n","loss: 0.18704, smth: 0.24454:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.17868, smth: 0.22807:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.17868, smth: 0.22807:  50%|█████     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.34880, smth: 0.25222:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.34880, smth: 0.25222:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.43361, smth: 0.28245:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.43361, smth: 0.28245:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.13404, smth: 0.26125:  75%|███████▌  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.13404, smth: 0.26125:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.45876, smth: 0.28594:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.45876, smth: 0.28594: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:55 2021 Epoch 93, lr: 0.0029341, train loss: 0.28594, train auc: 0.99389, val loss: 0.39647, val auc: 0.98469\n","Thu Jun 10 01:17:55 2021 Epoch: 94\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.24723, smth: 0.24723:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.24723, smth: 0.24723:  12%|█▎        | 1/8 [00:02<00:16,  2.40s/it]\u001b[A\n","loss: 0.23008, smth: 0.23865:  12%|█▎        | 1/8 [00:03<00:16,  2.40s/it]\u001b[A\n","loss: 0.23008, smth: 0.23865:  25%|██▌       | 2/8 [00:03<00:11,  1.90s/it]\u001b[A\n","loss: 0.34568, smth: 0.27433:  25%|██▌       | 2/8 [00:04<00:11,  1.90s/it]\u001b[A\n","loss: 0.34568, smth: 0.27433:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.24908, smth: 0.26801:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.24908, smth: 0.26801:  50%|█████     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.30509, smth: 0.27543:  50%|█████     | 4/8 [00:07<00:06,  1.52s/it]\u001b[A\n","loss: 0.30509, smth: 0.27543:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.41342, smth: 0.29843:  62%|██████▎   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.41342, smth: 0.29843:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.34876, smth: 0.30562:  75%|███████▌  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.34876, smth: 0.30562:  88%|████████▊ | 7/8 [00:09<00:01,  1.36s/it]\u001b[A\n","loss: 0.19393, smth: 0.29166:  88%|████████▊ | 7/8 [00:09<00:01,  1.36s/it]\u001b[A\n","loss: 0.19393, smth: 0.29166: 100%|██████████| 8/8 [00:10<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:06 2021 Epoch 94, lr: 0.0028911, train loss: 0.29166, train auc: 0.99280, val loss: 0.56298, val auc: 0.97906\n","Thu Jun 10 01:18:06 2021 Epoch: 95\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29330, smth: 0.29330:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.29330, smth: 0.29330:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.30422, smth: 0.29876:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.30422, smth: 0.29876:  25%|██▌       | 2/8 [00:04<00:14,  2.48s/it]\u001b[A\n","loss: 0.35214, smth: 0.31655:  25%|██▌       | 2/8 [00:05<00:14,  2.48s/it]\u001b[A\n","loss: 0.35214, smth: 0.31655:  38%|███▊      | 3/8 [00:05<00:10,  2.18s/it]\u001b[A\n","loss: 0.35110, smth: 0.32519:  38%|███▊      | 3/8 [00:06<00:10,  2.18s/it]\u001b[A\n","loss: 0.35110, smth: 0.32519:  50%|█████     | 4/8 [00:06<00:06,  1.74s/it]\u001b[A\n","loss: 0.23851, smth: 0.30785:  50%|█████     | 4/8 [00:07<00:06,  1.74s/it]\u001b[A\n","loss: 0.23851, smth: 0.30785:  62%|██████▎   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.34212, smth: 0.31356:  62%|██████▎   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.34212, smth: 0.31356:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.25888, smth: 0.30575:  75%|███████▌  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.25888, smth: 0.30575:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.35641, smth: 0.31208:  88%|████████▊ | 7/8 [00:10<00:01,  1.31s/it]\u001b[A\n","loss: 0.35641, smth: 0.31208: 100%|██████████| 8/8 [00:10<00:00,  1.28s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:17 2021 Epoch 95, lr: 0.0028479, train loss: 0.31208, train auc: 0.99141, val loss: 0.43549, val auc: 0.98219\n","Thu Jun 10 01:18:17 2021 Epoch: 96\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22743, smth: 0.22743:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22743, smth: 0.22743:  12%|█▎        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.17585, smth: 0.20164:  12%|█▎        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.17585, smth: 0.20164:  25%|██▌       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.26992, smth: 0.22440:  25%|██▌       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.26992, smth: 0.22440:  38%|███▊      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 0.34485, smth: 0.25451:  38%|███▊      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 0.34485, smth: 0.25451:  50%|█████     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.27376, smth: 0.25836:  50%|█████     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.27376, smth: 0.25836:  62%|██████▎   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.25182, smth: 0.25727:  62%|██████▎   | 5/8 [00:08<00:04,  1.60s/it]\u001b[A\n","loss: 0.25182, smth: 0.25727:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.30242, smth: 0.26372:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.30242, smth: 0.26372:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.45220, smth: 0.28728:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.45220, smth: 0.28728: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:27 2021 Epoch 96, lr: 0.0028047, train loss: 0.28728, train auc: 0.99270, val loss: 0.43166, val auc: 0.98360\n","Thu Jun 10 01:18:27 2021 Epoch: 97\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28366, smth: 0.28366:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.28366, smth: 0.28366:  12%|█▎        | 1/8 [00:02<00:17,  2.55s/it]\u001b[A\n","loss: 0.24950, smth: 0.26658:  12%|█▎        | 1/8 [00:03<00:17,  2.55s/it]\u001b[A\n","loss: 0.24950, smth: 0.26658:  25%|██▌       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.31764, smth: 0.28360:  25%|██▌       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.31764, smth: 0.28360:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.28921, smth: 0.28500:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.28921, smth: 0.28500:  50%|█████     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.18074, smth: 0.26415:  50%|█████     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A\n","loss: 0.18074, smth: 0.26415:  62%|██████▎   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.30751, smth: 0.27138:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.30751, smth: 0.27138:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.35666, smth: 0.28356:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.35666, smth: 0.28356:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.16396, smth: 0.26861:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.16396, smth: 0.26861: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:37 2021 Epoch 97, lr: 0.0027613, train loss: 0.26861, train auc: 0.99425, val loss: 0.39885, val auc: 0.98511\n","Thu Jun 10 01:18:37 2021 Epoch: 98\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22981, smth: 0.22981:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22981, smth: 0.22981:  12%|█▎        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.35497, smth: 0.29239:  12%|█▎        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.35497, smth: 0.29239:  25%|██▌       | 2/8 [00:03<00:12,  2.05s/it]\u001b[A\n","loss: 0.30069, smth: 0.29515:  25%|██▌       | 2/8 [00:04<00:12,  2.05s/it]\u001b[A\n","loss: 0.30069, smth: 0.29515:  38%|███▊      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.32851, smth: 0.30349:  38%|███▊      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.32851, smth: 0.30349:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.35404, smth: 0.31360:  50%|█████     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.35404, smth: 0.31360:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.26647, smth: 0.30575:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.26647, smth: 0.30575:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.22343, smth: 0.29399:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.22343, smth: 0.29399:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26765, smth: 0.29069:  88%|████████▊ | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26765, smth: 0.29069: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.99it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:47 2021 Epoch 98, lr: 0.0027179, train loss: 0.29069, train auc: 0.99444, val loss: 0.44949, val auc: 0.98308\n","Thu Jun 10 01:18:47 2021 Epoch: 99\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31167, smth: 0.31167:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31167, smth: 0.31167:  12%|█▎        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.18623, smth: 0.24895:  12%|█▎        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.18623, smth: 0.24895:  25%|██▌       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.26198, smth: 0.25330:  25%|██▌       | 2/8 [00:04<00:14,  2.40s/it]\u001b[A\n","loss: 0.26198, smth: 0.25330:  38%|███▊      | 3/8 [00:04<00:09,  1.99s/it]\u001b[A\n","loss: 0.24065, smth: 0.25013:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.24065, smth: 0.25013:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.18870, smth: 0.23785:  50%|█████     | 4/8 [00:07<00:06,  1.61s/it]\u001b[A\n","loss: 0.18870, smth: 0.23785:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.14564, smth: 0.22248:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.14564, smth: 0.22248:  75%|███████▌  | 6/8 [00:07<00:02,  1.35s/it]\u001b[A\n","loss: 0.28153, smth: 0.23091:  75%|███████▌  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.28153, smth: 0.23091:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.17412, smth: 0.22381:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.17412, smth: 0.22381: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:58 2021 Epoch 99, lr: 0.0026744, train loss: 0.22381, train auc: 0.99441, val loss: 0.45270, val auc: 0.98357\n","Thu Jun 10 01:18:58 2021 Epoch: 100\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25846, smth: 0.25846:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25846, smth: 0.25846:  12%|█▎        | 1/8 [00:02<00:18,  2.60s/it]\u001b[A\n","loss: 0.21154, smth: 0.23500:  12%|█▎        | 1/8 [00:03<00:18,  2.60s/it]\u001b[A\n","loss: 0.21154, smth: 0.23500:  25%|██▌       | 2/8 [00:03<00:12,  2.05s/it]\u001b[A\n","loss: 0.34600, smth: 0.27200:  25%|██▌       | 2/8 [00:04<00:12,  2.05s/it]\u001b[A\n","loss: 0.34600, smth: 0.27200:  38%|███▊      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.26129, smth: 0.26932:  38%|███▊      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.26129, smth: 0.26932:  50%|█████     | 4/8 [00:05<00:06,  1.50s/it]\u001b[A\n","loss: 0.14933, smth: 0.24532:  50%|█████     | 4/8 [00:07<00:06,  1.50s/it]\u001b[A\n","loss: 0.14933, smth: 0.24532:  62%|██████▎   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.23300, smth: 0.24327:  62%|██████▎   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.23300, smth: 0.24327:  75%|███████▌  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.33406, smth: 0.25624:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.33406, smth: 0.25624:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.23457, smth: 0.25353:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.23457, smth: 0.25353: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:08 2021 Epoch 100, lr: 0.0026308, train loss: 0.25353, train auc: 0.99523, val loss: 0.40947, val auc: 0.98557\n","Thu Jun 10 01:19:08 2021 Epoch: 101\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.36275, smth: 0.36275:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.36275, smth: 0.36275:  12%|█▎        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.20573, smth: 0.28424:  12%|█▎        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.20573, smth: 0.28424:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.28265, smth: 0.28371:  25%|██▌       | 2/8 [00:05<00:12,  2.14s/it]\u001b[A\n","loss: 0.28265, smth: 0.28371:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32875, smth: 0.29497:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32875, smth: 0.29497:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.32514, smth: 0.30101:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.32514, smth: 0.30101:  62%|██████▎   | 5/8 [00:07<00:05,  1.69s/it]\u001b[A\n","loss: 0.16602, smth: 0.27851:  62%|██████▎   | 5/8 [00:08<00:05,  1.69s/it]\u001b[A\n","loss: 0.16602, smth: 0.27851:  75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.18412, smth: 0.26503:  75%|███████▌  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.18412, smth: 0.26503:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.11381, smth: 0.24612:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.11381, smth: 0.24612: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:18 2021 Epoch 101, lr: 0.0025872, train loss: 0.24612, train auc: 0.99403, val loss: 0.42697, val auc: 0.98421\n","Thu Jun 10 01:19:18 2021 Epoch: 102\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25703, smth: 0.25703:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25703, smth: 0.25703:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.19822, smth: 0.22763:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.19822, smth: 0.22763:  25%|██▌       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.22159, smth: 0.22562:  25%|██▌       | 2/8 [00:05<00:13,  2.23s/it]\u001b[A\n","loss: 0.22159, smth: 0.22562:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.15511, smth: 0.20799:  38%|███▊      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.15511, smth: 0.20799:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.24341, smth: 0.21507:  50%|█████     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.24341, smth: 0.21507:  62%|██████▎   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.27529, smth: 0.22511:  62%|██████▎   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.27529, smth: 0.22511:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.19073, smth: 0.22020:  75%|███████▌  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.19073, smth: 0.22020:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.24660, smth: 0.22350:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.24660, smth: 0.22350: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:28 2021 Epoch 102, lr: 0.0025436, train loss: 0.22350, train auc: 0.99402, val loss: 0.45656, val auc: 0.98432\n","Thu Jun 10 01:19:28 2021 Epoch: 103\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15465, smth: 0.15465:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15465, smth: 0.15465:  12%|█▎        | 1/8 [00:02<00:18,  2.63s/it]\u001b[A\n","loss: 0.17242, smth: 0.16353:  12%|█▎        | 1/8 [00:03<00:18,  2.63s/it]\u001b[A\n","loss: 0.17242, smth: 0.16353:  25%|██▌       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.19943, smth: 0.17550:  25%|██▌       | 2/8 [00:04<00:12,  2.07s/it]\u001b[A\n","loss: 0.19943, smth: 0.17550:  38%|███▊      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.17238, smth: 0.17472:  38%|███▊      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.17238, smth: 0.17472:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.13641, smth: 0.16705:  50%|█████     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.13641, smth: 0.16705:  62%|██████▎   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.12657, smth: 0.16031:  62%|██████▎   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.12657, smth: 0.16031:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.16707, smth: 0.16127:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.16707, smth: 0.16127:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.14954, smth: 0.15981:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.14954, smth: 0.15981: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:38 2021 Epoch 103, lr: 0.0025000, train loss: 0.15981, train auc: 0.99573, val loss: 0.49147, val auc: 0.98466\n","Thu Jun 10 01:19:38 2021 Epoch: 104\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25284, smth: 0.25284:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25284, smth: 0.25284:  12%|█▎        | 1/8 [00:02<00:17,  2.56s/it]\u001b[A\n","loss: 0.17743, smth: 0.21513:  12%|█▎        | 1/8 [00:03<00:17,  2.56s/it]\u001b[A\n","loss: 0.17743, smth: 0.21513:  25%|██▌       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.19812, smth: 0.20946:  25%|██▌       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.19812, smth: 0.20946:  38%|███▊      | 3/8 [00:04<00:09,  1.81s/it]\u001b[A\n","loss: 0.22521, smth: 0.21340:  38%|███▊      | 3/8 [00:05<00:09,  1.81s/it]\u001b[A\n","loss: 0.22521, smth: 0.21340:  50%|█████     | 4/8 [00:05<00:05,  1.50s/it]\u001b[A\n","loss: 0.24211, smth: 0.21914:  50%|█████     | 4/8 [00:06<00:05,  1.50s/it]\u001b[A\n","loss: 0.24211, smth: 0.21914:  62%|██████▎   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.27368, smth: 0.22823:  62%|██████▎   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.27368, smth: 0.22823:  75%|███████▌  | 6/8 [00:07<00:02,  1.19s/it]\u001b[A\n","loss: 0.25964, smth: 0.23272:  75%|███████▌  | 6/8 [00:08<00:02,  1.19s/it]\u001b[A\n","loss: 0.25964, smth: 0.23272:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.14464, smth: 0.22171:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.14464, smth: 0.22171: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:48 2021 Epoch 104, lr: 0.0024564, train loss: 0.22171, train auc: 0.99390, val loss: 0.46420, val auc: 0.98469\n","Thu Jun 10 01:19:48 2021 Epoch: 105\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22315, smth: 0.22315:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22315, smth: 0.22315:  12%|█▎        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.22965, smth: 0.22640:  12%|█▎        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.22965, smth: 0.22640:  25%|██▌       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.17968, smth: 0.21083:  25%|██▌       | 2/8 [00:05<00:12,  2.07s/it]\u001b[A\n","loss: 0.17968, smth: 0.21083:  38%|███▊      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.18364, smth: 0.20403:  38%|███▊      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.18364, smth: 0.20403:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.34648, smth: 0.23252:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.34648, smth: 0.23252:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.20776, smth: 0.22839:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.20776, smth: 0.22839:  75%|███████▌  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.32311, smth: 0.24192:  75%|███████▌  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.32311, smth: 0.24192:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.15674, smth: 0.23128:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.15674, smth: 0.23128: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:58 2021 Epoch 105, lr: 0.0024128, train loss: 0.23128, train auc: 0.99312, val loss: 0.42929, val auc: 0.98650\n","Thu Jun 10 01:19:58 2021 Epoch: 106\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22694, smth: 0.22694:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22694, smth: 0.22694:  12%|█▎        | 1/8 [00:02<00:18,  2.58s/it]\u001b[A\n","loss: 0.21216, smth: 0.21955:  12%|█▎        | 1/8 [00:03<00:18,  2.58s/it]\u001b[A\n","loss: 0.21216, smth: 0.21955:  25%|██▌       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.21486, smth: 0.21799:  25%|██▌       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.21486, smth: 0.21799:  38%|███▊      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.18675, smth: 0.21018:  38%|███▊      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.18675, smth: 0.21018:  50%|█████     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.18581, smth: 0.20530:  50%|█████     | 4/8 [00:06<00:06,  1.57s/it]\u001b[A\n","loss: 0.18581, smth: 0.20530:  62%|██████▎   | 5/8 [00:06<00:04,  1.43s/it]\u001b[A\n","loss: 0.11983, smth: 0.19106:  62%|██████▎   | 5/8 [00:07<00:04,  1.43s/it]\u001b[A\n","loss: 0.11983, smth: 0.19106:  75%|███████▌  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 0.27882, smth: 0.20360:  75%|███████▌  | 6/8 [00:08<00:02,  1.22s/it]\u001b[A\n","loss: 0.27882, smth: 0.20360:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.28226, smth: 0.21343:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.28226, smth: 0.21343: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:07 2021 Epoch 106, lr: 0.0023692, train loss: 0.21343, train auc: 0.99351, val loss: 0.38702, val auc: 0.98744\n","Thu Jun 10 01:20:07 2021 Epoch: 107\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22848, smth: 0.22848:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22848, smth: 0.22848:  12%|█▎        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 0.20065, smth: 0.21456:  12%|█▎        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 0.20065, smth: 0.21456:  25%|██▌       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.28140, smth: 0.23684:  25%|██▌       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.28140, smth: 0.23684:  38%|███▊      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.23167, smth: 0.23555:  38%|███▊      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.23167, smth: 0.23555:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.17749, smth: 0.22394:  50%|█████     | 4/8 [00:07<00:06,  1.59s/it]\u001b[A\n","loss: 0.17749, smth: 0.22394:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.28625, smth: 0.23432:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.28625, smth: 0.23432:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.38487, smth: 0.25583:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.38487, smth: 0.25583:  88%|████████▊ | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.18317, smth: 0.24675:  88%|████████▊ | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.18317, smth: 0.24675: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.85it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:17 2021 Epoch 107, lr: 0.0023256, train loss: 0.24675, train auc: 0.99578, val loss: 0.48940, val auc: 0.98342\n","Thu Jun 10 01:20:17 2021 Epoch: 108\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21614, smth: 0.21614:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21614, smth: 0.21614:  12%|█▎        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.22353, smth: 0.21984:  12%|█▎        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.22353, smth: 0.21984:  25%|██▌       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.28777, smth: 0.24248:  25%|██▌       | 2/8 [00:05<00:13,  2.26s/it]\u001b[A\n","loss: 0.28777, smth: 0.24248:  38%|███▊      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 0.27196, smth: 0.24985:  38%|███▊      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 0.27196, smth: 0.24985:  50%|█████     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.21024, smth: 0.24193:  50%|█████     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.21024, smth: 0.24193:  62%|██████▎   | 5/8 [00:07<00:05,  1.67s/it]\u001b[A\n","loss: 0.25489, smth: 0.24409:  62%|██████▎   | 5/8 [00:08<00:05,  1.67s/it]\u001b[A\n","loss: 0.25489, smth: 0.24409:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.22697, smth: 0.24164:  75%|███████▌  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.22697, smth: 0.24164:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.25890, smth: 0.24380:  88%|████████▊ | 7/8 [00:10<00:01,  1.30s/it]\u001b[A\n","loss: 0.25890, smth: 0.24380: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.02it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:28 2021 Epoch 108, lr: 0.0022821, train loss: 0.24380, train auc: 0.99407, val loss: 0.40146, val auc: 0.98654\n","Thu Jun 10 01:20:28 2021 Epoch: 109\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16176, smth: 0.16176:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16176, smth: 0.16176:  12%|█▎        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.22814, smth: 0.19495:  12%|█▎        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.22814, smth: 0.19495:  25%|██▌       | 2/8 [00:03<00:13,  2.24s/it]\u001b[A\n","loss: 0.26622, smth: 0.21871:  25%|██▌       | 2/8 [00:05<00:13,  2.24s/it]\u001b[A\n","loss: 0.26622, smth: 0.21871:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.30024, smth: 0.23909:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.30024, smth: 0.23909:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.22953, smth: 0.23718:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.22953, smth: 0.23718:  62%|██████▎   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.33802, smth: 0.25399:  62%|██████▎   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.33802, smth: 0.25399:  75%|███████▌  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.28578, smth: 0.25853:  75%|███████▌  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.28578, smth: 0.25853:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.24141, smth: 0.25639:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.24141, smth: 0.25639: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:38 2021 Epoch 109, lr: 0.0022387, train loss: 0.25639, train auc: 0.99296, val loss: 0.36063, val auc: 0.98781\n","Thu Jun 10 01:20:38 2021 Epoch: 110\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.30881, smth: 0.30881:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.30881, smth: 0.30881:  12%|█▎        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.23340, smth: 0.27111:  12%|█▎        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.23340, smth: 0.27111:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.11285, smth: 0.21835:  25%|██▌       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.11285, smth: 0.21835:  38%|███▊      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.24338, smth: 0.22461:  38%|███▊      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.24338, smth: 0.22461:  50%|█████     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.21939, smth: 0.22357:  50%|█████     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.21939, smth: 0.22357:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.22197, smth: 0.22330:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.22197, smth: 0.22330:  75%|███████▌  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.19603, smth: 0.21940:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.19603, smth: 0.21940:  88%|████████▊ | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.29076, smth: 0.22832:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.29076, smth: 0.22832: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:48 2021 Epoch 110, lr: 0.0021953, train loss: 0.22832, train auc: 0.99434, val loss: 0.34178, val auc: 0.98735\n","Thu Jun 10 01:20:48 2021 Epoch: 111\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16376, smth: 0.16376:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16376, smth: 0.16376:  12%|█▎        | 1/8 [00:02<00:17,  2.46s/it]\u001b[A\n","loss: 0.21851, smth: 0.19114:  12%|█▎        | 1/8 [00:03<00:17,  2.46s/it]\u001b[A\n","loss: 0.21851, smth: 0.19114:  25%|██▌       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.21368, smth: 0.19865:  25%|██▌       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.21368, smth: 0.19865:  38%|███▊      | 3/8 [00:04<00:08,  1.69s/it]\u001b[A\n","loss: 0.20479, smth: 0.20019:  38%|███▊      | 3/8 [00:05<00:08,  1.69s/it]\u001b[A\n","loss: 0.20479, smth: 0.20019:  50%|█████     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.19246, smth: 0.19864:  50%|█████     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.19246, smth: 0.19864:  62%|██████▎   | 5/8 [00:06<00:03,  1.26s/it]\u001b[A\n","loss: 0.23772, smth: 0.20515:  62%|██████▎   | 5/8 [00:06<00:03,  1.26s/it]\u001b[A\n","loss: 0.23772, smth: 0.20515:  75%|███████▌  | 6/8 [00:06<00:02,  1.13s/it]\u001b[A\n","loss: 0.23881, smth: 0.20996:  75%|███████▌  | 6/8 [00:07<00:02,  1.13s/it]\u001b[A\n","loss: 0.23881, smth: 0.20996:  88%|████████▊ | 7/8 [00:07<00:01,  1.12s/it]\u001b[A\n","loss: 0.30437, smth: 0.22176:  88%|████████▊ | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 0.30437, smth: 0.22176: 100%|██████████| 8/8 [00:08<00:00,  1.05s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:57 2021 Epoch 111, lr: 0.0021521, train loss: 0.22176, train auc: 0.99568, val loss: 0.39411, val auc: 0.98633\n","Thu Jun 10 01:20:57 2021 Epoch: 112\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.23403, smth: 0.23403:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.23403, smth: 0.23403:  12%|█▎        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.16341, smth: 0.19872:  12%|█▎        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.16341, smth: 0.19872:  25%|██▌       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.24568, smth: 0.21437:  25%|██▌       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.24568, smth: 0.21437:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.24846, smth: 0.22289:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.24846, smth: 0.22289:  50%|█████     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.26018, smth: 0.23035:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.26018, smth: 0.23035:  62%|██████▎   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.15525, smth: 0.21783:  62%|██████▎   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.15525, smth: 0.21783:  75%|███████▌  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.16995, smth: 0.21099:  75%|███████▌  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.16995, smth: 0.21099:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.12509, smth: 0.20025:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.12509, smth: 0.20025: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:07 2021 Epoch 112, lr: 0.0021089, train loss: 0.20025, train auc: 0.99612, val loss: 0.38276, val auc: 0.98717\n","Thu Jun 10 01:21:07 2021 Epoch: 113\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19584, smth: 0.19584:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19584, smth: 0.19584:  12%|█▎        | 1/8 [00:02<00:17,  2.50s/it]\u001b[A\n","loss: 0.15328, smth: 0.17456:  12%|█▎        | 1/8 [00:03<00:17,  2.50s/it]\u001b[A\n","loss: 0.15328, smth: 0.17456:  25%|██▌       | 2/8 [00:03<00:11,  1.97s/it]\u001b[A\n","loss: 0.17760, smth: 0.17557:  25%|██▌       | 2/8 [00:04<00:11,  1.97s/it]\u001b[A\n","loss: 0.17760, smth: 0.17557:  38%|███▊      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.31776, smth: 0.21112:  38%|███▊      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.31776, smth: 0.21112:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.19176, smth: 0.20725:  50%|█████     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.19176, smth: 0.20725:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.16488, smth: 0.20019:  62%|██████▎   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.16488, smth: 0.20019:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.14426, smth: 0.19220:  75%|███████▌  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.14426, smth: 0.19220:  88%|████████▊ | 7/8 [00:09<00:01,  1.34s/it]\u001b[A\n","loss: 0.37103, smth: 0.21455:  88%|████████▊ | 7/8 [00:09<00:01,  1.34s/it]\u001b[A\n","loss: 0.37103, smth: 0.21455: 100%|██████████| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:17 2021 Epoch 113, lr: 0.0020659, train loss: 0.21455, train auc: 0.99508, val loss: 0.42515, val auc: 0.98714\n","Thu Jun 10 01:21:17 2021 Epoch: 114\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.23674, smth: 0.23674:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.23674, smth: 0.23674:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.24925, smth: 0.24299:  12%|█▎        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.24925, smth: 0.24299:  25%|██▌       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.22937, smth: 0.23845:  25%|██▌       | 2/8 [00:04<00:14,  2.35s/it]\u001b[A\n","loss: 0.22937, smth: 0.23845:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.14974, smth: 0.21628:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.14974, smth: 0.21628:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.18030, smth: 0.20908:  50%|█████     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.18030, smth: 0.20908:  62%|██████▎   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.16519, smth: 0.20177:  62%|██████▎   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.16519, smth: 0.20177:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.23865, smth: 0.20704:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.23865, smth: 0.20704:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.21808, smth: 0.20842:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.21808, smth: 0.20842: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:27 2021 Epoch 114, lr: 0.0020230, train loss: 0.20842, train auc: 0.99394, val loss: 0.41503, val auc: 0.98662\n","Thu Jun 10 01:21:27 2021 Epoch: 115\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17171, smth: 0.17171:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.17171, smth: 0.17171:  12%|█▎        | 1/8 [00:03<00:23,  3.33s/it]\u001b[A\n","loss: 0.21665, smth: 0.19418:  12%|█▎        | 1/8 [00:04<00:23,  3.33s/it]\u001b[A\n","loss: 0.21665, smth: 0.19418:  25%|██▌       | 2/8 [00:04<00:15,  2.56s/it]\u001b[A\n","loss: 0.14142, smth: 0.17659:  25%|██▌       | 2/8 [00:05<00:15,  2.56s/it]\u001b[A\n","loss: 0.14142, smth: 0.17659:  38%|███▊      | 3/8 [00:05<00:11,  2.30s/it]\u001b[A\n","loss: 0.15126, smth: 0.17026:  38%|███▊      | 3/8 [00:06<00:11,  2.30s/it]\u001b[A\n","loss: 0.15126, smth: 0.17026:  50%|█████     | 4/8 [00:06<00:07,  1.86s/it]\u001b[A\n","loss: 0.24394, smth: 0.18500:  50%|█████     | 4/8 [00:07<00:07,  1.86s/it]\u001b[A\n","loss: 0.24394, smth: 0.18500:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.21535, smth: 0.19006:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.21535, smth: 0.19006:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.24036, smth: 0.19724:  75%|███████▌  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.24036, smth: 0.19724:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.20312, smth: 0.19798:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.20312, smth: 0.19798: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:38 2021 Epoch 115, lr: 0.0019802, train loss: 0.19798, train auc: 0.99661, val loss: 0.46790, val auc: 0.98461\n","Thu Jun 10 01:21:38 2021 Epoch: 116\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29430, smth: 0.29430:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29430, smth: 0.29430:  12%|█▎        | 1/8 [00:02<00:20,  2.99s/it]\u001b[A\n","loss: 0.19633, smth: 0.24532:  12%|█▎        | 1/8 [00:03<00:20,  2.99s/it]\u001b[A\n","loss: 0.19633, smth: 0.24532:  25%|██▌       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.24717, smth: 0.24593:  25%|██▌       | 2/8 [00:05<00:13,  2.32s/it]\u001b[A\n","loss: 0.24717, smth: 0.24593:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.19078, smth: 0.23215:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.19078, smth: 0.23215:  50%|█████     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.25299, smth: 0.23631:  50%|█████     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.25299, smth: 0.23631:  62%|██████▎   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.24208, smth: 0.23728:  62%|██████▎   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.24208, smth: 0.23728:  75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.10906, smth: 0.21896:  75%|███████▌  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.10906, smth: 0.21896:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23021, smth: 0.22037:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23021, smth: 0.22037: 100%|██████████| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:48 2021 Epoch 116, lr: 0.0019376, train loss: 0.22037, train auc: 0.99530, val loss: 0.59413, val auc: 0.98149\n","Thu Jun 10 01:21:48 2021 Epoch: 117\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32001, smth: 0.32001:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32001, smth: 0.32001:  12%|█▎        | 1/8 [00:02<00:17,  2.57s/it]\u001b[A\n","loss: 0.30060, smth: 0.31030:  12%|█▎        | 1/8 [00:03<00:17,  2.57s/it]\u001b[A\n","loss: 0.30060, smth: 0.31030:  25%|██▌       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.19835, smth: 0.27298:  25%|██▌       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.19835, smth: 0.27298:  38%|███▊      | 3/8 [00:04<00:09,  1.85s/it]\u001b[A\n","loss: 0.15962, smth: 0.24464:  38%|███▊      | 3/8 [00:05<00:09,  1.85s/it]\u001b[A\n","loss: 0.15962, smth: 0.24464:  50%|█████     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.15066, smth: 0.22585:  50%|█████     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A\n","loss: 0.15066, smth: 0.22585:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.13076, smth: 0.21000:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.13076, smth: 0.21000:  75%|███████▌  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.22310, smth: 0.21187:  75%|███████▌  | 6/8 [00:09<00:02,  1.30s/it]\u001b[A\n","loss: 0.22310, smth: 0.21187:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.12458, smth: 0.20096:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.12458, smth: 0.20096: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:58 2021 Epoch 117, lr: 0.0018952, train loss: 0.20096, train auc: 0.99314, val loss: 0.59656, val auc: 0.98339\n","Thu Jun 10 01:21:59 2021 Epoch: 118\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19436, smth: 0.19436:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19436, smth: 0.19436:  12%|█▎        | 1/8 [00:02<00:17,  2.47s/it]\u001b[A\n","loss: 0.18819, smth: 0.19127:  12%|█▎        | 1/8 [00:03<00:17,  2.47s/it]\u001b[A\n","loss: 0.18819, smth: 0.19127:  25%|██▌       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.23639, smth: 0.20631:  25%|██▌       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.23639, smth: 0.20631:  38%|███▊      | 3/8 [00:04<00:08,  1.69s/it]\u001b[A\n","loss: 0.32644, smth: 0.23634:  38%|███▊      | 3/8 [00:04<00:08,  1.69s/it]\u001b[A\n","loss: 0.32644, smth: 0.23634:  50%|█████     | 4/8 [00:05<00:05,  1.42s/it]\u001b[A\n","loss: 0.18200, smth: 0.22548:  50%|█████     | 4/8 [00:06<00:05,  1.42s/it]\u001b[A\n","loss: 0.18200, smth: 0.22548:  62%|██████▎   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.29972, smth: 0.23785:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.29972, smth: 0.23785:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.26063, smth: 0.24110:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.26063, smth: 0.24110:  88%|████████▊ | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.13275, smth: 0.22756:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.13275, smth: 0.22756: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:08 2021 Epoch 118, lr: 0.0018530, train loss: 0.22756, train auc: 0.99684, val loss: 0.46676, val auc: 0.98586\n","Thu Jun 10 01:22:08 2021 Epoch: 119\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21024, smth: 0.21024:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21024, smth: 0.21024:  12%|█▎        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.24161, smth: 0.22593:  12%|█▎        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.24161, smth: 0.22593:  25%|██▌       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.15937, smth: 0.20374:  25%|██▌       | 2/8 [00:05<00:13,  2.21s/it]\u001b[A\n","loss: 0.15937, smth: 0.20374:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.11788, smth: 0.18228:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.11788, smth: 0.18228:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.27582, smth: 0.20099:  50%|█████     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.27582, smth: 0.20099:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.22047, smth: 0.20423:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.22047, smth: 0.20423:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.21095, smth: 0.20519:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.21095, smth: 0.20519:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.21924, smth: 0.20695:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.21924, smth: 0.20695: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:18 2021 Epoch 119, lr: 0.0018109, train loss: 0.20695, train auc: 0.99450, val loss: 0.43858, val auc: 0.98663\n","Thu Jun 10 01:22:18 2021 Epoch: 120\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22260, smth: 0.22260:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22260, smth: 0.22260:  12%|█▎        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.12109, smth: 0.17184:  12%|█▎        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.12109, smth: 0.17184:  25%|██▌       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.15102, smth: 0.16490:  25%|██▌       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.15102, smth: 0.16490:  38%|███▊      | 3/8 [00:04<00:08,  1.78s/it]\u001b[A\n","loss: 0.17292, smth: 0.16691:  38%|███▊      | 3/8 [00:05<00:08,  1.78s/it]\u001b[A\n","loss: 0.17292, smth: 0.16691:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.27075, smth: 0.18768:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.27075, smth: 0.18768:  62%|██████▎   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.23190, smth: 0.19505:  62%|██████▎   | 5/8 [00:08<00:04,  1.42s/it]\u001b[A\n","loss: 0.23190, smth: 0.19505:  75%|███████▌  | 6/8 [00:08<00:02,  1.46s/it]\u001b[A\n","loss: 0.14446, smth: 0.18782:  75%|███████▌  | 6/8 [00:08<00:02,  1.46s/it]\u001b[A\n","loss: 0.14446, smth: 0.18782:  88%|████████▊ | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.14384, smth: 0.18232:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.14384, smth: 0.18232: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:28 2021 Epoch 120, lr: 0.0017691, train loss: 0.18232, train auc: 0.99673, val loss: 0.42079, val auc: 0.98715\n","Thu Jun 10 01:22:28 2021 Epoch: 121\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16990, smth: 0.16990:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16990, smth: 0.16990:  12%|█▎        | 1/8 [00:02<00:17,  2.48s/it]\u001b[A\n","loss: 0.19348, smth: 0.18169:  12%|█▎        | 1/8 [00:03<00:17,  2.48s/it]\u001b[A\n","loss: 0.19348, smth: 0.18169:  25%|██▌       | 2/8 [00:03<00:11,  1.98s/it]\u001b[A\n","loss: 0.19599, smth: 0.18646:  25%|██▌       | 2/8 [00:04<00:11,  1.98s/it]\u001b[A\n","loss: 0.19599, smth: 0.18646:  38%|███▊      | 3/8 [00:04<00:08,  1.80s/it]\u001b[A\n","loss: 0.23542, smth: 0.19870:  38%|███▊      | 3/8 [00:05<00:08,  1.80s/it]\u001b[A\n","loss: 0.23542, smth: 0.19870:  50%|█████     | 4/8 [00:05<00:05,  1.48s/it]\u001b[A\n","loss: 0.12672, smth: 0.18430:  50%|█████     | 4/8 [00:06<00:05,  1.48s/it]\u001b[A\n","loss: 0.12672, smth: 0.18430:  62%|██████▎   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.13979, smth: 0.17688:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.13979, smth: 0.17688:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.12908, smth: 0.17006:  75%|███████▌  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.12908, smth: 0.17006:  88%|████████▊ | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.20636, smth: 0.17459:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.20636, smth: 0.17459: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:38 2021 Epoch 121, lr: 0.0017275, train loss: 0.17459, train auc: 0.99694, val loss: 0.47540, val auc: 0.98499\n","Thu Jun 10 01:22:38 2021 Epoch: 122\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18439, smth: 0.18439:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18439, smth: 0.18439:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.12994, smth: 0.15717:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.12994, smth: 0.15717:  25%|██▌       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.20241, smth: 0.17225:  25%|██▌       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.20241, smth: 0.17225:  38%|███▊      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.18586, smth: 0.17565:  38%|███▊      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.18586, smth: 0.17565:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.13001, smth: 0.16652:  50%|█████     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.13001, smth: 0.16652:  62%|██████▎   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.16417, smth: 0.16613:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.16417, smth: 0.16613:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.29213, smth: 0.18413:  75%|███████▌  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.29213, smth: 0.18413:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.38408, smth: 0.20912:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.38408, smth: 0.20912: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:48 2021 Epoch 122, lr: 0.0016861, train loss: 0.20912, train auc: 0.99532, val loss: 0.55646, val auc: 0.98325\n","Thu Jun 10 01:22:48 2021 Epoch: 123\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32407, smth: 0.32407:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32407, smth: 0.32407:  12%|█▎        | 1/8 [00:02<00:19,  2.79s/it]\u001b[A\n","loss: 0.21489, smth: 0.26948:  12%|█▎        | 1/8 [00:03<00:19,  2.79s/it]\u001b[A\n","loss: 0.21489, smth: 0.26948:  25%|██▌       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.19380, smth: 0.24426:  25%|██▌       | 2/8 [00:04<00:13,  2.18s/it]\u001b[A\n","loss: 0.19380, smth: 0.24426:  38%|███▊      | 3/8 [00:04<00:09,  1.85s/it]\u001b[A\n","loss: 0.23418, smth: 0.24174:  38%|███▊      | 3/8 [00:05<00:09,  1.85s/it]\u001b[A\n","loss: 0.23418, smth: 0.24174:  50%|█████     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.10643, smth: 0.21468:  50%|█████     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.10643, smth: 0.21468:  62%|██████▎   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.24772, smth: 0.22018:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.24772, smth: 0.22018:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.10488, smth: 0.20371:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.10488, smth: 0.20371:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.32875, smth: 0.21934:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.32875, smth: 0.21934: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:58 2021 Epoch 123, lr: 0.0016449, train loss: 0.21934, train auc: 0.99474, val loss: 0.58633, val auc: 0.98164\n","Thu Jun 10 01:22:58 2021 Epoch: 124\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19060, smth: 0.19060:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19060, smth: 0.19060:  12%|█▎        | 1/8 [00:02<00:17,  2.53s/it]\u001b[A\n","loss: 0.17416, smth: 0.18238:  12%|█▎        | 1/8 [00:03<00:17,  2.53s/it]\u001b[A\n","loss: 0.17416, smth: 0.18238:  25%|██▌       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.09839, smth: 0.15438:  25%|██▌       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.09839, smth: 0.15438:  38%|███▊      | 3/8 [00:04<00:09,  1.84s/it]\u001b[A\n","loss: 0.34528, smth: 0.20211:  38%|███▊      | 3/8 [00:05<00:09,  1.84s/it]\u001b[A\n","loss: 0.34528, smth: 0.20211:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.15366, smth: 0.19242:  50%|█████     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.15366, smth: 0.19242:  62%|██████▎   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.27784, smth: 0.20666:  62%|██████▎   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.27784, smth: 0.20666:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.17992, smth: 0.20284:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.17992, smth: 0.20284:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.40327, smth: 0.22789:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.40327, smth: 0.22789: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.99it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:07 2021 Epoch 124, lr: 0.0016041, train loss: 0.22789, train auc: 0.99449, val loss: 0.52510, val auc: 0.98350\n","Thu Jun 10 01:23:07 2021 Epoch: 125\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21638, smth: 0.21638:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21638, smth: 0.21638:  12%|█▎        | 1/8 [00:02<00:18,  2.61s/it]\u001b[A\n","loss: 0.22241, smth: 0.21939:  12%|█▎        | 1/8 [00:03<00:18,  2.61s/it]\u001b[A\n","loss: 0.22241, smth: 0.21939:  25%|██▌       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.19604, smth: 0.21161:  25%|██▌       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.19604, smth: 0.21161:  38%|███▊      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.18724, smth: 0.20552:  38%|███▊      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.18724, smth: 0.20552:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.23297, smth: 0.21101:  50%|█████     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 0.23297, smth: 0.21101:  62%|██████▎   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.30742, smth: 0.22708:  62%|██████▎   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.30742, smth: 0.22708:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.14118, smth: 0.21480:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.14118, smth: 0.21480:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.24792, smth: 0.21894:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.24792, smth: 0.21894: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:17 2021 Epoch 125, lr: 0.0015635, train loss: 0.21894, train auc: 0.99645, val loss: 0.41478, val auc: 0.98642\n","Thu Jun 10 01:23:17 2021 Epoch: 126\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28358, smth: 0.28358:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.28358, smth: 0.28358:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.19916, smth: 0.24137:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.19916, smth: 0.24137:  25%|██▌       | 2/8 [00:03<00:14,  2.47s/it]\u001b[A\n","loss: 0.19098, smth: 0.22457:  25%|██▌       | 2/8 [00:05<00:14,  2.47s/it]\u001b[A\n","loss: 0.19098, smth: 0.22457:  38%|███▊      | 3/8 [00:05<00:11,  2.27s/it]\u001b[A\n","loss: 0.20144, smth: 0.21879:  38%|███▊      | 3/8 [00:06<00:11,  2.27s/it]\u001b[A\n","loss: 0.20144, smth: 0.21879:  50%|█████     | 4/8 [00:06<00:07,  1.82s/it]\u001b[A\n","loss: 0.29160, smth: 0.23335:  50%|█████     | 4/8 [00:08<00:07,  1.82s/it]\u001b[A\n","loss: 0.29160, smth: 0.23335:  62%|██████▎   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.24746, smth: 0.23570:  62%|██████▎   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.24746, smth: 0.23570:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.14993, smth: 0.22345:  75%|███████▌  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.14993, smth: 0.22345:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.13896, smth: 0.21289:  88%|████████▊ | 7/8 [00:10<00:01,  1.25s/it]\u001b[A\n","loss: 0.13896, smth: 0.21289: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:27 2021 Epoch 126, lr: 0.0015232, train loss: 0.21289, train auc: 0.99570, val loss: 0.38780, val auc: 0.98658\n","Thu Jun 10 01:23:27 2021 Epoch: 127\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19671, smth: 0.19671:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19671, smth: 0.19671:  12%|█▎        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.28885, smth: 0.24278:  12%|█▎        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.28885, smth: 0.24278:  25%|██▌       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.28696, smth: 0.25751:  25%|██▌       | 2/8 [00:05<00:13,  2.18s/it]\u001b[A\n","loss: 0.28696, smth: 0.25751:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.14910, smth: 0.23040:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.14910, smth: 0.23040:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.22486, smth: 0.22930:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.22486, smth: 0.22930:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.26295, smth: 0.23490:  62%|██████▎   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.26295, smth: 0.23490:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.12025, smth: 0.21853:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.12025, smth: 0.21853:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.16779, smth: 0.21218:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.16779, smth: 0.21218: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:37 2021 Epoch 127, lr: 0.0014832, train loss: 0.21218, train auc: 0.99548, val loss: 0.43956, val auc: 0.98479\n","Thu Jun 10 01:23:37 2021 Epoch: 128\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12391, smth: 0.12391:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12391, smth: 0.12391:  12%|█▎        | 1/8 [00:02<00:16,  2.42s/it]\u001b[A\n","loss: 0.19165, smth: 0.15778:  12%|█▎        | 1/8 [00:03<00:16,  2.42s/it]\u001b[A\n","loss: 0.19165, smth: 0.15778:  25%|██▌       | 2/8 [00:03<00:11,  1.93s/it]\u001b[A\n","loss: 0.19508, smth: 0.17021:  25%|██▌       | 2/8 [00:04<00:11,  1.93s/it]\u001b[A\n","loss: 0.19508, smth: 0.17021:  38%|███▊      | 3/8 [00:04<00:09,  1.84s/it]\u001b[A\n","loss: 0.12670, smth: 0.15934:  38%|███▊      | 3/8 [00:05<00:09,  1.84s/it]\u001b[A\n","loss: 0.12670, smth: 0.15934:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.15665, smth: 0.15880:  50%|█████     | 4/8 [00:07<00:06,  1.53s/it]\u001b[A\n","loss: 0.15665, smth: 0.15880:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.24264, smth: 0.17277:  62%|██████▎   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.24264, smth: 0.17277:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.11753, smth: 0.16488:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.11753, smth: 0.16488:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.28066, smth: 0.17935:  88%|████████▊ | 7/8 [00:09<00:01,  1.13s/it]\u001b[A\n","loss: 0.28066, smth: 0.17935: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:47 2021 Epoch 128, lr: 0.0014435, train loss: 0.17935, train auc: 0.99494, val loss: 0.54825, val auc: 0.98259\n","Thu Jun 10 01:23:47 2021 Epoch: 129\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.27442, smth: 0.27442:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.27442, smth: 0.27442:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.12960, smth: 0.20201:  12%|█▎        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.12960, smth: 0.20201:  25%|██▌       | 2/8 [00:03<00:14,  2.45s/it]\u001b[A\n","loss: 0.19620, smth: 0.20007:  25%|██▌       | 2/8 [00:05<00:14,  2.45s/it]\u001b[A\n","loss: 0.19620, smth: 0.20007:  38%|███▊      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.06969, smth: 0.16748:  38%|███▊      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.06969, smth: 0.16748:  50%|█████     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.16027, smth: 0.16604:  50%|█████     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.16027, smth: 0.16604:  62%|██████▎   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.27039, smth: 0.18343:  62%|██████▎   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.27039, smth: 0.18343:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.15567, smth: 0.17946:  75%|███████▌  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.15567, smth: 0.17946:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.04837, smth: 0.16308:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.04837, smth: 0.16308: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:57 2021 Epoch 129, lr: 0.0014041, train loss: 0.16308, train auc: 0.99692, val loss: 0.54291, val auc: 0.98378\n","Thu Jun 10 01:23:57 2021 Epoch: 130\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10956, smth: 0.10956:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10956, smth: 0.10956:  12%|█▎        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.09433, smth: 0.10195:  12%|█▎        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.09433, smth: 0.10195:  25%|██▌       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.16503, smth: 0.12298:  25%|██▌       | 2/8 [00:05<00:12,  2.15s/it]\u001b[A\n","loss: 0.16503, smth: 0.12298:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.17448, smth: 0.13585:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.17448, smth: 0.13585:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.23349, smth: 0.15538:  50%|█████     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.23349, smth: 0.15538:  62%|██████▎   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.12475, smth: 0.15027:  62%|██████▎   | 5/8 [00:08<00:04,  1.60s/it]\u001b[A\n","loss: 0.12475, smth: 0.15027:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.23343, smth: 0.16215:  75%|███████▌  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.23343, smth: 0.16215:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.17754, smth: 0.16408:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.17754, smth: 0.16408: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.98it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.63it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:08 2021 Epoch 130, lr: 0.0013650, train loss: 0.16408, train auc: 0.99628, val loss: 0.51342, val auc: 0.98434\n","Thu Jun 10 01:24:08 2021 Epoch: 131\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16418, smth: 0.16418:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16418, smth: 0.16418:  12%|█▎        | 1/8 [00:02<00:20,  2.95s/it]\u001b[A\n","loss: 0.08542, smth: 0.12480:  12%|█▎        | 1/8 [00:03<00:20,  2.95s/it]\u001b[A\n","loss: 0.08542, smth: 0.12480:  25%|██▌       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.32237, smth: 0.19066:  25%|██▌       | 2/8 [00:04<00:13,  2.29s/it]\u001b[A\n","loss: 0.32237, smth: 0.19066:  38%|███▊      | 3/8 [00:04<00:09,  1.98s/it]\u001b[A\n","loss: 0.19259, smth: 0.19114:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.19259, smth: 0.19114:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.16570, smth: 0.18605:  50%|█████     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.16570, smth: 0.18605:  62%|██████▎   | 5/8 [00:07<00:05,  1.68s/it]\u001b[A\n","loss: 0.17511, smth: 0.18423:  62%|██████▎   | 5/8 [00:08<00:05,  1.68s/it]\u001b[A\n","loss: 0.17511, smth: 0.18423:  75%|███████▌  | 6/8 [00:08<00:02,  1.49s/it]\u001b[A\n","loss: 0.14180, smth: 0.17817:  75%|███████▌  | 6/8 [00:09<00:02,  1.49s/it]\u001b[A\n","loss: 0.14180, smth: 0.17817:  88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.08867, smth: 0.16698:  88%|████████▊ | 7/8 [00:10<00:01,  1.35s/it]\u001b[A\n","loss: 0.08867, smth: 0.16698: 100%|██████████| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:18 2021 Epoch 131, lr: 0.0013263, train loss: 0.16698, train auc: 0.99563, val loss: 0.40554, val auc: 0.98834\n","Thu Jun 10 01:24:18 2021 Epoch: 132\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13046, smth: 0.13046:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13046, smth: 0.13046:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.15615, smth: 0.14330:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.15615, smth: 0.14330:  25%|██▌       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.14422, smth: 0.14361:  25%|██▌       | 2/8 [00:04<00:13,  2.23s/it]\u001b[A\n","loss: 0.14422, smth: 0.14361:  38%|███▊      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.18762, smth: 0.15461:  38%|███▊      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.18762, smth: 0.15461:  50%|█████     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.12214, smth: 0.14812:  50%|█████     | 4/8 [00:07<00:06,  1.58s/it]\u001b[A\n","loss: 0.12214, smth: 0.14812:  62%|██████▎   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.29318, smth: 0.17229:  62%|██████▎   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.29318, smth: 0.17229:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.16834, smth: 0.17173:  75%|███████▌  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.16834, smth: 0.17173:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.08643, smth: 0.16107:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.08643, smth: 0.16107: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:29 2021 Epoch 132, lr: 0.0012880, train loss: 0.16107, train auc: 0.99629, val loss: 0.38921, val auc: 0.98914\n","Thu Jun 10 01:24:29 2021 Epoch: 133\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14472, smth: 0.14472:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14472, smth: 0.14472:  12%|█▎        | 1/8 [00:02<00:20,  2.94s/it]\u001b[A\n","loss: 0.12632, smth: 0.13552:  12%|█▎        | 1/8 [00:03<00:20,  2.94s/it]\u001b[A\n","loss: 0.12632, smth: 0.13552:  25%|██▌       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.09870, smth: 0.12325:  25%|██▌       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.09870, smth: 0.12325:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.16998, smth: 0.13493:  38%|███▊      | 3/8 [00:06<00:10,  2.11s/it]\u001b[A\n","loss: 0.16998, smth: 0.13493:  50%|█████     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.14739, smth: 0.13742:  50%|█████     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.14739, smth: 0.13742:  62%|██████▎   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.14199, smth: 0.13818:  62%|██████▎   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.14199, smth: 0.13818:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.18481, smth: 0.14485:  75%|███████▌  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.18481, smth: 0.14485:  88%|████████▊ | 7/8 [00:09<00:01,  1.39s/it]\u001b[A\n","loss: 0.24457, smth: 0.15731:  88%|████████▊ | 7/8 [00:10<00:01,  1.39s/it]\u001b[A\n","loss: 0.24457, smth: 0.15731: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:40 2021 Epoch 133, lr: 0.0012500, train loss: 0.15731, train auc: 0.99615, val loss: 0.48225, val auc: 0.98675\n","Thu Jun 10 01:24:40 2021 Epoch: 134\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.09781, smth: 0.09781:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.09781, smth: 0.09781:  12%|█▎        | 1/8 [00:03<00:22,  3.16s/it]\u001b[A\n","loss: 0.18882, smth: 0.14332:  12%|█▎        | 1/8 [00:03<00:22,  3.16s/it]\u001b[A\n","loss: 0.18882, smth: 0.14332:  25%|██▌       | 2/8 [00:03<00:14,  2.44s/it]\u001b[A\n","loss: 0.12962, smth: 0.13875:  25%|██▌       | 2/8 [00:05<00:14,  2.44s/it]\u001b[A\n","loss: 0.12962, smth: 0.13875:  38%|███▊      | 3/8 [00:05<00:10,  2.12s/it]\u001b[A\n","loss: 0.25494, smth: 0.16780:  38%|███▊      | 3/8 [00:05<00:10,  2.12s/it]\u001b[A\n","loss: 0.25494, smth: 0.16780:  50%|█████     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.21005, smth: 0.17625:  50%|█████     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.21005, smth: 0.17625:  62%|██████▎   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.21241, smth: 0.18228:  62%|██████▎   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.21241, smth: 0.18228:  75%|███████▌  | 6/8 [00:08<00:02,  1.40s/it]\u001b[A\n","loss: 0.16340, smth: 0.17958:  75%|███████▌  | 6/8 [00:09<00:02,  1.40s/it]\u001b[A\n","loss: 0.16340, smth: 0.17958:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.19016, smth: 0.18090:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.19016, smth: 0.18090: 100%|██████████| 8/8 [00:10<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:50 2021 Epoch 134, lr: 0.0012124, train loss: 0.18090, train auc: 0.99655, val loss: 0.41290, val auc: 0.98801\n","Thu Jun 10 01:24:50 2021 Epoch: 135\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21972, smth: 0.21972:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21972, smth: 0.21972:  12%|█▎        | 1/8 [00:02<00:20,  2.89s/it]\u001b[A\n","loss: 0.17729, smth: 0.19851:  12%|█▎        | 1/8 [00:03<00:20,  2.89s/it]\u001b[A\n","loss: 0.17729, smth: 0.19851:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.21238, smth: 0.20313:  25%|██▌       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.21238, smth: 0.20313:  38%|███▊      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.15598, smth: 0.19134:  38%|███▊      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.15598, smth: 0.19134:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.29888, smth: 0.21285:  50%|█████     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.29888, smth: 0.21285:  62%|██████▎   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.21078, smth: 0.21250:  62%|██████▎   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.21078, smth: 0.21250:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.24024, smth: 0.21647:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.24024, smth: 0.21647:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.34265, smth: 0.23224:  88%|████████▊ | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.34265, smth: 0.23224: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:00 2021 Epoch 135, lr: 0.0011752, train loss: 0.23224, train auc: 0.99432, val loss: 0.40404, val auc: 0.98791\n","Thu Jun 10 01:25:00 2021 Epoch: 136\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22106, smth: 0.22106:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22106, smth: 0.22106:  12%|█▎        | 1/8 [00:02<00:20,  2.90s/it]\u001b[A\n","loss: 0.21344, smth: 0.21725:  12%|█▎        | 1/8 [00:03<00:20,  2.90s/it]\u001b[A\n","loss: 0.21344, smth: 0.21725:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.14921, smth: 0.19457:  25%|██▌       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.14921, smth: 0.19457:  38%|███▊      | 3/8 [00:04<00:09,  1.94s/it]\u001b[A\n","loss: 0.18169, smth: 0.19135:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.18169, smth: 0.19135:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.13754, smth: 0.18059:  50%|█████     | 4/8 [00:07<00:06,  1.59s/it]\u001b[A\n","loss: 0.13754, smth: 0.18059:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.20477, smth: 0.18462:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.20477, smth: 0.18462:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.17493, smth: 0.18324:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.17493, smth: 0.18324:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.14061, smth: 0.17791:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.14061, smth: 0.17791: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:10 2021 Epoch 136, lr: 0.0011384, train loss: 0.17791, train auc: 0.99698, val loss: 0.36919, val auc: 0.98844\n","Thu Jun 10 01:25:10 2021 Epoch: 137\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25101, smth: 0.25101:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25101, smth: 0.25101:  12%|█▎        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.21847, smth: 0.23474:  12%|█▎        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.21847, smth: 0.23474:  25%|██▌       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.17345, smth: 0.21431:  25%|██▌       | 2/8 [00:04<00:13,  2.29s/it]\u001b[A\n","loss: 0.17345, smth: 0.21431:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.17412, smth: 0.20426:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.17412, smth: 0.20426:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.24073, smth: 0.21156:  50%|█████     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.24073, smth: 0.21156:  62%|██████▎   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.26692, smth: 0.22079:  62%|██████▎   | 5/8 [00:08<00:04,  1.61s/it]\u001b[A\n","loss: 0.26692, smth: 0.22079:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.10152, smth: 0.20375:  75%|███████▌  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.10152, smth: 0.20375:  88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.24450, smth: 0.20884:  88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.24450, smth: 0.20884: 100%|██████████| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:21 2021 Epoch 137, lr: 0.0011020, train loss: 0.20884, train auc: 0.99559, val loss: 0.42842, val auc: 0.98765\n","Thu Jun 10 01:25:21 2021 Epoch: 138\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15826, smth: 0.15826:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15826, smth: 0.15826:  12%|█▎        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.25728, smth: 0.20777:  12%|█▎        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.25728, smth: 0.20777:  25%|██▌       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.22042, smth: 0.21199:  25%|██▌       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.22042, smth: 0.21199:  38%|███▊      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.16940, smth: 0.20134:  38%|███▊      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.16940, smth: 0.20134:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.20690, smth: 0.20245:  50%|█████     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.20690, smth: 0.20245:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.10728, smth: 0.18659:  62%|██████▎   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.10728, smth: 0.18659:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.19275, smth: 0.18747:  75%|███████▌  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.19275, smth: 0.18747:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.21902, smth: 0.19141:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.21902, smth: 0.19141: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:31 2021 Epoch 138, lr: 0.0010661, train loss: 0.19141, train auc: 0.99658, val loss: 0.42001, val auc: 0.98695\n","Thu Jun 10 01:25:31 2021 Epoch: 139\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26328, smth: 0.26328:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26328, smth: 0.26328:  12%|█▎        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.25817, smth: 0.26073:  12%|█▎        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.25817, smth: 0.26073:  25%|██▌       | 2/8 [00:03<00:13,  2.28s/it]\u001b[A\n","loss: 0.18886, smth: 0.23677:  25%|██▌       | 2/8 [00:04<00:13,  2.28s/it]\u001b[A\n","loss: 0.18886, smth: 0.23677:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.20653, smth: 0.22921:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.20653, smth: 0.22921:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.16228, smth: 0.21582:  50%|█████     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.16228, smth: 0.21582:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.12726, smth: 0.20106:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.12726, smth: 0.20106:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.17614, smth: 0.19750:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.17614, smth: 0.19750:  88%|████████▊ | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.16678, smth: 0.19366:  88%|████████▊ | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.16678, smth: 0.19366: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:42 2021 Epoch 139, lr: 0.0010305, train loss: 0.19366, train auc: 0.99532, val loss: 0.47707, val auc: 0.98679\n","Thu Jun 10 01:25:42 2021 Epoch: 140\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26568, smth: 0.26568:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26568, smth: 0.26568:  12%|█▎        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.19004, smth: 0.22786:  12%|█▎        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.19004, smth: 0.22786:  25%|██▌       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.19515, smth: 0.21696:  25%|██▌       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.19515, smth: 0.21696:  38%|███▊      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.21131, smth: 0.21555:  38%|███▊      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.21131, smth: 0.21555:  50%|█████     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.08907, smth: 0.19025:  50%|█████     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.08907, smth: 0.19025:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.20670, smth: 0.19299:  62%|██████▎   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.20670, smth: 0.19299:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.21464, smth: 0.19608:  75%|███████▌  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.21464, smth: 0.19608:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.18233, smth: 0.19436:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.18233, smth: 0.19436: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:52 2021 Epoch 140, lr: 0.0009955, train loss: 0.19436, train auc: 0.99647, val loss: 0.42942, val auc: 0.98599\n","Thu Jun 10 01:25:52 2021 Epoch: 141\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21119, smth: 0.21119:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21119, smth: 0.21119:  12%|█▎        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.19771, smth: 0.20445:  12%|█▎        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.19771, smth: 0.20445:  25%|██▌       | 2/8 [00:03<00:13,  2.24s/it]\u001b[A\n","loss: 0.12508, smth: 0.17799:  25%|██▌       | 2/8 [00:04<00:13,  2.24s/it]\u001b[A\n","loss: 0.12508, smth: 0.17799:  38%|███▊      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.08335, smth: 0.15433:  38%|███▊      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.08335, smth: 0.15433:  50%|█████     | 4/8 [00:05<00:06,  1.50s/it]\u001b[A\n","loss: 0.12205, smth: 0.14788:  50%|█████     | 4/8 [00:06<00:06,  1.50s/it]\u001b[A\n","loss: 0.12205, smth: 0.14788:  62%|██████▎   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.16035, smth: 0.14996:  62%|██████▎   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.16035, smth: 0.14996:  75%|███████▌  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 0.19764, smth: 0.15677:  75%|███████▌  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 0.19764, smth: 0.15677:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.12160, smth: 0.15237:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.12160, smth: 0.15237: 100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:02 2021 Epoch 141, lr: 0.0009608, train loss: 0.15237, train auc: 0.99643, val loss: 0.40550, val auc: 0.98620\n","Thu Jun 10 01:26:02 2021 Epoch: 142\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.09275, smth: 0.09275:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.09275, smth: 0.09275:  12%|█▎        | 1/8 [00:02<00:19,  2.83s/it]\u001b[A\n","loss: 0.10690, smth: 0.09983:  12%|█▎        | 1/8 [00:03<00:19,  2.83s/it]\u001b[A\n","loss: 0.10690, smth: 0.09983:  25%|██▌       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.20480, smth: 0.13482:  25%|██▌       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.20480, smth: 0.13482:  38%|███▊      | 3/8 [00:04<00:09,  1.95s/it]\u001b[A\n","loss: 0.26460, smth: 0.16726:  38%|███▊      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.26460, smth: 0.16726:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.24206, smth: 0.18222:  50%|█████     | 4/8 [00:07<00:06,  1.61s/it]\u001b[A\n","loss: 0.24206, smth: 0.18222:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.17065, smth: 0.18029:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.17065, smth: 0.18029:  75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.23666, smth: 0.18834:  75%|███████▌  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.23666, smth: 0.18834:  88%|████████▊ | 7/8 [00:09<00:01,  1.39s/it]\u001b[A\n","loss: 0.24436, smth: 0.19535:  88%|████████▊ | 7/8 [00:10<00:01,  1.39s/it]\u001b[A\n","loss: 0.24436, smth: 0.19535: 100%|██████████| 8/8 [00:10<00:00,  1.28s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.00it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.63it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:12 2021 Epoch 142, lr: 0.0009267, train loss: 0.19535, train auc: 0.99617, val loss: 0.41284, val auc: 0.98605\n","Thu Jun 10 01:26:12 2021 Epoch: 143\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11698, smth: 0.11698:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.11698, smth: 0.11698:  12%|█▎        | 1/8 [00:03<00:22,  3.17s/it]\u001b[A\n","loss: 0.12477, smth: 0.12088:  12%|█▎        | 1/8 [00:03<00:22,  3.17s/it]\u001b[A\n","loss: 0.12477, smth: 0.12088:  25%|██▌       | 2/8 [00:03<00:14,  2.44s/it]\u001b[A\n","loss: 0.17312, smth: 0.13829:  25%|██▌       | 2/8 [00:05<00:14,  2.44s/it]\u001b[A\n","loss: 0.17312, smth: 0.13829:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.14521, smth: 0.14002:  38%|███▊      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.14521, smth: 0.14002:  50%|█████     | 4/8 [00:06<00:06,  1.72s/it]\u001b[A\n","loss: 0.12792, smth: 0.13760:  50%|█████     | 4/8 [00:08<00:06,  1.72s/it]\u001b[A\n","loss: 0.12792, smth: 0.13760:  62%|██████▎   | 5/8 [00:08<00:05,  1.82s/it]\u001b[A\n","loss: 0.12467, smth: 0.13545:  62%|██████▎   | 5/8 [00:08<00:05,  1.82s/it]\u001b[A\n","loss: 0.12467, smth: 0.13545:  75%|███████▌  | 6/8 [00:08<00:02,  1.48s/it]\u001b[A\n","loss: 0.15826, smth: 0.13871:  75%|███████▌  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.15826, smth: 0.13871:  88%|████████▊ | 7/8 [00:09<00:01,  1.33s/it]\u001b[A\n","loss: 0.21671, smth: 0.14846:  88%|████████▊ | 7/8 [00:10<00:01,  1.33s/it]\u001b[A\n","loss: 0.21671, smth: 0.14846: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:23 2021 Epoch 143, lr: 0.0008930, train loss: 0.14846, train auc: 0.99712, val loss: 0.39224, val auc: 0.98844\n","Thu Jun 10 01:26:23 2021 Epoch: 144\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22539, smth: 0.22539:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22539, smth: 0.22539:  12%|█▎        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.11380, smth: 0.16959:  12%|█▎        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.11380, smth: 0.16959:  25%|██▌       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.14463, smth: 0.16127:  25%|██▌       | 2/8 [00:05<00:13,  2.23s/it]\u001b[A\n","loss: 0.14463, smth: 0.16127:  38%|███▊      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.17018, smth: 0.16350:  38%|███▊      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.17018, smth: 0.16350:  50%|█████     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.11902, smth: 0.15460:  50%|█████     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.11902, smth: 0.15460:  62%|██████▎   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.05156, smth: 0.13743:  62%|██████▎   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.05156, smth: 0.13743:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.17463, smth: 0.14274:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.17463, smth: 0.14274:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.25899, smth: 0.15727:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.25899, smth: 0.15727: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:33 2021 Epoch 144, lr: 0.0008599, train loss: 0.15727, train auc: 0.99690, val loss: 0.39985, val auc: 0.98840\n","Thu Jun 10 01:26:33 2021 Epoch: 145\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15824, smth: 0.15824:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15824, smth: 0.15824:  12%|█▎        | 1/8 [00:03<00:21,  3.01s/it]\u001b[A\n","loss: 0.19263, smth: 0.17544:  12%|█▎        | 1/8 [00:03<00:21,  3.01s/it]\u001b[A\n","loss: 0.19263, smth: 0.17544:  25%|██▌       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.14012, smth: 0.16366:  25%|██▌       | 2/8 [00:04<00:14,  2.35s/it]\u001b[A\n","loss: 0.14012, smth: 0.16366:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.17496, smth: 0.16649:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.17496, smth: 0.16649:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.11302, smth: 0.15579:  50%|█████     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.11302, smth: 0.15579:  62%|██████▎   | 5/8 [00:07<00:05,  1.75s/it]\u001b[A\n","loss: 0.17607, smth: 0.15917:  62%|██████▎   | 5/8 [00:08<00:05,  1.75s/it]\u001b[A\n","loss: 0.17607, smth: 0.15917:  75%|███████▌  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.09735, smth: 0.15034:  75%|███████▌  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.09735, smth: 0.15034:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.15013, smth: 0.15031:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.15013, smth: 0.15031: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:44 2021 Epoch 145, lr: 0.0008272, train loss: 0.15031, train auc: 0.99762, val loss: 0.41492, val auc: 0.98806\n","Thu Jun 10 01:26:44 2021 Epoch: 146\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10220, smth: 0.10220:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10220, smth: 0.10220:  12%|█▎        | 1/8 [00:02<00:17,  2.45s/it]\u001b[A\n","loss: 0.20888, smth: 0.15554:  12%|█▎        | 1/8 [00:03<00:17,  2.45s/it]\u001b[A\n","loss: 0.20888, smth: 0.15554:  25%|██▌       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.17271, smth: 0.16127:  25%|██▌       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.17271, smth: 0.16127:  38%|███▊      | 3/8 [00:04<00:08,  1.74s/it]\u001b[A\n","loss: 0.11571, smth: 0.14988:  38%|███▊      | 3/8 [00:05<00:08,  1.74s/it]\u001b[A\n","loss: 0.11571, smth: 0.14988:  50%|█████     | 4/8 [00:05<00:05,  1.46s/it]\u001b[A\n","loss: 0.19417, smth: 0.15874:  50%|█████     | 4/8 [00:07<00:05,  1.46s/it]\u001b[A\n","loss: 0.19417, smth: 0.15874:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25336, smth: 0.17451:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25336, smth: 0.17451:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.08291, smth: 0.16142:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.08291, smth: 0.16142:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.44277, smth: 0.19659:  88%|████████▊ | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.44277, smth: 0.19659: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:54 2021 Epoch 146, lr: 0.0007950, train loss: 0.19659, train auc: 0.99719, val loss: 0.42819, val auc: 0.98745\n","Thu Jun 10 01:26:54 2021 Epoch: 147\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.34373, smth: 0.34373:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.34373, smth: 0.34373:  12%|█▎        | 1/8 [00:02<00:18,  2.66s/it]\u001b[A\n","loss: 0.12376, smth: 0.23374:  12%|█▎        | 1/8 [00:03<00:18,  2.66s/it]\u001b[A\n","loss: 0.12376, smth: 0.23374:  25%|██▌       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.09720, smth: 0.18823:  25%|██▌       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.09720, smth: 0.18823:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.14639, smth: 0.17777:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.14639, smth: 0.17777:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.07580, smth: 0.15737:  50%|█████     | 4/8 [00:06<00:06,  1.63s/it]\u001b[A\n","loss: 0.07580, smth: 0.15737:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.16474, smth: 0.15860:  62%|██████▎   | 5/8 [00:08<00:04,  1.52s/it]\u001b[A\n","loss: 0.16474, smth: 0.15860:  75%|███████▌  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.20102, smth: 0.16466:  75%|███████▌  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.20102, smth: 0.16466:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23212, smth: 0.17309:  88%|████████▊ | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23212, smth: 0.17309: 100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:04 2021 Epoch 147, lr: 0.0007634, train loss: 0.17309, train auc: 0.99682, val loss: 0.38738, val auc: 0.98775\n","Thu Jun 10 01:27:04 2021 Epoch: 148\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25674, smth: 0.25674:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25674, smth: 0.25674:  12%|█▎        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.12467, smth: 0.19070:  12%|█▎        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.12467, smth: 0.19070:  25%|██▌       | 2/8 [00:03<00:13,  2.31s/it]\u001b[A\n","loss: 0.20261, smth: 0.19467:  25%|██▌       | 2/8 [00:04<00:13,  2.31s/it]\u001b[A\n","loss: 0.20261, smth: 0.19467:  38%|███▊      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.17202, smth: 0.18901:  38%|███▊      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.17202, smth: 0.18901:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.09184, smth: 0.16957:  50%|█████     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.09184, smth: 0.16957:  62%|██████▎   | 5/8 [00:06<00:04,  1.49s/it]\u001b[A\n","loss: 0.20696, smth: 0.17580:  62%|██████▎   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.20696, smth: 0.17580:  75%|███████▌  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.18760, smth: 0.17749:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.18760, smth: 0.17749:  88%|████████▊ | 7/8 [00:08<00:01,  1.28s/it]\u001b[A\n","loss: 0.07054, smth: 0.16412:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.07054, smth: 0.16412: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:14 2021 Epoch 148, lr: 0.0007322, train loss: 0.16412, train auc: 0.99677, val loss: 0.39121, val auc: 0.98626\n","Thu Jun 10 01:27:14 2021 Epoch: 149\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18152, smth: 0.18152:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18152, smth: 0.18152:  12%|█▎        | 1/8 [00:02<00:20,  2.94s/it]\u001b[A\n","loss: 0.15417, smth: 0.16784:  12%|█▎        | 1/8 [00:03<00:20,  2.94s/it]\u001b[A\n","loss: 0.15417, smth: 0.16784:  25%|██▌       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.13157, smth: 0.15575:  25%|██▌       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.13157, smth: 0.15575:  38%|███▊      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.20636, smth: 0.16840:  38%|███▊      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.20636, smth: 0.16840:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.16244, smth: 0.16721:  50%|█████     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.16244, smth: 0.16721:  62%|██████▎   | 5/8 [00:07<00:05,  1.73s/it]\u001b[A\n","loss: 0.27918, smth: 0.18587:  62%|██████▎   | 5/8 [00:08<00:05,  1.73s/it]\u001b[A\n","loss: 0.27918, smth: 0.18587:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.14854, smth: 0.18054:  75%|███████▌  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.14854, smth: 0.18054:  88%|████████▊ | 7/8 [00:09<00:01,  1.36s/it]\u001b[A\n","loss: 0.22020, smth: 0.18549:  88%|████████▊ | 7/8 [00:10<00:01,  1.36s/it]\u001b[A\n","loss: 0.22020, smth: 0.18549: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:25 2021 Epoch 149, lr: 0.0007017, train loss: 0.18549, train auc: 0.99659, val loss: 0.38954, val auc: 0.98653\n","Thu Jun 10 01:27:25 2021 Epoch: 150\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12492, smth: 0.12492:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12492, smth: 0.12492:  12%|█▎        | 1/8 [00:02<00:15,  2.28s/it]\u001b[A\n","loss: 0.18185, smth: 0.15339:  12%|█▎        | 1/8 [00:02<00:15,  2.28s/it]\u001b[A\n","loss: 0.18185, smth: 0.15339:  25%|██▌       | 2/8 [00:03<00:11,  1.83s/it]\u001b[A\n","loss: 0.24930, smth: 0.18536:  25%|██▌       | 2/8 [00:04<00:11,  1.83s/it]\u001b[A\n","loss: 0.24930, smth: 0.18536:  38%|███▊      | 3/8 [00:04<00:08,  1.71s/it]\u001b[A\n","loss: 0.08679, smth: 0.16072:  38%|███▊      | 3/8 [00:05<00:08,  1.71s/it]\u001b[A\n","loss: 0.08679, smth: 0.16072:  50%|█████     | 4/8 [00:05<00:05,  1.44s/it]\u001b[A\n","loss: 0.13094, smth: 0.15476:  50%|█████     | 4/8 [00:06<00:05,  1.44s/it]\u001b[A\n","loss: 0.13094, smth: 0.15476:  62%|██████▎   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.10279, smth: 0.14610:  62%|██████▎   | 5/8 [00:07<00:04,  1.39s/it]\u001b[A\n","loss: 0.10279, smth: 0.14610:  75%|███████▌  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.13058, smth: 0.14388:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.13058, smth: 0.14388:  88%|████████▊ | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.43128, smth: 0.17981:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.43128, smth: 0.17981: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.54it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:35 2021 Epoch 150, lr: 0.0006716, train loss: 0.17981, train auc: 0.99592, val loss: 0.37742, val auc: 0.98789\n","Thu Jun 10 01:27:35 2021 Epoch: 151\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.23775, smth: 0.23775:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.23775, smth: 0.23775:  12%|█▎        | 1/8 [00:03<00:21,  3.13s/it]\u001b[A\n","loss: 0.08748, smth: 0.16262:  12%|█▎        | 1/8 [00:03<00:21,  3.13s/it]\u001b[A\n","loss: 0.08748, smth: 0.16262:  25%|██▌       | 2/8 [00:03<00:14,  2.43s/it]\u001b[A\n","loss: 0.22493, smth: 0.18339:  25%|██▌       | 2/8 [00:05<00:14,  2.43s/it]\u001b[A\n","loss: 0.22493, smth: 0.18339:  38%|███▊      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.12755, smth: 0.16943:  38%|███▊      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.12755, smth: 0.16943:  50%|█████     | 4/8 [00:05<00:06,  1.69s/it]\u001b[A\n","loss: 0.15555, smth: 0.16665:  50%|█████     | 4/8 [00:07<00:06,  1.69s/it]\u001b[A\n","loss: 0.15555, smth: 0.16665:  62%|██████▎   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.16481, smth: 0.16635:  62%|██████▎   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.16481, smth: 0.16635:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.13228, smth: 0.16148:  75%|███████▌  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.13228, smth: 0.16148:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.17020, smth: 0.16257:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.17020, smth: 0.16257: 100%|██████████| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.65it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:45 2021 Epoch 151, lr: 0.0006421, train loss: 0.16257, train auc: 0.99774, val loss: 0.42239, val auc: 0.98572\n","Thu Jun 10 01:27:46 2021 Epoch: 152\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31209, smth: 0.31209:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31209, smth: 0.31209:  12%|█▎        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.18052, smth: 0.24630:  12%|█▎        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.18052, smth: 0.24630:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.16876, smth: 0.22045:  25%|██▌       | 2/8 [00:05<00:12,  2.14s/it]\u001b[A\n","loss: 0.16876, smth: 0.22045:  38%|███▊      | 3/8 [00:05<00:10,  2.04s/it]\u001b[A\n","loss: 0.17460, smth: 0.20899:  38%|███▊      | 3/8 [00:05<00:10,  2.04s/it]\u001b[A\n","loss: 0.17460, smth: 0.20899:  50%|█████     | 4/8 [00:06<00:06,  1.66s/it]\u001b[A\n","loss: 0.15700, smth: 0.19859:  50%|█████     | 4/8 [00:08<00:06,  1.66s/it]\u001b[A\n","loss: 0.15700, smth: 0.19859:  62%|██████▎   | 5/8 [00:08<00:05,  1.79s/it]\u001b[A\n","loss: 0.10349, smth: 0.18274:  62%|██████▎   | 5/8 [00:08<00:05,  1.79s/it]\u001b[A\n","loss: 0.10349, smth: 0.18274:  75%|███████▌  | 6/8 [00:08<00:02,  1.47s/it]\u001b[A\n","loss: 0.11430, smth: 0.17296:  75%|███████▌  | 6/8 [00:09<00:02,  1.47s/it]\u001b[A\n","loss: 0.11430, smth: 0.17296:  88%|████████▊ | 7/8 [00:10<00:01,  1.37s/it]\u001b[A\n","loss: 0.13661, smth: 0.16842:  88%|████████▊ | 7/8 [00:10<00:01,  1.37s/it]\u001b[A\n","loss: 0.13661, smth: 0.16842: 100%|██████████| 8/8 [00:10<00:00,  1.32s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.02it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:57 2021 Epoch 152, lr: 0.0006132, train loss: 0.16842, train auc: 0.99698, val loss: 0.42443, val auc: 0.98604\n","Thu Jun 10 01:27:57 2021 Epoch: 153\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13147, smth: 0.13147:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13147, smth: 0.13147:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.07546, smth: 0.10347:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.07546, smth: 0.10347:  25%|██▌       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.13826, smth: 0.11506:  25%|██▌       | 2/8 [00:04<00:13,  2.23s/it]\u001b[A\n","loss: 0.13826, smth: 0.11506:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.16947, smth: 0.12867:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.16947, smth: 0.12867:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.16262, smth: 0.13546:  50%|█████     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.16262, smth: 0.13546:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.18648, smth: 0.14396:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.18648, smth: 0.14396:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.12006, smth: 0.14055:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.12006, smth: 0.14055:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.12013, smth: 0.13799:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.12013, smth: 0.13799: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:07 2021 Epoch 153, lr: 0.0005849, train loss: 0.13799, train auc: 0.99748, val loss: 0.40105, val auc: 0.98739\n","Thu Jun 10 01:28:07 2021 Epoch: 154\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14928, smth: 0.14928:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14928, smth: 0.14928:  12%|█▎        | 1/8 [00:02<00:20,  2.97s/it]\u001b[A\n","loss: 0.09169, smth: 0.12049:  12%|█▎        | 1/8 [00:03<00:20,  2.97s/it]\u001b[A\n","loss: 0.09169, smth: 0.12049:  25%|██▌       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.18555, smth: 0.14217:  25%|██▌       | 2/8 [00:05<00:13,  2.30s/it]\u001b[A\n","loss: 0.18555, smth: 0.14217:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.23067, smth: 0.16430:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.23067, smth: 0.16430:  50%|█████     | 4/8 [00:05<00:06,  1.66s/it]\u001b[A\n","loss: 0.12723, smth: 0.15688:  50%|█████     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.12723, smth: 0.15688:  62%|██████▎   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.13577, smth: 0.15336:  62%|██████▎   | 5/8 [00:08<00:04,  1.53s/it]\u001b[A\n","loss: 0.13577, smth: 0.15336:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.12851, smth: 0.14981:  75%|███████▌  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.12851, smth: 0.14981:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.16048, smth: 0.15115:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.16048, smth: 0.15115: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:17 2021 Epoch 154, lr: 0.0005571, train loss: 0.15115, train auc: 0.99768, val loss: 0.39047, val auc: 0.98818\n","Thu Jun 10 01:28:17 2021 Epoch: 155\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.20465, smth: 0.20465:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.20465, smth: 0.20465:  12%|█▎        | 1/8 [00:03<00:23,  3.34s/it]\u001b[A\n","loss: 0.18594, smth: 0.19530:  12%|█▎        | 1/8 [00:03<00:23,  3.34s/it]\u001b[A\n","loss: 0.18594, smth: 0.19530:  25%|██▌       | 2/8 [00:04<00:15,  2.57s/it]\u001b[A\n","loss: 0.13071, smth: 0.17377:  25%|██▌       | 2/8 [00:05<00:15,  2.57s/it]\u001b[A\n","loss: 0.13071, smth: 0.17377:  38%|███▊      | 3/8 [00:05<00:11,  2.23s/it]\u001b[A\n","loss: 0.13545, smth: 0.16419:  38%|███▊      | 3/8 [00:06<00:11,  2.23s/it]\u001b[A\n","loss: 0.13545, smth: 0.16419:  50%|█████     | 4/8 [00:06<00:07,  1.80s/it]\u001b[A\n","loss: 0.14287, smth: 0.15992:  50%|█████     | 4/8 [00:07<00:07,  1.80s/it]\u001b[A\n","loss: 0.14287, smth: 0.15992:  62%|██████▎   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.09597, smth: 0.14927:  62%|██████▎   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.09597, smth: 0.14927:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.15991, smth: 0.15079:  75%|███████▌  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.15991, smth: 0.15079:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.25784, smth: 0.16417:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.25784, smth: 0.16417: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:27 2021 Epoch 155, lr: 0.0005300, train loss: 0.16417, train auc: 0.99682, val loss: 0.41544, val auc: 0.98686\n","Thu Jun 10 01:28:27 2021 Epoch: 156\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11765, smth: 0.11765:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.11765, smth: 0.11765:  12%|█▎        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.17083, smth: 0.14424:  12%|█▎        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.17083, smth: 0.14424:  25%|██▌       | 2/8 [00:04<00:15,  2.51s/it]\u001b[A\n","loss: 0.24607, smth: 0.17818:  25%|██▌       | 2/8 [00:05<00:15,  2.51s/it]\u001b[A\n","loss: 0.24607, smth: 0.17818:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.10413, smth: 0.15967:  38%|███▊      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.10413, smth: 0.15967:  50%|█████     | 4/8 [00:05<00:06,  1.70s/it]\u001b[A\n","loss: 0.21883, smth: 0.17150:  50%|█████     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.21883, smth: 0.17150:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.13722, smth: 0.16579:  62%|██████▎   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.13722, smth: 0.16579:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.13460, smth: 0.16133:  75%|███████▌  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.13460, smth: 0.16133:  88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.08201, smth: 0.15142:  88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.08201, smth: 0.15142: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:38 2021 Epoch 156, lr: 0.0005034, train loss: 0.15142, train auc: 0.99744, val loss: 0.42526, val auc: 0.98642\n","Thu Jun 10 01:28:38 2021 Epoch: 157\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18129, smth: 0.18129:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18129, smth: 0.18129:  12%|█▎        | 1/8 [00:02<00:18,  2.60s/it]\u001b[A\n","loss: 0.09824, smth: 0.13977:  12%|█▎        | 1/8 [00:03<00:18,  2.60s/it]\u001b[A\n","loss: 0.09824, smth: 0.13977:  25%|██▌       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.12510, smth: 0.13488:  25%|██▌       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.12510, smth: 0.13488:  38%|███▊      | 3/8 [00:04<00:08,  1.70s/it]\u001b[A\n","loss: 0.06927, smth: 0.11848:  38%|███▊      | 3/8 [00:05<00:08,  1.70s/it]\u001b[A\n","loss: 0.06927, smth: 0.11848:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.15092, smth: 0.12497:  50%|█████     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.15092, smth: 0.12497:  62%|██████▎   | 5/8 [00:06<00:04,  1.40s/it]\u001b[A\n","loss: 0.05510, smth: 0.11332:  62%|██████▎   | 5/8 [00:07<00:04,  1.40s/it]\u001b[A\n","loss: 0.05510, smth: 0.11332:  75%|███████▌  | 6/8 [00:07<00:02,  1.38s/it]\u001b[A\n","loss: 0.10885, smth: 0.11268:  75%|███████▌  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.10885, smth: 0.11268:  88%|████████▊ | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.16707, smth: 0.11948:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.16707, smth: 0.11948: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:48 2021 Epoch 157, lr: 0.0004775, train loss: 0.11948, train auc: 0.99832, val loss: 0.40530, val auc: 0.98776\n","Thu Jun 10 01:28:48 2021 Epoch: 158\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17455, smth: 0.17455:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17455, smth: 0.17455:  12%|█▎        | 1/8 [00:02<00:17,  2.51s/it]\u001b[A\n","loss: 0.18603, smth: 0.18029:  12%|█▎        | 1/8 [00:03<00:17,  2.51s/it]\u001b[A\n","loss: 0.18603, smth: 0.18029:  25%|██▌       | 2/8 [00:03<00:11,  2.00s/it]\u001b[A\n","loss: 0.20528, smth: 0.18862:  25%|██▌       | 2/8 [00:04<00:11,  2.00s/it]\u001b[A\n","loss: 0.20528, smth: 0.18862:  38%|███▊      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.14396, smth: 0.17745:  38%|███▊      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.14396, smth: 0.17745:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.21772, smth: 0.18551:  50%|█████     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.21772, smth: 0.18551:  62%|██████▎   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.10678, smth: 0.17239:  62%|██████▎   | 5/8 [00:07<00:04,  1.39s/it]\u001b[A\n","loss: 0.10678, smth: 0.17239:  75%|███████▌  | 6/8 [00:07<00:02,  1.18s/it]\u001b[A\n","loss: 0.10046, smth: 0.16211:  75%|███████▌  | 6/8 [00:08<00:02,  1.18s/it]\u001b[A\n","loss: 0.10046, smth: 0.16211:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.26906, smth: 0.17548:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.26906, smth: 0.17548: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:57 2021 Epoch 158, lr: 0.0004521, train loss: 0.17548, train auc: 0.99770, val loss: 0.43307, val auc: 0.98606\n","Thu Jun 10 01:28:57 2021 Epoch: 159\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16595, smth: 0.16595:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16595, smth: 0.16595:  12%|█▎        | 1/8 [00:02<00:20,  2.87s/it]\u001b[A\n","loss: 0.09170, smth: 0.12882:  12%|█▎        | 1/8 [00:03<00:20,  2.87s/it]\u001b[A\n","loss: 0.09170, smth: 0.12882:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.09601, smth: 0.11789:  25%|██▌       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.09601, smth: 0.11789:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.18445, smth: 0.13453:  38%|███▊      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.18445, smth: 0.13453:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.20636, smth: 0.14889:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.20636, smth: 0.14889:  62%|██████▎   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.20376, smth: 0.15804:  62%|██████▎   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.20376, smth: 0.15804:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37308, smth: 0.18876:  75%|███████▌  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37308, smth: 0.18876:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.18590, smth: 0.18840:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.18590, smth: 0.18840: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:07 2021 Epoch 159, lr: 0.0004274, train loss: 0.18840, train auc: 0.99730, val loss: 0.42855, val auc: 0.98634\n","Thu Jun 10 01:29:07 2021 Epoch: 160\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.05978, smth: 0.05978:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.05978, smth: 0.05978:  12%|█▎        | 1/8 [00:03<00:21,  3.12s/it]\u001b[A\n","loss: 0.25872, smth: 0.15925:  12%|█▎        | 1/8 [00:03<00:21,  3.12s/it]\u001b[A\n","loss: 0.25872, smth: 0.15925:  25%|██▌       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.15344, smth: 0.15731:  25%|██▌       | 2/8 [00:04<00:14,  2.40s/it]\u001b[A\n","loss: 0.15344, smth: 0.15731:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.10755, smth: 0.14487:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.10755, smth: 0.14487:  50%|█████     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.13133, smth: 0.14216:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.13133, smth: 0.14216:  62%|██████▎   | 5/8 [00:06<00:04,  1.45s/it]\u001b[A\n","loss: 0.21497, smth: 0.15430:  62%|██████▎   | 5/8 [00:07<00:04,  1.45s/it]\u001b[A\n","loss: 0.21497, smth: 0.15430:  75%|███████▌  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.14910, smth: 0.15356:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.14910, smth: 0.15356:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.11717, smth: 0.14901:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.11717, smth: 0.14901: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:17 2021 Epoch 160, lr: 0.0004033, train loss: 0.14901, train auc: 0.99817, val loss: 0.45808, val auc: 0.98601\n","Thu Jun 10 01:29:17 2021 Epoch: 161\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15011, smth: 0.15011:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15011, smth: 0.15011:  12%|█▎        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.19186, smth: 0.17099:  12%|█▎        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.19186, smth: 0.17099:  25%|██▌       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.23841, smth: 0.19346:  25%|██▌       | 2/8 [00:04<00:14,  2.40s/it]\u001b[A\n","loss: 0.23841, smth: 0.19346:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.12333, smth: 0.17593:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.12333, smth: 0.17593:  50%|█████     | 4/8 [00:05<00:06,  1.65s/it]\u001b[A\n","loss: 0.12436, smth: 0.16562:  50%|█████     | 4/8 [00:07<00:06,  1.65s/it]\u001b[A\n","loss: 0.12436, smth: 0.16562:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.10634, smth: 0.15574:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.10634, smth: 0.15574:  75%|███████▌  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.10841, smth: 0.14898:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.10841, smth: 0.14898:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.24309, smth: 0.16074:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.24309, smth: 0.16074: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:27 2021 Epoch 161, lr: 0.0003799, train loss: 0.16074, train auc: 0.99801, val loss: 0.42621, val auc: 0.98648\n","Thu Jun 10 01:29:27 2021 Epoch: 162\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17433, smth: 0.17433:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17433, smth: 0.17433:  12%|█▎        | 1/8 [00:02<00:19,  2.82s/it]\u001b[A\n","loss: 0.11626, smth: 0.14530:  12%|█▎        | 1/8 [00:03<00:19,  2.82s/it]\u001b[A\n","loss: 0.11626, smth: 0.14530:  25%|██▌       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.21322, smth: 0.16794:  25%|██▌       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.21322, smth: 0.16794:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.15383, smth: 0.16441:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.15383, smth: 0.16441:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.17023, smth: 0.16557:  50%|█████     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.17023, smth: 0.16557:  62%|██████▎   | 5/8 [00:06<00:04,  1.45s/it]\u001b[A\n","loss: 0.15574, smth: 0.16393:  62%|██████▎   | 5/8 [00:07<00:04,  1.45s/it]\u001b[A\n","loss: 0.15574, smth: 0.16393:  75%|███████▌  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.23092, smth: 0.17350:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.23092, smth: 0.17350:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.17951, smth: 0.17426:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.17951, smth: 0.17426: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:37 2021 Epoch 162, lr: 0.0003571, train loss: 0.17426, train auc: 0.99765, val loss: 0.39325, val auc: 0.98725\n","Thu Jun 10 01:29:37 2021 Epoch: 163\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21194, smth: 0.21194:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21194, smth: 0.21194:  12%|█▎        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 0.16229, smth: 0.18712:  12%|█▎        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 0.16229, smth: 0.18712:  25%|██▌       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.25829, smth: 0.21084:  25%|██▌       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.25829, smth: 0.21084:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.06937, smth: 0.17547:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.06937, smth: 0.17547:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.08162, smth: 0.15670:  50%|█████     | 4/8 [00:06<00:06,  1.60s/it]\u001b[A\n","loss: 0.08162, smth: 0.15670:  62%|██████▎   | 5/8 [00:06<00:04,  1.44s/it]\u001b[A\n","loss: 0.12643, smth: 0.15166:  62%|██████▎   | 5/8 [00:07<00:04,  1.44s/it]\u001b[A\n","loss: 0.12643, smth: 0.15166:  75%|███████▌  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 0.11691, smth: 0.14669:  75%|███████▌  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 0.11691, smth: 0.14669:  88%|████████▊ | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n","loss: 0.11656, smth: 0.14293:  88%|████████▊ | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n","loss: 0.11656, smth: 0.14293: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:46 2021 Epoch 163, lr: 0.0003349, train loss: 0.14293, train auc: 0.99714, val loss: 0.41691, val auc: 0.98671\n","Thu Jun 10 01:29:46 2021 Epoch: 164\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14145, smth: 0.14145:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14145, smth: 0.14145:  12%|█▎        | 1/8 [00:02<00:19,  2.80s/it]\u001b[A\n","loss: 0.17760, smth: 0.15952:  12%|█▎        | 1/8 [00:03<00:19,  2.80s/it]\u001b[A\n","loss: 0.17760, smth: 0.15952:  25%|██▌       | 2/8 [00:03<00:13,  2.19s/it]\u001b[A\n","loss: 0.12421, smth: 0.14775:  25%|██▌       | 2/8 [00:04<00:13,  2.19s/it]\u001b[A\n","loss: 0.12421, smth: 0.14775:  38%|███▊      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.25496, smth: 0.17455:  38%|███▊      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.25496, smth: 0.17455:  50%|█████     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.10863, smth: 0.16137:  50%|█████     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.10863, smth: 0.16137:  62%|██████▎   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.21128, smth: 0.16969:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.21128, smth: 0.16969:  75%|███████▌  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 0.10825, smth: 0.16091:  75%|███████▌  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 0.10825, smth: 0.16091:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.21528, smth: 0.16771:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.21528, smth: 0.16771: 100%|██████████| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:56 2021 Epoch 164, lr: 0.0003135, train loss: 0.16771, train auc: 0.99742, val loss: 0.40619, val auc: 0.98658\n","Thu Jun 10 01:29:56 2021 Epoch: 165\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18675, smth: 0.18675:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18675, smth: 0.18675:  12%|█▎        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 0.10816, smth: 0.14745:  12%|█▎        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 0.10816, smth: 0.14745:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.12837, smth: 0.14109:  25%|██▌       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.12837, smth: 0.14109:  38%|███▊      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.18167, smth: 0.15124:  38%|███▊      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.18167, smth: 0.15124:  50%|█████     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.11418, smth: 0.14382:  50%|█████     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.11418, smth: 0.14382:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.14832, smth: 0.14457:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.14832, smth: 0.14457:  75%|███████▌  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.11832, smth: 0.14082:  75%|███████▌  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.11832, smth: 0.14082:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.24058, smth: 0.15329:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.24058, smth: 0.15329: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.91it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:05 2021 Epoch 165, lr: 0.0002926, train loss: 0.15329, train auc: 0.99748, val loss: 0.42259, val auc: 0.98630\n","Thu Jun 10 01:30:05 2021 Epoch: 166\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16013, smth: 0.16013:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16013, smth: 0.16013:  12%|█▎        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.10616, smth: 0.13315:  12%|█▎        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.10616, smth: 0.13315:  25%|██▌       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.20311, smth: 0.15647:  25%|██▌       | 2/8 [00:04<00:12,  2.15s/it]\u001b[A\n","loss: 0.20311, smth: 0.15647:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.15022, smth: 0.15491:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.15022, smth: 0.15491:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.10059, smth: 0.14404:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.10059, smth: 0.14404:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.17702, smth: 0.14954:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.17702, smth: 0.14954:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.19346, smth: 0.15581:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.19346, smth: 0.15581:  88%|████████▊ | 7/8 [00:08<00:01,  1.28s/it]\u001b[A\n","loss: 0.07390, smth: 0.14557:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.07390, smth: 0.14557: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:15 2021 Epoch 166, lr: 0.0002725, train loss: 0.14557, train auc: 0.99727, val loss: 0.44077, val auc: 0.98604\n","Thu Jun 10 01:30:15 2021 Epoch: 167\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12597, smth: 0.12597:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12597, smth: 0.12597:  12%|█▎        | 1/8 [00:02<00:18,  2.60s/it]\u001b[A\n","loss: 0.09721, smth: 0.11159:  12%|█▎        | 1/8 [00:03<00:18,  2.60s/it]\u001b[A\n","loss: 0.09721, smth: 0.11159:  25%|██▌       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.06627, smth: 0.09649:  25%|██▌       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.06627, smth: 0.09649:  38%|███▊      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.27604, smth: 0.14137:  38%|███▊      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.27604, smth: 0.14137:  50%|█████     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.14617, smth: 0.14233:  50%|█████     | 4/8 [00:07<00:06,  1.57s/it]\u001b[A\n","loss: 0.14617, smth: 0.14233:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.09448, smth: 0.13436:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.09448, smth: 0.13436:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.04615, smth: 0.12176:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.04615, smth: 0.12176:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.05943, smth: 0.11397:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.05943, smth: 0.11397: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:25 2021 Epoch 167, lr: 0.0002530, train loss: 0.11397, train auc: 0.99808, val loss: 0.46053, val auc: 0.98579\n","Thu Jun 10 01:30:25 2021 Epoch: 168\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10238, smth: 0.10238:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10238, smth: 0.10238:  12%|█▎        | 1/8 [00:02<00:20,  2.99s/it]\u001b[A\n","loss: 0.19012, smth: 0.14625:  12%|█▎        | 1/8 [00:03<00:20,  2.99s/it]\u001b[A\n","loss: 0.19012, smth: 0.14625:  25%|██▌       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.17644, smth: 0.15632:  25%|██▌       | 2/8 [00:05<00:13,  2.32s/it]\u001b[A\n","loss: 0.17644, smth: 0.15632:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.18848, smth: 0.16436:  38%|███▊      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.18848, smth: 0.16436:  50%|█████     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.22597, smth: 0.17668:  50%|█████     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.22597, smth: 0.17668:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25844, smth: 0.19031:  62%|██████▎   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25844, smth: 0.19031:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.10524, smth: 0.17815:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.10524, smth: 0.17815:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.16396, smth: 0.17638:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.16396, smth: 0.17638: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.21it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:35 2021 Epoch 168, lr: 0.0002342, train loss: 0.17638, train auc: 0.99776, val loss: 0.47086, val auc: 0.98580\n","Thu Jun 10 01:30:35 2021 Epoch: 169\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17320, smth: 0.17320:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17320, smth: 0.17320:  12%|█▎        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.10856, smth: 0.14088:  12%|█▎        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.10856, smth: 0.14088:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.05775, smth: 0.11317:  25%|██▌       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.05775, smth: 0.11317:  38%|███▊      | 3/8 [00:04<00:09,  1.97s/it]\u001b[A\n","loss: 0.08089, smth: 0.10510:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.08089, smth: 0.10510:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.17794, smth: 0.11967:  50%|█████     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.17794, smth: 0.11967:  62%|██████▎   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.12705, smth: 0.12090:  62%|██████▎   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.12705, smth: 0.12090:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.10137, smth: 0.11811:  75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.10137, smth: 0.11811:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.16940, smth: 0.12452:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.16940, smth: 0.12452: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:46 2021 Epoch 169, lr: 0.0002161, train loss: 0.12452, train auc: 0.99844, val loss: 0.43357, val auc: 0.98675\n","Thu Jun 10 01:30:46 2021 Epoch: 170\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17333, smth: 0.17333:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17333, smth: 0.17333:  12%|█▎        | 1/8 [00:02<00:18,  2.70s/it]\u001b[A\n","loss: 0.17184, smth: 0.17259:  12%|█▎        | 1/8 [00:03<00:18,  2.70s/it]\u001b[A\n","loss: 0.17184, smth: 0.17259:  25%|██▌       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.21688, smth: 0.18735:  25%|██▌       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.21688, smth: 0.18735:  38%|███▊      | 3/8 [00:04<00:09,  1.94s/it]\u001b[A\n","loss: 0.08344, smth: 0.16137:  38%|███▊      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.08344, smth: 0.16137:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.24841, smth: 0.17878:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.24841, smth: 0.17878:  62%|██████▎   | 5/8 [00:06<00:04,  1.41s/it]\u001b[A\n","loss: 0.12654, smth: 0.17007:  62%|██████▎   | 5/8 [00:07<00:04,  1.41s/it]\u001b[A\n","loss: 0.12654, smth: 0.17007:  75%|███████▌  | 6/8 [00:07<00:02,  1.21s/it]\u001b[A\n","loss: 0.11995, smth: 0.16291:  75%|███████▌  | 6/8 [00:08<00:02,  1.21s/it]\u001b[A\n","loss: 0.11995, smth: 0.16291:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.27506, smth: 0.17693:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.27506, smth: 0.17693: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:55 2021 Epoch 170, lr: 0.0001987, train loss: 0.17693, train auc: 0.99828, val loss: 0.41823, val auc: 0.98698\n","Thu Jun 10 01:30:55 2021 Epoch: 171\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28006, smth: 0.28006:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.28006, smth: 0.28006:  12%|█▎        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.26611, smth: 0.27308:  12%|█▎        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.26611, smth: 0.27308:  25%|██▌       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.08185, smth: 0.20934:  25%|██▌       | 2/8 [00:04<00:13,  2.26s/it]\u001b[A\n","loss: 0.08185, smth: 0.20934:  38%|███▊      | 3/8 [00:04<00:09,  1.97s/it]\u001b[A\n","loss: 0.11097, smth: 0.18474:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.11097, smth: 0.18474:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.13629, smth: 0.17505:  50%|█████     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.13629, smth: 0.17505:  62%|██████▎   | 5/8 [00:06<00:04,  1.49s/it]\u001b[A\n","loss: 0.11988, smth: 0.16586:  62%|██████▎   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.11988, smth: 0.16586:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.23689, smth: 0.17600:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.23689, smth: 0.17600:  88%|████████▊ | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.11294, smth: 0.16812:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.11294, smth: 0.16812: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.86it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.54it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:05 2021 Epoch 171, lr: 0.0001820, train loss: 0.16812, train auc: 0.99850, val loss: 0.40949, val auc: 0.98742\n","Thu Jun 10 01:31:05 2021 Epoch: 172\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11517, smth: 0.11517:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.11517, smth: 0.11517:  12%|█▎        | 1/8 [00:02<00:19,  2.76s/it]\u001b[A\n","loss: 0.16426, smth: 0.13972:  12%|█▎        | 1/8 [00:03<00:19,  2.76s/it]\u001b[A\n","loss: 0.16426, smth: 0.13972:  25%|██▌       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.14588, smth: 0.14177:  25%|██▌       | 2/8 [00:04<00:13,  2.18s/it]\u001b[A\n","loss: 0.14588, smth: 0.14177:  38%|███▊      | 3/8 [00:05<00:09,  1.96s/it]\u001b[A\n","loss: 0.12788, smth: 0.13830:  38%|███▊      | 3/8 [00:05<00:09,  1.96s/it]\u001b[A\n","loss: 0.12788, smth: 0.13830:  50%|█████     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.18738, smth: 0.14812:  50%|█████     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.18738, smth: 0.14812:  62%|██████▎   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.20935, smth: 0.15832:  62%|██████▎   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.20935, smth: 0.15832:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.06927, smth: 0.14560:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.06927, smth: 0.14560:  88%|████████▊ | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 0.13458, smth: 0.14422:  88%|████████▊ | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 0.13458, smth: 0.14422: 100%|██████████| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:15 2021 Epoch 172, lr: 0.0001660, train loss: 0.14422, train auc: 0.99843, val loss: 0.42925, val auc: 0.98706\n","Thu Jun 10 01:31:15 2021 Epoch: 173\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13060, smth: 0.13060:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13060, smth: 0.13060:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.08340, smth: 0.10700:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.08340, smth: 0.10700:  25%|██▌       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.12862, smth: 0.11421:  25%|██▌       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.12862, smth: 0.11421:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.16907, smth: 0.12792:  38%|███▊      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.16907, smth: 0.12792:  50%|█████     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.08822, smth: 0.11998:  50%|█████     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.08822, smth: 0.11998:  62%|██████▎   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.18740, smth: 0.13122:  62%|██████▎   | 5/8 [00:08<00:04,  1.51s/it]\u001b[A\n","loss: 0.18740, smth: 0.13122:  75%|███████▌  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.06526, smth: 0.12180:  75%|███████▌  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.06526, smth: 0.12180:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.14744, smth: 0.12500:  88%|████████▊ | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.14744, smth: 0.12500: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:25 2021 Epoch 173, lr: 0.0001508, train loss: 0.12500, train auc: 0.99800, val loss: 0.43995, val auc: 0.98684\n","Thu Jun 10 01:31:25 2021 Epoch: 174\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16859, smth: 0.16859:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16859, smth: 0.16859:  12%|█▎        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.08533, smth: 0.12696:  12%|█▎        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.08533, smth: 0.12696:  25%|██▌       | 2/8 [00:03<00:12,  2.08s/it]\u001b[A\n","loss: 0.14345, smth: 0.13246:  25%|██▌       | 2/8 [00:04<00:12,  2.08s/it]\u001b[A\n","loss: 0.14345, smth: 0.13246:  38%|███▊      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.20164, smth: 0.14975:  38%|███▊      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.20164, smth: 0.14975:  50%|█████     | 4/8 [00:05<00:06,  1.54s/it]\u001b[A\n","loss: 0.09462, smth: 0.13873:  50%|█████     | 4/8 [00:06<00:06,  1.54s/it]\u001b[A\n","loss: 0.09462, smth: 0.13873:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.10893, smth: 0.13376:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.10893, smth: 0.13376:  75%|███████▌  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.17023, smth: 0.13897:  75%|███████▌  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.17023, smth: 0.13897:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14275, smth: 0.13944:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14275, smth: 0.13944: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:35 2021 Epoch 174, lr: 0.0001362, train loss: 0.13944, train auc: 0.99779, val loss: 0.40915, val auc: 0.98781\n","Thu Jun 10 01:31:35 2021 Epoch: 175\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26870, smth: 0.26870:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.26870, smth: 0.26870:  12%|█▎        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.20046, smth: 0.23458:  12%|█▎        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.20046, smth: 0.23458:  25%|██▌       | 2/8 [00:03<00:14,  2.49s/it]\u001b[A\n","loss: 0.15534, smth: 0.20816:  25%|██▌       | 2/8 [00:04<00:14,  2.49s/it]\u001b[A\n","loss: 0.15534, smth: 0.20816:  38%|███▊      | 3/8 [00:04<00:10,  2.05s/it]\u001b[A\n","loss: 0.13178, smth: 0.18907:  38%|███▊      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.13178, smth: 0.18907:  50%|█████     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.12422, smth: 0.17610:  50%|█████     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.12422, smth: 0.17610:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.12179, smth: 0.16705:  62%|██████▎   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.12179, smth: 0.16705:  75%|███████▌  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.15937, smth: 0.16595:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.15937, smth: 0.16595:  88%|████████▊ | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.09338, smth: 0.15688:  88%|████████▊ | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.09338, smth: 0.15688: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:45 2021 Epoch 175, lr: 0.0001224, train loss: 0.15688, train auc: 0.99825, val loss: 0.42441, val auc: 0.98749\n","Thu Jun 10 01:31:45 2021 Epoch: 176\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14234, smth: 0.14234:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14234, smth: 0.14234:  12%|█▎        | 1/8 [00:02<00:16,  2.38s/it]\u001b[A\n","loss: 0.12612, smth: 0.13423:  12%|█▎        | 1/8 [00:03<00:16,  2.38s/it]\u001b[A\n","loss: 0.12612, smth: 0.13423:  25%|██▌       | 2/8 [00:03<00:11,  1.89s/it]\u001b[A\n","loss: 0.09998, smth: 0.12281:  25%|██▌       | 2/8 [00:04<00:11,  1.89s/it]\u001b[A\n","loss: 0.09998, smth: 0.12281:  38%|███▊      | 3/8 [00:04<00:08,  1.67s/it]\u001b[A\n","loss: 0.17765, smth: 0.13652:  38%|███▊      | 3/8 [00:05<00:08,  1.67s/it]\u001b[A\n","loss: 0.17765, smth: 0.13652:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.12065, smth: 0.13335:  50%|█████     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.12065, smth: 0.13335:  62%|██████▎   | 5/8 [00:06<00:03,  1.31s/it]\u001b[A\n","loss: 0.12230, smth: 0.13151:  62%|██████▎   | 5/8 [00:07<00:03,  1.31s/it]\u001b[A\n","loss: 0.12230, smth: 0.13151:  75%|███████▌  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.08323, smth: 0.12461:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.08323, smth: 0.12461:  88%|████████▊ | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.05122, smth: 0.11544:  88%|████████▊ | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.05122, smth: 0.11544: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.64it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:55 2021 Epoch 176, lr: 0.0001092, train loss: 0.11544, train auc: 0.99837, val loss: 0.42366, val auc: 0.98706\n","Thu Jun 10 01:31:55 2021 Epoch: 177\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19401, smth: 0.19401:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19401, smth: 0.19401:  12%|█▎        | 1/8 [00:02<00:18,  2.65s/it]\u001b[A\n","loss: 0.27446, smth: 0.23424:  12%|█▎        | 1/8 [00:03<00:18,  2.65s/it]\u001b[A\n","loss: 0.27446, smth: 0.23424:  25%|██▌       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.25075, smth: 0.23974:  25%|██▌       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.25075, smth: 0.23974:  38%|███▊      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.07892, smth: 0.19953:  38%|███▊      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.07892, smth: 0.19953:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.14758, smth: 0.18914:  50%|█████     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.14758, smth: 0.18914:  62%|██████▎   | 5/8 [00:06<00:04,  1.35s/it]\u001b[A\n","loss: 0.11287, smth: 0.17643:  62%|██████▎   | 5/8 [00:07<00:04,  1.35s/it]\u001b[A\n","loss: 0.11287, smth: 0.17643:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.09969, smth: 0.16547:  75%|███████▌  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.09969, smth: 0.16547:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.16987, smth: 0.16602:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.16987, smth: 0.16602: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:05 2021 Epoch 177, lr: 0.0000968, train loss: 0.16602, train auc: 0.99767, val loss: 0.40045, val auc: 0.98806\n","Thu Jun 10 01:32:05 2021 Epoch: 178\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19588, smth: 0.19588:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19588, smth: 0.19588:  12%|█▎        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.09092, smth: 0.14340:  12%|█▎        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.09092, smth: 0.14340:  25%|██▌       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.17918, smth: 0.15533:  25%|██▌       | 2/8 [00:05<00:13,  2.25s/it]\u001b[A\n","loss: 0.17918, smth: 0.15533:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.11874, smth: 0.14618:  38%|███▊      | 3/8 [00:06<00:10,  2.06s/it]\u001b[A\n","loss: 0.11874, smth: 0.14618:  50%|█████     | 4/8 [00:06<00:06,  1.69s/it]\u001b[A\n","loss: 0.17128, smth: 0.15120:  50%|█████     | 4/8 [00:07<00:06,  1.69s/it]\u001b[A\n","loss: 0.17128, smth: 0.15120:  62%|██████▎   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.15698, smth: 0.15216:  62%|██████▎   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.15698, smth: 0.15216:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.13845, smth: 0.15020:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.13845, smth: 0.15020:  88%|████████▊ | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.22899, smth: 0.16005:  88%|████████▊ | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.22899, smth: 0.16005: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:15 2021 Epoch 178, lr: 0.0000852, train loss: 0.16005, train auc: 0.99724, val loss: 0.43318, val auc: 0.98690\n","Thu Jun 10 01:32:15 2021 Epoch: 179\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10943, smth: 0.10943:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10943, smth: 0.10943:  12%|█▎        | 1/8 [00:02<00:20,  2.98s/it]\u001b[A\n","loss: 0.12003, smth: 0.11473:  12%|█▎        | 1/8 [00:03<00:20,  2.98s/it]\u001b[A\n","loss: 0.12003, smth: 0.11473:  25%|██▌       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.19145, smth: 0.14030:  25%|██▌       | 2/8 [00:04<00:13,  2.32s/it]\u001b[A\n","loss: 0.19145, smth: 0.14030:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.14797, smth: 0.14222:  38%|███▊      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.14797, smth: 0.14222:  50%|█████     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.14171, smth: 0.14212:  50%|█████     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 0.14171, smth: 0.14212:  62%|██████▎   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.14754, smth: 0.14302:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.14754, smth: 0.14302:  75%|███████▌  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.19664, smth: 0.15068:  75%|███████▌  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.19664, smth: 0.15068:  88%|████████▊ | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.09264, smth: 0.14343:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.09264, smth: 0.14343: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:25 2021 Epoch 179, lr: 0.0000743, train loss: 0.14343, train auc: 0.99728, val loss: 0.42815, val auc: 0.98652\n","Thu Jun 10 01:32:25 2021 Epoch: 180\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16585, smth: 0.16585:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16585, smth: 0.16585:  12%|█▎        | 1/8 [00:02<00:17,  2.53s/it]\u001b[A\n","loss: 0.17763, smth: 0.17174:  12%|█▎        | 1/8 [00:03<00:17,  2.53s/it]\u001b[A\n","loss: 0.17763, smth: 0.17174:  25%|██▌       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.16556, smth: 0.16968:  25%|██▌       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.16556, smth: 0.16968:  38%|███▊      | 3/8 [00:04<00:08,  1.75s/it]\u001b[A\n","loss: 0.10474, smth: 0.15344:  38%|███▊      | 3/8 [00:05<00:08,  1.75s/it]\u001b[A\n","loss: 0.10474, smth: 0.15344:  50%|█████     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.07794, smth: 0.13834:  50%|█████     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.07794, smth: 0.13834:  62%|██████▎   | 5/8 [00:06<00:04,  1.44s/it]\u001b[A\n","loss: 0.10095, smth: 0.13211:  62%|██████▎   | 5/8 [00:07<00:04,  1.44s/it]\u001b[A\n","loss: 0.10095, smth: 0.13211:  75%|███████▌  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.14927, smth: 0.13456:  75%|███████▌  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.14927, smth: 0.13456:  88%|████████▊ | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.17942, smth: 0.14017:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.17942, smth: 0.14017: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:34 2021 Epoch 180, lr: 0.0000641, train loss: 0.14017, train auc: 0.99831, val loss: 0.43070, val auc: 0.98661\n","Thu Jun 10 01:32:34 2021 Epoch: 181\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08492, smth: 0.08492:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08492, smth: 0.08492:  12%|█▎        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.12036, smth: 0.10264:  12%|█▎        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.12036, smth: 0.10264:  25%|██▌       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.13501, smth: 0.11343:  25%|██▌       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.13501, smth: 0.11343:  38%|███▊      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.14055, smth: 0.12021:  38%|███▊      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.14055, smth: 0.12021:  50%|█████     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.14451, smth: 0.12507:  50%|█████     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.14451, smth: 0.12507:  62%|██████▎   | 5/8 [00:06<00:04,  1.49s/it]\u001b[A\n","loss: 0.10274, smth: 0.12135:  62%|██████▎   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.10274, smth: 0.12135:  75%|███████▌  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.09083, smth: 0.11699:  75%|███████▌  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.09083, smth: 0.11699:  88%|████████▊ | 7/8 [00:08<00:01,  1.24s/it]\u001b[A\n","loss: 0.11763, smth: 0.11707:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.11763, smth: 0.11707: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:44 2021 Epoch 181, lr: 0.0000546, train loss: 0.11707, train auc: 0.99794, val loss: 0.44045, val auc: 0.98613\n","Thu Jun 10 01:32:44 2021 Epoch: 182\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17050, smth: 0.17050:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17050, smth: 0.17050:  12%|█▎        | 1/8 [00:02<00:17,  2.51s/it]\u001b[A\n","loss: 0.12084, smth: 0.14567:  12%|█▎        | 1/8 [00:03<00:17,  2.51s/it]\u001b[A\n","loss: 0.12084, smth: 0.14567:  25%|██▌       | 2/8 [00:03<00:11,  1.99s/it]\u001b[A\n","loss: 0.09647, smth: 0.12927:  25%|██▌       | 2/8 [00:04<00:11,  1.99s/it]\u001b[A\n","loss: 0.09647, smth: 0.12927:  38%|███▊      | 3/8 [00:04<00:08,  1.80s/it]\u001b[A\n","loss: 0.12085, smth: 0.12717:  38%|███▊      | 3/8 [00:05<00:08,  1.80s/it]\u001b[A\n","loss: 0.12085, smth: 0.12717:  50%|█████     | 4/8 [00:05<00:05,  1.50s/it]\u001b[A\n","loss: 0.08144, smth: 0.11802:  50%|█████     | 4/8 [00:06<00:05,  1.50s/it]\u001b[A\n","loss: 0.08144, smth: 0.11802:  62%|██████▎   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.13071, smth: 0.12014:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.13071, smth: 0.12014:  75%|███████▌  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.11970, smth: 0.12007:  75%|███████▌  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.11970, smth: 0.12007:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.16779, smth: 0.12604:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.16779, smth: 0.12604: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:54 2021 Epoch 182, lr: 0.0000459, train loss: 0.12604, train auc: 0.99651, val loss: 0.44270, val auc: 0.98645\n","Thu Jun 10 01:32:54 2021 Epoch: 183\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.09814, smth: 0.09814:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.09814, smth: 0.09814:  12%|█▎        | 1/8 [00:02<00:17,  2.45s/it]\u001b[A\n","loss: 0.11978, smth: 0.10896:  12%|█▎        | 1/8 [00:03<00:17,  2.45s/it]\u001b[A\n","loss: 0.11978, smth: 0.10896:  25%|██▌       | 2/8 [00:03<00:11,  1.95s/it]\u001b[A\n","loss: 0.08619, smth: 0.10137:  25%|██▌       | 2/8 [00:04<00:11,  1.95s/it]\u001b[A\n","loss: 0.08619, smth: 0.10137:  38%|███▊      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.14054, smth: 0.11116:  38%|███▊      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.14054, smth: 0.11116:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.16199, smth: 0.12133:  50%|█████     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.16199, smth: 0.12133:  62%|██████▎   | 5/8 [00:06<00:04,  1.34s/it]\u001b[A\n","loss: 0.15193, smth: 0.12643:  62%|██████▎   | 5/8 [00:07<00:04,  1.34s/it]\u001b[A\n","loss: 0.15193, smth: 0.12643:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.13302, smth: 0.12737:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.13302, smth: 0.12737:  88%|████████▊ | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.13681, smth: 0.12855:  88%|████████▊ | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.13681, smth: 0.12855: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:04 2021 Epoch 183, lr: 0.0000380, train loss: 0.12855, train auc: 0.99747, val loss: 0.45148, val auc: 0.98651\n","Thu Jun 10 01:33:04 2021 Epoch: 184\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08094, smth: 0.08094:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08094, smth: 0.08094:  12%|█▎        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.14225, smth: 0.11160:  12%|█▎        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.14225, smth: 0.11160:  25%|██▌       | 2/8 [00:03<00:13,  2.27s/it]\u001b[A\n","loss: 0.06285, smth: 0.09535:  25%|██▌       | 2/8 [00:04<00:13,  2.27s/it]\u001b[A\n","loss: 0.06285, smth: 0.09535:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.10863, smth: 0.09867:  38%|███▊      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.10863, smth: 0.09867:  50%|█████     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.16364, smth: 0.11166:  50%|█████     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.16364, smth: 0.11166:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.10913, smth: 0.11124:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.10913, smth: 0.11124:  75%|███████▌  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.14499, smth: 0.11606:  75%|███████▌  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.14499, smth: 0.11606:  88%|████████▊ | 7/8 [00:08<00:01,  1.24s/it]\u001b[A\n","loss: 0.04209, smth: 0.10681:  88%|████████▊ | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.04209, smth: 0.10681: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.97it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.59it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:14 2021 Epoch 184, lr: 0.0000308, train loss: 0.10681, train auc: 0.99756, val loss: 0.46452, val auc: 0.98634\n","Thu Jun 10 01:33:14 2021 Epoch: 185\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08871, smth: 0.08871:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08871, smth: 0.08871:  12%|█▎        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.13433, smth: 0.11152:  12%|█▎        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.13433, smth: 0.11152:  25%|██▌       | 2/8 [00:03<00:13,  2.27s/it]\u001b[A\n","loss: 0.18420, smth: 0.13575:  25%|██▌       | 2/8 [00:05<00:13,  2.27s/it]\u001b[A\n","loss: 0.18420, smth: 0.13575:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.10451, smth: 0.12794:  38%|███▊      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.10451, smth: 0.12794:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.08251, smth: 0.11885:  50%|█████     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.08251, smth: 0.11885:  62%|██████▎   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.10321, smth: 0.11625:  62%|██████▎   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.10321, smth: 0.11625:  75%|███████▌  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.10077, smth: 0.11403:  75%|███████▌  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.10077, smth: 0.11403:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.21700, smth: 0.12691:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.21700, smth: 0.12691: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.88it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:24 2021 Epoch 185, lr: 0.0000243, train loss: 0.12691, train auc: 0.99843, val loss: 0.46904, val auc: 0.98630\n","Thu Jun 10 01:33:24 2021 Epoch: 186\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14755, smth: 0.14755:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14755, smth: 0.14755:  12%|█▎        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.13195, smth: 0.13975:  12%|█▎        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.13195, smth: 0.13975:  25%|██▌       | 2/8 [00:03<00:12,  2.16s/it]\u001b[A\n","loss: 0.13150, smth: 0.13700:  25%|██▌       | 2/8 [00:04<00:12,  2.16s/it]\u001b[A\n","loss: 0.13150, smth: 0.13700:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.16763, smth: 0.14466:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.16763, smth: 0.14466:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.19337, smth: 0.15440:  50%|█████     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.19337, smth: 0.15440:  62%|██████▎   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.20188, smth: 0.16231:  62%|██████▎   | 5/8 [00:08<00:04,  1.61s/it]\u001b[A\n","loss: 0.20188, smth: 0.16231:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.12854, smth: 0.15749:  75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.12854, smth: 0.15749:  88%|████████▊ | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.10996, smth: 0.15155:  88%|████████▊ | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.10996, smth: 0.15155: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.61it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:34 2021 Epoch 186, lr: 0.0000186, train loss: 0.15155, train auc: 0.99783, val loss: 0.47112, val auc: 0.98630\n","Thu Jun 10 01:33:34 2021 Epoch: 187\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12273, smth: 0.12273:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12273, smth: 0.12273:  12%|█▎        | 1/8 [00:02<00:16,  2.38s/it]\u001b[A\n","loss: 0.16400, smth: 0.14337:  12%|█▎        | 1/8 [00:03<00:16,  2.38s/it]\u001b[A\n","loss: 0.16400, smth: 0.14337:  25%|██▌       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.14375, smth: 0.14349:  25%|██▌       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.14375, smth: 0.14349:  38%|███▊      | 3/8 [00:04<00:08,  1.71s/it]\u001b[A\n","loss: 0.09919, smth: 0.13242:  38%|███▊      | 3/8 [00:05<00:08,  1.71s/it]\u001b[A\n","loss: 0.09919, smth: 0.13242:  50%|█████     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.11274, smth: 0.12848:  50%|█████     | 4/8 [00:06<00:06,  1.57s/it]\u001b[A\n","loss: 0.11274, smth: 0.12848:  62%|██████▎   | 5/8 [00:06<00:04,  1.34s/it]\u001b[A\n","loss: 0.24970, smth: 0.14869:  62%|██████▎   | 5/8 [00:08<00:04,  1.34s/it]\u001b[A\n","loss: 0.24970, smth: 0.14869:  75%|███████▌  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.09290, smth: 0.14072:  75%|███████▌  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.09290, smth: 0.14072:  88%|████████▊ | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.28994, smth: 0.15937:  88%|████████▊ | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.28994, smth: 0.15937: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:44 2021 Epoch 187, lr: 0.0000137, train loss: 0.15937, train auc: 0.99707, val loss: 0.47106, val auc: 0.98597\n","Thu Jun 10 01:33:44 2021 Epoch: 188\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.05052, smth: 0.05052:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.05052, smth: 0.05052:  12%|█▎        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.24440, smth: 0.14746:  12%|█▎        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.24440, smth: 0.14746:  25%|██▌       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.16334, smth: 0.15275:  25%|██▌       | 2/8 [00:04<00:13,  2.26s/it]\u001b[A\n","loss: 0.16334, smth: 0.15275:  38%|███▊      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.09060, smth: 0.13722:  38%|███▊      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.09060, smth: 0.13722:  50%|█████     | 4/8 [00:05<00:06,  1.54s/it]\u001b[A\n","loss: 0.07114, smth: 0.12400:  50%|█████     | 4/8 [00:06<00:06,  1.54s/it]\u001b[A\n","loss: 0.07114, smth: 0.12400:  62%|██████▎   | 5/8 [00:06<00:04,  1.51s/it]\u001b[A\n","loss: 0.13711, smth: 0.12619:  62%|██████▎   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.13711, smth: 0.12619:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.06510, smth: 0.11746:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.06510, smth: 0.11746:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.15644, smth: 0.12233:  88%|████████▊ | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.15644, smth: 0.12233: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.94it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.57it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:54 2021 Epoch 188, lr: 0.0000095, train loss: 0.12233, train auc: 0.99756, val loss: 0.45908, val auc: 0.98647\n","Thu Jun 10 01:33:54 2021 Epoch: 189\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12943, smth: 0.12943:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12943, smth: 0.12943:  12%|█▎        | 1/8 [00:02<00:18,  2.65s/it]\u001b[A\n","loss: 0.12140, smth: 0.12542:  12%|█▎        | 1/8 [00:03<00:18,  2.65s/it]\u001b[A\n","loss: 0.12140, smth: 0.12542:  25%|██▌       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.15549, smth: 0.13544:  25%|██▌       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.15549, smth: 0.13544:  38%|███▊      | 3/8 [00:04<00:09,  1.85s/it]\u001b[A\n","loss: 0.12087, smth: 0.13180:  38%|███▊      | 3/8 [00:05<00:09,  1.85s/it]\u001b[A\n","loss: 0.12087, smth: 0.13180:  50%|█████     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.13725, smth: 0.13289:  50%|█████     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.13725, smth: 0.13289:  62%|██████▎   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.22044, smth: 0.14748:  62%|██████▎   | 5/8 [00:07<00:04,  1.39s/it]\u001b[A\n","loss: 0.22044, smth: 0.14748:  75%|███████▌  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.14679, smth: 0.14738:  75%|███████▌  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.14679, smth: 0.14738:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.11926, smth: 0.14387:  88%|████████▊ | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.11926, smth: 0.14387: 100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:03 2021 Epoch 189, lr: 0.0000061, train loss: 0.14387, train auc: 0.99724, val loss: 0.45720, val auc: 0.98614\n","Thu Jun 10 01:34:03 2021 Epoch: 190\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11311, smth: 0.11311:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.11311, smth: 0.11311:  12%|█▎        | 1/8 [00:02<00:16,  2.40s/it]\u001b[A\n","loss: 0.12304, smth: 0.11807:  12%|█▎        | 1/8 [00:03<00:16,  2.40s/it]\u001b[A\n","loss: 0.12304, smth: 0.11807:  25%|██▌       | 2/8 [00:03<00:11,  1.91s/it]\u001b[A\n","loss: 0.13092, smth: 0.12236:  25%|██▌       | 2/8 [00:04<00:11,  1.91s/it]\u001b[A\n","loss: 0.13092, smth: 0.12236:  38%|███▊      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.16317, smth: 0.13256:  38%|███▊      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.16317, smth: 0.13256:  50%|█████     | 4/8 [00:05<00:05,  1.44s/it]\u001b[A\n","loss: 0.20620, smth: 0.14729:  50%|█████     | 4/8 [00:06<00:05,  1.44s/it]\u001b[A\n","loss: 0.20620, smth: 0.14729:  62%|██████▎   | 5/8 [00:06<00:04,  1.52s/it]\u001b[A\n","loss: 0.10472, smth: 0.14019:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.10472, smth: 0.14019:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.08195, smth: 0.13187:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.08195, smth: 0.13187:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.15464, smth: 0.13472:  88%|████████▊ | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.15464, smth: 0.13472: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.97it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:12 2021 Epoch 190, lr: 0.0000034, train loss: 0.13472, train auc: 0.99795, val loss: 0.47983, val auc: 0.98618\n","Thu Jun 10 01:34:12 2021 Epoch: 191\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12233, smth: 0.12233:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12233, smth: 0.12233:  12%|█▎        | 1/8 [00:02<00:18,  2.68s/it]\u001b[A\n","loss: 0.16417, smth: 0.14325:  12%|█▎        | 1/8 [00:03<00:18,  2.68s/it]\u001b[A\n","loss: 0.16417, smth: 0.14325:  25%|██▌       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.08460, smth: 0.12370:  25%|██▌       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.08460, smth: 0.12370:  38%|███▊      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.16192, smth: 0.13325:  38%|███▊      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.16192, smth: 0.13325:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.12949, smth: 0.13250:  50%|█████     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.12949, smth: 0.13250:  62%|██████▎   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.17447, smth: 0.13950:  62%|██████▎   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.17447, smth: 0.13950:  75%|███████▌  | 6/8 [00:07<00:02,  1.20s/it]\u001b[A\n","loss: 0.06978, smth: 0.12954:  75%|███████▌  | 6/8 [00:08<00:02,  1.20s/it]\u001b[A\n","loss: 0.06978, smth: 0.12954:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.13284, smth: 0.12995:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.13284, smth: 0.12995: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.92it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.53it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:22 2021 Epoch 191, lr: 0.0000015, train loss: 0.12995, train auc: 0.99747, val loss: 0.47076, val auc: 0.98591\n","Thu Jun 10 01:34:22 2021 Epoch: 192\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08355, smth: 0.08355:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08355, smth: 0.08355:  12%|█▎        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.11079, smth: 0.09717:  12%|█▎        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.11079, smth: 0.09717:  25%|██▌       | 2/8 [00:03<00:12,  2.05s/it]\u001b[A\n","loss: 0.19303, smth: 0.12912:  25%|██▌       | 2/8 [00:05<00:12,  2.05s/it]\u001b[A\n","loss: 0.19303, smth: 0.12912:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.12081, smth: 0.12704:  38%|███▊      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.12081, smth: 0.12704:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.12538, smth: 0.12671:  50%|█████     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.12538, smth: 0.12671:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.14518, smth: 0.12979:  62%|██████▎   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.14518, smth: 0.12979:  75%|███████▌  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.21024, smth: 0.14128:  75%|███████▌  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.21024, smth: 0.14128:  88%|████████▊ | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.23773, smth: 0.15334:  88%|████████▊ | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.23773, smth: 0.15334: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:32 2021 Epoch 192, lr: 0.0000004, train loss: 0.15334, train auc: 0.99875, val loss: 0.46048, val auc: 0.98678\n","Thu Jun 10 01:34:32 2021 Epoch: 193\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16677, smth: 0.16677:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16677, smth: 0.16677:  12%|█▎        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.17777, smth: 0.17227:  12%|█▎        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.17777, smth: 0.17227:  25%|██▌       | 2/8 [00:03<00:13,  2.19s/it]\u001b[A\n","loss: 0.15130, smth: 0.16528:  25%|██▌       | 2/8 [00:04<00:13,  2.19s/it]\u001b[A\n","loss: 0.15130, smth: 0.16528:  38%|███▊      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.10386, smth: 0.14993:  38%|███▊      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.10386, smth: 0.14993:  50%|█████     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.04562, smth: 0.12907:  50%|█████     | 4/8 [00:07<00:06,  1.58s/it]\u001b[A\n","loss: 0.04562, smth: 0.12907:  62%|██████▎   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.21052, smth: 0.14264:  62%|██████▎   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.21052, smth: 0.14264:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.14517, smth: 0.14300:  75%|███████▌  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.14517, smth: 0.14300:  88%|████████▊ | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.05834, smth: 0.13242:  88%|████████▊ | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.05834, smth: 0.13242: 100%|██████████| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:42 2021 Epoch 193, lr: 0.0000000, train loss: 0.13242, train auc: 0.99752, val loss: 0.43660, val auc: 0.98644\n","Thu Jun 10 01:34:42 2021 Epoch: 194\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13204, smth: 0.13204:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13204, smth: 0.13204:  12%|█▎        | 1/8 [00:02<00:18,  2.63s/it]\u001b[A\n","loss: 0.20079, smth: 0.16642:  12%|█▎        | 1/8 [00:03<00:18,  2.63s/it]\u001b[A\n","loss: 0.20079, smth: 0.16642:  25%|██▌       | 2/8 [00:03<00:12,  2.08s/it]\u001b[A\n","loss: 0.14923, smth: 0.16069:  25%|██▌       | 2/8 [00:04<00:12,  2.08s/it]\u001b[A\n","loss: 0.14923, smth: 0.16069:  38%|███▊      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.16200, smth: 0.16101:  38%|███▊      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.16200, smth: 0.16101:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.12420, smth: 0.15365:  50%|█████     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.12420, smth: 0.15365:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.15022, smth: 0.15308:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.15022, smth: 0.15308:  75%|███████▌  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.25446, smth: 0.16756:  75%|███████▌  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.25446, smth: 0.16756:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.08600, smth: 0.15737:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.08600, smth: 0.15737: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:51 2021 Epoch 194, lr: 0.0000004, train loss: 0.15737, train auc: 0.99805, val loss: 0.46612, val auc: 0.98578\n","Thu Jun 10 01:34:51 2021 Epoch: 195\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15512, smth: 0.15512:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15512, smth: 0.15512:  12%|█▎        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 0.14965, smth: 0.15238:  12%|█▎        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 0.14965, smth: 0.15238:  25%|██▌       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.12200, smth: 0.14226:  25%|██▌       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.12200, smth: 0.14226:  38%|███▊      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.16390, smth: 0.14767:  38%|███▊      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.16390, smth: 0.14767:  50%|█████     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.07952, smth: 0.13404:  50%|█████     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.07952, smth: 0.13404:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.19670, smth: 0.14448:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.19670, smth: 0.14448:  75%|███████▌  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.27658, smth: 0.16335:  75%|███████▌  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.27658, smth: 0.16335:  88%|████████▊ | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.12312, smth: 0.15832:  88%|████████▊ | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.12312, smth: 0.15832: 100%|██████████| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:02 2021 Epoch 195, lr: 0.0000015, train loss: 0.15832, train auc: 0.99722, val loss: 0.45451, val auc: 0.98611\n","Thu Jun 10 01:35:02 2021 Epoch: 196\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10895, smth: 0.10895:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10895, smth: 0.10895:  12%|█▎        | 1/8 [00:02<00:18,  2.65s/it]\u001b[A\n","loss: 0.12280, smth: 0.11587:  12%|█▎        | 1/8 [00:03<00:18,  2.65s/it]\u001b[A\n","loss: 0.12280, smth: 0.11587:  25%|██▌       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.26128, smth: 0.16434:  25%|██▌       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.26128, smth: 0.16434:  38%|███▊      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.09628, smth: 0.14733:  38%|███▊      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.09628, smth: 0.14733:  50%|█████     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.10183, smth: 0.13823:  50%|█████     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.10183, smth: 0.13823:  62%|██████▎   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.10680, smth: 0.13299:  62%|██████▎   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.10680, smth: 0.13299:  75%|███████▌  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.21540, smth: 0.14476:  75%|███████▌  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.21540, smth: 0.14476:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14586, smth: 0.14490:  88%|████████▊ | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14586, smth: 0.14490: 100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:12 2021 Epoch 196, lr: 0.0000034, train loss: 0.14490, train auc: 0.99841, val loss: 0.45959, val auc: 0.98620\n","Thu Jun 10 01:35:12 2021 Epoch: 197\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18912, smth: 0.18912:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18912, smth: 0.18912:  12%|█▎        | 1/8 [00:02<00:20,  2.98s/it]\u001b[A\n","loss: 0.21479, smth: 0.20195:  12%|█▎        | 1/8 [00:03<00:20,  2.98s/it]\u001b[A\n","loss: 0.21479, smth: 0.20195:  25%|██▌       | 2/8 [00:03<00:13,  2.31s/it]\u001b[A\n","loss: 0.10449, smth: 0.16947:  25%|██▌       | 2/8 [00:04<00:13,  2.31s/it]\u001b[A\n","loss: 0.10449, smth: 0.16947:  38%|███▊      | 3/8 [00:04<00:09,  1.98s/it]\u001b[A\n","loss: 0.15986, smth: 0.16707:  38%|███▊      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.15986, smth: 0.16707:  50%|█████     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.09481, smth: 0.15262:  50%|█████     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.09481, smth: 0.15262:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.12559, smth: 0.14811:  62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.12559, smth: 0.14811:  75%|███████▌  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.16815, smth: 0.15097:  75%|███████▌  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.16815, smth: 0.15097:  88%|████████▊ | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.11770, smth: 0.14681:  88%|████████▊ | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.11770, smth: 0.14681: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.57it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:21 2021 Epoch 197, lr: 0.0000061, train loss: 0.14681, train auc: 0.99736, val loss: 0.44678, val auc: 0.98659\n","Thu Jun 10 01:35:21 2021 Epoch: 198\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08704, smth: 0.08704:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08704, smth: 0.08704:  12%|█▎        | 1/8 [00:02<00:18,  2.70s/it]\u001b[A\n","loss: 0.10192, smth: 0.09448:  12%|█▎        | 1/8 [00:03<00:18,  2.70s/it]\u001b[A\n","loss: 0.10192, smth: 0.09448:  25%|██▌       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.11343, smth: 0.10079:  25%|██▌       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.11343, smth: 0.10079:  38%|███▊      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.09296, smth: 0.09884:  38%|███▊      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.09296, smth: 0.09884:  50%|█████     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.18976, smth: 0.11702:  50%|█████     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.18976, smth: 0.11702:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.10176, smth: 0.11448:  62%|██████▎   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.10176, smth: 0.11448:  75%|███████▌  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.10106, smth: 0.11256:  75%|███████▌  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.10106, smth: 0.11256:  88%|████████▊ | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.19736, smth: 0.12316:  88%|████████▊ | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.19736, smth: 0.12316: 100%|██████████| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:31 2021 Epoch 198, lr: 0.0000095, train loss: 0.12316, train auc: 0.99783, val loss: 0.45003, val auc: 0.98630\n","Thu Jun 10 01:35:31 2021 Epoch: 199\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11278, smth: 0.11278:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.11278, smth: 0.11278:  12%|█▎        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.13626, smth: 0.12452:  12%|█▎        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.13626, smth: 0.12452:  25%|██▌       | 2/8 [00:03<00:14,  2.36s/it]\u001b[A\n","loss: 0.13262, smth: 0.12722:  25%|██▌       | 2/8 [00:05<00:14,  2.36s/it]\u001b[A\n","loss: 0.13262, smth: 0.12722:  38%|███▊      | 3/8 [00:05<00:10,  2.20s/it]\u001b[A\n","loss: 0.09638, smth: 0.11951:  38%|███▊      | 3/8 [00:06<00:10,  2.20s/it]\u001b[A\n","loss: 0.09638, smth: 0.11951:  50%|█████     | 4/8 [00:06<00:07,  1.77s/it]\u001b[A\n","loss: 0.13961, smth: 0.12353:  50%|█████     | 4/8 [00:07<00:07,  1.77s/it]\u001b[A\n","loss: 0.13961, smth: 0.12353:  62%|██████▎   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.17691, smth: 0.13243:  62%|██████▎   | 5/8 [00:08<00:04,  1.57s/it]\u001b[A\n","loss: 0.17691, smth: 0.13243:  75%|███████▌  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.08848, smth: 0.12615:  75%|███████▌  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.08848, smth: 0.12615:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.12513, smth: 0.12602:  88%|████████▊ | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.12513, smth: 0.12602: 100%|██████████| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:42 2021 Epoch 199, lr: 0.0000137, train loss: 0.12602, train auc: 0.99794, val loss: 0.43351, val auc: 0.98699\n","Thu Jun 10 01:35:42 2021 Epoch: 200\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17104, smth: 0.17104:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17104, smth: 0.17104:  12%|█▎        | 1/8 [00:02<00:17,  2.54s/it]\u001b[A\n","loss: 0.11597, smth: 0.14350:  12%|█▎        | 1/8 [00:03<00:17,  2.54s/it]\u001b[A\n","loss: 0.11597, smth: 0.14350:  25%|██▌       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.18125, smth: 0.15609:  25%|██▌       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.18125, smth: 0.15609:  38%|███▊      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.10487, smth: 0.14328:  38%|███▊      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.10487, smth: 0.14328:  50%|█████     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.16007, smth: 0.14664:  50%|█████     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.16007, smth: 0.14664:  62%|██████▎   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.22252, smth: 0.15929:  62%|██████▎   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.22252, smth: 0.15929:  75%|███████▌  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 0.14017, smth: 0.15656:  75%|███████▌  | 6/8 [00:08<00:02,  1.22s/it]\u001b[A\n","loss: 0.14017, smth: 0.15656:  88%|████████▊ | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.08360, smth: 0.14744:  88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.08360, smth: 0.14744: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|█████     | 1/2 [00:00<00:00,  2.96it/s]\u001b[A\n","100%|██████████| 2/2 [00:00<00:00,  3.60it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:51 2021 Epoch 200, lr: 0.0000186, train loss: 0.14744, train auc: 0.99842, val loss: 0.45193, val auc: 0.98641\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOaBS7IhxkKk","executionInfo":{"status":"ok","timestamp":1623289671197,"user_tz":-480,"elapsed":313,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"553be18f-1e5e-4301-ef99-3219dedec094"},"source":["model = model.to(device)\n","model.load_state_dict(torch.load(os.path.join(f'{kernel_type}_best_fold{use_fold}.pth')))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"yJvIm17JzdmW"},"source":["# test_file_list = test['Filename'][:10000].values.tolist()\n","test_file_list = test['Filename'].values.tolist()\n","test_loader = data.DataLoader(PANNsDataset(test_file_list, None, training=False), \n","                             batch_size=16, \n","                             shuffle=False,\n","                             num_workers=2,\n","                             pin_memory=True,\n","                             drop_last=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UssJbITwxhqV"},"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"URDc17zoxt4b","executionInfo":{"status":"ok","timestamp":1623289747559,"user_tz":-480,"elapsed":68688,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"71d6a071-604b-4e17-86f0-0b5c73718eee"},"source":["PREDS = []\n","bar = tqdm(test_loader)\n","for d in bar:\n","    data = d['waveform']\n","    data = data.to(device)\n","    with torch.no_grad():\n","        logits = model(data)\n","\n","        # print(torch.exp(logits['clipwise_output']))\n","        # print(torch.exp(logits['clipwise_output']).sum(axis=-1))\n","        # input()\n","        \n","        PREDS.append(torch.exp(logits['clipwise_output']))\n","\n","        # if pred_num != target_num:\n","        #     plt.imshow(data.squeeze().cpu().numpy())\n","        #     plt.axis('off')\n","        #     plt.show()\n","        #     input('next...')\n","\n","PREDS = torch.cat(PREDS).detach().cpu().numpy()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","  0%|          | 0/1875 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 1/1875 [00:00<30:56,  1.01it/s]\u001b[A\n","  0%|          | 6/1875 [00:01<21:48,  1.43it/s]\u001b[A\n","  1%|          | 10/1875 [00:01<15:31,  2.00it/s]\u001b[A\n","  1%|          | 14/1875 [00:01<11:10,  2.78it/s]\u001b[A\n","  1%|          | 18/1875 [00:01<08:09,  3.80it/s]\u001b[A\n","  1%|          | 22/1875 [00:01<06:02,  5.12it/s]\u001b[A\n","  1%|▏         | 26/1875 [00:01<04:35,  6.71it/s]\u001b[A\n","  2%|▏         | 30/1875 [00:01<03:28,  8.87it/s]\u001b[A\n","  2%|▏         | 33/1875 [00:02<02:45, 11.11it/s]\u001b[A\n","  2%|▏         | 36/1875 [00:02<02:17, 13.39it/s]\u001b[A\n","  2%|▏         | 40/1875 [00:02<01:54, 15.97it/s]\u001b[A\n","  2%|▏         | 43/1875 [00:02<01:39, 18.46it/s]\u001b[A\n","  2%|▏         | 46/1875 [00:02<01:29, 20.38it/s]\u001b[A\n","  3%|▎         | 49/1875 [00:02<01:21, 22.48it/s]\u001b[A\n","  3%|▎         | 52/1875 [00:02<01:16, 23.80it/s]\u001b[A\n","  3%|▎         | 55/1875 [00:02<01:12, 25.23it/s]\u001b[A\n","  3%|▎         | 58/1875 [00:02<01:11, 25.37it/s]\u001b[A\n","  3%|▎         | 62/1875 [00:03<01:11, 25.32it/s]\u001b[A\n","  4%|▎         | 66/1875 [00:03<01:07, 26.63it/s]\u001b[A\n","  4%|▎         | 69/1875 [00:03<01:07, 26.95it/s]\u001b[A\n","  4%|▍         | 72/1875 [00:03<01:08, 26.45it/s]\u001b[A\n","  4%|▍         | 76/1875 [00:03<01:08, 26.18it/s]\u001b[A\n","  4%|▍         | 80/1875 [00:03<01:03, 28.09it/s]\u001b[A\n","  4%|▍         | 83/1875 [00:03<01:03, 28.06it/s]\u001b[A\n","  5%|▍         | 86/1875 [00:03<01:06, 26.73it/s]\u001b[A\n","  5%|▍         | 90/1875 [00:04<01:06, 26.89it/s]\u001b[A\n","  5%|▍         | 93/1875 [00:04<01:04, 27.57it/s]\u001b[A\n","  5%|▌         | 96/1875 [00:04<01:03, 27.94it/s]\u001b[A\n","  5%|▌         | 100/1875 [00:04<01:01, 28.77it/s]\u001b[A\n","  5%|▌         | 103/1875 [00:04<01:02, 28.35it/s]\u001b[A\n","  6%|▌         | 106/1875 [00:04<01:02, 28.29it/s]\u001b[A\n","  6%|▌         | 109/1875 [00:04<01:02, 28.34it/s]\u001b[A\n","  6%|▌         | 112/1875 [00:04<01:06, 26.34it/s]\u001b[A\n","  6%|▌         | 116/1875 [00:05<01:02, 28.09it/s]\u001b[A\n","  6%|▋         | 119/1875 [00:05<01:03, 27.84it/s]\u001b[A\n","  7%|▋         | 123/1875 [00:05<01:05, 26.59it/s]\u001b[A\n","  7%|▋         | 126/1875 [00:05<01:03, 27.42it/s]\u001b[A\n","  7%|▋         | 129/1875 [00:05<01:03, 27.61it/s]\u001b[A\n","  7%|▋         | 132/1875 [00:05<01:03, 27.56it/s]\u001b[A\n","  7%|▋         | 135/1875 [00:05<01:02, 27.74it/s]\u001b[A\n","  7%|▋         | 138/1875 [00:05<01:04, 26.95it/s]\u001b[A\n","  8%|▊         | 142/1875 [00:06<01:05, 26.27it/s]\u001b[A\n","  8%|▊         | 146/1875 [00:06<01:03, 27.11it/s]\u001b[A\n","  8%|▊         | 150/1875 [00:06<00:59, 28.76it/s]\u001b[A\n","  8%|▊         | 153/1875 [00:06<00:59, 29.02it/s]\u001b[A\n","  8%|▊         | 156/1875 [00:06<00:59, 28.85it/s]\u001b[A\n","  8%|▊         | 159/1875 [00:06<01:05, 26.30it/s]\u001b[A\n","  9%|▊         | 163/1875 [00:06<01:03, 27.04it/s]\u001b[A\n","  9%|▉         | 166/1875 [00:06<01:02, 27.13it/s]\u001b[A\n","  9%|▉         | 169/1875 [00:06<01:01, 27.78it/s]\u001b[A\n","  9%|▉         | 173/1875 [00:07<01:01, 27.50it/s]\u001b[A\n","  9%|▉         | 177/1875 [00:07<01:00, 27.91it/s]\u001b[A\n"," 10%|▉         | 181/1875 [00:07<01:03, 26.87it/s]\u001b[A\n"," 10%|▉         | 185/1875 [00:07<00:59, 28.25it/s]\u001b[A\n"," 10%|█         | 188/1875 [00:07<01:00, 27.84it/s]\u001b[A\n"," 10%|█         | 192/1875 [00:07<01:02, 27.10it/s]\u001b[A\n"," 10%|█         | 196/1875 [00:07<01:00, 27.83it/s]\u001b[A\n"," 11%|█         | 200/1875 [00:08<00:56, 29.79it/s]\u001b[A\n"," 11%|█         | 204/1875 [00:08<01:01, 27.32it/s]\u001b[A\n"," 11%|█         | 207/1875 [00:08<01:00, 27.80it/s]\u001b[A\n"," 11%|█         | 210/1875 [00:08<01:00, 27.65it/s]\u001b[A\n"," 11%|█▏        | 214/1875 [00:08<00:56, 29.55it/s]\u001b[A\n"," 12%|█▏        | 218/1875 [00:08<01:01, 26.97it/s]\u001b[A\n"," 12%|█▏        | 221/1875 [00:08<01:01, 26.97it/s]\u001b[A\n"," 12%|█▏        | 224/1875 [00:08<01:01, 26.90it/s]\u001b[A\n"," 12%|█▏        | 227/1875 [00:09<00:59, 27.58it/s]\u001b[A\n"," 12%|█▏        | 231/1875 [00:09<00:56, 29.29it/s]\u001b[A\n"," 12%|█▏        | 234/1875 [00:09<01:03, 25.95it/s]\u001b[A\n"," 13%|█▎        | 237/1875 [00:09<01:02, 26.21it/s]\u001b[A\n"," 13%|█▎        | 241/1875 [00:09<00:57, 28.31it/s]\u001b[A\n"," 13%|█▎        | 244/1875 [00:09<00:56, 28.71it/s]\u001b[A\n"," 13%|█▎        | 247/1875 [00:09<01:05, 24.87it/s]\u001b[A\n"," 13%|█▎        | 251/1875 [00:09<00:59, 27.50it/s]\u001b[A\n"," 14%|█▎        | 254/1875 [00:10<00:58, 27.54it/s]\u001b[A\n"," 14%|█▎        | 257/1875 [00:10<00:57, 27.97it/s]\u001b[A\n"," 14%|█▍        | 261/1875 [00:10<00:54, 29.51it/s]\u001b[A\n"," 14%|█▍        | 265/1875 [00:10<00:57, 27.77it/s]\u001b[A\n"," 14%|█▍        | 268/1875 [00:10<00:59, 26.83it/s]\u001b[A\n"," 15%|█▍        | 272/1875 [00:10<01:04, 24.90it/s]\u001b[A\n"," 15%|█▍        | 276/1875 [00:10<00:57, 28.01it/s]\u001b[A\n"," 15%|█▍        | 279/1875 [00:10<00:56, 28.17it/s]\u001b[A\n"," 15%|█▌        | 282/1875 [00:11<00:57, 27.63it/s]\u001b[A\n"," 15%|█▌        | 285/1875 [00:11<00:57, 27.54it/s]\u001b[A\n"," 15%|█▌        | 289/1875 [00:11<00:59, 26.76it/s]\u001b[A\n"," 16%|█▌        | 292/1875 [00:11<00:57, 27.48it/s]\u001b[A\n"," 16%|█▌        | 296/1875 [00:11<00:54, 29.09it/s]\u001b[A\n"," 16%|█▌        | 299/1875 [00:11<00:54, 29.18it/s]\u001b[A\n"," 16%|█▌        | 302/1875 [00:11<00:55, 28.55it/s]\u001b[A\n"," 16%|█▋        | 305/1875 [00:11<00:56, 27.64it/s]\u001b[A\n"," 16%|█▋        | 309/1875 [00:12<00:58, 26.57it/s]\u001b[A\n"," 17%|█▋        | 312/1875 [00:12<00:58, 26.65it/s]\u001b[A\n"," 17%|█▋        | 316/1875 [00:12<00:53, 29.01it/s]\u001b[A\n"," 17%|█▋        | 319/1875 [00:12<01:05, 23.79it/s]\u001b[A\n"," 17%|█▋        | 323/1875 [00:12<00:57, 26.90it/s]\u001b[A\n"," 17%|█▋        | 326/1875 [00:12<00:58, 26.57it/s]\u001b[A\n"," 18%|█▊        | 329/1875 [00:12<00:58, 26.22it/s]\u001b[A\n"," 18%|█▊        | 333/1875 [00:12<00:58, 26.28it/s]\u001b[A\n"," 18%|█▊        | 337/1875 [00:13<00:54, 28.38it/s]\u001b[A\n"," 18%|█▊        | 340/1875 [00:13<00:54, 28.29it/s]\u001b[A\n"," 18%|█▊        | 343/1875 [00:13<00:55, 27.41it/s]\u001b[A\n"," 19%|█▊        | 347/1875 [00:13<00:57, 26.49it/s]\u001b[A\n"," 19%|█▊        | 350/1875 [00:13<00:57, 26.49it/s]\u001b[A\n"," 19%|█▉        | 354/1875 [00:13<00:53, 28.57it/s]\u001b[A\n"," 19%|█▉        | 357/1875 [00:13<00:55, 27.12it/s]\u001b[A\n"," 19%|█▉        | 361/1875 [00:13<00:57, 26.20it/s]\u001b[A\n"," 20%|█▉        | 366/1875 [00:14<00:51, 29.52it/s]\u001b[A\n"," 20%|█▉        | 370/1875 [00:14<00:56, 26.85it/s]\u001b[A\n"," 20%|█▉        | 373/1875 [00:14<00:54, 27.65it/s]\u001b[A\n"," 20%|██        | 377/1875 [00:14<00:59, 24.98it/s]\u001b[A\n"," 20%|██        | 382/1875 [00:14<00:51, 28.86it/s]\u001b[A\n"," 21%|██        | 386/1875 [00:14<00:55, 26.94it/s]\u001b[A\n"," 21%|██        | 389/1875 [00:14<00:54, 27.43it/s]\u001b[A\n"," 21%|██        | 392/1875 [00:15<00:52, 28.04it/s]\u001b[A\n"," 21%|██        | 395/1875 [00:15<00:53, 27.53it/s]\u001b[A\n"," 21%|██        | 398/1875 [00:15<00:54, 27.23it/s]\u001b[A\n"," 21%|██▏       | 401/1875 [00:15<00:53, 27.72it/s]\u001b[A\n"," 22%|██▏       | 404/1875 [00:15<00:52, 27.83it/s]\u001b[A\n"," 22%|██▏       | 407/1875 [00:15<00:52, 28.20it/s]\u001b[A\n"," 22%|██▏       | 411/1875 [00:15<00:54, 26.87it/s]\u001b[A\n"," 22%|██▏       | 415/1875 [00:15<00:50, 29.05it/s]\u001b[A\n"," 22%|██▏       | 418/1875 [00:15<00:51, 28.19it/s]\u001b[A\n"," 22%|██▏       | 421/1875 [00:16<00:51, 28.44it/s]\u001b[A\n"," 23%|██▎       | 424/1875 [00:16<00:50, 28.78it/s]\u001b[A\n"," 23%|██▎       | 427/1875 [00:16<01:00, 24.01it/s]\u001b[A\n"," 23%|██▎       | 432/1875 [00:16<00:52, 27.65it/s]\u001b[A\n"," 23%|██▎       | 436/1875 [00:16<00:56, 25.34it/s]\u001b[A\n"," 23%|██▎       | 440/1875 [00:16<00:51, 28.12it/s]\u001b[A\n"," 24%|██▎       | 444/1875 [00:16<00:50, 28.56it/s]\u001b[A\n"," 24%|██▍       | 448/1875 [00:17<00:53, 26.90it/s]\u001b[A\n"," 24%|██▍       | 451/1875 [00:17<00:52, 27.25it/s]\u001b[A\n"," 24%|██▍       | 455/1875 [00:17<00:48, 29.11it/s]\u001b[A\n"," 24%|██▍       | 459/1875 [00:17<00:53, 26.55it/s]\u001b[A\n"," 25%|██▍       | 462/1875 [00:17<00:52, 26.75it/s]\u001b[A\n"," 25%|██▍       | 466/1875 [00:17<00:53, 26.13it/s]\u001b[A\n"," 25%|██▌       | 470/1875 [00:17<00:49, 28.36it/s]\u001b[A\n"," 25%|██▌       | 473/1875 [00:17<00:51, 27.43it/s]\u001b[A\n"," 25%|██▌       | 476/1875 [00:18<00:51, 27.17it/s]\u001b[A\n"," 26%|██▌       | 480/1875 [00:18<00:52, 26.36it/s]\u001b[A\n"," 26%|██▌       | 483/1875 [00:18<00:52, 26.74it/s]\u001b[A\n"," 26%|██▌       | 486/1875 [00:18<00:53, 26.11it/s]\u001b[A\n"," 26%|██▌       | 490/1875 [00:18<00:54, 25.52it/s]\u001b[A\n"," 26%|██▋       | 494/1875 [00:18<00:48, 28.49it/s]\u001b[A\n"," 27%|██▋       | 497/1875 [00:18<00:49, 27.82it/s]\u001b[A\n"," 27%|██▋       | 501/1875 [00:18<00:45, 30.36it/s]\u001b[A\n"," 27%|██▋       | 505/1875 [00:19<00:46, 29.63it/s]\u001b[A\n"," 27%|██▋       | 509/1875 [00:19<00:48, 28.04it/s]\u001b[A\n"," 27%|██▋       | 512/1875 [00:19<00:48, 28.39it/s]\u001b[A\n"," 27%|██▋       | 515/1875 [00:19<00:47, 28.50it/s]\u001b[A\n"," 28%|██▊       | 518/1875 [00:19<00:56, 24.09it/s]\u001b[A\n"," 28%|██▊       | 521/1875 [00:19<00:54, 24.68it/s]\u001b[A\n"," 28%|██▊       | 525/1875 [00:19<00:48, 27.60it/s]\u001b[A\n"," 28%|██▊       | 529/1875 [00:19<00:45, 29.76it/s]\u001b[A\n"," 28%|██▊       | 533/1875 [00:20<00:49, 27.28it/s]\u001b[A\n"," 29%|██▊       | 536/1875 [00:20<00:49, 27.19it/s]\u001b[A\n"," 29%|██▉       | 540/1875 [00:20<00:44, 29.83it/s]\u001b[A\n"," 29%|██▉       | 544/1875 [00:20<00:51, 25.62it/s]\u001b[A\n"," 29%|██▉       | 549/1875 [00:20<00:50, 26.02it/s]\u001b[A\n"," 30%|██▉       | 554/1875 [00:20<00:44, 29.47it/s]\u001b[A\n"," 30%|██▉       | 558/1875 [00:21<00:48, 27.23it/s]\u001b[A\n"," 30%|██▉       | 561/1875 [00:21<00:47, 27.38it/s]\u001b[A\n"," 30%|███       | 564/1875 [00:21<00:47, 27.53it/s]\u001b[A\n"," 30%|███       | 568/1875 [00:21<00:44, 29.37it/s]\u001b[A\n"," 31%|███       | 572/1875 [00:21<00:49, 26.09it/s]\u001b[A\n"," 31%|███       | 575/1875 [00:21<00:57, 22.56it/s]\u001b[A\n"," 31%|███       | 580/1875 [00:21<00:49, 26.20it/s]\u001b[A\n"," 31%|███       | 584/1875 [00:22<00:51, 25.07it/s]\u001b[A\n"," 31%|███▏      | 588/1875 [00:22<00:46, 27.95it/s]\u001b[A\n"," 32%|███▏      | 592/1875 [00:22<00:43, 29.71it/s]\u001b[A\n"," 32%|███▏      | 596/1875 [00:22<00:44, 28.84it/s]\u001b[A\n"," 32%|███▏      | 600/1875 [00:22<00:45, 28.15it/s]\u001b[A\n"," 32%|███▏      | 603/1875 [00:22<00:44, 28.27it/s]\u001b[A\n"," 32%|███▏      | 606/1875 [00:22<00:47, 26.69it/s]\u001b[A\n"," 33%|███▎      | 610/1875 [00:22<00:48, 26.25it/s]\u001b[A\n"," 33%|███▎      | 614/1875 [00:23<00:44, 28.48it/s]\u001b[A\n"," 33%|███▎      | 617/1875 [00:23<00:44, 28.34it/s]\u001b[A\n"," 33%|███▎      | 620/1875 [00:23<00:53, 23.55it/s]\u001b[A\n"," 33%|███▎      | 625/1875 [00:23<00:46, 27.14it/s]\u001b[A\n"," 34%|███▎      | 629/1875 [00:23<00:48, 25.45it/s]\u001b[A\n"," 34%|███▎      | 632/1875 [00:23<00:49, 24.93it/s]\u001b[A\n"," 34%|███▍      | 636/1875 [00:23<00:47, 26.12it/s]\u001b[A\n"," 34%|███▍      | 640/1875 [00:24<00:46, 26.59it/s]\u001b[A\n"," 34%|███▍      | 644/1875 [00:24<00:46, 26.47it/s]\u001b[A\n"," 35%|███▍      | 648/1875 [00:24<00:44, 27.33it/s]\u001b[A\n"," 35%|███▍      | 652/1875 [00:24<00:44, 27.55it/s]\u001b[A\n"," 35%|███▍      | 655/1875 [00:24<00:43, 28.18it/s]\u001b[A\n"," 35%|███▌      | 658/1875 [00:24<00:45, 26.83it/s]\u001b[A\n"," 35%|███▌      | 662/1875 [00:24<00:45, 26.74it/s]\u001b[A\n"," 35%|███▌      | 665/1875 [00:24<00:44, 27.34it/s]\u001b[A\n"," 36%|███▌      | 668/1875 [00:25<00:44, 26.95it/s]\u001b[A\n"," 36%|███▌      | 672/1875 [00:25<00:41, 29.26it/s]\u001b[A\n"," 36%|███▌      | 676/1875 [00:25<00:42, 28.18it/s]\u001b[A\n"," 36%|███▋      | 680/1875 [00:25<00:42, 28.12it/s]\u001b[A\n"," 36%|███▋      | 684/1875 [00:25<00:40, 29.71it/s]\u001b[A\n"," 37%|███▋      | 688/1875 [00:25<00:41, 28.34it/s]\u001b[A\n"," 37%|███▋      | 692/1875 [00:25<00:41, 28.71it/s]\u001b[A\n"," 37%|███▋      | 696/1875 [00:26<00:42, 28.01it/s]\u001b[A\n"," 37%|███▋      | 700/1875 [00:26<00:42, 27.87it/s]\u001b[A\n"," 38%|███▊      | 704/1875 [00:26<00:43, 26.94it/s]\u001b[A\n"," 38%|███▊      | 708/1875 [00:26<00:42, 27.69it/s]\u001b[A\n"," 38%|███▊      | 712/1875 [00:26<00:40, 29.03it/s]\u001b[A\n"," 38%|███▊      | 715/1875 [00:26<00:41, 27.90it/s]\u001b[A\n"," 38%|███▊      | 719/1875 [00:26<00:39, 29.27it/s]\u001b[A\n"," 39%|███▊      | 722/1875 [00:26<00:42, 27.00it/s]\u001b[A\n"," 39%|███▊      | 725/1875 [00:27<00:42, 26.98it/s]\u001b[A\n"," 39%|███▉      | 728/1875 [00:27<00:44, 25.85it/s]\u001b[A\n"," 39%|███▉      | 731/1875 [00:27<00:43, 26.54it/s]\u001b[A\n"," 39%|███▉      | 734/1875 [00:27<00:44, 25.65it/s]\u001b[A\n"," 39%|███▉      | 738/1875 [00:27<00:42, 26.90it/s]\u001b[A\n"," 40%|███▉      | 742/1875 [00:27<00:40, 27.68it/s]\u001b[A\n"," 40%|███▉      | 746/1875 [00:27<00:41, 26.89it/s]\u001b[A\n"," 40%|████      | 750/1875 [00:27<00:39, 28.56it/s]\u001b[A\n"," 40%|████      | 754/1875 [00:28<00:40, 27.54it/s]\u001b[A\n"," 40%|████      | 758/1875 [00:28<00:39, 28.04it/s]\u001b[A\n"," 41%|████      | 761/1875 [00:28<00:38, 28.57it/s]\u001b[A\n"," 41%|████      | 764/1875 [00:28<00:39, 27.82it/s]\u001b[A\n"," 41%|████      | 767/1875 [00:28<00:39, 27.74it/s]\u001b[A\n"," 41%|████      | 770/1875 [00:28<00:39, 27.81it/s]\u001b[A\n"," 41%|████▏     | 774/1875 [00:28<00:41, 26.76it/s]\u001b[A\n"," 41%|████▏     | 778/1875 [00:28<00:39, 27.54it/s]\u001b[A\n"," 42%|████▏     | 782/1875 [00:29<00:39, 27.79it/s]\u001b[A\n"," 42%|████▏     | 786/1875 [00:29<00:37, 28.75it/s]\u001b[A\n"," 42%|████▏     | 789/1875 [00:29<00:37, 28.98it/s]\u001b[A\n"," 42%|████▏     | 792/1875 [00:29<00:39, 27.64it/s]\u001b[A\n"," 42%|████▏     | 796/1875 [00:29<00:39, 27.26it/s]\u001b[A\n"," 43%|████▎     | 800/1875 [00:29<00:38, 27.61it/s]\u001b[A\n"," 43%|████▎     | 804/1875 [00:29<00:36, 29.47it/s]\u001b[A\n"," 43%|████▎     | 807/1875 [00:29<00:36, 29.57it/s]\u001b[A\n"," 43%|████▎     | 810/1875 [00:30<00:39, 27.16it/s]\u001b[A\n"," 43%|████▎     | 814/1875 [00:30<00:38, 27.28it/s]\u001b[A\n"," 44%|████▎     | 817/1875 [00:30<00:38, 27.62it/s]\u001b[A\n"," 44%|████▎     | 820/1875 [00:30<00:37, 28.14it/s]\u001b[A\n"," 44%|████▍     | 824/1875 [00:30<00:38, 26.99it/s]\u001b[A\n"," 44%|████▍     | 827/1875 [00:30<00:38, 27.29it/s]\u001b[A\n"," 44%|████▍     | 830/1875 [00:30<00:38, 27.42it/s]\u001b[A\n"," 44%|████▍     | 834/1875 [00:30<00:36, 28.67it/s]\u001b[A\n"," 45%|████▍     | 838/1875 [00:31<00:36, 28.06it/s]\u001b[A\n"," 45%|████▍     | 842/1875 [00:31<00:36, 28.14it/s]\u001b[A\n"," 45%|████▌     | 846/1875 [00:31<00:38, 26.52it/s]\u001b[A\n"," 45%|████▌     | 850/1875 [00:31<00:36, 28.39it/s]\u001b[A\n"," 45%|████▌     | 853/1875 [00:31<00:36, 27.96it/s]\u001b[A\n"," 46%|████▌     | 856/1875 [00:31<00:37, 27.36it/s]\u001b[A\n"," 46%|████▌     | 860/1875 [00:31<00:36, 27.55it/s]\u001b[A\n"," 46%|████▌     | 864/1875 [00:32<00:35, 28.63it/s]\u001b[A\n"," 46%|████▌     | 867/1875 [00:32<00:35, 28.76it/s]\u001b[A\n"," 46%|████▋     | 870/1875 [00:32<00:35, 28.36it/s]\u001b[A\n"," 47%|████▋     | 874/1875 [00:32<00:33, 30.01it/s]\u001b[A\n"," 47%|████▋     | 878/1875 [00:32<00:35, 28.39it/s]\u001b[A\n"," 47%|████▋     | 881/1875 [00:32<00:34, 28.50it/s]\u001b[A\n"," 47%|████▋     | 884/1875 [00:32<00:35, 28.25it/s]\u001b[A\n"," 47%|████▋     | 887/1875 [00:32<00:34, 28.67it/s]\u001b[A\n"," 48%|████▊     | 891/1875 [00:32<00:32, 30.20it/s]\u001b[A\n"," 48%|████▊     | 895/1875 [00:33<00:35, 27.59it/s]\u001b[A\n"," 48%|████▊     | 898/1875 [00:33<00:34, 28.24it/s]\u001b[A\n"," 48%|████▊     | 901/1875 [00:33<00:34, 28.07it/s]\u001b[A\n"," 48%|████▊     | 904/1875 [00:33<00:36, 26.86it/s]\u001b[A\n"," 48%|████▊     | 907/1875 [00:33<00:35, 27.41it/s]\u001b[A\n"," 49%|████▊     | 911/1875 [00:33<00:33, 28.91it/s]\u001b[A\n"," 49%|████▊     | 914/1875 [00:33<00:37, 25.89it/s]\u001b[A\n"," 49%|████▉     | 917/1875 [00:33<00:35, 26.72it/s]\u001b[A\n"," 49%|████▉     | 920/1875 [00:34<00:34, 27.57it/s]\u001b[A\n"," 49%|████▉     | 923/1875 [00:34<00:34, 27.70it/s]\u001b[A\n"," 49%|████▉     | 926/1875 [00:34<00:34, 27.78it/s]\u001b[A\n"," 50%|████▉     | 929/1875 [00:34<00:33, 28.21it/s]\u001b[A\n"," 50%|████▉     | 933/1875 [00:34<00:31, 30.05it/s]\u001b[A\n"," 50%|████▉     | 937/1875 [00:34<00:34, 26.92it/s]\u001b[A\n"," 50%|█████     | 941/1875 [00:34<00:32, 28.97it/s]\u001b[A\n"," 50%|█████     | 945/1875 [00:34<00:34, 26.86it/s]\u001b[A\n"," 51%|█████     | 949/1875 [00:35<00:31, 29.15it/s]\u001b[A\n"," 51%|█████     | 953/1875 [00:35<00:30, 29.77it/s]\u001b[A\n"," 51%|█████     | 957/1875 [00:35<00:33, 27.46it/s]\u001b[A\n"," 51%|█████▏    | 961/1875 [00:35<00:31, 28.94it/s]\u001b[A\n"," 51%|█████▏    | 964/1875 [00:35<00:35, 25.79it/s]\u001b[A\n"," 52%|█████▏    | 968/1875 [00:35<00:32, 28.19it/s]\u001b[A\n"," 52%|█████▏    | 971/1875 [00:35<00:32, 27.91it/s]\u001b[A\n"," 52%|█████▏    | 975/1875 [00:35<00:30, 29.68it/s]\u001b[A\n"," 52%|█████▏    | 979/1875 [00:36<00:32, 27.66it/s]\u001b[A\n"," 52%|█████▏    | 983/1875 [00:36<00:30, 29.30it/s]\u001b[A\n"," 53%|█████▎    | 987/1875 [00:36<00:33, 26.37it/s]\u001b[A\n"," 53%|█████▎    | 991/1875 [00:36<00:31, 28.46it/s]\u001b[A\n"," 53%|█████▎    | 995/1875 [00:36<00:32, 26.96it/s]\u001b[A\n"," 53%|█████▎    | 998/1875 [00:36<00:32, 26.65it/s]\u001b[A\n"," 53%|█████▎    | 1002/1875 [00:36<00:31, 28.07it/s]\u001b[A\n"," 54%|█████▎    | 1005/1875 [00:37<00:30, 28.10it/s]\u001b[A\n"," 54%|█████▍    | 1008/1875 [00:37<00:30, 28.14it/s]\u001b[A\n"," 54%|█████▍    | 1011/1875 [00:37<00:31, 27.44it/s]\u001b[A\n"," 54%|█████▍    | 1014/1875 [00:37<00:33, 25.68it/s]\u001b[A\n"," 54%|█████▍    | 1018/1875 [00:37<00:31, 27.20it/s]\u001b[A\n"," 55%|█████▍    | 1022/1875 [00:37<00:29, 29.10it/s]\u001b[A\n"," 55%|█████▍    | 1025/1875 [00:37<00:29, 28.40it/s]\u001b[A\n"," 55%|█████▍    | 1028/1875 [00:37<00:29, 28.84it/s]\u001b[A\n"," 55%|█████▍    | 1031/1875 [00:37<00:29, 28.34it/s]\u001b[A\n"," 55%|█████▌    | 1035/1875 [00:38<00:30, 27.73it/s]\u001b[A\n"," 55%|█████▌    | 1038/1875 [00:38<00:30, 27.78it/s]\u001b[A\n"," 56%|█████▌    | 1041/1875 [00:38<00:29, 27.83it/s]\u001b[A\n"," 56%|█████▌    | 1044/1875 [00:38<00:29, 27.98it/s]\u001b[A\n"," 56%|█████▌    | 1047/1875 [00:38<00:29, 27.95it/s]\u001b[A\n"," 56%|█████▌    | 1051/1875 [00:38<00:30, 27.23it/s]\u001b[A\n"," 56%|█████▋    | 1055/1875 [00:38<00:29, 27.81it/s]\u001b[A\n"," 56%|█████▋    | 1059/1875 [00:38<00:27, 29.39it/s]\u001b[A\n"," 57%|█████▋    | 1062/1875 [00:39<00:28, 28.55it/s]\u001b[A\n"," 57%|█████▋    | 1065/1875 [00:39<00:31, 25.50it/s]\u001b[A\n"," 57%|█████▋    | 1069/1875 [00:39<00:29, 27.78it/s]\u001b[A\n"," 57%|█████▋    | 1072/1875 [00:39<00:28, 27.71it/s]\u001b[A\n"," 57%|█████▋    | 1076/1875 [00:39<00:27, 29.59it/s]\u001b[A\n"," 58%|█████▊    | 1080/1875 [00:39<00:28, 27.54it/s]\u001b[A\n"," 58%|█████▊    | 1083/1875 [00:39<00:29, 27.28it/s]\u001b[A\n"," 58%|█████▊    | 1087/1875 [00:39<00:26, 29.34it/s]\u001b[A\n"," 58%|█████▊    | 1091/1875 [00:40<00:29, 27.02it/s]\u001b[A\n"," 58%|█████▊    | 1094/1875 [00:40<00:29, 26.60it/s]\u001b[A\n"," 59%|█████▊    | 1097/1875 [00:40<00:29, 26.40it/s]\u001b[A\n"," 59%|█████▊    | 1100/1875 [00:40<00:28, 26.83it/s]\u001b[A\n"," 59%|█████▉    | 1104/1875 [00:40<00:28, 26.73it/s]\u001b[A\n"," 59%|█████▉    | 1108/1875 [00:40<00:26, 28.43it/s]\u001b[A\n"," 59%|█████▉    | 1111/1875 [00:40<00:27, 27.85it/s]\u001b[A\n"," 59%|█████▉    | 1114/1875 [00:40<00:27, 27.90it/s]\u001b[A\n"," 60%|█████▉    | 1118/1875 [00:41<00:27, 27.39it/s]\u001b[A\n"," 60%|█████▉    | 1122/1875 [00:41<00:26, 28.41it/s]\u001b[A\n"," 60%|██████    | 1125/1875 [00:41<00:26, 28.08it/s]\u001b[A\n"," 60%|██████    | 1128/1875 [00:41<00:26, 28.59it/s]\u001b[A\n"," 60%|██████    | 1132/1875 [00:41<00:27, 26.80it/s]\u001b[A\n"," 61%|██████    | 1136/1875 [00:41<00:26, 28.41it/s]\u001b[A\n"," 61%|██████    | 1139/1875 [00:41<00:25, 28.68it/s]\u001b[A\n"," 61%|██████    | 1142/1875 [00:41<00:26, 27.25it/s]\u001b[A\n"," 61%|██████    | 1146/1875 [00:42<00:26, 27.29it/s]\u001b[A\n"," 61%|██████▏   | 1149/1875 [00:42<00:26, 27.27it/s]\u001b[A\n"," 61%|██████▏   | 1153/1875 [00:42<00:24, 29.22it/s]\u001b[A\n"," 62%|██████▏   | 1156/1875 [00:42<00:25, 28.05it/s]\u001b[A\n"," 62%|██████▏   | 1160/1875 [00:42<00:26, 26.59it/s]\u001b[A\n"," 62%|██████▏   | 1164/1875 [00:42<00:25, 28.34it/s]\u001b[A\n"," 62%|██████▏   | 1167/1875 [00:42<00:24, 28.44it/s]\u001b[A\n"," 62%|██████▏   | 1170/1875 [00:42<00:25, 27.94it/s]\u001b[A\n"," 63%|██████▎   | 1174/1875 [00:43<00:26, 26.88it/s]\u001b[A\n"," 63%|██████▎   | 1177/1875 [00:43<00:25, 27.11it/s]\u001b[A\n"," 63%|██████▎   | 1180/1875 [00:43<00:25, 27.65it/s]\u001b[A\n"," 63%|██████▎   | 1183/1875 [00:43<00:24, 28.14it/s]\u001b[A\n"," 63%|██████▎   | 1186/1875 [00:43<00:24, 28.15it/s]\u001b[A\n"," 63%|██████▎   | 1189/1875 [00:43<00:24, 28.57it/s]\u001b[A\n"," 64%|██████▎   | 1192/1875 [00:43<00:24, 27.61it/s]\u001b[A\n"," 64%|██████▍   | 1196/1875 [00:43<00:25, 26.26it/s]\u001b[A\n"," 64%|██████▍   | 1199/1875 [00:44<00:24, 27.17it/s]\u001b[A\n"," 64%|██████▍   | 1203/1875 [00:44<00:23, 28.68it/s]\u001b[A\n"," 64%|██████▍   | 1206/1875 [00:44<00:23, 29.03it/s]\u001b[A\n"," 64%|██████▍   | 1209/1875 [00:44<00:26, 24.79it/s]\u001b[A\n"," 65%|██████▍   | 1213/1875 [00:44<00:24, 27.24it/s]\u001b[A\n"," 65%|██████▍   | 1216/1875 [00:44<00:24, 27.06it/s]\u001b[A\n"," 65%|██████▌   | 1219/1875 [00:44<00:24, 27.06it/s]\u001b[A\n"," 65%|██████▌   | 1223/1875 [00:44<00:25, 25.98it/s]\u001b[A\n"," 65%|██████▌   | 1226/1875 [00:45<00:24, 26.16it/s]\u001b[A\n"," 66%|██████▌   | 1230/1875 [00:45<00:22, 28.25it/s]\u001b[A\n"," 66%|██████▌   | 1233/1875 [00:45<00:22, 28.14it/s]\u001b[A\n"," 66%|██████▌   | 1236/1875 [00:45<00:22, 27.82it/s]\u001b[A\n"," 66%|██████▌   | 1239/1875 [00:45<00:25, 25.01it/s]\u001b[A\n"," 66%|██████▋   | 1243/1875 [00:45<00:22, 27.82it/s]\u001b[A\n"," 66%|██████▋   | 1246/1875 [00:45<00:23, 26.97it/s]\u001b[A\n"," 67%|██████▋   | 1250/1875 [00:45<00:21, 28.91it/s]\u001b[A\n"," 67%|██████▋   | 1254/1875 [00:46<00:23, 26.57it/s]\u001b[A\n"," 67%|██████▋   | 1258/1875 [00:46<00:21, 28.70it/s]\u001b[A\n"," 67%|██████▋   | 1262/1875 [00:46<00:22, 27.41it/s]\u001b[A\n"," 67%|██████▋   | 1265/1875 [00:46<00:21, 28.13it/s]\u001b[A\n"," 68%|██████▊   | 1268/1875 [00:46<00:21, 28.08it/s]\u001b[A\n"," 68%|██████▊   | 1271/1875 [00:46<00:21, 28.03it/s]\u001b[A\n"," 68%|██████▊   | 1274/1875 [00:46<00:21, 27.84it/s]\u001b[A\n"," 68%|██████▊   | 1278/1875 [00:46<00:20, 29.57it/s]\u001b[A\n"," 68%|██████▊   | 1282/1875 [00:47<00:22, 26.65it/s]\u001b[A\n"," 69%|██████▊   | 1285/1875 [00:47<00:23, 25.60it/s]\u001b[A\n"," 69%|██████▊   | 1289/1875 [00:47<00:22, 25.62it/s]\u001b[A\n"," 69%|██████▉   | 1293/1875 [00:47<00:20, 28.46it/s]\u001b[A\n"," 69%|██████▉   | 1296/1875 [00:47<00:20, 27.88it/s]\u001b[A\n"," 69%|██████▉   | 1300/1875 [00:47<00:19, 30.26it/s]\u001b[A\n"," 70%|██████▉   | 1304/1875 [00:47<00:22, 25.93it/s]\u001b[A\n"," 70%|██████▉   | 1307/1875 [00:47<00:21, 26.41it/s]\u001b[A\n"," 70%|██████▉   | 1311/1875 [00:48<00:22, 25.61it/s]\u001b[A\n"," 70%|███████   | 1316/1875 [00:48<00:19, 29.18it/s]\u001b[A\n"," 70%|███████   | 1320/1875 [00:48<00:19, 28.54it/s]\u001b[A\n"," 71%|███████   | 1324/1875 [00:48<00:20, 27.39it/s]\u001b[A\n"," 71%|███████   | 1327/1875 [00:48<00:19, 27.72it/s]\u001b[A\n"," 71%|███████   | 1331/1875 [00:48<00:20, 26.26it/s]\u001b[A\n"," 71%|███████   | 1335/1875 [00:48<00:18, 28.54it/s]\u001b[A\n"," 71%|███████▏  | 1338/1875 [00:49<00:19, 27.56it/s]\u001b[A\n"," 72%|███████▏  | 1341/1875 [00:49<00:22, 23.97it/s]\u001b[A\n"," 72%|███████▏  | 1345/1875 [00:49<00:19, 26.51it/s]\u001b[A\n"," 72%|███████▏  | 1348/1875 [00:49<00:19, 27.41it/s]\u001b[A\n"," 72%|███████▏  | 1351/1875 [00:49<00:18, 28.05it/s]\u001b[A\n"," 72%|███████▏  | 1354/1875 [00:49<00:18, 27.68it/s]\u001b[A\n"," 72%|███████▏  | 1357/1875 [00:49<00:18, 27.34it/s]\u001b[A\n"," 73%|███████▎  | 1361/1875 [00:49<00:19, 26.21it/s]\u001b[A\n"," 73%|███████▎  | 1366/1875 [00:50<00:17, 29.54it/s]\u001b[A\n"," 73%|███████▎  | 1370/1875 [00:50<00:18, 27.17it/s]\u001b[A\n"," 73%|███████▎  | 1373/1875 [00:50<00:18, 27.28it/s]\u001b[A\n"," 73%|███████▎  | 1376/1875 [00:50<00:18, 27.40it/s]\u001b[A\n"," 74%|███████▎  | 1380/1875 [00:50<00:16, 29.74it/s]\u001b[A\n"," 74%|███████▍  | 1384/1875 [00:50<00:17, 27.49it/s]\u001b[A\n"," 74%|███████▍  | 1387/1875 [00:50<00:17, 28.07it/s]\u001b[A\n"," 74%|███████▍  | 1390/1875 [00:50<00:17, 27.08it/s]\u001b[A\n"," 74%|███████▍  | 1394/1875 [00:51<00:16, 29.11it/s]\u001b[A\n"," 75%|███████▍  | 1397/1875 [00:51<00:19, 24.00it/s]\u001b[A\n"," 75%|███████▍  | 1402/1875 [00:51<00:17, 27.63it/s]\u001b[A\n"," 75%|███████▍  | 1406/1875 [00:51<00:17, 26.10it/s]\u001b[A\n"," 75%|███████▌  | 1409/1875 [00:51<00:17, 26.21it/s]\u001b[A\n"," 75%|███████▌  | 1412/1875 [00:51<00:17, 26.71it/s]\u001b[A\n"," 75%|███████▌  | 1415/1875 [00:51<00:17, 26.07it/s]\u001b[A\n"," 76%|███████▌  | 1418/1875 [00:51<00:17, 26.24it/s]\u001b[A\n"," 76%|███████▌  | 1422/1875 [00:52<00:15, 28.64it/s]\u001b[A\n"," 76%|███████▌  | 1425/1875 [00:52<00:17, 26.07it/s]\u001b[A\n"," 76%|███████▌  | 1429/1875 [00:52<00:15, 28.81it/s]\u001b[A\n"," 76%|███████▋  | 1433/1875 [00:52<00:17, 25.62it/s]\u001b[A\n"," 77%|███████▋  | 1437/1875 [00:52<00:15, 28.48it/s]\u001b[A\n"," 77%|███████▋  | 1441/1875 [00:52<00:15, 28.71it/s]\u001b[A\n"," 77%|███████▋  | 1445/1875 [00:52<00:15, 27.25it/s]\u001b[A\n"," 77%|███████▋  | 1448/1875 [00:53<00:15, 26.83it/s]\u001b[A\n"," 77%|███████▋  | 1451/1875 [00:53<00:15, 26.67it/s]\u001b[A\n"," 78%|███████▊  | 1455/1875 [00:53<00:16, 25.82it/s]\u001b[A\n"," 78%|███████▊  | 1458/1875 [00:53<00:15, 26.12it/s]\u001b[A\n"," 78%|███████▊  | 1462/1875 [00:53<00:14, 28.38it/s]\u001b[A\n"," 78%|███████▊  | 1465/1875 [00:53<00:17, 23.86it/s]\u001b[A\n"," 78%|███████▊  | 1470/1875 [00:53<00:14, 27.31it/s]\u001b[A\n"," 79%|███████▊  | 1474/1875 [00:53<00:13, 29.80it/s]\u001b[A\n"," 79%|███████▉  | 1478/1875 [00:54<00:15, 25.94it/s]\u001b[A\n"," 79%|███████▉  | 1483/1875 [00:54<00:14, 26.80it/s]\u001b[A\n"," 79%|███████▉  | 1487/1875 [00:54<00:13, 28.93it/s]\u001b[A\n"," 80%|███████▉  | 1491/1875 [00:54<00:14, 26.25it/s]\u001b[A\n"," 80%|███████▉  | 1495/1875 [00:54<00:13, 27.64it/s]\u001b[A\n"," 80%|███████▉  | 1499/1875 [00:54<00:12, 29.42it/s]\u001b[A\n"," 80%|████████  | 1503/1875 [00:55<00:13, 27.06it/s]\u001b[A\n"," 80%|████████  | 1506/1875 [00:55<00:13, 27.05it/s]\u001b[A\n"," 80%|████████  | 1509/1875 [00:55<00:13, 27.07it/s]\u001b[A\n"," 81%|████████  | 1513/1875 [00:55<00:13, 26.31it/s]\u001b[A\n"," 81%|████████  | 1517/1875 [00:55<00:12, 28.87it/s]\u001b[A\n"," 81%|████████  | 1520/1875 [00:55<00:12, 29.02it/s]\u001b[A\n"," 81%|████████  | 1523/1875 [00:55<00:12, 29.04it/s]\u001b[A\n"," 81%|████████▏ | 1526/1875 [00:55<00:12, 28.78it/s]\u001b[A\n"," 82%|████████▏ | 1529/1875 [00:55<00:12, 28.64it/s]\u001b[A\n"," 82%|████████▏ | 1532/1875 [00:56<00:11, 28.62it/s]\u001b[A\n"," 82%|████████▏ | 1535/1875 [00:56<00:14, 23.80it/s]\u001b[A\n"," 82%|████████▏ | 1540/1875 [00:56<00:12, 27.42it/s]\u001b[A\n"," 82%|████████▏ | 1544/1875 [00:56<00:13, 25.15it/s]\u001b[A\n"," 83%|████████▎ | 1547/1875 [00:56<00:12, 25.78it/s]\u001b[A\n"," 83%|████████▎ | 1551/1875 [00:56<00:11, 28.59it/s]\u001b[A\n"," 83%|████████▎ | 1555/1875 [00:56<00:10, 29.70it/s]\u001b[A\n"," 83%|████████▎ | 1559/1875 [00:57<00:11, 27.61it/s]\u001b[A\n"," 83%|████████▎ | 1562/1875 [00:57<00:11, 27.43it/s]\u001b[A\n"," 84%|████████▎ | 1566/1875 [00:57<00:10, 29.16it/s]\u001b[A\n"," 84%|████████▎ | 1570/1875 [00:57<00:11, 26.78it/s]\u001b[A\n"," 84%|████████▍ | 1573/1875 [00:57<00:11, 26.10it/s]\u001b[A\n"," 84%|████████▍ | 1577/1875 [00:57<00:11, 25.28it/s]\u001b[A\n"," 84%|████████▍ | 1582/1875 [00:57<00:10, 29.26it/s]\u001b[A\n"," 85%|████████▍ | 1586/1875 [00:58<00:11, 25.70it/s]\u001b[A\n"," 85%|████████▍ | 1590/1875 [00:58<00:10, 28.29it/s]\u001b[A\n"," 85%|████████▌ | 1594/1875 [00:58<00:10, 25.69it/s]\u001b[A\n"," 85%|████████▌ | 1598/1875 [00:58<00:09, 28.33it/s]\u001b[A\n"," 85%|████████▌ | 1602/1875 [00:58<00:10, 26.18it/s]\u001b[A\n"," 86%|████████▌ | 1606/1875 [00:58<00:09, 28.55it/s]\u001b[A\n"," 86%|████████▌ | 1610/1875 [00:58<00:10, 25.96it/s]\u001b[A\n"," 86%|████████▌ | 1614/1875 [00:59<00:09, 28.13it/s]\u001b[A\n"," 86%|████████▌ | 1617/1875 [00:59<00:11, 23.23it/s]\u001b[A\n"," 87%|████████▋ | 1622/1875 [00:59<00:09, 26.80it/s]\u001b[A\n"," 87%|████████▋ | 1626/1875 [00:59<00:08, 29.26it/s]\u001b[A\n"," 87%|████████▋ | 1630/1875 [00:59<00:08, 27.32it/s]\u001b[A\n"," 87%|████████▋ | 1633/1875 [00:59<00:09, 26.33it/s]\u001b[A\n"," 87%|████████▋ | 1637/1875 [00:59<00:08, 28.97it/s]\u001b[A\n"," 88%|████████▊ | 1641/1875 [01:00<00:09, 25.99it/s]\u001b[A\n"," 88%|████████▊ | 1645/1875 [01:00<00:08, 28.11it/s]\u001b[A\n"," 88%|████████▊ | 1649/1875 [01:00<00:07, 28.90it/s]\u001b[A\n"," 88%|████████▊ | 1653/1875 [01:00<00:08, 27.34it/s]\u001b[A\n"," 88%|████████▊ | 1656/1875 [01:00<00:08, 26.37it/s]\u001b[A\n"," 89%|████████▊ | 1660/1875 [01:00<00:08, 25.88it/s]\u001b[A\n"," 89%|████████▊ | 1663/1875 [01:00<00:07, 26.55it/s]\u001b[A\n"," 89%|████████▉ | 1666/1875 [01:00<00:07, 27.25it/s]\u001b[A\n"," 89%|████████▉ | 1670/1875 [01:01<00:07, 29.00it/s]\u001b[A\n"," 89%|████████▉ | 1674/1875 [01:01<00:07, 27.03it/s]\u001b[A\n"," 89%|████████▉ | 1677/1875 [01:01<00:07, 27.24it/s]\u001b[A\n"," 90%|████████▉ | 1681/1875 [01:01<00:06, 29.74it/s]\u001b[A\n"," 90%|████████▉ | 1685/1875 [01:01<00:07, 25.74it/s]\u001b[A\n"," 90%|█████████ | 1689/1875 [01:01<00:06, 28.18it/s]\u001b[A\n"," 90%|█████████ | 1693/1875 [01:01<00:06, 29.53it/s]\u001b[A\n"," 91%|█████████ | 1697/1875 [01:02<00:06, 28.04it/s]\u001b[A\n"," 91%|█████████ | 1700/1875 [01:02<00:07, 23.24it/s]\u001b[A\n"," 91%|█████████ | 1704/1875 [01:02<00:06, 26.29it/s]\u001b[A\n"," 91%|█████████ | 1708/1875 [01:02<00:05, 29.20it/s]\u001b[A\n"," 91%|█████████▏| 1712/1875 [01:02<00:06, 25.66it/s]\u001b[A\n"," 92%|█████████▏| 1716/1875 [01:02<00:05, 28.47it/s]\u001b[A\n"," 92%|█████████▏| 1720/1875 [01:02<00:06, 25.65it/s]\u001b[A\n"," 92%|█████████▏| 1724/1875 [01:03<00:05, 28.47it/s]\u001b[A\n"," 92%|█████████▏| 1728/1875 [01:03<00:05, 28.56it/s]\u001b[A\n"," 92%|█████████▏| 1732/1875 [01:03<00:05, 27.38it/s]\u001b[A\n"," 93%|█████████▎| 1735/1875 [01:03<00:05, 27.49it/s]\u001b[A\n"," 93%|█████████▎| 1738/1875 [01:03<00:05, 25.84it/s]\u001b[A\n"," 93%|█████████▎| 1742/1875 [01:03<00:05, 25.75it/s]\u001b[A\n"," 93%|█████████▎| 1747/1875 [01:03<00:04, 29.24it/s]\u001b[A\n"," 93%|█████████▎| 1751/1875 [01:03<00:04, 28.76it/s]\u001b[A\n"," 94%|█████████▎| 1755/1875 [01:04<00:04, 26.94it/s]\u001b[A\n"," 94%|█████████▍| 1760/1875 [01:04<00:04, 27.18it/s]\u001b[A\n"," 94%|█████████▍| 1763/1875 [01:04<00:04, 27.57it/s]\u001b[A\n"," 94%|█████████▍| 1767/1875 [01:04<00:03, 29.48it/s]\u001b[A\n"," 94%|█████████▍| 1771/1875 [01:04<00:03, 26.42it/s]\u001b[A\n"," 95%|█████████▍| 1775/1875 [01:04<00:03, 28.25it/s]\u001b[A\n"," 95%|█████████▍| 1779/1875 [01:05<00:03, 26.88it/s]\u001b[A\n"," 95%|█████████▌| 1783/1875 [01:05<00:03, 29.21it/s]\u001b[A\n"," 95%|█████████▌| 1787/1875 [01:05<00:03, 25.79it/s]\u001b[A\n"," 96%|█████████▌| 1792/1875 [01:05<00:02, 29.48it/s]\u001b[A\n"," 96%|█████████▌| 1796/1875 [01:05<00:02, 26.59it/s]\u001b[A\n"," 96%|█████████▌| 1799/1875 [01:05<00:02, 26.48it/s]\u001b[A\n"," 96%|█████████▌| 1802/1875 [01:05<00:02, 26.71it/s]\u001b[A\n"," 96%|█████████▋| 1806/1875 [01:06<00:02, 26.03it/s]\u001b[A\n"," 97%|█████████▋| 1810/1875 [01:06<00:02, 28.61it/s]\u001b[A\n"," 97%|█████████▋| 1814/1875 [01:06<00:02, 25.30it/s]\u001b[A\n"," 97%|█████████▋| 1819/1875 [01:06<00:01, 28.94it/s]\u001b[A\n"," 97%|█████████▋| 1823/1875 [01:06<00:01, 28.44it/s]\u001b[A\n"," 97%|█████████▋| 1827/1875 [01:06<00:01, 27.45it/s]\u001b[A\n"," 98%|█████████▊| 1830/1875 [01:06<00:01, 22.99it/s]\u001b[A\n"," 98%|█████████▊| 1835/1875 [01:07<00:01, 26.70it/s]\u001b[A\n"," 98%|█████████▊| 1839/1875 [01:07<00:01, 27.10it/s]\u001b[A\n"," 98%|█████████▊| 1842/1875 [01:07<00:01, 24.99it/s]\u001b[A\n"," 99%|█████████▊| 1847/1875 [01:07<00:00, 28.37it/s]\u001b[A\n"," 99%|█████████▊| 1851/1875 [01:07<00:00, 28.15it/s]\u001b[A\n"," 99%|█████████▉| 1855/1875 [01:07<00:00, 27.58it/s]\u001b[A\n"," 99%|█████████▉| 1859/1875 [01:07<00:00, 29.13it/s]\u001b[A\n"," 99%|█████████▉| 1863/1875 [01:08<00:00, 26.80it/s]\u001b[A\n","100%|█████████▉| 1866/1875 [01:08<00:00, 25.69it/s]\u001b[A\n","100%|█████████▉| 1870/1875 [01:08<00:00, 25.48it/s]\u001b[A\n","100%|██████████| 1875/1875 [01:08<00:00, 27.38it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"BdQ558ooxwQJ","executionInfo":{"status":"ok","timestamp":1623289748009,"user_tz":-480,"elapsed":460,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e22387c1-adb1-4c49-8df5-b2da32baef6f"},"source":["# test.iloc[:10000, 1:] = F.softmax(torch.tensor(PREDS), dim=-1).numpy()\n","# test.iloc[:10000, 1:] = torch.tensor(PREDS).numpy()\n","test.iloc[:, 1:] = torch.tensor(PREDS).numpy()\n","test['Filename'] = test['Filename'].str.replace('.wav', '')\n","test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Filename</th>\n","      <th>Barking</th>\n","      <th>Howling</th>\n","      <th>Crying</th>\n","      <th>COSmoke</th>\n","      <th>GlassBreaking</th>\n","      <th>Other</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>public_00001</td>\n","      <td>6.144940e-02</td>\n","      <td>5.453053e-01</td>\n","      <td>3.228939e-01</td>\n","      <td>1.126858e-02</td>\n","      <td>7.859051e-03</td>\n","      <td>0.051224</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>public_00002</td>\n","      <td>2.654521e-07</td>\n","      <td>3.957806e-07</td>\n","      <td>1.911393e-07</td>\n","      <td>5.934786e-10</td>\n","      <td>1.516890e-05</td>\n","      <td>0.999984</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>public_00003</td>\n","      <td>9.999226e-01</td>\n","      <td>3.045627e-05</td>\n","      <td>1.043902e-05</td>\n","      <td>7.652030e-08</td>\n","      <td>1.474413e-05</td>\n","      <td>0.000022</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>public_00004</td>\n","      <td>2.964883e-01</td>\n","      <td>1.444015e-01</td>\n","      <td>2.364393e-01</td>\n","      <td>8.931077e-02</td>\n","      <td>8.475937e-02</td>\n","      <td>0.148601</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>public_00005</td>\n","      <td>3.901064e-02</td>\n","      <td>1.406257e-01</td>\n","      <td>7.239787e-01</td>\n","      <td>1.721349e-02</td>\n","      <td>3.508842e-02</td>\n","      <td>0.044083</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29995</th>\n","      <td>private_19996</td>\n","      <td>2.312374e-03</td>\n","      <td>2.550300e-05</td>\n","      <td>2.088708e-03</td>\n","      <td>9.953142e-01</td>\n","      <td>1.351796e-04</td>\n","      <td>0.000124</td>\n","    </tr>\n","    <tr>\n","      <th>29996</th>\n","      <td>private_19997</td>\n","      <td>1.420989e-01</td>\n","      <td>1.024586e-01</td>\n","      <td>1.133607e-01</td>\n","      <td>6.746402e-02</td>\n","      <td>2.826460e-01</td>\n","      <td>0.291972</td>\n","    </tr>\n","    <tr>\n","      <th>29997</th>\n","      <td>private_19998</td>\n","      <td>8.714300e-10</td>\n","      <td>2.151173e-09</td>\n","      <td>7.284987e-10</td>\n","      <td>1.612620e-13</td>\n","      <td>1.130452e-07</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>29998</th>\n","      <td>private_19999</td>\n","      <td>1.540704e-03</td>\n","      <td>1.168827e-02</td>\n","      <td>9.866976e-01</td>\n","      <td>3.251455e-06</td>\n","      <td>6.343484e-06</td>\n","      <td>0.000064</td>\n","    </tr>\n","    <tr>\n","      <th>29999</th>\n","      <td>private_20000</td>\n","      <td>5.067265e-05</td>\n","      <td>9.946993e-01</td>\n","      <td>5.197752e-03</td>\n","      <td>1.310457e-08</td>\n","      <td>5.470592e-07</td>\n","      <td>0.000052</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30000 rows × 7 columns</p>\n","</div>"],"text/plain":["            Filename       Barking  ...  GlassBreaking     Other\n","0       public_00001  6.144940e-02  ...   7.859051e-03  0.051224\n","1       public_00002  2.654521e-07  ...   1.516890e-05  0.999984\n","2       public_00003  9.999226e-01  ...   1.474413e-05  0.000022\n","3       public_00004  2.964883e-01  ...   8.475937e-02  0.148601\n","4       public_00005  3.901064e-02  ...   3.508842e-02  0.044083\n","...              ...           ...  ...            ...       ...\n","29995  private_19996  2.312374e-03  ...   1.351796e-04  0.000124\n","29996  private_19997  1.420989e-01  ...   2.826460e-01  0.291972\n","29997  private_19998  8.714300e-10  ...   1.130452e-07  1.000000\n","29998  private_19999  1.540704e-03  ...   6.343484e-06  0.000064\n","29999  private_20000  5.067265e-05  ...   5.470592e-07  0.000052\n","\n","[30000 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"vILIh030xx_J"},"source":["test.to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:59.844806Z","iopub.status.busy":"2020-08-14T10:26:59.843872Z","iopub.status.idle":"2020-08-14T11:06:08.770335Z","shell.execute_reply":"2020-08-14T11:06:08.767243Z"},"papermill":{"duration":2348.968697,"end_time":"2020-08-14T11:06:08.770508","exception":false,"start_time":"2020-08-14T10:26:59.801811","status":"completed"},"tags":[],"id":"leCrNXYxEIAo"},"source":["# from catalyst import dl\n","\n","# warnings.simplefilter(\"ignore\")\n","\n","# runner = SupervisedRunner(\n","#     input_key=\"waveform\",\n","#     output_key=\"logits\",\n","#     target_key=\"targets\",\n","#     loss_key=\"loss\"\n","# )\n","\n","# runner.train(\n","#     model=model,\n","#     criterion=criterion,\n","#     loaders=loaders,\n","#     optimizer=optimizer,\n","#     scheduler=scheduler,\n","#     num_epochs=10,\n","#     verbose=True,\n","#     logdir=None, #f\"fold0\",\n","#     callbacks=[\n","#         dl.AUCCallback(input_key=\"logits\", target_key=\"targets\")  #callbacks,\n","#     ],\n","#     valid_loader=\"valid\",\n","#     valid_metric=\"loss\"\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.484059,"end_time":"2020-08-14T11:06:25.607820","exception":false,"start_time":"2020-08-14T11:06:25.123761","status":"completed"},"tags":[],"id":"sSmzxzEfEIAo"},"source":["Seems it's learning something.\n","\n","Now I'll show how this model works in the inference phase. I'll use trained model of this which I trained by myself using the data of this competition in my local environment.\n","\n","Since [several concerns](https://www.kaggle.com/c/birdsong-recognition/discussion/172356) are expressed about over-sharing of top solutions during competition, and since I do respect those people who have worked hard to improve their scores, I would not make trained weight in common and would not share how I trained this model."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.970323,"end_time":"2020-08-14T11:06:27.858184","exception":false,"start_time":"2020-08-14T11:06:26.887861","status":"completed"},"tags":[],"id":"aorWjX4EEIAo"},"source":["## Prediction with SED model"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:28.697434Z","iopub.status.busy":"2020-08-14T11:06:28.695751Z","iopub.status.idle":"2020-08-14T11:06:28.698107Z","shell.execute_reply":"2020-08-14T11:06:28.698557Z"},"papermill":{"duration":0.413231,"end_time":"2020-08-14T11:06:28.698692","exception":false,"start_time":"2020-08-14T11:06:28.285461","status":"completed"},"tags":[],"id":"uO1KiQjcEIAo"},"source":["model_config = {\n","    \"sample_rate\": 32000,\n","    \"window_size\": 1024,\n","    \"hop_size\": 320,\n","    \"mel_bins\": 64,\n","    \"fmin\": 50,\n","    \"fmax\": 14000,\n","    \"classes_num\": 264\n","}\n","\n","weights_path = \"../input/birdcall-pannsatt-aux-weak/best.pth\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:29.628436Z","iopub.status.busy":"2020-08-14T11:06:29.626386Z","iopub.status.idle":"2020-08-14T11:06:29.629299Z","shell.execute_reply":"2020-08-14T11:06:29.630949Z"},"papermill":{"duration":0.538009,"end_time":"2020-08-14T11:06:29.631164","exception":false,"start_time":"2020-08-14T11:06:29.093155","status":"completed"},"tags":[],"id":"CmczvMtCEIAo"},"source":["def get_model(config: dict, weights_path: str):\n","    model = PANNsCNN14Att(**config)\n","    checkpoint = torch.load(weights_path)\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    device = torch.device(\"cuda\")\n","    model.to(device)\n","    model.eval()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:30.517376Z","iopub.status.busy":"2020-08-14T11:06:30.515796Z","iopub.status.idle":"2020-08-14T11:06:30.518163Z","shell.execute_reply":"2020-08-14T11:06:30.518675Z"},"papermill":{"duration":0.436469,"end_time":"2020-08-14T11:06:30.518838","exception":false,"start_time":"2020-08-14T11:06:30.082369","status":"completed"},"tags":[],"id":"7va6HNrCEIAo"},"source":["def prediction_for_clip(test_df: pd.DataFrame,\n","                        clip: np.ndarray, \n","                        model: PANNsCNN14Att,\n","                        threshold=0.5):\n","    PERIOD = 30\n","    audios = []\n","    y = clip.astype(np.float32)\n","    len_y = len(y)\n","    start = 0\n","    end = PERIOD * SR\n","    while True:\n","        y_batch = y[start:end].astype(np.float32)\n","        if len(y_batch) != PERIOD * SR:\n","            y_pad = np.zeros(PERIOD * SR, dtype=np.float32)\n","            y_pad[:len(y_batch)] = y_batch\n","            audios.append(y_pad)\n","            break\n","        start = end\n","        end += PERIOD * SR\n","        audios.append(y_batch)\n","        \n","    array = np.asarray(audios)\n","    tensors = torch.from_numpy(array)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    model.eval()\n","    estimated_event_list = []\n","    global_time = 0.0\n","    site = test_df[\"site\"].values[0]\n","    audio_id = test_df[\"audio_id\"].values[0]\n","    for image in progress_bar(tensors):\n","        image = image.view(1, image.size(0))\n","        image = image.to(device)\n","\n","        with torch.no_grad():\n","            prediction = model(image)\n","            framewise_outputs = prediction[\"framewise_output\"].detach(\n","                ).cpu().numpy()[0]\n","                \n","        thresholded = framewise_outputs >= threshold\n","\n","        for target_idx in range(thresholded.shape[1]):\n","            if thresholded[:, target_idx].mean() == 0:\n","                pass\n","            else:\n","                detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n","                head_idx = 0\n","                tail_idx = 0\n","                while True:\n","                    if (tail_idx + 1 == len(detected)) or (\n","                            detected[tail_idx + 1] - \n","                            detected[tail_idx] != 1):\n","                        onset = 0.01 * detected[\n","                            head_idx] + global_time\n","                        offset = 0.01 * detected[\n","                            tail_idx] + global_time\n","                        onset_idx = detected[head_idx]\n","                        offset_idx = detected[tail_idx]\n","                        max_confidence = framewise_outputs[\n","                            onset_idx:offset_idx, target_idx].max()\n","                        mean_confidence = framewise_outputs[\n","                            onset_idx:offset_idx, target_idx].mean()\n","                        estimated_event = {\n","                            \"site\": site,\n","                            \"audio_id\": audio_id,\n","                            \"ebird_code\": INV_BIRD_CODE[target_idx],\n","                            \"onset\": onset,\n","                            \"offset\": offset,\n","                            \"max_confidence\": max_confidence,\n","                            \"mean_confidence\": mean_confidence\n","                        }\n","                        estimated_event_list.append(estimated_event)\n","                        head_idx = tail_idx + 1\n","                        tail_idx = tail_idx + 1\n","                        if head_idx >= len(detected):\n","                            break\n","                    else:\n","                        tail_idx += 1\n","        global_time += PERIOD\n","        \n","    prediction_df = pd.DataFrame(estimated_event_list)\n","    return prediction_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:31.416619Z","iopub.status.busy":"2020-08-14T11:06:31.415703Z","iopub.status.idle":"2020-08-14T11:06:31.418878Z","shell.execute_reply":"2020-08-14T11:06:31.418386Z"},"papermill":{"duration":0.411637,"end_time":"2020-08-14T11:06:31.418987","exception":false,"start_time":"2020-08-14T11:06:31.007350","status":"completed"},"tags":[],"id":"aHRriBcgEIAp"},"source":["def prediction(test_df: pd.DataFrame,\n","               test_audio: Path,\n","               model_config: dict,\n","               weights_path: str,\n","               threshold=0.5):\n","    model = get_model(model_config, weights_path)\n","    unique_audio_id = test_df.audio_id.unique()\n","\n","    warnings.filterwarnings(\"ignore\")\n","    prediction_dfs = []\n","    for audio_id in unique_audio_id:\n","        with timer(f\"Loading {audio_id}\"):\n","            clip, _ = librosa.load(test_audio / (audio_id + \".mp3\"),\n","                                   sr=SR,\n","                                   mono=True,\n","                                   res_type=\"kaiser_fast\")\n","        \n","        test_df_for_audio_id = test_df.query(\n","            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n","        with timer(f\"Prediction on {audio_id}\"):\n","            prediction_df = prediction_for_clip(test_df_for_audio_id,\n","                                                clip=clip,\n","                                                model=model,\n","                                                threshold=threshold)\n","\n","        prediction_dfs.append(prediction_df)\n","    \n","    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n","    return prediction_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:32.264948Z","iopub.status.busy":"2020-08-14T11:06:32.263886Z","iopub.status.idle":"2020-08-14T11:06:52.100417Z","shell.execute_reply":"2020-08-14T11:06:52.101005Z"},"papermill":{"duration":20.287908,"end_time":"2020-08-14T11:06:52.101194","exception":false,"start_time":"2020-08-14T11:06:31.813286","status":"completed"},"tags":[],"id":"BAmgqI6wEIAp","outputId":"2276b18e-3018-4611-b39b-88d6f3747382"},"source":["prediction_df = prediction(test_df=test,\n","                           test_audio=TEST_AUDIO_DIR,\n","                           model_config=model_config,\n","                           weights_path=weights_path,\n","                           threshold=0.5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Loading 41e6fe6504a34bf6846938ba78d13df1] start\n","[Loading 41e6fe6504a34bf6846938ba78d13df1] done in 2.31 s\n","[Prediction on 41e6fe6504a34bf6846938ba78d13df1] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 41e6fe6504a34bf6846938ba78d13df1] done in 0.60 s\n","[Loading cce64fffafed40f2b2f3d3413ec1c4c2] start\n","[Loading cce64fffafed40f2b2f3d3413ec1c4c2] done in 0.81 s\n","[Prediction on cce64fffafed40f2b2f3d3413ec1c4c2] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on cce64fffafed40f2b2f3d3413ec1c4c2] done in 0.07 s\n","[Loading 99af324c881246949408c0b1ae54271f] start\n","[Loading 99af324c881246949408c0b1ae54271f] done in 0.82 s\n","[Prediction on 99af324c881246949408c0b1ae54271f] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 99af324c881246949408c0b1ae54271f] done in 0.08 s\n","[Loading 6ab74e177aa149468a39ca10beed6222] start\n","[Loading 6ab74e177aa149468a39ca10beed6222] done in 0.76 s\n","[Prediction on 6ab74e177aa149468a39ca10beed6222] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 6ab74e177aa149468a39ca10beed6222] done in 0.08 s\n","[Loading b2fd3f01e9284293a1e33f9c811a2ed6] start\n","[Loading b2fd3f01e9284293a1e33f9c811a2ed6] done in 0.78 s\n","[Prediction on b2fd3f01e9284293a1e33f9c811a2ed6] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on b2fd3f01e9284293a1e33f9c811a2ed6] done in 0.07 s\n","[Loading de62b37ebba749d2abf29d4a493ea5d4] start\n","[Loading de62b37ebba749d2abf29d4a493ea5d4] done in 0.44 s\n","[Prediction on de62b37ebba749d2abf29d4a493ea5d4] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on de62b37ebba749d2abf29d4a493ea5d4] done in 0.04 s\n","[Loading 8680a8dd845d40f296246dbed0d37394] start\n","[Loading 8680a8dd845d40f296246dbed0d37394] done in 0.88 s\n","[Prediction on 8680a8dd845d40f296246dbed0d37394] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 8680a8dd845d40f296246dbed0d37394] done in 0.07 s\n","[Loading 940d546e5eb745c9a74bce3f35efa1f9] start\n","[Loading 940d546e5eb745c9a74bce3f35efa1f9] done in 1.22 s\n","[Prediction on 940d546e5eb745c9a74bce3f35efa1f9] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='3' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [3/3 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 940d546e5eb745c9a74bce3f35efa1f9] done in 0.11 s\n","[Loading 07ab324c602e4afab65ddbcc746c31b5] start\n","[Loading 07ab324c602e4afab65ddbcc746c31b5] done in 0.66 s\n","[Prediction on 07ab324c602e4afab65ddbcc746c31b5] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 07ab324c602e4afab65ddbcc746c31b5] done in 0.04 s\n","[Loading 899616723a32409c996f6f3441646c2a] start\n","[Loading 899616723a32409c996f6f3441646c2a] done in 1.17 s\n","[Prediction on 899616723a32409c996f6f3441646c2a] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 899616723a32409c996f6f3441646c2a] done in 0.08 s\n","[Loading 9cc5d9646f344f1bbb52640a988fe902] start\n","[Loading 9cc5d9646f344f1bbb52640a988fe902] done in 3.42 s\n","[Prediction on 9cc5d9646f344f1bbb52640a988fe902] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='9' class='' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [9/9 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 9cc5d9646f344f1bbb52640a988fe902] done in 0.32 s\n","[Loading a56e20a518684688a9952add8a9d5213] start\n","[Loading a56e20a518684688a9952add8a9d5213] done in 0.74 s\n","[Prediction on a56e20a518684688a9952add8a9d5213] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on a56e20a518684688a9952add8a9d5213] done in 0.08 s\n","[Loading 96779836288745728306903d54e264dd] start\n","[Loading 96779836288745728306903d54e264dd] done in 0.59 s\n","[Prediction on 96779836288745728306903d54e264dd] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 96779836288745728306903d54e264dd] done in 0.04 s\n","[Loading f77783ba4c6641bc918b034a18c23e53] start\n","[Loading f77783ba4c6641bc918b034a18c23e53] done in 0.47 s\n","[Prediction on f77783ba4c6641bc918b034a18c23e53] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on f77783ba4c6641bc918b034a18c23e53] done in 0.04 s\n","[Loading 856b194b097441958697c2bcd1f63982] start\n","[Loading 856b194b097441958697c2bcd1f63982] done in 0.71 s\n","[Prediction on 856b194b097441958697c2bcd1f63982] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 856b194b097441958697c2bcd1f63982] done in 0.04 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:52.923275Z","iopub.status.busy":"2020-08-14T11:06:52.922629Z","iopub.status.idle":"2020-08-14T11:06:52.941913Z","shell.execute_reply":"2020-08-14T11:06:52.942405Z"},"papermill":{"duration":0.437985,"end_time":"2020-08-14T11:06:52.942559","exception":false,"start_time":"2020-08-14T11:06:52.504574","status":"completed"},"tags":[],"id":"n_psXKK6EIAp","outputId":"d279790c-1bb5-4dd9-d512-bdaf669659cf"},"source":["prediction_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>site</th>\n","      <th>audio_id</th>\n","      <th>ebird_code</th>\n","      <th>onset</th>\n","      <th>offset</th>\n","      <th>max_confidence</th>\n","      <th>mean_confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>0.96</td>\n","      <td>2.23</td>\n","      <td>0.985395</td>\n","      <td>0.897037</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>7.04</td>\n","      <td>7.67</td>\n","      <td>0.526611</td>\n","      <td>0.519470</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>11.20</td>\n","      <td>12.15</td>\n","      <td>0.956318</td>\n","      <td>0.928820</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>14.40</td>\n","      <td>15.03</td>\n","      <td>0.809643</td>\n","      <td>0.805571</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>20.16</td>\n","      <td>21.43</td>\n","      <td>0.987058</td>\n","      <td>0.917030</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>18.24</td>\n","      <td>19.19</td>\n","      <td>0.885321</td>\n","      <td>0.789641</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>19.84</td>\n","      <td>22.07</td>\n","      <td>0.983291</td>\n","      <td>0.913020</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>23.04</td>\n","      <td>23.67</td>\n","      <td>0.669237</td>\n","      <td>0.624711</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>25.92</td>\n","      <td>26.87</td>\n","      <td>0.950051</td>\n","      <td>0.897717</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>27.84</td>\n","      <td>28.79</td>\n","      <td>0.991087</td>\n","      <td>0.899111</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 7 columns</p>\n","</div>"],"text/plain":["       site                          audio_id ebird_code  onset  offset  \\\n","0    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly   0.96    2.23   \n","1    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly   7.04    7.67   \n","2    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly  11.20   12.15   \n","3    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly  14.40   15.03   \n","4    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly  20.16   21.43   \n","..      ...                               ...        ...    ...     ...   \n","195  site_3  856b194b097441958697c2bcd1f63982     aldfly  18.24   19.19   \n","196  site_3  856b194b097441958697c2bcd1f63982     aldfly  19.84   22.07   \n","197  site_3  856b194b097441958697c2bcd1f63982     aldfly  23.04   23.67   \n","198  site_3  856b194b097441958697c2bcd1f63982     aldfly  25.92   26.87   \n","199  site_3  856b194b097441958697c2bcd1f63982     aldfly  27.84   28.79   \n","\n","     max_confidence  mean_confidence  \n","0          0.985395         0.897037  \n","1          0.526611         0.519470  \n","2          0.956318         0.928820  \n","3          0.809643         0.805571  \n","4          0.987058         0.917030  \n","..              ...              ...  \n","195        0.885321         0.789641  \n","196        0.983291         0.913020  \n","197        0.669237         0.624711  \n","198        0.950051         0.897717  \n","199        0.991087         0.899111  \n","\n","[200 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.448352,"end_time":"2020-08-14T11:06:53.824923","exception":false,"start_time":"2020-08-14T11:06:53.376571","status":"completed"},"tags":[],"id":"xpta1cQPEIAq"},"source":["## Postprocess\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:54.667626Z","iopub.status.busy":"2020-08-14T11:06:54.666862Z","iopub.status.idle":"2020-08-14T11:06:54.711744Z","shell.execute_reply":"2020-08-14T11:06:54.711222Z"},"papermill":{"duration":0.476478,"end_time":"2020-08-14T11:06:54.711863","exception":false,"start_time":"2020-08-14T11:06:54.235385","status":"completed"},"tags":[],"id":"bQHVLzN7EIAq"},"source":["labels = {}\n","\n","for audio_id, sub_df in prediction_df.groupby(\"audio_id\"):\n","    events = sub_df[[\"ebird_code\", \"onset\", \"offset\", \"max_confidence\", \"site\"]].values\n","    n_events = len(events)\n","    removed_event = []\n","    # Overlap deletion: this part may not be necessary\n","    # I deleted this part in other model and found there's no difference on the public LB score.\n","    for i in range(n_events):\n","        for j in range(n_events):\n","            if i == j:\n","                continue\n","            if i in removed_event:\n","                continue\n","            if j in removed_event:\n","                continue\n","            \n","            event_i = events[i]\n","            event_j = events[j]\n","            \n","            if (event_i[1] - event_j[2] >= 0) or (event_j[1] - event_i[2] >= 0):\n","                pass\n","            else:\n","                later_onset = max(event_i[1], event_j[1])\n","                sooner_onset = min(event_i[1], event_j[1])\n","                sooner_offset = min(event_i[2], event_j[2])\n","                later_offset = max(event_i[2], event_j[2])\n","\n","                intersection = sooner_offset - later_onset\n","                union = later_offset - sooner_onset\n","                \n","                iou = intersection / union\n","                if iou > 0.4:\n","                    if event_i[3] > event_j[3]:\n","                        removed_event.append(j)\n","                    else:\n","                        removed_event.append(i)\n","\n","    site = events[0][4]\n","    for i in range(n_events):\n","        if i in removed_event:\n","            continue\n","        event = events[i][0]\n","        onset = events[i][1]\n","        offset = events[i][2]\n","        if site in {\"site_1\", \"site_2\"}:\n","            start_section = int((onset // 5) * 5) + 5\n","            end_section = int((offset // 5) * 5) + 5\n","            cur_section = start_section\n","\n","            row_id = f\"{site}_{audio_id}_{start_section}\"\n","            if labels.get(row_id) is not None:\n","                labels[row_id].add(event)\n","            else:\n","                labels[row_id] = set()\n","                labels[row_id].add(event)\n","\n","            while cur_section != end_section:\n","                cur_section += 5\n","                row_id = f\"{site}_{audio_id}_{cur_section}\"\n","                if labels.get(row_id) is not None:\n","                    labels[row_id].add(event)\n","                else:\n","                    labels[row_id] = set()\n","                    labels[row_id].add(event)\n","        else:\n","            row_id = f\"{site}_{audio_id}\"\n","            if labels.get(row_id) is not None:\n","                labels[row_id].add(event)\n","            else:\n","                labels[row_id] = set()\n","                labels[row_id].add(event)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:55.584674Z","iopub.status.busy":"2020-08-14T11:06:55.583531Z","iopub.status.idle":"2020-08-14T11:06:55.588323Z","shell.execute_reply":"2020-08-14T11:06:55.587801Z"},"papermill":{"duration":0.421735,"end_time":"2020-08-14T11:06:55.588428","exception":false,"start_time":"2020-08-14T11:06:55.166693","status":"completed"},"tags":[],"id":"n4bVOnKMEIAq","outputId":"d3a77ae3-ac79-4ae9-e334-eb4d1fa6df2b"},"source":["for key in labels:\n","    labels[key] = \" \".join(sorted(list(labels[key])))\n","    \n","    \n","row_ids = list(labels.keys())\n","birds = list(labels.values())\n","post_processed = pd.DataFrame({\n","    \"row_id\": row_ids,\n","    \"birds\": birds\n","})\n","post_processed.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>birds</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_10</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_15</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_25</td>\n","      <td>redcro</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       row_id   birds\n","0   site_2_07ab324c602e4afab65ddbcc746c31b5_5  aldfly\n","1  site_2_07ab324c602e4afab65ddbcc746c31b5_10  aldfly\n","2  site_2_07ab324c602e4afab65ddbcc746c31b5_15  aldfly\n","3  site_2_07ab324c602e4afab65ddbcc746c31b5_25  redcro\n","4   site_1_41e6fe6504a34bf6846938ba78d13df1_5  aldfly"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:56.691972Z","iopub.status.busy":"2020-08-14T11:06:56.690948Z","iopub.status.idle":"2020-08-14T11:06:56.903190Z","shell.execute_reply":"2020-08-14T11:06:56.902599Z"},"papermill":{"duration":0.819428,"end_time":"2020-08-14T11:06:56.903303","exception":false,"start_time":"2020-08-14T11:06:56.083875","status":"completed"},"tags":[],"id":"0twOoktlEIAq","outputId":"6db86e93-4c17-46b5-e8cd-ddf51dd3dd03"},"source":["all_row_id = test[[\"row_id\"]]\n","submission = all_row_id.merge(post_processed, on=\"row_id\", how=\"left\")\n","submission = submission.fillna(\"nocall\")\n","submission.to_csv(\"submission.csv\", index=False)\n","submission.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>birds</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_10</td>\n","      <td>aldfly fiespa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_15</td>\n","      <td>aldfly moudov</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_20</td>\n","      <td>aldfly chswar</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_25</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_10</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_15</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_20</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_25</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_30</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_35</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_5</td>\n","      <td>hamfly</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_10</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_15</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_20</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_25</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_30</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_35</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>site_1_6ab74e177aa149468a39ca10beed6222_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        row_id          birds\n","0    site_1_41e6fe6504a34bf6846938ba78d13df1_5         aldfly\n","1   site_1_41e6fe6504a34bf6846938ba78d13df1_10  aldfly fiespa\n","2   site_1_41e6fe6504a34bf6846938ba78d13df1_15  aldfly moudov\n","3   site_1_41e6fe6504a34bf6846938ba78d13df1_20  aldfly chswar\n","4   site_1_41e6fe6504a34bf6846938ba78d13df1_25         aldfly\n","5    site_1_cce64fffafed40f2b2f3d3413ec1c4c2_5         aldfly\n","6   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_10         nocall\n","7   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_15         aldfly\n","8   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_20         nocall\n","9   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_25         nocall\n","10  site_1_cce64fffafed40f2b2f3d3413ec1c4c2_30         nocall\n","11  site_1_cce64fffafed40f2b2f3d3413ec1c4c2_35         aldfly\n","12   site_1_99af324c881246949408c0b1ae54271f_5         hamfly\n","13  site_1_99af324c881246949408c0b1ae54271f_10         aldfly\n","14  site_1_99af324c881246949408c0b1ae54271f_15         aldfly\n","15  site_1_99af324c881246949408c0b1ae54271f_20         aldfly\n","16  site_1_99af324c881246949408c0b1ae54271f_25         aldfly\n","17  site_1_99af324c881246949408c0b1ae54271f_30         aldfly\n","18  site_1_99af324c881246949408c0b1ae54271f_35         aldfly\n","19   site_1_6ab74e177aa149468a39ca10beed6222_5         aldfly"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.406457,"end_time":"2020-08-14T11:06:57.714194","exception":false,"start_time":"2020-08-14T11:06:57.307737","status":"completed"},"tags":[],"id":"BNGASdj8EIAr"},"source":["## EOF"]},{"cell_type":"code","metadata":{"papermill":{"duration":0.454931,"end_time":"2020-08-14T11:06:58.586733","exception":false,"start_time":"2020-08-14T11:06:58.131802","status":"completed"},"tags":[],"id":"xXXdz-RPEIAr"},"source":[""],"execution_count":null,"outputs":[]}]}