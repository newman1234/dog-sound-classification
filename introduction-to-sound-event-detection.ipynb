{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"papermill":{"duration":2433.885786,"end_time":"2020-08-14T11:07:00.653314","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-08-14T10:26:26.767528","version":"2.1.0"},"colab":{"name":"introduction-to-sound-event-detection.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ck66G62eCBLM","executionInfo":{"status":"ok","timestamp":1623286706986,"user_tz":-480,"elapsed":69236,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e90b6e3d-5c68-4043-c04c-a01855e5eae9"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/Colab\\ Notebooks/esun_tbrain/dog_sound"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/MyDrive/Colab Notebooks/esun_tbrain/dog_sound\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3_Ga8Nshm-s","executionInfo":{"status":"ok","timestamp":1623286706987,"user_tz":-480,"elapsed":9,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"385a0f9e-4c94-41d1-c20a-b997365d0d5c"},"source":["%cd /gdrive/MyDrive/Colab\\ Notebooks/esun_tbrain/dog_sound"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/MyDrive/Colab Notebooks/esun_tbrain/dog_sound\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvlv1nFNCBu0","executionInfo":{"status":"ok","timestamp":1623286803960,"user_tz":-480,"elapsed":96978,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e27a0603-cf77-466c-957d-1bb892457888"},"source":["!mkdir -p /content/data/ && unzip data/train.zip -d /content/data/ && unzip data/public_test.zip -d /content/data/ && unzip data/private_test.zip -d /content/data/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","  inflating: /content/data/private_test/private_12035.wav  \n","  inflating: /content/data/private_test/private_05549.wav  \n","  inflating: /content/data/private_test/private_06040.wav  \n","  inflating: /content/data/private_test/private_09373.wav  \n","  inflating: /content/data/private_test/private_16253.wav  \n","  inflating: /content/data/private_test/private_19160.wav  \n","  inflating: /content/data/private_test/private_00431.wav  \n","  inflating: /content/data/private_test/private_10622.wav  \n","  inflating: /content/data/private_test/private_07438.wav  \n","  inflating: /content/data/private_test/private_10144.wav  \n","  inflating: /content/data/private_test/private_00357.wav  \n","  inflating: /content/data/private_test/private_18518.wav  \n","  inflating: /content/data/private_test/private_16535.wav  \n","  inflating: /content/data/private_test/private_19606.wav  \n","  inflating: /content/data/private_test/private_01049.wav  \n","  inflating: /content/data/private_test/private_06726.wav  \n","  inflating: /content/data/private_test/private_09415.wav  \n","  inflating: /content/data/private_test/private_03886.wav  \n","  inflating: /content/data/private_test/private_12753.wav  \n","  inflating: /content/data/private_test/private_02540.wav  \n","  inflating: /content/data/private_test/private_14322.wav  \n","  inflating: /content/data/private_test/private_04131.wav  \n","  inflating: /content/data/private_test/private_12747.wav  \n","  inflating: /content/data/private_test/private_03892.wav  \n","  inflating: /content/data/private_test/private_02554.wav  \n","  inflating: /content/data/private_test/private_15028.wav  \n","  inflating: /content/data/private_test/private_14336.wav  \n","  inflating: /content/data/private_test/private_13459.wav  \n","  inflating: /content/data/private_test/private_04125.wav  \n","  inflating: /content/data/private_test/private_10150.wav  \n","  inflating: /content/data/private_test/private_00343.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00343.wav  \n","  inflating: /content/data/private_test/private_16521.wav  \n","  inflating: /content/data/private_test/private_19612.wav  \n","  inflating: /content/data/private_test/private_06732.wav  \n","  inflating: /content/data/private_test/private_09401.wav  \n","  inflating: /content/data/private_test/private_06054.wav  \n","  inflating: /content/data/private_test/private_09367.wav  \n","  inflating: /content/data/private_test/private_11528.wav  \n","  inflating: /content/data/private_test/private_16247.wav  \n","  inflating: /content/data/private_test/private_19174.wav  \n","  inflating: /content/data/private_test/private_17159.wav  \n","  inflating: /content/data/private_test/private_00425.wav  \n","  inflating: /content/data/private_test/private_10636.wav  \n","  inflating: /content/data/private_test/private_08079.wav  \n","  inflating: /content/data/private_test/private_04643.wav  \n","  inflating: /content/data/private_test/private_15996.wav  \n","  inflating: /content/data/private_test/private_14450.wav  \n","  inflating: /content/data/private_test/private_02232.wav  \n","  inflating: /content/data/private_test/private_12021.wav  \n","  inflating: /content/data/private_test/private_11500.wav  \n","  inflating: /content/data/private_test/private_01713.wav  \n","  inflating: /content/data/private_test/private_18242.wav  \n","  inflating: /content/data/private_test/private_17171.wav  \n","  inflating: /content/data/private_test/private_08051.wav  \n","  inflating: /content/data/private_test/private_07362.wav  \n","  inflating: /content/data/private_test/private_13317.wav  \n","  inflating: /content/data/private_test/private_14478.wav  \n","  inflating: /content/data/private_test/private_03104.wav  \n","  inflating: /content/data/private_test/private_15766.wav  \n","  inflating: /content/data/private_test/private_05575.wav  \n","  inflating: /content/data/private_test/private_12009.wav  \n","  inflating: /content/data/private_test/private_05213.wav  \n","  inflating: /content/data/private_test/private_15000.wav  \n","  inflating: /content/data/private_test/private_03662.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03662.wav  \n","  inflating: /content/data/private_test/private_13471.wav  \n","  inflating: /content/data/private_test/private_08737.wav  \n","  inflating: /content/data/private_test/private_10178.wav  \n","  inflating: /content/data/private_test/private_07404.wav  \n","  inflating: /content/data/private_test/private_18524.wav  \n","  inflating: /content/data/private_test/private_17617.wav  \n","  inflating: /content/data/private_test/private_01075.wav  \n","  inflating: /content/data/private_test/private_16509.wav  \n","  inflating: /content/data/private_test/private_11266.wav  \n","  inflating: /content/data/private_test/private_09429.wav  \n","  inflating: /content/data/private_test/private_08723.wav  \n","  inflating: /content/data/private_test/private_07410.wav  \n","  inflating: /content/data/private_test/private_18530.wav  \n","  inflating: /content/data/private_test/private_17603.wav  \n","  inflating: /content/data/private_test/private_01061.wav  \n","  inflating: /content/data/private_test/private_11272.wav  \n","  inflating: /content/data/private_test/private_05207.wav  \n","  inflating: /content/data/private_test/private_15014.wav  \n","  inflating: /content/data/private_test/private_02568.wav  \n","  inflating: /content/data/private_test/private_03676.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03676.wav  \n","  inflating: /content/data/private_test/private_04119.wav  \n","  inflating: /content/data/private_test/private_13465.wav  \n","  inflating: /content/data/private_test/private_13303.wav  \n","  inflating: /content/data/private_test/private_03110.wav  \n","  inflating: /content/data/private_test/private_15772.wav  \n","  inflating: /content/data/private_test/private_05561.wav  \n","  inflating: /content/data/private_test/private_11514.wav  \n","  inflating: /content/data/private_test/private_06068.wav  \n","  inflating: /content/data/private_test/private_01707.wav  \n","  inflating: /content/data/private_test/private_19148.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19148.wav  \n","  inflating: /content/data/private_test/private_18256.wav  \n","  inflating: /content/data/private_test/private_00419.wav  \n","  inflating: /content/data/private_test/private_17165.wav  \n","  inflating: /content/data/private_test/private_08045.wav  \n","  inflating: /content/data/private_test/private_07376.wav  \n","  inflating: /content/data/private_test/private_01934.wav  \n","  inflating: /content/data/private_test/private_06083.wav  \n","  inflating: /content/data/private_test/private_16290.wav  \n","  inflating: /content/data/private_test/private_10839.wav  \n","  inflating: /content/data/private_test/private_15799.wav  \n","  inflating: /content/data/private_test/private_15941.wav  \n","  inflating: /content/data/private_test/private_04694.wav  \n","  inflating: /content/data/private_test/private_14487.wav  \n","  inflating: /content/data/private_test/private_12948.wav  \n","  inflating: /content/data/private_test/private_03845.wav  \n","  inflating: /content/data/private_test/private_12790.wav  \n","  inflating: /content/data/private_test/private_02583.wav  \n","  inflating: /content/data/private_test/private_08910.wav  \n","  inflating: /content/data/private_test/private_17830.wav  \n","  inflating: /content/data/private_test/private_11299.wav  \n","  inflating: /content/data/private_test/private_10187.wav  \n","  inflating: /content/data/private_test/private_00394.wav  \n","  inflating: /content/data/private_test/private_08904.wav  \n","  inflating: /content/data/private_test/private_17824.wav  \n","  inflating: /content/data/private_test/private_19809.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19809.wav  \n","  inflating: /content/data/private_test/private_10193.wav  \n","  inflating: /content/data/private_test/private_06929.wav  \n","  inflating: /content/data/private_test/private_00380.wav  \n","  inflating: /content/data/private_test/private_03689.wav  \n","  inflating: /content/data/private_test/private_12784.wav  \n","  inflating: /content/data/private_test/private_03851.wav  \n","  inflating: /content/data/private_test/private_02597.wav  \n","  inflating: /content/data/private_test/private_04858.wav  \n","  inflating: /content/data/private_test/private_04680.wav  \n","  inflating: /content/data/private_test/private_15955.wav  \n","  inflating: /content/data/private_test/private_14493.wav  \n","  inflating: /content/data/private_test/private_07389.wav  \n","  inflating: /content/data/private_test/private_01920.wav  \n","  inflating: /content/data/private_test/private_06097.wav  \n","  inflating: /content/data/private_test/private_16284.wav  \n","  inflating: /content/data/private_test/private_04870.wav  \n","  inflating: /content/data/private_test/private_18281.wav  \n","  inflating: /content/data/private_test/private_01908.wav  \n","  inflating: /content/data/private_test/private_08092.wav  \n","  inflating: /content/data/private_test/private_10805.wav  \n","  inflating: /content/data/private_test/private_19821.wav  \n","  inflating: /content/data/private_test/private_06901.wav  \n","  inflating: /content/data/private_test/private_12974.wav  \n","  inflating: /content/data/private_test/private_03879.wav  \n","  inflating: /content/data/private_test/private_12960.wav  \n","  inflating: /content/data/private_test/private_08938.wav  \n","  inflating: /content/data/private_test/private_17818.wav  \n","  inflating: /content/data/private_test/private_19835.wav  \n","  inflating: /content/data/private_test/private_06915.wav  \n","  inflating: /content/data/private_test/private_18295.wav  \n","  inflating: /content/data/private_test/private_08086.wav  \n","  inflating: /content/data/private_test/private_09398.wav  \n","  inflating: /content/data/private_test/private_10811.wav  \n","  inflating: /content/data/private_test/private_04864.wav  \n","  inflating: /content/data/private_test/private_15969.wav  \n","  inflating: /content/data/private_test/private_12141.wav  \n","  inflating: /content/data/private_test/private_02352.wav  \n","  inflating: /content/data/private_test/private_14530.wav  \n","  inflating: /content/data/private_test/private_04723.wav  \n","  inflating: /content/data/private_test/private_01883.wav  \n","  inflating: /content/data/private_test/private_10756.wav  \n","  inflating: /content/data/private_test/private_08119.wav  \n","  inflating: /content/data/private_test/private_17039.wav  \n","  inflating: /content/data/private_test/private_00545.wav  \n","  inflating: /content/data/private_test/private_16327.wav  \n","  inflating: /content/data/private_test/private_19014.wav  \n","  inflating: /content/data/private_test/private_06134.wav  \n","  inflating: /content/data/private_test/private_09207.wav  \n","  inflating: /content/data/private_test/private_11448.wav  \n","  inflating: /content/data/private_test/private_17987.wav  \n","  inflating: /content/data/private_test/private_06652.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06652.wav  \n","  inflating: /content/data/private_test/private_09561.wav  \n","  inflating: /content/data/private_test/private_16441.wav  \n","  inflating: /content/data/private_test/private_19772.wav  \n","  inflating: /content/data/private_test/private_00223.wav  \n","  inflating: /content/data/private_test/private_10030.wav  \n","  inflating: /content/data/private_test/private_13539.wav  \n","  inflating: /content/data/private_test/private_04045.wav  \n","  inflating: /content/data/private_test/private_14256.wav  \n","  inflating: /content/data/private_test/private_02434.wav  \n","  inflating: /content/data/private_test/private_15148.wav  \n","  inflating: /content/data/private_test/private_12627.wav  \n","  inflating: /content/data/private_test/private_04051.wav  \n","  inflating: /content/data/private_test/private_14242.wav  \n","  inflating: /content/data/private_test/private_02420.wav  \n","  inflating: /content/data/private_test/private_12633.wav  \n","  inflating: /content/data/private_test/private_06646.wav  \n","  inflating: /content/data/private_test/private_17993.wav  \n","  inflating: /content/data/private_test/private_09575.wav  \n","  inflating: /content/data/private_test/private_16455.wav  \n","  inflating: /content/data/private_test/private_19766.wav  \n","  inflating: /content/data/private_test/private_01129.wav  \n","  inflating: /content/data/private_test/private_00237.wav  \n","  inflating: /content/data/private_test/private_18478.wav  \n","  inflating: /content/data/private_test/private_07558.wav  \n","  inflating: /content/data/private_test/private_10024.wav  \n","  inflating: /content/data/private_test/private_10742.wav  \n","  inflating: /content/data/private_test/private_01897.wav  \n","  inflating: /content/data/private_test/private_00551.wav  \n","  inflating: /content/data/private_test/private_16333.wav  \n","  inflating: /content/data/private_test/private_19000.wav  \n","  inflating: /content/data/private_test/private_06120.wav  \n","  inflating: /content/data/private_test/private_09213.wav  \n","  inflating: /content/data/private_test/private_12155.wav  \n","  inflating: /content/data/private_test/private_05429.wav  \n","  inflating: /content/data/private_test/private_02346.wav  \n","  inflating: /content/data/private_test/private_03058.wav  \n","  inflating: /content/data/private_test/private_14524.wav  \n","  inflating: /content/data/private_test/private_04737.wav  \n","  inflating: /content/data/private_test/private_08125.wav  \n","  inflating: /content/data/private_test/private_07216.wav  \n","  inflating: /content/data/private_test/private_18336.wav  \n","  inflating: /content/data/private_test/private_00579.wav  \n","  inflating: /content/data/private_test/private_17005.wav  \n","  inflating: /content/data/private_test/private_01667.wav  \n","  inflating: /content/data/private_test/private_19028.wav  \n","  inflating: /content/data/private_test/private_11474.wav  \n","  inflating: /content/data/private_test/private_06108.wav  \n","  inflating: /content/data/private_test/private_05401.wav  \n","  inflating: /content/data/private_test/private_15612.wav  \n","  inflating: /content/data/private_test/private_03070.wav  \n","  inflating: /content/data/private_test/private_13263.wav  \n","  inflating: /content/data/private_test/private_04079.wav  \n","  inflating: /content/data/private_test/private_13505.wav  \n","  inflating: /content/data/private_test/private_03716.wav  \n","  inflating: /content/data/private_test/private_15174.wav  \n","  inflating: /content/data/private_test/private_02408.wav  \n","  inflating: /content/data/private_test/private_05367.wav  \n","  inflating: /content/data/private_test/private_11312.wav  \n","  inflating: /content/data/private_test/private_01101.wav  \n","  inflating: /content/data/private_test/private_18450.wav  \n","  inflating: /content/data/private_test/private_17763.wav  \n","  inflating: /content/data/private_test/private_08643.wav  \n","  inflating: /content/data/private_test/private_19996.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19996.wav  \n","  inflating: /content/data/private_test/private_07570.wav  \n","  inflating: /content/data/private_test/private_11306.wav  \n","  inflating: /content/data/private_test/private_09549.wav  \n","  inflating: /content/data/private_test/private_01115.wav  \n","  inflating: /content/data/private_test/private_16469.wav  \n","  inflating: /content/data/private_test/private_18444.wav  \n","  inflating: /content/data/private_test/private_17777.wav  \n","  inflating: /content/data/private_test/private_19982.wav  \n","  inflating: /content/data/private_test/private_08657.wav  \n","  inflating: /content/data/private_test/private_10018.wav  \n","  inflating: /content/data/private_test/private_07564.wav  \n","  inflating: /content/data/private_test/private_13511.wav  \n","  inflating: /content/data/private_test/private_03702.wav  \n","  inflating: /content/data/private_test/private_15160.wav  \n","  inflating: /content/data/private_test/private_05373.wav  \n","  inflating: /content/data/private_test/private_05415.wav  \n","  inflating: /content/data/private_test/private_12169.wav  \n","  inflating: /content/data/private_test/private_15606.wav  \n","  inflating: /content/data/private_test/private_14518.wav  \n","  inflating: /content/data/private_test/private_03064.wav  \n","  inflating: /content/data/private_test/private_13277.wav  \n","  inflating: /content/data/private_test/private_08131.wav  \n","  inflating: /content/data/private_test/private_07202.wav  \n","  inflating: /content/data/private_test/private_18322.wav  \n","  inflating: /content/data/private_test/private_17011.wav  \n","  inflating: /content/data/private_test/private_01673.wav  \n","  inflating: /content/data/private_test/private_11460.wav  \n","  inflating: /content/data/private_test/private_01698.wav  \n","  inflating: /content/data/private_test/private_01840.wav  \n","  inflating: /content/data/private_test/private_10795.wav  \n","  inflating: /content/data/private_test/private_00586.wav  \n","  inflating: /content/data/private_test/private_15835.wav  \n","  inflating: /content/data/private_test/private_12182.wav  \n","  inflating: /content/data/private_test/private_02391.wav  \n","  inflating: /content/data/private_test/private_04938.wav  \n","  inflating: /content/data/private_test/private_03931.wav  \n","  inflating: /content/data/private_test/private_05398.wav  \n","  inflating: /content/data/private_test/private_04086.wav  \n","  inflating: /content/data/private_test/private_14295.wav  \n","  inflating: /content/data/private_test/private_06849.wav  \n","  inflating: /content/data/private_test/private_19969.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19969.wav  \n","  inflating: /content/data/private_test/private_17944.wav  \n","  inflating: /content/data/private_test/private_06691.wav  \n","  inflating: /content/data/private_test/private_16482.wav  \n","  inflating: /content/data/private_test/private_08864.wav  \n","  inflating: /content/data/private_test/private_17788.wav  \n","  inflating: /content/data/private_test/private_06685.wav  \n","  inflating: /content/data/private_test/private_17950.wav  \n","  inflating: /content/data/private_test/private_16496.wav  \n","  inflating: /content/data/private_test/private_08870.wav  \n","  inflating: /content/data/private_test/private_03925.wav  \n","  inflating: /content/data/private_test/private_04092.wav  \n","  inflating: /content/data/private_test/private_12828.wav  \n","  inflating: /content/data/private_test/private_14281.wav  \n","  inflating: /content/data/private_test/private_13288.wav  \n","  inflating: /content/data/private_test/private_15821.wav  \n","  inflating: /content/data/private_test/private_12196.wav  \n","  inflating: /content/data/private_test/private_02385.wav  \n","  inflating: /content/data/private_test/private_10959.wav  \n","  inflating: /content/data/private_test/private_10781.wav  \n","  inflating: /content/data/private_test/private_01854.wav  \n","  inflating: /content/data/private_test/private_00592.wav  \n","  inflating: /content/data/private_test/private_15809.wav  \n","  inflating: /content/data/private_test/private_04904.wav  \n","  inflating: /content/data/private_test/private_10971.wav  \n","  inflating: /content/data/private_test/private_18493.wav  \n","  inflating: /content/data/private_test/private_06875.wav  \n","  inflating: /content/data/private_test/private_08680.wav  \n","  inflating: /content/data/private_test/private_19955.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19955.wav  \n","  inflating: /content/data/private_test/private_17978.wav  \n","  inflating: /content/data/private_test/private_08858.wav  \n","  inflating: /content/data/private_test/private_12800.wav  \n","  inflating: /content/data/private_test/private_03919.wav  \n","  inflating: /content/data/private_test/private_12814.wav  \n","  inflating: /content/data/private_test/private_18487.wav  \n","  inflating: /content/data/private_test/private_06861.wav  \n","  inflating: /content/data/private_test/private_19941.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19941.wav  \n","  inflating: /content/data/private_test/private_08694.wav  \n","  inflating: /content/data/private_test/private_19799.wav  \n","  inflating: /content/data/private_test/private_10965.wav  \n","  inflating: /content/data/private_test/private_01868.wav  \n","  inflating: /content/data/private_test/private_04910.wav  \n","  inflating: /content/data/private_test/private_05159.wav  \n","  inflating: /content/data/private_test/private_12425.wav  \n","  inflating: /content/data/private_test/private_02636.wav  \n","  inflating: /content/data/private_test/private_14054.wav  \n","  inflating: /content/data/private_test/private_03528.wav  \n","  inflating: /content/data/private_test/private_04247.wav  \n","  inflating: /content/data/private_test/private_10232.wav  \n","  inflating: /content/data/private_test/private_00021.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00021.wav  \n","  inflating: /content/data/private_test/private_19570.wav  \n","  inflating: /content/data/private_test/private_07996.wav  \n","  inflating: /content/data/private_test/private_16643.wav  \n","  inflating: /content/data/private_test/private_09763.wav  \n","  inflating: /content/data/private_test/private_06450.wav  \n","  inflating: /content/data/private_test/private_09005.wav  \n","  inflating: /content/data/private_test/private_06336.wav  \n","  inflating: /content/data/private_test/private_19216.wav  \n","  inflating: /content/data/private_test/private_01459.wav  \n","  inflating: /content/data/private_test/private_16125.wav  \n","  inflating: /content/data/private_test/private_11892.wav  \n","  inflating: /content/data/private_test/private_00747.wav  \n","  inflating: /content/data/private_test/private_18108.wav  \n","  inflating: /content/data/private_test/private_10554.wav  \n","  inflating: /content/data/private_test/private_07028.wav  \n","  inflating: /content/data/private_test/private_04521.wav  \n","  inflating: /content/data/private_test/private_14732.wav  \n","  inflating: /content/data/private_test/private_02150.wav  \n","  inflating: /content/data/private_test/private_12343.wav  \n","  inflating: /content/data/private_test/private_04535.wav  \n","  inflating: /content/data/private_test/private_13049.wav  \n","  inflating: /content/data/private_test/private_14726.wav  \n","  inflating: /content/data/private_test/private_15438.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15438.wav  \n","  inflating: /content/data/private_test/private_02144.wav  \n","  inflating: /content/data/private_test/private_12357.wav  \n","  inflating: /content/data/private_test/private_09011.wav  \n","  inflating: /content/data/private_test/private_06322.wav  \n","  inflating: /content/data/private_test/private_19202.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19202.wav  \n","  inflating: /content/data/private_test/private_16131.wav  \n","  inflating: /content/data/private_test/private_00753.wav  \n","  inflating: /content/data/private_test/private_11886.wav  \n","  inflating: /content/data/private_test/private_10540.wav  \n","  inflating: /content/data/private_test/private_10226.wav  \n","  inflating: /content/data/private_test/private_08469.wav  \n","  inflating: /content/data/private_test/private_00035.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00035.wav  \n","  inflating: /content/data/private_test/private_17549.wav  \n","  inflating: /content/data/private_test/private_19564.wav  \n","  inflating: /content/data/private_test/private_16657.wav  \n","  inflating: /content/data/private_test/private_07982.wav  \n","  inflating: /content/data/private_test/private_09777.wav  \n","  inflating: /content/data/private_test/private_11138.wav  \n","  inflating: /content/data/private_test/private_06444.wav  \n","  inflating: /content/data/private_test/private_12431.wav  \n","  inflating: /content/data/private_test/private_02622.wav  \n","  inflating: /content/data/private_test/private_14040.wav  \n","  inflating: /content/data/private_test/private_04253.wav  \n","  inflating: /content/data/private_test/private_07772.wav  \n","  inflating: /content/data/private_test/private_08441.wav  \n","  inflating: /content/data/private_test/private_17561.wav  \n","  inflating: /content/data/private_test/private_18652.wav  \n","  inflating: /content/data/private_test/private_09987.wav  \n","  inflating: /content/data/private_test/private_01303.wav  \n","  inflating: /content/data/private_test/private_11110.wav  \n","  inflating: /content/data/private_test/private_12419.wav  \n","  inflating: /content/data/private_test/private_05165.wav  \n","  inflating: /content/data/private_test/private_15376.wav  \n","  inflating: /content/data/private_test/private_03514.wav  \n","  inflating: /content/data/private_test/private_14068.wav  \n","  inflating: /content/data/private_test/private_13707.wav  \n","  inflating: /content/data/private_test/private_13061.wav  \n","  inflating: /content/data/private_test/private_03272.wav  \n","  inflating: /content/data/private_test/private_15410.wav  \n","  inflating: /content/data/private_test/private_05603.wav  \n","  inflating: /content/data/private_test/private_11676.wav  \n","  inflating: /content/data/private_test/private_09039.wav  \n","  inflating: /content/data/private_test/private_16119.wav  \n","  inflating: /content/data/private_test/private_01465.wav  \n","  inflating: /content/data/private_test/private_17207.wav  \n","  inflating: /content/data/private_test/private_18134.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18134.wav  \n","  inflating: /content/data/private_test/private_07014.wav  \n","  inflating: /content/data/private_test/private_08327.wav  \n","  inflating: /content/data/private_test/private_10568.wav  \n","  inflating: /content/data/private_test/private_11662.wav  \n","  inflating: /content/data/private_test/private_01471.wav  \n","  inflating: /content/data/private_test/private_17213.wav  \n","  inflating: /content/data/private_test/private_18120.wav  \n","  inflating: /content/data/private_test/private_07000.wav  \n","  inflating: /content/data/private_test/private_08333.wav  \n","  inflating: /content/data/private_test/private_13075.wav  \n","  inflating: /content/data/private_test/private_04509.wav  \n","  inflating: /content/data/private_test/private_03266.wav  \n","  inflating: /content/data/private_test/private_02178.wav  \n","  inflating: /content/data/private_test/private_15404.wav  \n","  inflating: /content/data/private_test/private_05617.wav  \n","  inflating: /content/data/private_test/private_05171.wav  \n","  inflating: /content/data/private_test/private_15362.wav  \n","  inflating: /content/data/private_test/private_03500.wav  \n","  inflating: /content/data/private_test/private_13713.wav  \n","  inflating: /content/data/private_test/private_07766.wav  \n","  inflating: /content/data/private_test/private_08455.wav  \n","  inflating: /content/data/private_test/private_17575.wav  \n","  inflating: /content/data/private_test/private_09993.wav  \n","  inflating: /content/data/private_test/private_18646.wav  \n","  inflating: /content/data/private_test/private_00009.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00009.wav  \n","  inflating: /content/data/private_test/private_01317.wav  \n","  inflating: /content/data/private_test/private_19558.wav  \n","  inflating: /content/data/private_test/private_06478.wav  \n","  inflating: /content/data/private_test/private_11104.wav  \n","  inflating: /content/data/private_test/private_07955.wav  \n","  inflating: /content/data/private_test/private_16680.wav  \n","  inflating: /content/data/private_test/private_18875.wav  \n","  inflating: /content/data/private_test/private_06493.wav  \n","  inflating: /content/data/private_test/private_16858.wav  \n","  inflating: /content/data/private_test/private_09978.wav  \n","  inflating: /content/data/private_test/private_14097.wav  \n","  inflating: /content/data/private_test/private_04284.wav  \n","  inflating: /content/data/private_test/private_15389.wav  \n","  inflating: /content/data/private_test/private_13920.wav  \n","  inflating: /content/data/private_test/private_02193.wav  \n","  inflating: /content/data/private_test/private_14929.wav  \n","  inflating: /content/data/private_test/private_12380.wav  \n","  inflating: /content/data/private_test/private_05824.wav  \n","  inflating: /content/data/private_test/private_11851.wav  \n","  inflating: /content/data/private_test/private_00784.wav  \n","  inflating: /content/data/private_test/private_10597.wav  \n","  inflating: /content/data/private_test/private_11689.wav  \n","  inflating: /content/data/private_test/private_00790.wav  \n","  inflating: /content/data/private_test/private_11845.wav  \n","  inflating: /content/data/private_test/private_10583.wav  \n","  inflating: /content/data/private_test/private_00948.wav  \n","  inflating: /content/data/private_test/private_02187.wav  \n","  inflating: /content/data/private_test/private_12394.wav  \n","  inflating: /content/data/private_test/private_05830.wav  \n","  inflating: /content/data/private_test/private_03299.wav  \n","  inflating: /content/data/private_test/private_14083.wav  \n","  inflating: /content/data/private_test/private_04290.wav  \n","  inflating: /content/data/private_test/private_02839.wav  \n","  inflating: /content/data/private_test/private_13934.wav  \n","  inflating: /content/data/private_test/private_16694.wav  \n","  inflating: /content/data/private_test/private_07941.wav  \n","  inflating: /content/data/private_test/private_18861.wav  \n","  inflating: /content/data/private_test/private_06487.wav  \n","  inflating: /content/data/private_test/private_07799.wav  \n","  inflating: /content/data/private_test/private_02811.wav  \n","  inflating: /content/data/private_test/private_07969.wav  \n","  inflating: /content/data/private_test/private_18849.wav  \n","  inflating: /content/data/private_test/private_16864.wav  \n","  inflating: /content/data/private_test/private_08482.wav  \n","  inflating: /content/data/private_test/private_18691.wav  \n","  inflating: /content/data/private_test/private_09944.wav  \n","  inflating: /content/data/private_test/private_00960.wav  \n","  inflating: /content/data/private_test/private_14915.wav  \n","  inflating: /content/data/private_test/private_05818.wav  \n","  inflating: /content/data/private_test/private_14901.wav  \n","  inflating: /content/data/private_test/private_11879.wav  \n","  inflating: /content/data/private_test/private_00974.wav  \n","  inflating: /content/data/private_test/private_09788.wav  \n","  inflating: /content/data/private_test/private_16870.wav  \n","  inflating: /content/data/private_test/private_08496.wav  \n","  inflating: /content/data/private_test/private_09950.wav  \n","  inflating: /content/data/private_test/private_18685.wav  \n","  inflating: /content/data/private_test/private_02805.wav  \n","  inflating: /content/data/private_test/private_13908.wav  \n","  inflating: /content/data/private_test/private_00975.wav  \n","  inflating: /content/data/private_test/private_11878.wav  \n","  inflating: /content/data/private_test/private_14900.wav  \n","  inflating: /content/data/private_test/private_13909.wav  \n","  inflating: /content/data/private_test/private_02804.wav  \n","  inflating: /content/data/private_test/private_18684.wav  \n","  inflating: /content/data/private_test/private_09951.wav  \n","  inflating: /content/data/private_test/private_08497.wav  \n","  inflating: /content/data/private_test/private_16871.wav  \n","  inflating: /content/data/private_test/private_09789.wav  \n","  inflating: /content/data/private_test/private_09945.wav  \n","  inflating: /content/data/private_test/private_18690.wav  \n","  inflating: /content/data/private_test/private_08483.wav  \n","  inflating: /content/data/private_test/private_16865.wav  \n","  inflating: /content/data/private_test/private_18848.wav  \n","  inflating: /content/data/private_test/private_07968.wav  \n","  inflating: /content/data/private_test/private_02810.wav  \n","  inflating: /content/data/private_test/private_05819.wav  \n","  inflating: /content/data/private_test/private_14914.wav  \n","  inflating: /content/data/private_test/private_00961.wav  \n","  inflating: /content/data/private_test/private_03298.wav  \n","  inflating: /content/data/private_test/private_05831.wav  \n","  inflating: /content/data/private_test/private_12395.wav  \n","  inflating: /content/data/private_test/private_02186.wav  \n","  inflating: /content/data/private_test/private_00949.wav  \n","  inflating: /content/data/private_test/private_10582.wav  \n","  inflating: /content/data/private_test/private_11844.wav  \n","  inflating: /content/data/private_test/private_00791.wav  \n","  inflating: /content/data/private_test/private_07798.wav  \n","  inflating: /content/data/private_test/private_06486.wav  \n","  inflating: /content/data/private_test/private_18860.wav  \n","  inflating: /content/data/private_test/private_07940.wav  \n","  inflating: /content/data/private_test/private_16695.wav  \n","  inflating: /content/data/private_test/private_13935.wav  \n","  inflating: /content/data/private_test/private_02838.wav  \n","  inflating: /content/data/private_test/private_04291.wav  \n","  inflating: /content/data/private_test/private_14082.wav  \n","  inflating: /content/data/private_test/private_13921.wav  \n","  inflating: /content/data/private_test/private_15388.wav  \n","  inflating: /content/data/private_test/private_04285.wav  \n","  inflating: /content/data/private_test/private_14096.wav  \n","  inflating: /content/data/private_test/private_09979.wav  \n","  inflating: /content/data/private_test/private_16859.wav  \n","  inflating: /content/data/private_test/private_06492.wav  \n","  inflating: /content/data/private_test/private_18874.wav  \n","  inflating: /content/data/private_test/private_16681.wav  \n","  inflating: /content/data/private_test/private_07954.wav  \n","  inflating: /content/data/private_test/private_11688.wav  \n","  inflating: /content/data/private_test/private_10596.wav  \n","  inflating: /content/data/private_test/private_00785.wav  \n","  inflating: /content/data/private_test/private_11850.wav  \n","  inflating: /content/data/private_test/private_05825.wav  \n","  inflating: /content/data/private_test/private_12381.wav  \n","  inflating: /content/data/private_test/private_14928.wav  \n","  inflating: /content/data/private_test/private_02192.wav  \n","  inflating: /content/data/private_test/private_05616.wav  \n","  inflating: /content/data/private_test/private_15405.wav  \n","  inflating: /content/data/private_test/private_02179.wav  \n","  inflating: /content/data/private_test/private_03267.wav  \n","  inflating: /content/data/private_test/private_04508.wav  \n","  inflating: /content/data/private_test/private_13074.wav  \n","  inflating: /content/data/private_test/private_08332.wav  \n","  inflating: /content/data/private_test/private_07001.wav  \n","  inflating: /content/data/private_test/private_18121.wav  \n","  inflating: /content/data/private_test/private_17212.wav  \n","  inflating: /content/data/private_test/private_01470.wav  \n","  inflating: /content/data/private_test/private_11663.wav  \n","  inflating: /content/data/private_test/private_11105.wav  \n","  inflating: /content/data/private_test/private_06479.wav  \n","  inflating: /content/data/private_test/private_01316.wav  \n","  inflating: /content/data/private_test/private_19559.wav  \n","  inflating: /content/data/private_test/private_18647.wav  \n","  inflating: /content/data/private_test/private_09992.wav  \n","  inflating: /content/data/private_test/private_00008.wav  \n","  inflating: /content/data/private_test/private_17574.wav  \n","  inflating: /content/data/private_test/private_08454.wav  \n","  inflating: /content/data/private_test/private_07767.wav  \n","  inflating: /content/data/private_test/private_13712.wav  \n","  inflating: /content/data/private_test/private_03501.wav  \n","  inflating: /content/data/private_test/private_15363.wav  \n","  inflating: /content/data/private_test/private_05170.wav  \n","  inflating: /content/data/private_test/private_13706.wav  \n","  inflating: /content/data/private_test/private_14069.wav  \n","  inflating: /content/data/private_test/private_03515.wav  \n","  inflating: /content/data/private_test/private_15377.wav  \n","  inflating: /content/data/private_test/private_05164.wav  \n","  inflating: /content/data/private_test/private_12418.wav  \n","  inflating: /content/data/private_test/private_11111.wav  \n","  inflating: /content/data/private_test/private_01302.wav  \n","  inflating: /content/data/private_test/private_09986.wav  \n","  inflating: /content/data/private_test/private_18653.wav  \n","  inflating: /content/data/private_test/private_17560.wav  \n","  inflating: /content/data/private_test/private_08440.wav  \n","  inflating: /content/data/private_test/private_07773.wav  \n","  inflating: /content/data/private_test/private_08326.wav  \n","  inflating: /content/data/private_test/private_10569.wav  \n","  inflating: /content/data/private_test/private_07015.wav  \n","  inflating: /content/data/private_test/private_18135.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18135.wav  \n","  inflating: /content/data/private_test/private_17206.wav  \n","  inflating: /content/data/private_test/private_01464.wav  \n","  inflating: /content/data/private_test/private_16118.wav  \n","  inflating: /content/data/private_test/private_11677.wav  \n","  inflating: /content/data/private_test/private_09038.wav  \n","  inflating: /content/data/private_test/private_05602.wav  \n","  inflating: /content/data/private_test/private_15411.wav  \n","  inflating: /content/data/private_test/private_03273.wav  \n","  inflating: /content/data/private_test/private_13060.wav  \n","  inflating: /content/data/private_test/private_10541.wav  \n","  inflating: /content/data/private_test/private_11887.wav  \n","  inflating: /content/data/private_test/private_00752.wav  \n","  inflating: /content/data/private_test/private_16130.wav  \n","  inflating: /content/data/private_test/private_19203.wav  \n","  inflating: /content/data/private_test/private_06323.wav  \n","  inflating: /content/data/private_test/private_09010.wav  \n","  inflating: /content/data/private_test/private_12356.wav  \n","  inflating: /content/data/private_test/private_02145.wav  \n","  inflating: /content/data/private_test/private_15439.wav  \n","  inflating: /content/data/private_test/private_14727.wav  \n","  inflating: /content/data/private_test/private_13048.wav  \n","  inflating: /content/data/private_test/private_04534.wav  \n","  inflating: /content/data/private_test/private_04252.wav  \n","  inflating: /content/data/private_test/private_14041.wav  \n","  inflating: /content/data/private_test/private_02623.wav  \n","  inflating: /content/data/private_test/private_12430.wav  \n","  inflating: /content/data/private_test/private_06445.wav  \n","  inflating: /content/data/private_test/private_09776.wav  \n","  inflating: /content/data/private_test/private_11139.wav  \n","  inflating: /content/data/private_test/private_07983.wav  \n","  inflating: /content/data/private_test/private_16656.wav  \n","  inflating: /content/data/private_test/private_19565.wav  \n","  inflating: /content/data/private_test/private_17548.wav  \n","  inflating: /content/data/private_test/private_00034.wav  \n","  inflating: /content/data/private_test/private_10227.wav  \n","  inflating: /content/data/private_test/private_08468.wav  \n","  inflating: /content/data/private_test/private_06451.wav  \n","  inflating: /content/data/private_test/private_09762.wav  \n","  inflating: /content/data/private_test/private_16642.wav  \n","  inflating: /content/data/private_test/private_07997.wav  \n","  inflating: /content/data/private_test/private_19571.wav  \n","  inflating: /content/data/private_test/private_00020.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00020.wav  \n","  inflating: /content/data/private_test/private_10233.wav  \n","  inflating: /content/data/private_test/private_04246.wav  \n","  inflating: /content/data/private_test/private_03529.wav  \n","  inflating: /content/data/private_test/private_14055.wav  \n","  inflating: /content/data/private_test/private_02637.wav  \n","  inflating: /content/data/private_test/private_12424.wav  \n","  inflating: /content/data/private_test/private_05158.wav  \n","  inflating: /content/data/private_test/private_12342.wav  \n","  inflating: /content/data/private_test/private_02151.wav  \n","  inflating: /content/data/private_test/private_14733.wav  \n","  inflating: /content/data/private_test/private_04520.wav  \n","  inflating: /content/data/private_test/private_07029.wav  \n","  inflating: /content/data/private_test/private_10555.wav  \n","  inflating: /content/data/private_test/private_00746.wav  \n","  inflating: /content/data/private_test/private_11893.wav  \n","  inflating: /content/data/private_test/private_18109.wav  \n","  inflating: /content/data/private_test/private_16124.wav  \n","  inflating: /content/data/private_test/private_19217.wav  \n","  inflating: /content/data/private_test/private_01458.wav  \n","  inflating: /content/data/private_test/private_06337.wav  \n","  inflating: /content/data/private_test/private_09004.wav  \n","  inflating: /content/data/private_test/private_19798.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19798.wav  \n","  inflating: /content/data/private_test/private_08695.wav  \n","  inflating: /content/data/private_test/private_19940.wav  \n","  inflating: /content/data/private_test/private_06860.wav  \n","  inflating: /content/data/private_test/private_18486.wav  \n","  inflating: /content/data/private_test/private_12815.wav  \n","  inflating: /content/data/private_test/private_03918.wav  \n","  inflating: /content/data/private_test/private_04911.wav  \n","  inflating: /content/data/private_test/private_01869.wav  \n","  inflating: /content/data/private_test/private_10964.wav  \n","  inflating: /content/data/private_test/private_10970.wav  \n","  inflating: /content/data/private_test/private_04905.wav  \n","  inflating: /content/data/private_test/private_15808.wav  \n","  inflating: /content/data/private_test/private_12801.wav  \n","  inflating: /content/data/private_test/private_08859.wav  \n","  inflating: /content/data/private_test/private_17979.wav  \n","  inflating: /content/data/private_test/private_19954.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19954.wav  \n","  inflating: /content/data/private_test/private_08681.wav  \n","  inflating: /content/data/private_test/private_06874.wav  \n","  inflating: /content/data/private_test/private_18492.wav  \n","  inflating: /content/data/private_test/private_14280.wav  \n","  inflating: /content/data/private_test/private_12829.wav  \n","  inflating: /content/data/private_test/private_04093.wav  \n","  inflating: /content/data/private_test/private_03924.wav  \n","  inflating: /content/data/private_test/private_08871.wav  \n","  inflating: /content/data/private_test/private_16497.wav  \n","  inflating: /content/data/private_test/private_17951.wav  \n","  inflating: /content/data/private_test/private_06684.wav  \n","  inflating: /content/data/private_test/private_17789.wav  \n","  inflating: /content/data/private_test/private_00593.wav  \n","  inflating: /content/data/private_test/private_01855.wav  \n","  inflating: /content/data/private_test/private_10780.wav  \n","  inflating: /content/data/private_test/private_10958.wav  \n","  inflating: /content/data/private_test/private_02384.wav  \n","  inflating: /content/data/private_test/private_12197.wav  \n","  inflating: /content/data/private_test/private_15820.wav  \n","  inflating: /content/data/private_test/private_13289.wav  \n","  inflating: /content/data/private_test/private_04939.wav  \n","  inflating: /content/data/private_test/private_02390.wav  \n","  inflating: /content/data/private_test/private_12183.wav  \n","  inflating: /content/data/private_test/private_15834.wav  \n","  inflating: /content/data/private_test/private_00587.wav  \n","  inflating: /content/data/private_test/private_10794.wav  \n","  inflating: /content/data/private_test/private_01841.wav  \n","  inflating: /content/data/private_test/private_01699.wav  \n","  inflating: /content/data/private_test/private_08865.wav  \n","  inflating: /content/data/private_test/private_16483.wav  \n","  inflating: /content/data/private_test/private_06690.wav  \n","  inflating: /content/data/private_test/private_17945.wav  \n","  inflating: /content/data/private_test/private_19968.wav  \n","  inflating: /content/data/private_test/private_06848.wav  \n","  inflating: /content/data/private_test/private_14294.wav  \n","  inflating: /content/data/private_test/private_04087.wav  \n","  inflating: /content/data/private_test/private_05399.wav  \n","  inflating: /content/data/private_test/private_03930.wav  \n","  inflating: /content/data/private_test/private_05372.wav  \n","  inflating: /content/data/private_test/private_15161.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15161.wav  \n","  inflating: /content/data/private_test/private_03703.wav  \n","  inflating: /content/data/private_test/private_13510.wav  \n","  inflating: /content/data/private_test/private_07565.wav  \n","  inflating: /content/data/private_test/private_08656.wav  \n","  inflating: /content/data/private_test/private_19983.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19983.wav  \n","  inflating: /content/data/private_test/private_10019.wav  \n","  inflating: /content/data/private_test/private_17776.wav  \n","  inflating: /content/data/private_test/private_18445.wav  \n","  inflating: /content/data/private_test/private_16468.wav  \n","  inflating: /content/data/private_test/private_01114.wav  \n","  inflating: /content/data/private_test/private_11307.wav  \n","  inflating: /content/data/private_test/private_09548.wav  \n","  inflating: /content/data/private_test/private_11461.wav  \n","  inflating: /content/data/private_test/private_01672.wav  \n","  inflating: /content/data/private_test/private_17010.wav  \n","  inflating: /content/data/private_test/private_18323.wav  \n","  inflating: /content/data/private_test/private_07203.wav  \n","  inflating: /content/data/private_test/private_08130.wav  \n","  inflating: /content/data/private_test/private_13276.wav  \n","  inflating: /content/data/private_test/private_03065.wav  \n","  inflating: /content/data/private_test/private_14519.wav  \n","  inflating: /content/data/private_test/private_15607.wav  \n","  inflating: /content/data/private_test/private_12168.wav  \n","  inflating: /content/data/private_test/private_05414.wav  \n","  inflating: /content/data/private_test/private_13262.wav  \n","  inflating: /content/data/private_test/private_03071.wav  \n","  inflating: /content/data/private_test/private_15613.wav  \n","  inflating: /content/data/private_test/private_05400.wav  \n","  inflating: /content/data/private_test/private_06109.wav  \n","  inflating: /content/data/private_test/private_11475.wav  \n","  inflating: /content/data/private_test/private_01666.wav  \n","  inflating: /content/data/private_test/private_19029.wav  \n","  inflating: /content/data/private_test/private_17004.wav  \n","  inflating: /content/data/private_test/private_18337.wav  \n","  inflating: /content/data/private_test/private_00578.wav  \n","  inflating: /content/data/private_test/private_07217.wav  \n","  inflating: /content/data/private_test/private_08124.wav  \n","  inflating: /content/data/private_test/private_07571.wav  \n","  inflating: /content/data/private_test/private_19997.wav  \n","  inflating: /content/data/private_test/private_08642.wav  \n","  inflating: /content/data/private_test/private_17762.wav  \n","  inflating: /content/data/private_test/private_18451.wav  \n","  inflating: /content/data/private_test/private_01100.wav  \n","  inflating: /content/data/private_test/private_11313.wav  \n","  inflating: /content/data/private_test/private_05366.wav  \n","  inflating: /content/data/private_test/private_02409.wav  \n","  inflating: /content/data/private_test/private_15175.wav  \n","  inflating: /content/data/private_test/private_03717.wav  \n","  inflating: /content/data/private_test/private_13504.wav  \n","  inflating: /content/data/private_test/private_04078.wav  \n","  inflating: /content/data/private_test/private_10025.wav  \n","  inflating: /content/data/private_test/private_07559.wav  \n","  inflating: /content/data/private_test/private_00236.wav  \n","  inflating: /content/data/private_test/private_18479.wav  \n","  inflating: /content/data/private_test/private_19767.wav  \n","  inflating: /content/data/private_test/private_01128.wav  \n","  inflating: /content/data/private_test/private_16454.wav  \n","  inflating: /content/data/private_test/private_09574.wav  \n","  inflating: /content/data/private_test/private_17992.wav  \n","  inflating: /content/data/private_test/private_06647.wav  \n","  inflating: /content/data/private_test/private_12632.wav  \n","  inflating: /content/data/private_test/private_02421.wav  \n","  inflating: /content/data/private_test/private_14243.wav  \n","  inflating: /content/data/private_test/private_04050.wav  \n","  inflating: /content/data/private_test/private_04736.wav  \n","  inflating: /content/data/private_test/private_14525.wav  \n","  inflating: /content/data/private_test/private_03059.wav  \n","  inflating: /content/data/private_test/private_02347.wav  \n","  inflating: /content/data/private_test/private_05428.wav  \n","  inflating: /content/data/private_test/private_12154.wav  \n","  inflating: /content/data/private_test/private_09212.wav  \n","  inflating: /content/data/private_test/private_06121.wav  \n","  inflating: /content/data/private_test/private_19001.wav  \n","  inflating: /content/data/private_test/private_16332.wav  \n","  inflating: /content/data/private_test/private_00550.wav  \n","  inflating: /content/data/private_test/private_01896.wav  \n","  inflating: /content/data/private_test/private_10743.wav  \n","  inflating: /content/data/private_test/private_09206.wav  \n","  inflating: /content/data/private_test/private_11449.wav  \n","  inflating: /content/data/private_test/private_06135.wav  \n","  inflating: /content/data/private_test/private_19015.wav  \n","  inflating: /content/data/private_test/private_16326.wav  \n","  inflating: /content/data/private_test/private_00544.wav  \n","  inflating: /content/data/private_test/private_17038.wav  \n","  inflating: /content/data/private_test/private_10757.wav  \n","  inflating: /content/data/private_test/private_01882.wav  \n","  inflating: /content/data/private_test/private_08118.wav  \n","  inflating: /content/data/private_test/private_04722.wav  \n","  inflating: /content/data/private_test/private_14531.wav  \n","  inflating: /content/data/private_test/private_02353.wav  \n","  inflating: /content/data/private_test/private_12140.wav  \n","  inflating: /content/data/private_test/private_12626.wav  \n","  inflating: /content/data/private_test/private_15149.wav  \n","  inflating: /content/data/private_test/private_02435.wav  \n","  inflating: /content/data/private_test/private_14257.wav  \n","  inflating: /content/data/private_test/private_04044.wav  \n","  inflating: /content/data/private_test/private_13538.wav  \n","  inflating: /content/data/private_test/private_10031.wav  \n","  inflating: /content/data/private_test/private_00222.wav  \n","  inflating: /content/data/private_test/private_19773.wav  \n","  inflating: /content/data/private_test/private_16440.wav  \n","  inflating: /content/data/private_test/private_09560.wav  \n","  inflating: /content/data/private_test/private_06653.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06653.wav  \n","  inflating: /content/data/private_test/private_17986.wav  \n","  inflating: /content/data/private_test/private_06914.wav  \n","  inflating: /content/data/private_test/private_19834.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19834.wav  \n","  inflating: /content/data/private_test/private_17819.wav  \n","  inflating: /content/data/private_test/private_08939.wav  \n","  inflating: /content/data/private_test/private_12961.wav  \n","  inflating: /content/data/private_test/private_15968.wav  \n","  inflating: /content/data/private_test/private_04865.wav  \n","  inflating: /content/data/private_test/private_10810.wav  \n","  inflating: /content/data/private_test/private_09399.wav  \n","  inflating: /content/data/private_test/private_08087.wav  \n","  inflating: /content/data/private_test/private_18294.wav  \n","  inflating: /content/data/private_test/private_10804.wav  \n","  inflating: /content/data/private_test/private_01909.wav  \n","  inflating: /content/data/private_test/private_08093.wav  \n","  inflating: /content/data/private_test/private_18280.wav  \n","  inflating: /content/data/private_test/private_04871.wav  \n","  inflating: /content/data/private_test/private_03878.wav  \n","  inflating: /content/data/private_test/private_12975.wav  \n","  inflating: /content/data/private_test/private_06900.wav  \n","  inflating: /content/data/private_test/private_19820.wav  \n","  inflating: /content/data/private_test/private_02596.wav  \n","  inflating: /content/data/private_test/private_03850.wav  \n","  inflating: /content/data/private_test/private_12785.wav  \n","  inflating: /content/data/private_test/private_03688.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03688.wav  \n","  inflating: /content/data/private_test/private_00381.wav  \n","  inflating: /content/data/private_test/private_06928.wav  \n","  inflating: /content/data/private_test/private_19808.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19808.wav  \n","  inflating: /content/data/private_test/private_10192.wav  \n","  inflating: /content/data/private_test/private_17825.wav  \n","  inflating: /content/data/private_test/private_08905.wav  \n","  inflating: /content/data/private_test/private_16285.wav  \n","  inflating: /content/data/private_test/private_06096.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06096.wav  \n","  inflating: /content/data/private_test/private_01921.wav  \n","  inflating: /content/data/private_test/private_07388.wav  \n","  inflating: /content/data/private_test/private_14492.wav  \n","  inflating: /content/data/private_test/private_15954.wav  \n","  inflating: /content/data/private_test/private_04681.wav  \n","  inflating: /content/data/private_test/private_04859.wav  \n","  inflating: /content/data/private_test/private_14486.wav  \n","  inflating: /content/data/private_test/private_04695.wav  \n","  inflating: /content/data/private_test/private_15940.wav  \n","  inflating: /content/data/private_test/private_15798.wav  \n","  inflating: /content/data/private_test/private_10838.wav  \n","  inflating: /content/data/private_test/private_16291.wav  \n","  inflating: /content/data/private_test/private_06082.wav  \n","  inflating: /content/data/private_test/private_01935.wav  \n","  inflating: /content/data/private_test/private_00395.wav  \n","  inflating: /content/data/private_test/private_10186.wav  \n","  inflating: /content/data/private_test/private_11298.wav  \n","  inflating: /content/data/private_test/private_17831.wav  \n","  inflating: /content/data/private_test/private_08911.wav  \n","  inflating: /content/data/private_test/private_02582.wav  \n","  inflating: /content/data/private_test/private_12791.wav  \n","  inflating: /content/data/private_test/private_03844.wav  \n","  inflating: /content/data/private_test/private_12949.wav  \n","  inflating: /content/data/private_test/private_13464.wav  \n","  inflating: /content/data/private_test/private_04118.wav  \n","  inflating: /content/data/private_test/private_03677.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03677.wav  \n","  inflating: /content/data/private_test/private_02569.wav  \n","  inflating: /content/data/private_test/private_15015.wav  \n","  inflating: /content/data/private_test/private_05206.wav  \n","  inflating: /content/data/private_test/private_11273.wav  \n","  inflating: /content/data/private_test/private_01060.wav  \n","  inflating: /content/data/private_test/private_17602.wav  \n","  inflating: /content/data/private_test/private_18531.wav  \n","  inflating: /content/data/private_test/private_07411.wav  \n","  inflating: /content/data/private_test/private_08722.wav  \n","  inflating: /content/data/private_test/private_07377.wav  \n","  inflating: /content/data/private_test/private_08044.wav  \n","  inflating: /content/data/private_test/private_17164.wav  \n","  inflating: /content/data/private_test/private_18257.wav  \n","  inflating: /content/data/private_test/private_00418.wav  \n","  inflating: /content/data/private_test/private_01706.wav  \n","  inflating: /content/data/private_test/private_19149.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19149.wav  \n","  inflating: /content/data/private_test/private_06069.wav  \n","  inflating: /content/data/private_test/private_11515.wav  \n","  inflating: /content/data/private_test/private_05560.wav  \n","  inflating: /content/data/private_test/private_15773.wav  \n","  inflating: /content/data/private_test/private_03111.wav  \n","  inflating: /content/data/private_test/private_13302.wav  \n","  inflating: /content/data/private_test/private_12008.wav  \n","  inflating: /content/data/private_test/private_05574.wav  \n","  inflating: /content/data/private_test/private_15767.wav  \n","  inflating: /content/data/private_test/private_03105.wav  \n","  inflating: /content/data/private_test/private_14479.wav  \n","  inflating: /content/data/private_test/private_13316.wav  \n","  inflating: /content/data/private_test/private_07363.wav  \n","  inflating: /content/data/private_test/private_08050.wav  \n","  inflating: /content/data/private_test/private_17170.wav  \n","  inflating: /content/data/private_test/private_18243.wav  \n","  inflating: /content/data/private_test/private_01712.wav  \n","  inflating: /content/data/private_test/private_11501.wav  \n","  inflating: /content/data/private_test/private_11267.wav  \n","  inflating: /content/data/private_test/private_09428.wav  \n","  inflating: /content/data/private_test/private_16508.wav  \n","  inflating: /content/data/private_test/private_01074.wav  \n","  inflating: /content/data/private_test/private_17616.wav  \n","  inflating: /content/data/private_test/private_18525.wav  \n","  inflating: /content/data/private_test/private_07405.wav  \n","  inflating: /content/data/private_test/private_08736.wav  \n","  inflating: /content/data/private_test/private_10179.wav  \n","  inflating: /content/data/private_test/private_13470.wav  \n","  inflating: /content/data/private_test/private_03663.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03663.wav  \n","  inflating: /content/data/private_test/private_15001.wav  \n","  inflating: /content/data/private_test/private_05212.wav  \n","  inflating: /content/data/private_test/private_09400.wav  \n","  inflating: /content/data/private_test/private_06733.wav  \n","  inflating: /content/data/private_test/private_19613.wav  \n","  inflating: /content/data/private_test/private_16520.wav  \n","  inflating: /content/data/private_test/private_00342.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00342.wav  \n","  inflating: /content/data/private_test/private_10151.wav  \n","  inflating: /content/data/private_test/private_04124.wav  \n","  inflating: /content/data/private_test/private_13458.wav  \n","  inflating: /content/data/private_test/private_14337.wav  \n","  inflating: /content/data/private_test/private_15029.wav  \n","  inflating: /content/data/private_test/private_02555.wav  \n","  inflating: /content/data/private_test/private_03893.wav  \n","  inflating: /content/data/private_test/private_12746.wav  \n","  inflating: /content/data/private_test/private_12020.wav  \n","  inflating: /content/data/private_test/private_02233.wav  \n","  inflating: /content/data/private_test/private_14451.wav  \n","  inflating: /content/data/private_test/private_15997.wav  \n","  inflating: /content/data/private_test/private_04642.wav  \n","  inflating: /content/data/private_test/private_10637.wav  \n","  inflating: /content/data/private_test/private_08078.wav  \n","  inflating: /content/data/private_test/private_00424.wav  \n","  inflating: /content/data/private_test/private_17158.wav  \n","  inflating: /content/data/private_test/private_19175.wav  \n","  inflating: /content/data/private_test/private_16246.wav  \n","  inflating: /content/data/private_test/private_09366.wav  \n","  inflating: /content/data/private_test/private_11529.wav  \n","  inflating: /content/data/private_test/private_06055.wav  \n","  inflating: /content/data/private_test/private_10623.wav  \n","  inflating: /content/data/private_test/private_00430.wav  \n","  inflating: /content/data/private_test/private_19161.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19161.wav  \n","  inflating: /content/data/private_test/private_16252.wav  \n","  inflating: /content/data/private_test/private_09372.wav  \n","  inflating: /content/data/private_test/private_06041.wav  \n","  inflating: /content/data/private_test/private_05548.wav  \n","  inflating: /content/data/private_test/private_12034.wav  \n","  inflating: /content/data/private_test/private_02227.wav  \n","  inflating: /content/data/private_test/private_14445.wav  \n","  inflating: /content/data/private_test/private_03139.wav  \n","  inflating: /content/data/private_test/private_04656.wav  \n","  inflating: /content/data/private_test/private_15983.wav  \n","  inflating: /content/data/private_test/private_04130.wav  \n","  inflating: /content/data/private_test/private_14323.wav  \n","  inflating: /content/data/private_test/private_02541.wav  \n","  inflating: /content/data/private_test/private_12752.wav  \n","  inflating: /content/data/private_test/private_03887.wav  \n","  inflating: /content/data/private_test/private_09414.wav  \n","  inflating: /content/data/private_test/private_06727.wav  \n","  inflating: /content/data/private_test/private_19607.wav  \n","  inflating: /content/data/private_test/private_01048.wav  \n","  inflating: /content/data/private_test/private_16534.wav  \n","  inflating: /content/data/private_test/private_00356.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00356.wav  \n","  inflating: /content/data/private_test/private_18519.wav  \n","  inflating: /content/data/private_test/private_10145.wav  \n","  inflating: /content/data/private_test/private_07439.wav  \n","  inflating: /content/data/private_test/private_18096.wav  \n","  inflating: /content/data/private_test/private_08285.wav  \n","  inflating: /content/data/private_test/private_00801.wav  \n","  inflating: /content/data/private_test/private_19388.wav  \n","  inflating: /content/data/private_test/private_14874.wav  \n","  inflating: /content/data/private_test/private_05979.wav  \n","  inflating: /content/data/private_test/private_02970.wav  \n","  inflating: /content/data/private_test/private_07808.wav  \n","  inflating: /content/data/private_test/private_18928.wav  \n","  inflating: /content/data/private_test/private_16905.wav  \n","  inflating: /content/data/private_test/private_09825.wav  \n","  inflating: /content/data/private_test/private_16911.wav  \n","  inflating: /content/data/private_test/private_09831.wav  \n","  inflating: /content/data/private_test/private_02964.wav  \n","  inflating: /content/data/private_test/private_13869.wav  \n","  inflating: /content/data/private_test/private_14860.wav  \n","  inflating: /content/data/private_test/private_11918.wav  \n","  inflating: /content/data/private_test/private_18082.wav  \n","  inflating: /content/data/private_test/private_08291.wav  \n","  inflating: /content/data/private_test/private_00815.wav  \n","  inflating: /content/data/private_test/private_14848.wav  \n","  inflating: /content/data/private_test/private_04483.wav  \n","  inflating: /content/data/private_test/private_05945.wav  \n","  inflating: /content/data/private_test/private_14690.wav  \n","  inflating: /content/data/private_test/private_17399.wav  \n","  inflating: /content/data/private_test/private_11930.wav  \n","  inflating: /content/data/private_test/private_06294.wav  \n","  inflating: /content/data/private_test/private_16087.wav  \n","  inflating: /content/data/private_test/private_07834.wav  \n","  inflating: /content/data/private_test/private_18914.wav  \n","  inflating: /content/data/private_test/private_16939.wav  \n","  inflating: /content/data/private_test/private_10390.wav  \n","  inflating: /content/data/private_test/private_09819.wav  \n","  inflating: /content/data/private_test/private_00183.wav  \n","  inflating: /content/data/private_test/private_13699.wav  \n","  inflating: /content/data/private_test/private_12587.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_12587.wav  \n","  inflating: /content/data/private_test/private_13841.wav  \n","  inflating: /content/data/private_test/private_02794.wav  \n","  inflating: /content/data/private_test/private_02958.wav  \n","  inflating: /content/data/private_test/private_12593.wav  \n","  inflating: /content/data/private_test/private_02780.wav  \n","  inflating: /content/data/private_test/private_13855.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13855.wav  \n","  inflating: /content/data/private_test/private_07820.wav  \n","  inflating: /content/data/private_test/private_01289.wav  \n","  inflating: /content/data/private_test/private_18900.wav  \n","  inflating: /content/data/private_test/private_10384.wav  \n","  inflating: /content/data/private_test/private_00197.wav  \n","  inflating: /content/data/private_test/private_11924.wav  \n","  inflating: /content/data/private_test/private_06280.wav  \n","  inflating: /content/data/private_test/private_00829.wav  \n","  inflating: /content/data/private_test/private_16093.wav  \n","  inflating: /content/data/private_test/private_05789.wav  \n","  inflating: /content/data/private_test/private_04497.wav  \n","  inflating: /content/data/private_test/private_14684.wav  \n","  inflating: /content/data/private_test/private_05951.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_05951.wav  \n","  inflating: /content/data/private_test/private_13100.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13100.wav  \n","  inflating: /content/data/private_test/private_03313.wav  \n","  inflating: /content/data/private_test/private_15571.wav  \n","  inflating: /content/data/private_test/private_05762.wav  \n","  inflating: /content/data/private_test/private_11717.wav  \n","  inflating: /content/data/private_test/private_09158.wav  \n","  inflating: /content/data/private_test/private_01504.wav  \n","  inflating: /content/data/private_test/private_16078.wav  \n","  inflating: /content/data/private_test/private_18055.wav  \n","  inflating: /content/data/private_test/private_17366.wav  \n","  inflating: /content/data/private_test/private_08246.wav  \n","  inflating: /content/data/private_test/private_10409.wav  \n","  inflating: /content/data/private_test/private_07175.wav  \n","  inflating: /content/data/private_test/private_08520.wav  \n","  inflating: /content/data/private_test/private_07613.wav  \n","  inflating: /content/data/private_test/private_18733.wav  \n","  inflating: /content/data/private_test/private_17400.wav  \n","  inflating: /content/data/private_test/private_01262.wav  \n","  inflating: /content/data/private_test/private_11071.wav  \n","  inflating: /content/data/private_test/private_05004.wav  \n","  inflating: /content/data/private_test/private_12578.wav  \n","  inflating: /content/data/private_test/private_15217.wav  \n","  inflating: /content/data/private_test/private_14109.wav  \n","  inflating: /content/data/private_test/private_03475.wav  \n","  inflating: /content/data/private_test/private_13666.wav  \n","  inflating: /content/data/private_test/private_05010.wav  \n","  inflating: /content/data/private_test/private_15203.wav  \n","  inflating: /content/data/private_test/private_03461.wav  \n","  inflating: /content/data/private_test/private_13672.wav  \n","  inflating: /content/data/private_test/private_08534.wav  \n","  inflating: /content/data/private_test/private_07607.wav  \n","  inflating: /content/data/private_test/private_18727.wav  \n","  inflating: /content/data/private_test/private_00168.wav  \n","  inflating: /content/data/private_test/private_17414.wav  \n","  inflating: /content/data/private_test/private_01276.wav  \n","  inflating: /content/data/private_test/private_19439.wav  \n","  inflating: /content/data/private_test/private_11065.wav  \n","  inflating: /content/data/private_test/private_06519.wav  \n","  inflating: /content/data/private_test/private_11703.wav  \n","  inflating: /content/data/private_test/private_01510.wav  \n","  inflating: /content/data/private_test/private_18041.wav  \n","  inflating: /content/data/private_test/private_17372.wav  \n","  inflating: /content/data/private_test/private_08252.wav  \n","  inflating: /content/data/private_test/private_07161.wav  \n","  inflating: /content/data/private_test/private_04468.wav  \n","  inflating: /content/data/private_test/private_13114.wav  \n","  inflating: /content/data/private_test/private_03307.wav  \n","  inflating: /content/data/private_test/private_15565.wav  \n","  inflating: /content/data/private_test/private_02019.wav  \n","  inflating: /content/data/private_test/private_05776.wav  \n","  inflating: /content/data/private_test/private_06257.wav  \n","  inflating: /content/data/private_test/private_09164.wav  \n","  inflating: /content/data/private_test/private_16044.wav  \n","  inflating: /content/data/private_test/private_19377.wav  \n","  inflating: /content/data/private_test/private_01538.wav  \n","  inflating: /content/data/private_test/private_00626.wav  \n","  inflating: /content/data/private_test/private_18069.wav  \n","  inflating: /content/data/private_test/private_07149.wav  \n","  inflating: /content/data/private_test/private_10435.wav  \n","  inflating: /content/data/private_test/private_04440.wav  \n","  inflating: /content/data/private_test/private_05986.wav  \n","  inflating: /content/data/private_test/private_14653.wav  \n","  inflating: /content/data/private_test/private_02031.wav  \n","  inflating: /content/data/private_test/private_12222.wav  \n","  inflating: /content/data/private_test/private_12544.wav  \n","  inflating: /content/data/private_test/private_05038.wav  \n","  inflating: /content/data/private_test/private_13882.wav  \n","  inflating: /content/data/private_test/private_02757.wav  \n","  inflating: /content/data/private_test/private_03449.wav  \n","  inflating: /content/data/private_test/private_14135.wav  \n","  inflating: /content/data/private_test/private_04326.wav  \n","  inflating: /content/data/private_test/private_10353.wav  \n","  inflating: /content/data/private_test/private_00140.wav  \n","  inflating: /content/data/private_test/private_16722.wav  \n","  inflating: /content/data/private_test/private_19411.wav  \n","  inflating: /content/data/private_test/private_06531.wav  \n","  inflating: /content/data/private_test/private_09602.wav  \n","  inflating: /content/data/private_test/private_10347.wav  \n","  inflating: /content/data/private_test/private_08508.wav  \n","  inflating: /content/data/private_test/private_17428.wav  \n","  inflating: /content/data/private_test/private_00154.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00154.wav  \n","  inflating: /content/data/private_test/private_16736.wav  \n","  inflating: /content/data/private_test/private_19405.wav  \n","  inflating: /content/data/private_test/private_06525.wav  \n","  inflating: /content/data/private_test/private_09616.wav  \n","  inflating: /content/data/private_test/private_11059.wav  \n","  inflating: /content/data/private_test/private_12550.wav  \n","  inflating: /content/data/private_test/private_02743.wav  \n","  inflating: /content/data/private_test/private_13896.wav  \n","  inflating: /content/data/private_test/private_14121.wav  \n","  inflating: /content/data/private_test/private_04332.wav  \n","  inflating: /content/data/private_test/private_13128.wav  \n","  inflating: /content/data/private_test/private_04454.wav  \n","  inflating: /content/data/private_test/private_14647.wav  \n","  inflating: /content/data/private_test/private_05992.wav  \n","  inflating: /content/data/private_test/private_02025.wav  \n","  inflating: /content/data/private_test/private_15559.wav  \n","  inflating: /content/data/private_test/private_12236.wav  \n","  inflating: /content/data/private_test/private_06243.wav  \n","  inflating: /content/data/private_test/private_09170.wav  \n","  inflating: /content/data/private_test/private_16050.wav  \n","  inflating: /content/data/private_test/private_19363.wav  \n","  inflating: /content/data/private_test/private_00632.wav  \n","  inflating: /content/data/private_test/private_10421.wav  \n","  inflating: /content/data/private_test/private_06533.wav  \n","  inflating: /content/data/private_test/private_09600.wav  \n","  inflating: /content/data/private_test/private_16720.wav  \n","  inflating: /content/data/private_test/private_19413.wav  \n","  inflating: /content/data/private_test/private_00142.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00142.wav  \n","  inflating: /content/data/private_test/private_10351.wav  \n","  inflating: /content/data/private_test/private_13658.wav  \n","  inflating: /content/data/private_test/private_04324.wav  \n","  inflating: /content/data/private_test/private_14137.wav  \n","  inflating: /content/data/private_test/private_02755.wav  \n","  inflating: /content/data/private_test/private_13880.wav  \n","  inflating: /content/data/private_test/private_15229.wav  \n","  inflating: /content/data/private_test/private_12546.wav  \n","  inflating: /content/data/private_test/private_12220.wav  \n","  inflating: /content/data/private_test/private_14889.wav  \n","  inflating: /content/data/private_test/private_02033.wav  \n","  inflating: /content/data/private_test/private_14651.wav  \n","  inflating: /content/data/private_test/private_05984.wav  \n","  inflating: /content/data/private_test/private_04442.wav  \n","  inflating: /content/data/private_test/private_10437.wav  \n","  inflating: /content/data/private_test/private_08278.wav  \n","  inflating: /content/data/private_test/private_17358.wav  \n","  inflating: /content/data/private_test/private_00624.wav  \n","  inflating: /content/data/private_test/private_16046.wav  \n","  inflating: /content/data/private_test/private_19375.wav  \n","  inflating: /content/data/private_test/private_06255.wav  \n","  inflating: /content/data/private_test/private_09166.wav  \n","  inflating: /content/data/private_test/private_11729.wav  \n","  inflating: /content/data/private_test/private_10423.wav  \n","  inflating: /content/data/private_test/private_00630.wav  \n","  inflating: /content/data/private_test/private_16052.wav  \n","  inflating: /content/data/private_test/private_19361.wav  \n","  inflating: /content/data/private_test/private_06241.wav  \n","  inflating: /content/data/private_test/private_09172.wav  \n","  inflating: /content/data/private_test/private_12234.wav  \n","  inflating: /content/data/private_test/private_05748.wav  \n","  inflating: /content/data/private_test/private_02027.wav  \n","  inflating: /content/data/private_test/private_03339.wav  \n","  inflating: /content/data/private_test/private_05990.wav  \n","  inflating: /content/data/private_test/private_14645.wav  \n","  inflating: /content/data/private_test/private_04456.wav  \n","  inflating: /content/data/private_test/private_02999.wav  \n","  inflating: /content/data/private_test/private_04330.wav  \n","  inflating: /content/data/private_test/private_14123.wav  \n","  inflating: /content/data/private_test/private_13894.wav  \n","  inflating: /content/data/private_test/private_02741.wav  \n","  inflating: /content/data/private_test/private_12552.wav  \n","  inflating: /content/data/private_test/private_06527.wav  \n","  inflating: /content/data/private_test/private_09614.wav  \n","  inflating: /content/data/private_test/private_16734.wav  \n","  inflating: /content/data/private_test/private_19407.wav  \n","  inflating: /content/data/private_test/private_01248.wav  \n","  inflating: /content/data/private_test/private_00156.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00156.wav  \n","  inflating: /content/data/private_test/private_18719.wav  \n","  inflating: /content/data/private_test/private_07639.wav  \n","  inflating: /content/data/private_test/private_10345.wav  \n","  inflating: /content/data/private_test/private_04318.wav  \n","  inflating: /content/data/private_test/private_13664.wav  \n","  inflating: /content/data/private_test/private_03477.wav  \n","  inflating: /content/data/private_test/private_15215.wav  \n","  inflating: /content/data/private_test/private_02769.wav  \n","  inflating: /content/data/private_test/private_05006.wav  \n","  inflating: /content/data/private_test/private_11073.wav  \n","  inflating: /content/data/private_test/private_01260.wav  \n","  inflating: /content/data/private_test/private_18731.wav  \n","  inflating: /content/data/private_test/private_17402.wav  \n","  inflating: /content/data/private_test/private_08522.wav  \n","  inflating: /content/data/private_test/private_07611.wav  \n","  inflating: /content/data/private_test/private_08244.wav  \n","  inflating: /content/data/private_test/private_07177.wav  \n","  inflating: /content/data/private_test/private_18057.wav  \n","  inflating: /content/data/private_test/private_00618.wav  \n","  inflating: /content/data/private_test/private_17364.wav  \n","  inflating: /content/data/private_test/private_01506.wav  \n","  inflating: /content/data/private_test/private_19349.wav  \n","  inflating: /content/data/private_test/private_11715.wav  \n","  inflating: /content/data/private_test/private_06269.wav  \n","  inflating: /content/data/private_test/private_05760.wav  \n","  inflating: /content/data/private_test/private_15573.wav  \n","  inflating: /content/data/private_test/private_03311.wav  \n","  inflating: /content/data/private_test/private_13102.wav  \n","  inflating: /content/data/private_test/private_05774.wav  \n","  inflating: /content/data/private_test/private_12208.wav  \n","  inflating: /content/data/private_test/private_15567.wav  \n","  inflating: /content/data/private_test/private_14679.wav  \n","  inflating: /content/data/private_test/private_03305.wav  \n","  inflating: /content/data/private_test/private_13116.wav  \n","  inflating: /content/data/private_test/private_08250.wav  \n","  inflating: /content/data/private_test/private_07163.wav  \n","  inflating: /content/data/private_test/private_18043.wav  \n","  inflating: /content/data/private_test/private_17370.wav  \n","  inflating: /content/data/private_test/private_01512.wav  \n","  inflating: /content/data/private_test/private_11701.wav  \n","  inflating: /content/data/private_test/private_11067.wav  \n","  inflating: /content/data/private_test/private_09628.wav  \n","  inflating: /content/data/private_test/private_01274.wav  \n","  inflating: /content/data/private_test/private_16708.wav  \n","  inflating: /content/data/private_test/private_18725.wav  \n","  inflating: /content/data/private_test/private_17416.wav  \n","  inflating: /content/data/private_test/private_08536.wav  \n","  inflating: /content/data/private_test/private_10379.wav  \n","  inflating: /content/data/private_test/private_07605.wav  \n","  inflating: /content/data/private_test/private_13670.wav  \n","  inflating: /content/data/private_test/private_03463.wav  \n","  inflating: /content/data/private_test/private_15201.wav  \n","  inflating: /content/data/private_test/private_05012.wav  \n","  inflating: /content/data/private_test/private_02796.wav  \n","  inflating: /content/data/private_test/private_13843.wav  \n","  inflating: /content/data/private_test/private_12585.wav  \n","  inflating: /content/data/private_test/private_03488.wav  \n","  inflating: /content/data/private_test/private_00181.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00181.wav  \n","  inflating: /content/data/private_test/private_10392.wav  \n","  inflating: /content/data/private_test/private_18916.wav  \n","  inflating: /content/data/private_test/private_07836.wav  \n","  inflating: /content/data/private_test/private_16085.wav  \n","  inflating: /content/data/private_test/private_06296.wav  \n","  inflating: /content/data/private_test/private_07188.wav  \n","  inflating: /content/data/private_test/private_11932.wav  \n","  inflating: /content/data/private_test/private_14692.wav  \n","  inflating: /content/data/private_test/private_05947.wav  \n","  inflating: /content/data/private_test/private_04481.wav  \n","  inflating: /content/data/private_test/private_05953.wav  \n","  inflating: /content/data/private_test/private_14686.wav  \n","  inflating: /content/data/private_test/private_04495.wav  \n","  inflating: /content/data/private_test/private_15598.wav  \n","  inflating: /content/data/private_test/private_16091.wav  \n","  inflating: /content/data/private_test/private_06282.wav  \n","  inflating: /content/data/private_test/private_11926.wav  \n","  inflating: /content/data/private_test/private_00195.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00195.wav  \n","  inflating: /content/data/private_test/private_10386.wav  \n","  inflating: /content/data/private_test/private_18902.wav  \n","  inflating: /content/data/private_test/private_11098.wav  \n","  inflating: /content/data/private_test/private_07822.wav  \n","  inflating: /content/data/private_test/private_13857.wav  \n","  inflating: /content/data/private_test/private_02782.wav  \n","  inflating: /content/data/private_test/private_12591.wav  \n","  inflating: /content/data/private_test/private_09827.wav  \n","  inflating: /content/data/private_test/private_16907.wav  \n","  inflating: /content/data/private_test/private_02972.wav  \n","  inflating: /content/data/private_test/private_14876.wav  \n","  inflating: /content/data/private_test/private_00803.wav  \n","  inflating: /content/data/private_test/private_09199.wav  \n","  inflating: /content/data/private_test/private_08287.wav  \n","  inflating: /content/data/private_test/private_18094.wav  \n","  inflating: /content/data/private_test/private_00817.wav  \n","  inflating: /content/data/private_test/private_08293.wav  \n","  inflating: /content/data/private_test/private_18080.wav  \n","  inflating: /content/data/private_test/private_14862.wav  \n","  inflating: /content/data/private_test/private_02966.wav  \n","  inflating: /content/data/private_test/private_09833.wav  \n","  inflating: /content/data/private_test/private_16913.wav  \n","  inflating: /content/data/private_test/private_09364.wav  \n","  inflating: /content/data/private_test/private_06057.wav  \n","  inflating: /content/data/private_test/private_19177.wav  \n","  inflating: /content/data/private_test/private_01738.wav  \n","  inflating: /content/data/private_test/private_16244.wav  \n","  inflating: /content/data/private_test/private_00426.wav  \n","  inflating: /content/data/private_test/private_18269.wav  \n","  inflating: /content/data/private_test/private_10635.wav  \n","  inflating: /content/data/private_test/private_07349.wav  \n","  inflating: /content/data/private_test/private_04640.wav  \n","  inflating: /content/data/private_test/private_15995.wav  \n","  inflating: /content/data/private_test/private_14453.wav  \n","  inflating: /content/data/private_test/private_04898.wav  \n","  inflating: /content/data/private_test/private_02231.wav  \n","  inflating: /content/data/private_test/private_12022.wav  \n","  inflating: /content/data/private_test/private_05238.wav  \n","  inflating: /content/data/private_test/private_12744.wav  \n","  inflating: /content/data/private_test/private_03891.wav  \n","  inflating: /content/data/private_test/private_02557.wav  \n","  inflating: /content/data/private_test/private_14335.wav  \n","  inflating: /content/data/private_test/private_03649.wav  \n","  inflating: /content/data/private_test/private_04126.wav  \n","  inflating: /content/data/private_test/private_10153.wav  \n","  inflating: /content/data/private_test/private_00340.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00340.wav  \n","  inflating: /content/data/private_test/private_19611.wav  \n","  inflating: /content/data/private_test/private_16522.wav  \n","  inflating: /content/data/private_test/private_09402.wav  \n","  inflating: /content/data/private_test/private_06731.wav  \n","  inflating: /content/data/private_test/private_10147.wav  \n","  inflating: /content/data/private_test/private_08708.wav  \n","  inflating: /content/data/private_test/private_00354.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00354.wav  \n","  inflating: /content/data/private_test/private_17628.wav  \n","  inflating: /content/data/private_test/private_19605.wav  \n","  inflating: /content/data/private_test/private_16536.wav  \n","  inflating: /content/data/private_test/private_09416.wav  \n","  inflating: /content/data/private_test/private_11259.wav  \n","  inflating: /content/data/private_test/private_06725.wav  \n","  inflating: /content/data/private_test/private_03885.wav  \n","  inflating: /content/data/private_test/private_12750.wav  \n","  inflating: /content/data/private_test/private_02543.wav  \n","  inflating: /content/data/private_test/private_14321.wav  \n","  inflating: /content/data/private_test/private_12988.wav  \n","  inflating: /content/data/private_test/private_04132.wav  \n","  inflating: /content/data/private_test/private_15981.wav  \n","  inflating: /content/data/private_test/private_04654.wav  \n","  inflating: /content/data/private_test/private_13328.wav  \n","  inflating: /content/data/private_test/private_14447.wav  \n","  inflating: /content/data/private_test/private_15759.wav  \n","  inflating: /content/data/private_test/private_02225.wav  \n","  inflating: /content/data/private_test/private_12036.wav  \n","  inflating: /content/data/private_test/private_09370.wav  \n","  inflating: /content/data/private_test/private_06043.wav  \n","  inflating: /content/data/private_test/private_19163.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19163.wav  \n","  inflating: /content/data/private_test/private_16250.wav  \n","  inflating: /content/data/private_test/private_00432.wav  \n","  inflating: /content/data/private_test/private_10621.wav  \n","  inflating: /content/data/private_test/private_13300.wav  \n","  inflating: /content/data/private_test/private_03113.wav  \n","  inflating: /content/data/private_test/private_15771.wav  \n","  inflating: /content/data/private_test/private_05562.wav  \n","  inflating: /content/data/private_test/private_11517.wav  \n","  inflating: /content/data/private_test/private_09358.wav  \n","  inflating: /content/data/private_test/private_16278.wav  \n","  inflating: /content/data/private_test/private_01704.wav  \n","  inflating: /content/data/private_test/private_17166.wav  \n","  inflating: /content/data/private_test/private_18255.wav  \n","  inflating: /content/data/private_test/private_07375.wav  \n","  inflating: /content/data/private_test/private_08046.wav  \n","  inflating: /content/data/private_test/private_10609.wav  \n","  inflating: /content/data/private_test/private_07413.wav  \n","  inflating: /content/data/private_test/private_08720.wav  \n","  inflating: /content/data/private_test/private_17600.wav  \n","  inflating: /content/data/private_test/private_18533.wav  \n","  inflating: /content/data/private_test/private_01062.wav  \n","  inflating: /content/data/private_test/private_11271.wav  \n","  inflating: /content/data/private_test/private_12778.wav  \n","  inflating: /content/data/private_test/private_05204.wav  \n","  inflating: /content/data/private_test/private_15017.wav  \n","  inflating: /content/data/private_test/private_03675.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03675.wav  \n","  inflating: /content/data/private_test/private_14309.wav  \n","  inflating: /content/data/private_test/private_13466.wav  \n","  inflating: /content/data/private_test/private_05210.wav  \n","  inflating: /content/data/private_test/private_15003.wav  \n","  inflating: /content/data/private_test/private_03661.wav  \n","  inflating: /content/data/private_test/private_13472.wav  \n","  inflating: /content/data/private_test/private_07407.wav  \n","  inflating: /content/data/private_test/private_08734.wav  \n","  inflating: /content/data/private_test/private_17614.wav  \n","  inflating: /content/data/private_test/private_18527.wav  \n","  inflating: /content/data/private_test/private_00368.wav  \n","  inflating: /content/data/private_test/private_01076.wav  \n","  inflating: /content/data/private_test/private_19639.wav  \n","  inflating: /content/data/private_test/private_06719.wav  \n","  inflating: /content/data/private_test/private_11265.wav  \n","  inflating: /content/data/private_test/private_11503.wav  \n","  inflating: /content/data/private_test/private_01710.wav  \n","  inflating: /content/data/private_test/private_17172.wav  \n","  inflating: /content/data/private_test/private_18241.wav  \n","  inflating: /content/data/private_test/private_07361.wav  \n","  inflating: /content/data/private_test/private_08052.wav  \n","  inflating: /content/data/private_test/private_13314.wav  \n","  inflating: /content/data/private_test/private_04668.wav  \n","  inflating: /content/data/private_test/private_03107.wav  \n","  inflating: /content/data/private_test/private_02219.wav  \n","  inflating: /content/data/private_test/private_15765.wav  \n","  inflating: /content/data/private_test/private_05576.wav  \n","  inflating: /content/data/private_test/private_04683.wav  \n","  inflating: /content/data/private_test/private_15956.wav  \n","  inflating: /content/data/private_test/private_14490.wav  \n","  inflating: /content/data/private_test/private_17199.wav  \n","  inflating: /content/data/private_test/private_01923.wav  \n","  inflating: /content/data/private_test/private_06094.wav  \n","  inflating: /content/data/private_test/private_16287.wav  \n","  inflating: /content/data/private_test/private_08907.wav  \n","  inflating: /content/data/private_test/private_17827.wav  \n","  inflating: /content/data/private_test/private_10190.wav  \n","  inflating: /content/data/private_test/private_00383.wav  \n","  inflating: /content/data/private_test/private_13499.wav  \n","  inflating: /content/data/private_test/private_12787.wav  \n","  inflating: /content/data/private_test/private_03852.wav  \n","  inflating: /content/data/private_test/private_02594.wav  \n","  inflating: /content/data/private_test/private_03846.wav  \n","  inflating: /content/data/private_test/private_12793.wav  \n","  inflating: /content/data/private_test/private_02580.wav  \n","  inflating: /content/data/private_test/private_08913.wav  \n","  inflating: /content/data/private_test/private_01089.wav  \n","  inflating: /content/data/private_test/private_17833.wav  \n","  inflating: /content/data/private_test/private_10184.wav  \n","  inflating: /content/data/private_test/private_00397.wav  \n","  inflating: /content/data/private_test/private_01937.wav  \n","  inflating: /content/data/private_test/private_06080.wav  \n","  inflating: /content/data/private_test/private_16293.wav  \n","  inflating: /content/data/private_test/private_05589.wav  \n","  inflating: /content/data/private_test/private_15942.wav  \n","  inflating: /content/data/private_test/private_04697.wav  \n","  inflating: /content/data/private_test/private_14484.wav  \n","  inflating: /content/data/private_test/private_18296.wav  \n","  inflating: /content/data/private_test/private_08085.wav  \n","  inflating: /content/data/private_test/private_10812.wav  \n","  inflating: /content/data/private_test/private_19188.wav  \n","  inflating: /content/data/private_test/private_04867.wav  \n","  inflating: /content/data/private_test/private_12963.wav  \n","  inflating: /content/data/private_test/private_19836.wav  \n","  inflating: /content/data/private_test/private_06916.wav  \n","  inflating: /content/data/private_test/private_19822.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19822.wav  \n","  inflating: /content/data/private_test/private_06902.wav  \n","  inflating: /content/data/private_test/private_12977.wav  \n","  inflating: /content/data/private_test/private_04873.wav  \n","  inflating: /content/data/private_test/private_18282.wav  \n","  inflating: /content/data/private_test/private_08091.wav  \n","  inflating: /content/data/private_test/private_10806.wav  \n","  inflating: /content/data/private_test/private_10741.wav  \n","  inflating: /content/data/private_test/private_01894.wav  \n","  inflating: /content/data/private_test/private_00552.wav  \n","  inflating: /content/data/private_test/private_10999.wav  \n","  inflating: /content/data/private_test/private_19003.wav  \n","  inflating: /content/data/private_test/private_16330.wav  \n","  inflating: /content/data/private_test/private_09210.wav  \n","  inflating: /content/data/private_test/private_06123.wav  \n","  inflating: /content/data/private_test/private_12156.wav  \n","  inflating: /content/data/private_test/private_15639.wav  \n","  inflating: /content/data/private_test/private_02345.wav  \n","  inflating: /content/data/private_test/private_14527.wav  \n","  inflating: /content/data/private_test/private_04734.wav  \n","  inflating: /content/data/private_test/private_13248.wav  \n","  inflating: /content/data/private_test/private_04052.wav  \n","  inflating: /content/data/private_test/private_14241.wav  \n","  inflating: /content/data/private_test/private_02423.wav  \n","  inflating: /content/data/private_test/private_12630.wav  \n","  inflating: /content/data/private_test/private_09576.wav  \n","  inflating: /content/data/private_test/private_11339.wav  \n","  inflating: /content/data/private_test/private_06645.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06645.wav  \n","  inflating: /content/data/private_test/private_17990.wav  \n","  inflating: /content/data/private_test/private_19765.wav  \n","  inflating: /content/data/private_test/private_16456.wav  \n","  inflating: /content/data/private_test/private_00234.wav  \n","  inflating: /content/data/private_test/private_17748.wav  \n","  inflating: /content/data/private_test/private_10027.wav  \n","  inflating: /content/data/private_test/private_08668.wav  \n","  inflating: /content/data/private_test/private_09562.wav  \n","  inflating: /content/data/private_test/private_17984.wav  \n","  inflating: /content/data/private_test/private_06651.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06651.wav  \n","  inflating: /content/data/private_test/private_19771.wav  \n","  inflating: /content/data/private_test/private_16442.wav  \n","  inflating: /content/data/private_test/private_00220.wav  \n","  inflating: /content/data/private_test/private_06889.wav  \n","  inflating: /content/data/private_test/private_10033.wav  \n","  inflating: /content/data/private_test/private_04046.wav  \n","  inflating: /content/data/private_test/private_14255.wav  \n","  inflating: /content/data/private_test/private_03729.wav  \n","  inflating: /content/data/private_test/private_02437.wav  \n","  inflating: /content/data/private_test/private_05358.wav  \n","  inflating: /content/data/private_test/private_12624.wav  \n","  inflating: /content/data/private_test/private_12142.wav  \n","  inflating: /content/data/private_test/private_02351.wav  \n","  inflating: /content/data/private_test/private_14533.wav  \n","  inflating: /content/data/private_test/private_04720.wav  \n","  inflating: /content/data/private_test/private_01880.wav  \n","  inflating: /content/data/private_test/private_10755.wav  \n","  inflating: /content/data/private_test/private_07229.wav  \n","  inflating: /content/data/private_test/private_00546.wav  \n","  inflating: /content/data/private_test/private_18309.wav  \n","  inflating: /content/data/private_test/private_19017.wav  \n","  inflating: /content/data/private_test/private_01658.wav  \n","  inflating: /content/data/private_test/private_16324.wav  \n","  inflating: /content/data/private_test/private_09204.wav  \n","  inflating: /content/data/private_test/private_06137.wav  \n","  inflating: /content/data/private_test/private_05416.wav  \n","  inflating: /content/data/private_test/private_02379.wav  \n","  inflating: /content/data/private_test/private_15605.wav  \n","  inflating: /content/data/private_test/private_03067.wav  \n","  inflating: /content/data/private_test/private_13274.wav  \n","  inflating: /content/data/private_test/private_04708.wav  \n","  inflating: /content/data/private_test/private_07201.wav  \n","  inflating: /content/data/private_test/private_08132.wav  \n","  inflating: /content/data/private_test/private_17012.wav  \n","  inflating: /content/data/private_test/private_18321.wav  \n","  inflating: /content/data/private_test/private_01670.wav  \n","  inflating: /content/data/private_test/private_11463.wav  \n","  inflating: /content/data/private_test/private_06679.wav  \n","  inflating: /content/data/private_test/private_11305.wav  \n","  inflating: /content/data/private_test/private_01116.wav  \n","  inflating: /content/data/private_test/private_19759.wav  \n","  inflating: /content/data/private_test/private_17774.wav  \n","  inflating: /content/data/private_test/private_18447.wav  \n","  inflating: /content/data/private_test/private_00208.wav  \n","  inflating: /content/data/private_test/private_07567.wav  \n","  inflating: /content/data/private_test/private_19981.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19981.wav  \n","  inflating: /content/data/private_test/private_08654.wav  \n","  inflating: /content/data/private_test/private_13512.wav  \n","  inflating: /content/data/private_test/private_03701.wav  \n","  inflating: /content/data/private_test/private_15163.wav  \n","  inflating: /content/data/private_test/private_05370.wav  \n","  inflating: /content/data/private_test/private_13506.wav  \n","  inflating: /content/data/private_test/private_03715.wav  \n","  inflating: /content/data/private_test/private_14269.wav  \n","  inflating: /content/data/private_test/private_15177.wav  \n","  inflating: /content/data/private_test/private_12618.wav  \n","  inflating: /content/data/private_test/private_05364.wav  \n","  inflating: /content/data/private_test/private_11311.wav  \n","  inflating: /content/data/private_test/private_08898.wav  \n","  inflating: /content/data/private_test/private_01102.wav  \n","  inflating: /content/data/private_test/private_17760.wav  \n","  inflating: /content/data/private_test/private_18453.wav  \n","  inflating: /content/data/private_test/private_07573.wav  \n","  inflating: /content/data/private_test/private_08640.wav  \n","  inflating: /content/data/private_test/private_19995.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19995.wav  \n","  inflating: /content/data/private_test/private_07215.wav  \n","  inflating: /content/data/private_test/private_08126.wav  \n","  inflating: /content/data/private_test/private_10769.wav  \n","  inflating: /content/data/private_test/private_17006.wav  \n","  inflating: /content/data/private_test/private_18335.wav  \n","  inflating: /content/data/private_test/private_16318.wav  \n","  inflating: /content/data/private_test/private_01664.wav  \n","  inflating: /content/data/private_test/private_11477.wav  \n","  inflating: /content/data/private_test/private_09238.wav  \n","  inflating: /content/data/private_test/private_05402.wav  \n","  inflating: /content/data/private_test/private_15611.wav  \n","  inflating: /content/data/private_test/private_03073.wav  \n","  inflating: /content/data/private_test/private_13260.wav  \n","  inflating: /content/data/private_test/private_03098.wav  \n","  inflating: /content/data/private_test/private_15822.wav  \n","  inflating: /content/data/private_test/private_12195.wav  \n","  inflating: /content/data/private_test/private_02386.wav  \n","  inflating: /content/data/private_test/private_10782.wav  \n","  inflating: /content/data/private_test/private_01857.wav  \n","  inflating: /content/data/private_test/private_00591.wav  \n","  inflating: /content/data/private_test/private_07598.wav  \n","  inflating: /content/data/private_test/private_06686.wav  \n","  inflating: /content/data/private_test/private_17953.wav  \n","  inflating: /content/data/private_test/private_08873.wav  \n","  inflating: /content/data/private_test/private_16495.wav  \n","  inflating: /content/data/private_test/private_03926.wav  \n","  inflating: /content/data/private_test/private_04091.wav  \n","  inflating: /content/data/private_test/private_14282.wav  \n","  inflating: /content/data/private_test/private_15188.wav  \n","  inflating: /content/data/private_test/private_03932.wav  \n","  inflating: /content/data/private_test/private_04085.wav  \n","  inflating: /content/data/private_test/private_14296.wav  \n","  inflating: /content/data/private_test/private_17947.wav  \n","  inflating: /content/data/private_test/private_06692.wav  \n","  inflating: /content/data/private_test/private_08867.wav  \n","  inflating: /content/data/private_test/private_16481.wav  \n","  inflating: /content/data/private_test/private_11488.wav  \n","  inflating: /content/data/private_test/private_01843.wav  \n","  inflating: /content/data/private_test/private_10796.wav  \n","  inflating: /content/data/private_test/private_00585.wav  \n","  inflating: /content/data/private_test/private_15836.wav  \n","  inflating: /content/data/private_test/private_12181.wav  \n","  inflating: /content/data/private_test/private_02392.wav  \n","  inflating: /content/data/private_test/private_10966.wav  \n","  inflating: /content/data/private_test/private_04913.wav  \n","  inflating: /content/data/private_test/private_12817.wav  \n","  inflating: /content/data/private_test/private_06862.wav  \n","  inflating: /content/data/private_test/private_18484.wav  \n","  inflating: /content/data/private_test/private_19942.wav  \n","  inflating: /content/data/private_test/private_08697.wav  \n","  inflating: /content/data/private_test/private_09589.wav  \n","  inflating: /content/data/private_test/private_06876.wav  \n","  inflating: /content/data/private_test/private_18490.wav  \n","  inflating: /content/data/private_test/private_08683.wav  \n","  inflating: /content/data/private_test/private_19956.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19956.wav  \n","  inflating: /content/data/private_test/private_12803.wav  \n","  inflating: /content/data/private_test/private_04907.wav  \n","  inflating: /content/data/private_test/private_10972.wav  \n","  inflating: /content/data/private_test/private_07759.wav  \n","  inflating: /content/data/private_test/private_10225.wav  \n","  inflating: /content/data/private_test/private_00036.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00036.wav  \n","  inflating: /content/data/private_test/private_18679.wav  \n","  inflating: /content/data/private_test/private_16654.wav  \n","  inflating: /content/data/private_test/private_07981.wav  \n","  inflating: /content/data/private_test/private_19567.wav  \n","  inflating: /content/data/private_test/private_01328.wav  \n","  inflating: /content/data/private_test/private_06447.wav  \n","  inflating: /content/data/private_test/private_09774.wav  \n","  inflating: /content/data/private_test/private_12432.wav  \n","  inflating: /content/data/private_test/private_02621.wav  \n","  inflating: /content/data/private_test/private_14043.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_14043.wav  \n","  inflating: /content/data/private_test/private_04250.wav  \n","  inflating: /content/data/private_test/private_04536.wav  \n","  inflating: /content/data/private_test/private_03259.wav  \n","  inflating: /content/data/private_test/private_14725.wav  \n","  inflating: /content/data/private_test/private_02147.wav  \n","  inflating: /content/data/private_test/private_12354.wav  \n","  inflating: /content/data/private_test/private_05628.wav  \n","  inflating: /content/data/private_test/private_06321.wav  \n","  inflating: /content/data/private_test/private_00988.wav  \n","  inflating: /content/data/private_test/private_09012.wav  \n","  inflating: /content/data/private_test/private_16132.wav  \n","  inflating: /content/data/private_test/private_19201.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19201.wav  \n","  inflating: /content/data/private_test/private_00750.wav  \n","  inflating: /content/data/private_test/private_11885.wav  \n","  inflating: /content/data/private_test/private_10543.wav  \n","  inflating: /content/data/private_test/private_06335.wav  \n","  inflating: /content/data/private_test/private_09006.wav  \n","  inflating: /content/data/private_test/private_11649.wav  \n","  inflating: /content/data/private_test/private_16126.wav  \n","  inflating: /content/data/private_test/private_19215.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19215.wav  \n","  inflating: /content/data/private_test/private_17238.wav  \n","  inflating: /content/data/private_test/private_11891.wav  \n","  inflating: /content/data/private_test/private_00744.wav  \n","  inflating: /content/data/private_test/private_10557.wav  \n","  inflating: /content/data/private_test/private_08318.wav  \n","  inflating: /content/data/private_test/private_04522.wav  \n","  inflating: /content/data/private_test/private_14731.wav  \n","  inflating: /content/data/private_test/private_02153.wav  \n","  inflating: /content/data/private_test/private_12340.wav  \n","  inflating: /content/data/private_test/private_12426.wav  \n","  inflating: /content/data/private_test/private_02635.wav  \n","  inflating: /content/data/private_test/private_15349.wav  \n","  inflating: /content/data/private_test/private_14057.wav  \n","  inflating: /content/data/private_test/private_13738.wav  \n","  inflating: /content/data/private_test/private_04244.wav  \n","  inflating: /content/data/private_test/private_16898.wav  \n","  inflating: /content/data/private_test/private_10231.wav  \n","  inflating: /content/data/private_test/private_00022.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00022.wav  \n","  inflating: /content/data/private_test/private_07995.wav  \n","  inflating: /content/data/private_test/private_16640.wav  \n","  inflating: /content/data/private_test/private_19573.wav  \n","  inflating: /content/data/private_test/private_06453.wav  \n","  inflating: /content/data/private_test/private_09760.wav  \n","  inflating: /content/data/private_test/private_05172.wav  \n","  inflating: /content/data/private_test/private_15361.wav  \n","  inflating: /content/data/private_test/private_03503.wav  \n","  inflating: /content/data/private_test/private_13710.wav  \n","  inflating: /content/data/private_test/private_08456.wav  \n","  inflating: /content/data/private_test/private_10219.wav  \n","  inflating: /content/data/private_test/private_07765.wav  \n","  inflating: /content/data/private_test/private_09990.wav  \n","  inflating: /content/data/private_test/private_18645.wav  \n","  inflating: /content/data/private_test/private_17576.wav  \n","  inflating: /content/data/private_test/private_01314.wav  \n","  inflating: /content/data/private_test/private_16668.wav  \n","  inflating: /content/data/private_test/private_11107.wav  \n","  inflating: /content/data/private_test/private_09748.wav  \n","  inflating: /content/data/private_test/private_11661.wav  \n","  inflating: /content/data/private_test/private_01472.wav  \n","  inflating: /content/data/private_test/private_18123.wav  \n","  inflating: /content/data/private_test/private_17210.wav  \n","  inflating: /content/data/private_test/private_08330.wav  \n","  inflating: /content/data/private_test/private_07003.wav  \n","  inflating: /content/data/private_test/private_13076.wav  \n","  inflating: /content/data/private_test/private_14719.wav  \n","  inflating: /content/data/private_test/private_03265.wav  \n","  inflating: /content/data/private_test/private_15407.wav  \n","  inflating: /content/data/private_test/private_05614.wav  \n","  inflating: /content/data/private_test/private_12368.wav  \n","  inflating: /content/data/private_test/private_13062.wav  \n","  inflating: /content/data/private_test/private_03271.wav  \n","  inflating: /content/data/private_test/private_15413.wav  \n","  inflating: /content/data/private_test/private_05600.wav  \n","  inflating: /content/data/private_test/private_11675.wav  \n","  inflating: /content/data/private_test/private_06309.wav  \n","  inflating: /content/data/private_test/private_01466.wav  \n","  inflating: /content/data/private_test/private_19229.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19229.wav  \n","  inflating: /content/data/private_test/private_18137.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18137.wav  \n","  inflating: /content/data/private_test/private_00778.wav  \n","  inflating: /content/data/private_test/private_17204.wav  \n","  inflating: /content/data/private_test/private_08324.wav  \n","  inflating: /content/data/private_test/private_07017.wav  \n","  inflating: /content/data/private_test/private_08442.wav  \n","  inflating: /content/data/private_test/private_07771.wav  \n","  inflating: /content/data/private_test/private_18651.wav  \n","  inflating: /content/data/private_test/private_09984.wav  \n","  inflating: /content/data/private_test/private_17562.wav  \n","  inflating: /content/data/private_test/private_01300.wav  \n","  inflating: /content/data/private_test/private_18889.wav  \n","  inflating: /content/data/private_test/private_11113.wav  \n","  inflating: /content/data/private_test/private_05166.wav  \n","  inflating: /content/data/private_test/private_15375.wav  \n","  inflating: /content/data/private_test/private_02609.wav  \n","  inflating: /content/data/private_test/private_03517.wav  \n","  inflating: /content/data/private_test/private_04278.wav  \n","  inflating: /content/data/private_test/private_13704.wav  \n","  inflating: /content/data/private_test/private_14080.wav  \n","  inflating: /content/data/private_test/private_04293.wav  \n","  inflating: /content/data/private_test/private_13937.wav  \n","  inflating: /content/data/private_test/private_16697.wav  \n","  inflating: /content/data/private_test/private_07942.wav  \n","  inflating: /content/data/private_test/private_06484.wav  \n","  inflating: /content/data/private_test/private_18862.wav  \n","  inflating: /content/data/private_test/private_17589.wav  \n","  inflating: /content/data/private_test/private_00793.wav  \n","  inflating: /content/data/private_test/private_11846.wav  \n","  inflating: /content/data/private_test/private_10580.wav  \n","  inflating: /content/data/private_test/private_02184.wav  \n","  inflating: /content/data/private_test/private_12397.wav  \n","  inflating: /content/data/private_test/private_13089.wav  \n","  inflating: /content/data/private_test/private_05833.wav  \n","  inflating: /content/data/private_test/private_02190.wav  \n","  inflating: /content/data/private_test/private_12383.wav  \n","  inflating: /content/data/private_test/private_05827.wav  \n","  inflating: /content/data/private_test/private_11852.wav  \n","  inflating: /content/data/private_test/private_00787.wav  \n","  inflating: /content/data/private_test/private_10594.wav  \n","  inflating: /content/data/private_test/private_01499.wav  \n","  inflating: /content/data/private_test/private_07956.wav  \n","  inflating: /content/data/private_test/private_16683.wav  \n","  inflating: /content/data/private_test/private_06490.wav  \n","  inflating: /content/data/private_test/private_18876.wav  \n","  inflating: /content/data/private_test/private_14094.wav  \n","  inflating: /content/data/private_test/private_04287.wav  \n","  inflating: /content/data/private_test/private_05199.wav  \n","  inflating: /content/data/private_test/private_13923.wav  \n","  inflating: /content/data/private_test/private_19598.wav  \n","  inflating: /content/data/private_test/private_08495.wav  \n","  inflating: /content/data/private_test/private_16873.wav  \n","  inflating: /content/data/private_test/private_09953.wav  \n","  inflating: /content/data/private_test/private_18686.wav  \n","  inflating: /content/data/private_test/private_02806.wav  \n","  inflating: /content/data/private_test/private_14902.wav  \n","  inflating: /content/data/private_test/private_00977.wav  \n","  inflating: /content/data/private_test/private_00963.wav  \n","  inflating: /content/data/private_test/private_14916.wav  \n","  inflating: /content/data/private_test/private_02812.wav  \n","  inflating: /content/data/private_test/private_08481.wav  \n","  inflating: /content/data/private_test/private_16867.wav  \n","  inflating: /content/data/private_test/private_18692.wav  \n","  inflating: /content/data/private_test/private_09947.wav  \n","  inflating: /content/data/private_test/private_14917.wav  \n","  inflating: /content/data/private_test/private_00962.wav  \n","  inflating: /content/data/private_test/private_09946.wav  \n","  inflating: /content/data/private_test/private_18693.wav  \n","  inflating: /content/data/private_test/private_16866.wav  \n","  inflating: /content/data/private_test/private_08480.wav  \n","  inflating: /content/data/private_test/private_02813.wav  \n","  inflating: /content/data/private_test/private_02807.wav  \n","  inflating: /content/data/private_test/private_18687.wav  \n","  inflating: /content/data/private_test/private_09952.wav  \n","  inflating: /content/data/private_test/private_16872.wav  \n","  inflating: /content/data/private_test/private_08494.wav  \n","  inflating: /content/data/private_test/private_19599.wav  \n","  inflating: /content/data/private_test/private_00976.wav  \n","  inflating: /content/data/private_test/private_14903.wav  \n","  inflating: /content/data/private_test/private_01498.wav  \n","  inflating: /content/data/private_test/private_10595.wav  \n","  inflating: /content/data/private_test/private_00786.wav  \n","  inflating: /content/data/private_test/private_11853.wav  \n","  inflating: /content/data/private_test/private_05826.wav  \n","  inflating: /content/data/private_test/private_12382.wav  \n","  inflating: /content/data/private_test/private_02191.wav  \n","  inflating: /content/data/private_test/private_13922.wav  \n","  inflating: /content/data/private_test/private_05198.wav  \n","  inflating: /content/data/private_test/private_04286.wav  \n","  inflating: /content/data/private_test/private_14095.wav  \n","  inflating: /content/data/private_test/private_18877.wav  \n","  inflating: /content/data/private_test/private_06491.wav  \n","  inflating: /content/data/private_test/private_16682.wav  \n","  inflating: /content/data/private_test/private_07957.wav  \n","  inflating: /content/data/private_test/private_17588.wav  \n","  inflating: /content/data/private_test/private_18863.wav  \n","  inflating: /content/data/private_test/private_06485.wav  \n","  inflating: /content/data/private_test/private_07943.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_07943.wav  \n","  inflating: /content/data/private_test/private_16696.wav  \n","  inflating: /content/data/private_test/private_13936.wav  \n","  inflating: /content/data/private_test/private_04292.wav  \n","  inflating: /content/data/private_test/private_14081.wav  \n","  inflating: /content/data/private_test/private_05832.wav  \n","  inflating: /content/data/private_test/private_13088.wav  \n","  inflating: /content/data/private_test/private_12396.wav  \n","  inflating: /content/data/private_test/private_02185.wav  \n","  inflating: /content/data/private_test/private_10581.wav  \n","  inflating: /content/data/private_test/private_11847.wav  \n","  inflating: /content/data/private_test/private_00792.wav  \n","  inflating: /content/data/private_test/private_07016.wav  \n","  inflating: /content/data/private_test/private_08325.wav  \n","  inflating: /content/data/private_test/private_17205.wav  \n","  inflating: /content/data/private_test/private_18136.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18136.wav  \n","  inflating: /content/data/private_test/private_00779.wav  \n","  inflating: /content/data/private_test/private_01467.wav  \n","  inflating: /content/data/private_test/private_19228.wav  \n","  inflating: /content/data/private_test/private_06308.wav  \n","  inflating: /content/data/private_test/private_11674.wav  \n","  inflating: /content/data/private_test/private_05601.wav  \n","  inflating: /content/data/private_test/private_15412.wav  \n","  inflating: /content/data/private_test/private_03270.wav  \n","  inflating: /content/data/private_test/private_13063.wav  \n","  inflating: /content/data/private_test/private_13705.wav  \n","  inflating: /content/data/private_test/private_04279.wav  \n","  inflating: /content/data/private_test/private_03516.wav  \n","  inflating: /content/data/private_test/private_02608.wav  \n","  inflating: /content/data/private_test/private_15374.wav  \n","  inflating: /content/data/private_test/private_05167.wav  \n","  inflating: /content/data/private_test/private_18888.wav  \n","  inflating: /content/data/private_test/private_11112.wav  \n","  inflating: /content/data/private_test/private_01301.wav  \n","  inflating: /content/data/private_test/private_17563.wav  \n","  inflating: /content/data/private_test/private_09985.wav  \n","  inflating: /content/data/private_test/private_18650.wav  \n","  inflating: /content/data/private_test/private_07770.wav  \n","  inflating: /content/data/private_test/private_08443.wav  \n","  inflating: /content/data/private_test/private_11106.wav  \n","  inflating: /content/data/private_test/private_09749.wav  \n","  inflating: /content/data/private_test/private_16669.wav  \n","  inflating: /content/data/private_test/private_01315.wav  \n","  inflating: /content/data/private_test/private_17577.wav  \n","  inflating: /content/data/private_test/private_18644.wav  \n","  inflating: /content/data/private_test/private_09991.wav  \n","  inflating: /content/data/private_test/private_07764.wav  \n","  inflating: /content/data/private_test/private_08457.wav  \n","  inflating: /content/data/private_test/private_10218.wav  \n","  inflating: /content/data/private_test/private_13711.wav  \n","  inflating: /content/data/private_test/private_03502.wav  \n","  inflating: /content/data/private_test/private_15360.wav  \n","  inflating: /content/data/private_test/private_05173.wav  \n","  inflating: /content/data/private_test/private_12369.wav  \n","  inflating: /content/data/private_test/private_05615.wav  \n","  inflating: /content/data/private_test/private_15406.wav  \n","  inflating: /content/data/private_test/private_03264.wav  \n","  inflating: /content/data/private_test/private_14718.wav  \n","  inflating: /content/data/private_test/private_13077.wav  \n","  inflating: /content/data/private_test/private_07002.wav  \n","  inflating: /content/data/private_test/private_08331.wav  \n","  inflating: /content/data/private_test/private_17211.wav  \n","  inflating: /content/data/private_test/private_18122.wav  \n","  inflating: /content/data/private_test/private_01473.wav  \n","  inflating: /content/data/private_test/private_11660.wav  \n","  inflating: /content/data/private_test/private_12341.wav  \n","  inflating: /content/data/private_test/private_02152.wav  \n","  inflating: /content/data/private_test/private_14730.wav  \n","  inflating: /content/data/private_test/private_04523.wav  \n","  inflating: /content/data/private_test/private_10556.wav  \n","  inflating: /content/data/private_test/private_08319.wav  \n","  inflating: /content/data/private_test/private_00745.wav  \n","  inflating: /content/data/private_test/private_11890.wav  \n","  inflating: /content/data/private_test/private_17239.wav  \n","  inflating: /content/data/private_test/private_19214.wav  \n","  inflating: /content/data/private_test/private_16127.wav  \n","  inflating: /content/data/private_test/private_09007.wav  \n","  inflating: /content/data/private_test/private_11648.wav  \n","  inflating: /content/data/private_test/private_06334.wav  \n","  inflating: /content/data/private_test/private_09761.wav  \n","  inflating: /content/data/private_test/private_06452.wav  \n","  inflating: /content/data/private_test/private_19572.wav  \n","  inflating: /content/data/private_test/private_16641.wav  \n","  inflating: /content/data/private_test/private_07994.wav  \n","  inflating: /content/data/private_test/private_00023.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00023.wav  \n","  inflating: /content/data/private_test/private_10230.wav  \n","  inflating: /content/data/private_test/private_16899.wav  \n","  inflating: /content/data/private_test/private_04245.wav  \n","  inflating: /content/data/private_test/private_13739.wav  \n","  inflating: /content/data/private_test/private_14056.wav  \n","  inflating: /content/data/private_test/private_15348.wav  \n","  inflating: /content/data/private_test/private_02634.wav  \n","  inflating: /content/data/private_test/private_12427.wav  \n","  inflating: /content/data/private_test/private_04251.wav  \n","  inflating: /content/data/private_test/private_14042.wav  \n","  inflating: /content/data/private_test/private_02620.wav  \n","  inflating: /content/data/private_test/private_12433.wav  \n","  inflating: /content/data/private_test/private_09775.wav  \n","  inflating: /content/data/private_test/private_06446.wav  \n","  inflating: /content/data/private_test/private_19566.wav  \n","  inflating: /content/data/private_test/private_01329.wav  \n","  inflating: /content/data/private_test/private_07980.wav  \n","  inflating: /content/data/private_test/private_16655.wav  \n","  inflating: /content/data/private_test/private_00037.wav  \n","  inflating: /content/data/private_test/private_18678.wav  \n","  inflating: /content/data/private_test/private_10224.wav  \n","  inflating: /content/data/private_test/private_07758.wav  \n","  inflating: /content/data/private_test/private_10542.wav  \n","  inflating: /content/data/private_test/private_11884.wav  \n","  inflating: /content/data/private_test/private_00751.wav  \n","  inflating: /content/data/private_test/private_19200.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19200.wav  \n","  inflating: /content/data/private_test/private_16133.wav  \n","  inflating: /content/data/private_test/private_00989.wav  \n","  inflating: /content/data/private_test/private_09013.wav  \n","  inflating: /content/data/private_test/private_06320.wav  \n","  inflating: /content/data/private_test/private_05629.wav  \n","  inflating: /content/data/private_test/private_12355.wav  \n","  inflating: /content/data/private_test/private_02146.wav  \n","  inflating: /content/data/private_test/private_14724.wav  \n","  inflating: /content/data/private_test/private_03258.wav  \n","  inflating: /content/data/private_test/private_04537.wav  \n","  inflating: /content/data/private_test/private_12802.wav  \n","  inflating: /content/data/private_test/private_19957.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19957.wav  \n","  inflating: /content/data/private_test/private_08682.wav  \n","  inflating: /content/data/private_test/private_18491.wav  \n","  inflating: /content/data/private_test/private_06877.wav  \n","  inflating: /content/data/private_test/private_10973.wav  \n","  inflating: /content/data/private_test/private_04906.wav  \n","  inflating: /content/data/private_test/private_04912.wav  \n","  inflating: /content/data/private_test/private_10967.wav  \n","  inflating: /content/data/private_test/private_09588.wav  \n","  inflating: /content/data/private_test/private_08696.wav  \n","  inflating: /content/data/private_test/private_19943.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19943.wav  \n","  inflating: /content/data/private_test/private_18485.wav  \n","  inflating: /content/data/private_test/private_06863.wav  \n","  inflating: /content/data/private_test/private_12816.wav  \n","  inflating: /content/data/private_test/private_16480.wav  \n","  inflating: /content/data/private_test/private_08866.wav  \n","  inflating: /content/data/private_test/private_06693.wav  \n","  inflating: /content/data/private_test/private_17946.wav  \n","  inflating: /content/data/private_test/private_14297.wav  \n","  inflating: /content/data/private_test/private_04084.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_04084.wav  \n","  inflating: /content/data/private_test/private_03933.wav  \n","  inflating: /content/data/private_test/private_15189.wav  \n","  inflating: /content/data/private_test/private_02393.wav  \n","  inflating: /content/data/private_test/private_12180.wav  \n","  inflating: /content/data/private_test/private_15837.wav  \n","  inflating: /content/data/private_test/private_00584.wav  \n","  inflating: /content/data/private_test/private_10797.wav  \n","  inflating: /content/data/private_test/private_01842.wav  \n","  inflating: /content/data/private_test/private_11489.wav  \n","  inflating: /content/data/private_test/private_00590.wav  \n","  inflating: /content/data/private_test/private_01856.wav  \n","  inflating: /content/data/private_test/private_10783.wav  \n","  inflating: /content/data/private_test/private_02387.wav  \n","  inflating: /content/data/private_test/private_12194.wav  \n","  inflating: /content/data/private_test/private_15823.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15823.wav  \n","  inflating: /content/data/private_test/private_03099.wav  \n","  inflating: /content/data/private_test/private_14283.wav  \n","  inflating: /content/data/private_test/private_04090.wav  \n","  inflating: /content/data/private_test/private_03927.wav  \n","  inflating: /content/data/private_test/private_16494.wav  \n","  inflating: /content/data/private_test/private_08872.wav  \n","  inflating: /content/data/private_test/private_17952.wav  \n","  inflating: /content/data/private_test/private_06687.wav  \n","  inflating: /content/data/private_test/private_07599.wav  \n","  inflating: /content/data/private_test/private_19994.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19994.wav  \n","  inflating: /content/data/private_test/private_08641.wav  \n","  inflating: /content/data/private_test/private_07572.wav  \n","  inflating: /content/data/private_test/private_18452.wav  \n","  inflating: /content/data/private_test/private_17761.wav  \n","  inflating: /content/data/private_test/private_08899.wav  \n","  inflating: /content/data/private_test/private_01103.wav  \n","  inflating: /content/data/private_test/private_11310.wav  \n","  inflating: /content/data/private_test/private_05365.wav  \n","  inflating: /content/data/private_test/private_12619.wav  \n","  inflating: /content/data/private_test/private_15176.wav  \n","  inflating: /content/data/private_test/private_14268.wav  \n","  inflating: /content/data/private_test/private_03714.wav  \n","  inflating: /content/data/private_test/private_13507.wav  \n","  inflating: /content/data/private_test/private_13261.wav  \n","  inflating: /content/data/private_test/private_03072.wav  \n","  inflating: /content/data/private_test/private_15610.wav  \n","  inflating: /content/data/private_test/private_05403.wav  \n","  inflating: /content/data/private_test/private_11476.wav  \n","  inflating: /content/data/private_test/private_09239.wav  \n","  inflating: /content/data/private_test/private_01665.wav  \n","  inflating: /content/data/private_test/private_16319.wav  \n","  inflating: /content/data/private_test/private_18334.wav  \n","  inflating: /content/data/private_test/private_17007.wav  \n","  inflating: /content/data/private_test/private_08127.wav  \n","  inflating: /content/data/private_test/private_10768.wav  \n","  inflating: /content/data/private_test/private_07214.wav  \n","  inflating: /content/data/private_test/private_11462.wav  \n","  inflating: /content/data/private_test/private_01671.wav  \n","  inflating: /content/data/private_test/private_18320.wav  \n","  inflating: /content/data/private_test/private_17013.wav  \n","  inflating: /content/data/private_test/private_08133.wav  \n","  inflating: /content/data/private_test/private_07200.wav  \n","  inflating: /content/data/private_test/private_04709.wav  \n","  inflating: /content/data/private_test/private_13275.wav  \n","  inflating: /content/data/private_test/private_03066.wav  \n","  inflating: /content/data/private_test/private_15604.wav  \n","  inflating: /content/data/private_test/private_02378.wav  \n","  inflating: /content/data/private_test/private_05417.wav  \n","  inflating: /content/data/private_test/private_05371.wav  \n","  inflating: /content/data/private_test/private_15162.wav  \n","  inflating: /content/data/private_test/private_03700.wav  \n","  inflating: /content/data/private_test/private_13513.wav  \n","  inflating: /content/data/private_test/private_08655.wav  \n","  inflating: /content/data/private_test/private_19980.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19980.wav  \n","  inflating: /content/data/private_test/private_07566.wav  \n","  inflating: /content/data/private_test/private_18446.wav  \n","  inflating: /content/data/private_test/private_00209.wav  \n","  inflating: /content/data/private_test/private_17775.wav  \n","  inflating: /content/data/private_test/private_01117.wav  \n","  inflating: /content/data/private_test/private_19758.wav  \n","  inflating: /content/data/private_test/private_11304.wav  \n","  inflating: /content/data/private_test/private_06678.wav  \n","  inflating: /content/data/private_test/private_12625.wav  \n","  inflating: /content/data/private_test/private_05359.wav  \n","  inflating: /content/data/private_test/private_02436.wav  \n","  inflating: /content/data/private_test/private_03728.wav  \n","  inflating: /content/data/private_test/private_14254.wav  \n","  inflating: /content/data/private_test/private_04047.wav  \n","  inflating: /content/data/private_test/private_10032.wav  \n","  inflating: /content/data/private_test/private_06888.wav  \n","  inflating: /content/data/private_test/private_00221.wav  \n","  inflating: /content/data/private_test/private_16443.wav  \n","  inflating: /content/data/private_test/private_19770.wav  \n","  inflating: /content/data/private_test/private_06650.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06650.wav  \n","  inflating: /content/data/private_test/private_17985.wav  \n","  inflating: /content/data/private_test/private_09563.wav  \n","  inflating: /content/data/private_test/private_06136.wav  \n","  inflating: /content/data/private_test/private_09205.wav  \n","  inflating: /content/data/private_test/private_16325.wav  \n","  inflating: /content/data/private_test/private_19016.wav  \n","  inflating: /content/data/private_test/private_01659.wav  \n","  inflating: /content/data/private_test/private_00547.wav  \n","  inflating: /content/data/private_test/private_18308.wav  \n","  inflating: /content/data/private_test/private_07228.wav  \n","  inflating: /content/data/private_test/private_10754.wav  \n","  inflating: /content/data/private_test/private_01881.wav  \n","  inflating: /content/data/private_test/private_04721.wav  \n","  inflating: /content/data/private_test/private_14532.wav  \n","  inflating: /content/data/private_test/private_02350.wav  \n","  inflating: /content/data/private_test/private_12143.wav  \n","  inflating: /content/data/private_test/private_13249.wav  \n","  inflating: /content/data/private_test/private_04735.wav  \n","  inflating: /content/data/private_test/private_14526.wav  \n","  inflating: /content/data/private_test/private_02344.wav  \n","  inflating: /content/data/private_test/private_15638.wav  \n","  inflating: /content/data/private_test/private_12157.wav  \n","  inflating: /content/data/private_test/private_06122.wav  \n","  inflating: /content/data/private_test/private_09211.wav  \n","  inflating: /content/data/private_test/private_16331.wav  \n","  inflating: /content/data/private_test/private_10998.wav  \n","  inflating: /content/data/private_test/private_19002.wav  \n","  inflating: /content/data/private_test/private_00553.wav  \n","  inflating: /content/data/private_test/private_01895.wav  \n","  inflating: /content/data/private_test/private_10740.wav  \n","  inflating: /content/data/private_test/private_10026.wav  \n","  inflating: /content/data/private_test/private_08669.wav  \n","  inflating: /content/data/private_test/private_17749.wav  \n","  inflating: /content/data/private_test/private_00235.wav  \n","  inflating: /content/data/private_test/private_16457.wav  \n","  inflating: /content/data/private_test/private_19764.wav  \n","  inflating: /content/data/private_test/private_17991.wav  \n","  inflating: /content/data/private_test/private_06644.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06644.wav  \n","  inflating: /content/data/private_test/private_09577.wav  \n","  inflating: /content/data/private_test/private_11338.wav  \n","  inflating: /content/data/private_test/private_12631.wav  \n","  inflating: /content/data/private_test/private_02422.wav  \n","  inflating: /content/data/private_test/private_14240.wav  \n","  inflating: /content/data/private_test/private_04053.wav  \n","  inflating: /content/data/private_test/private_12976.wav  \n","  inflating: /content/data/private_test/private_06903.wav  \n","  inflating: /content/data/private_test/private_19823.wav  \n","  inflating: /content/data/private_test/private_10807.wav  \n","  inflating: /content/data/private_test/private_08090.wav  \n","  inflating: /content/data/private_test/private_18283.wav  \n","  inflating: /content/data/private_test/private_04872.wav  \n","  inflating: /content/data/private_test/private_04866.wav  \n","  inflating: /content/data/private_test/private_10813.wav  \n","  inflating: /content/data/private_test/private_19189.wav  \n","  inflating: /content/data/private_test/private_08084.wav  \n","  inflating: /content/data/private_test/private_18297.wav  \n","  inflating: /content/data/private_test/private_06917.wav  \n","  inflating: /content/data/private_test/private_19837.wav  \n","  inflating: /content/data/private_test/private_12962.wav  \n","  inflating: /content/data/private_test/private_00396.wav  \n","  inflating: /content/data/private_test/private_10185.wav  \n","  inflating: /content/data/private_test/private_17832.wav  \n","  inflating: /content/data/private_test/private_08912.wav  \n","  inflating: /content/data/private_test/private_01088.wav  \n","  inflating: /content/data/private_test/private_02581.wav  \n","  inflating: /content/data/private_test/private_12792.wav  \n","  inflating: /content/data/private_test/private_03847.wav  \n","  inflating: /content/data/private_test/private_14485.wav  \n","  inflating: /content/data/private_test/private_04696.wav  \n","  inflating: /content/data/private_test/private_15943.wav  \n","  inflating: /content/data/private_test/private_05588.wav  \n","  inflating: /content/data/private_test/private_16292.wav  \n","  inflating: /content/data/private_test/private_06081.wav  \n","  inflating: /content/data/private_test/private_01936.wav  \n","  inflating: /content/data/private_test/private_16286.wav  \n","  inflating: /content/data/private_test/private_06095.wav  \n","  inflating: /content/data/private_test/private_01922.wav  \n","  inflating: /content/data/private_test/private_17198.wav  \n","  inflating: /content/data/private_test/private_14491.wav  \n","  inflating: /content/data/private_test/private_15957.wav  \n","  inflating: /content/data/private_test/private_04682.wav  \n","  inflating: /content/data/private_test/private_02595.wav  \n","  inflating: /content/data/private_test/private_03853.wav  \n","  inflating: /content/data/private_test/private_12786.wav  \n","  inflating: /content/data/private_test/private_13498.wav  \n","  inflating: /content/data/private_test/private_00382.wav  \n","  inflating: /content/data/private_test/private_10191.wav  \n","  inflating: /content/data/private_test/private_17826.wav  \n","  inflating: /content/data/private_test/private_08906.wav  \n","  inflating: /content/data/private_test/private_11264.wav  \n","  inflating: /content/data/private_test/private_06718.wav  \n","  inflating: /content/data/private_test/private_01077.wav  \n","  inflating: /content/data/private_test/private_19638.wav  \n","  inflating: /content/data/private_test/private_18526.wav  \n","  inflating: /content/data/private_test/private_00369.wav  \n","  inflating: /content/data/private_test/private_17615.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_17615.wav  \n","  inflating: /content/data/private_test/private_08735.wav  \n","  inflating: /content/data/private_test/private_07406.wav  \n","  inflating: /content/data/private_test/private_13473.wav  \n","  inflating: /content/data/private_test/private_03660.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03660.wav  \n","  inflating: /content/data/private_test/private_15002.wav  \n","  inflating: /content/data/private_test/private_05211.wav  \n","  inflating: /content/data/private_test/private_05577.wav  \n","  inflating: /content/data/private_test/private_15764.wav  \n","  inflating: /content/data/private_test/private_02218.wav  \n","  inflating: /content/data/private_test/private_03106.wav  \n","  inflating: /content/data/private_test/private_04669.wav  \n","  inflating: /content/data/private_test/private_13315.wav  \n","  inflating: /content/data/private_test/private_08053.wav  \n","  inflating: /content/data/private_test/private_07360.wav  \n","  inflating: /content/data/private_test/private_18240.wav  \n","  inflating: /content/data/private_test/private_17173.wav  \n","  inflating: /content/data/private_test/private_01711.wav  \n","  inflating: /content/data/private_test/private_11502.wav  \n","  inflating: /content/data/private_test/private_08047.wav  \n","  inflating: /content/data/private_test/private_10608.wav  \n","  inflating: /content/data/private_test/private_07374.wav  \n","  inflating: /content/data/private_test/private_18254.wav  \n","  inflating: /content/data/private_test/private_17167.wav  \n","  inflating: /content/data/private_test/private_01705.wav  \n","  inflating: /content/data/private_test/private_16279.wav  \n","  inflating: /content/data/private_test/private_11516.wav  \n","  inflating: /content/data/private_test/private_09359.wav  \n","  inflating: /content/data/private_test/private_05563.wav  \n","  inflating: /content/data/private_test/private_15770.wav  \n","  inflating: /content/data/private_test/private_03112.wav  \n","  inflating: /content/data/private_test/private_13301.wav  \n","  inflating: /content/data/private_test/private_13467.wav  \n","  inflating: /content/data/private_test/private_14308.wav  \n","  inflating: /content/data/private_test/private_03674.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03674.wav  \n","  inflating: /content/data/private_test/private_15016.wav  \n","  inflating: /content/data/private_test/private_05205.wav  \n","  inflating: /content/data/private_test/private_12779.wav  \n","  inflating: /content/data/private_test/private_11270.wav  \n","  inflating: /content/data/private_test/private_01063.wav  \n","  inflating: /content/data/private_test/private_18532.wav  \n","  inflating: /content/data/private_test/private_17601.wav  \n","  inflating: /content/data/private_test/private_08721.wav  \n","  inflating: /content/data/private_test/private_07412.wav  \n","  inflating: /content/data/private_test/private_04133.wav  \n","  inflating: /content/data/private_test/private_12989.wav  \n","  inflating: /content/data/private_test/private_14320.wav  \n","  inflating: /content/data/private_test/private_02542.wav  \n","  inflating: /content/data/private_test/private_12751.wav  \n","  inflating: /content/data/private_test/private_03884.wav  \n","  inflating: /content/data/private_test/private_06724.wav  \n","  inflating: /content/data/private_test/private_09417.wav  \n","  inflating: /content/data/private_test/private_11258.wav  \n","  inflating: /content/data/private_test/private_16537.wav  \n","  inflating: /content/data/private_test/private_19604.wav  \n","  inflating: /content/data/private_test/private_17629.wav  \n","  inflating: /content/data/private_test/private_00355.wav  \n","  inflating: /content/data/private_test/private_10146.wav  \n","  inflating: /content/data/private_test/private_08709.wav  \n","  inflating: /content/data/private_test/private_10620.wav  \n","  inflating: /content/data/private_test/private_00433.wav  \n","  inflating: /content/data/private_test/private_16251.wav  \n","  inflating: /content/data/private_test/private_19162.wav  \n","  inflating: /content/data/private_test/private_06042.wav  \n","  inflating: /content/data/private_test/private_09371.wav  \n","  inflating: /content/data/private_test/private_12037.wav  \n","  inflating: /content/data/private_test/private_02224.wav  \n","  inflating: /content/data/private_test/private_15758.wav  \n","  inflating: /content/data/private_test/private_14446.wav  \n","  inflating: /content/data/private_test/private_13329.wav  \n","  inflating: /content/data/private_test/private_04655.wav  \n","  inflating: /content/data/private_test/private_15980.wav  \n","  inflating: /content/data/private_test/private_12023.wav  \n","  inflating: /content/data/private_test/private_02230.wav  \n","  inflating: /content/data/private_test/private_04899.wav  \n","  inflating: /content/data/private_test/private_14452.wav  \n","  inflating: /content/data/private_test/private_15994.wav  \n","  inflating: /content/data/private_test/private_04641.wav  \n","  inflating: /content/data/private_test/private_07348.wav  \n","  inflating: /content/data/private_test/private_10634.wav  \n","  inflating: /content/data/private_test/private_00427.wav  \n","  inflating: /content/data/private_test/private_18268.wav  \n","  inflating: /content/data/private_test/private_16245.wav  \n","  inflating: /content/data/private_test/private_19176.wav  \n","  inflating: /content/data/private_test/private_01739.wav  \n","  inflating: /content/data/private_test/private_06056.wav  \n","  inflating: /content/data/private_test/private_09365.wav  \n","  inflating: /content/data/private_test/private_06730.wav  \n","  inflating: /content/data/private_test/private_09403.wav  \n","  inflating: /content/data/private_test/private_16523.wav  \n","  inflating: /content/data/private_test/private_19610.wav  \n","  inflating: /content/data/private_test/private_00341.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00341.wav  \n","  inflating: /content/data/private_test/private_10152.wav  \n","  inflating: /content/data/private_test/private_04127.wav  \n","  inflating: /content/data/private_test/private_03648.wav  \n","  inflating: /content/data/private_test/private_14334.wav  \n","  inflating: /content/data/private_test/private_02556.wav  \n","  inflating: /content/data/private_test/private_03890.wav  \n","  inflating: /content/data/private_test/private_12745.wav  \n","  inflating: /content/data/private_test/private_05239.wav  \n","  inflating: /content/data/private_test/private_14863.wav  \n","  inflating: /content/data/private_test/private_18081.wav  \n","  inflating: /content/data/private_test/private_08292.wav  \n","  inflating: /content/data/private_test/private_00816.wav  \n","  inflating: /content/data/private_test/private_16912.wav  \n","  inflating: /content/data/private_test/private_09832.wav  \n","  inflating: /content/data/private_test/private_02967.wav  \n","  inflating: /content/data/private_test/private_02973.wav  \n","  inflating: /content/data/private_test/private_16906.wav  \n","  inflating: /content/data/private_test/private_09826.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_09826.wav  \n","  inflating: /content/data/private_test/private_18095.wav  \n","  inflating: /content/data/private_test/private_08286.wav  \n","  inflating: /content/data/private_test/private_00802.wav  \n","  inflating: /content/data/private_test/private_09198.wav  \n","  inflating: /content/data/private_test/private_14877.wav  \n","  inflating: /content/data/private_test/private_11927.wav  \n","  inflating: /content/data/private_test/private_06283.wav  \n","  inflating: /content/data/private_test/private_16090.wav  \n","  inflating: /content/data/private_test/private_15599.wav  \n","  inflating: /content/data/private_test/private_04494.wav  \n","  inflating: /content/data/private_test/private_14687.wav  \n","  inflating: /content/data/private_test/private_05952.wav  \n","  inflating: /content/data/private_test/private_12590.wav  \n","  inflating: /content/data/private_test/private_02783.wav  \n","  inflating: /content/data/private_test/private_13856.wav  \n","  inflating: /content/data/private_test/private_07823.wav  \n","  inflating: /content/data/private_test/private_18903.wav  \n","  inflating: /content/data/private_test/private_11099.wav  \n","  inflating: /content/data/private_test/private_10387.wav  \n","  inflating: /content/data/private_test/private_00194.wav  \n","  inflating: /content/data/private_test/private_07837.wav  \n","  inflating: /content/data/private_test/private_18917.wav  \n","  inflating: /content/data/private_test/private_10393.wav  \n","  inflating: /content/data/private_test/private_00180.wav  \n","  inflating: /content/data/private_test/private_03489.wav  \n","  inflating: /content/data/private_test/private_12584.wav  \n","  inflating: /content/data/private_test/private_13842.wav  \n","  inflating: /content/data/private_test/private_02797.wav  \n","  inflating: /content/data/private_test/private_04480.wav  \n","  inflating: /content/data/private_test/private_05946.wav  \n","  inflating: /content/data/private_test/private_14693.wav  \n","  inflating: /content/data/private_test/private_11933.wav  \n","  inflating: /content/data/private_test/private_07189.wav  \n","  inflating: /content/data/private_test/private_06297.wav  \n","  inflating: /content/data/private_test/private_16084.wav  \n","  inflating: /content/data/private_test/private_11700.wav  \n","  inflating: /content/data/private_test/private_01513.wav  \n","  inflating: /content/data/private_test/private_17371.wav  \n","  inflating: /content/data/private_test/private_18042.wav  \n","  inflating: /content/data/private_test/private_07162.wav  \n","  inflating: /content/data/private_test/private_08251.wav  \n","  inflating: /content/data/private_test/private_13117.wav  \n","  inflating: /content/data/private_test/private_03304.wav  \n","  inflating: /content/data/private_test/private_14678.wav  \n","  inflating: /content/data/private_test/private_15566.wav  \n","  inflating: /content/data/private_test/private_12209.wav  \n","  inflating: /content/data/private_test/private_05775.wav  \n","  inflating: /content/data/private_test/private_05013.wav  \n","  inflating: /content/data/private_test/private_15200.wav  \n","  inflating: /content/data/private_test/private_03462.wav  \n","  inflating: /content/data/private_test/private_13671.wav  \n","  inflating: /content/data/private_test/private_07604.wav  \n","  inflating: /content/data/private_test/private_08537.wav  \n","  inflating: /content/data/private_test/private_10378.wav  \n","  inflating: /content/data/private_test/private_17417.wav  \n","  inflating: /content/data/private_test/private_18724.wav  \n","  inflating: /content/data/private_test/private_16709.wav  \n","  inflating: /content/data/private_test/private_01275.wav  \n","  inflating: /content/data/private_test/private_11066.wav  \n","  inflating: /content/data/private_test/private_09629.wav  \n","  inflating: /content/data/private_test/private_07610.wav  \n","  inflating: /content/data/private_test/private_08523.wav  \n","  inflating: /content/data/private_test/private_17403.wav  \n","  inflating: /content/data/private_test/private_18730.wav  \n","  inflating: /content/data/private_test/private_01261.wav  \n","  inflating: /content/data/private_test/private_11072.wav  \n","  inflating: /content/data/private_test/private_05007.wav  \n","  inflating: /content/data/private_test/private_02768.wav  \n","  inflating: /content/data/private_test/private_15214.wav  \n","  inflating: /content/data/private_test/private_03476.wav  \n","  inflating: /content/data/private_test/private_13665.wav  \n","  inflating: /content/data/private_test/private_04319.wav  \n","  inflating: /content/data/private_test/private_13103.wav  \n","  inflating: /content/data/private_test/private_03310.wav  \n","  inflating: /content/data/private_test/private_15572.wav  \n","  inflating: /content/data/private_test/private_05761.wav  \n","  inflating: /content/data/private_test/private_06268.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06268.wav  \n","  inflating: /content/data/private_test/private_11714.wav  \n","  inflating: /content/data/private_test/private_01507.wav  \n","  inflating: /content/data/private_test/private_19348.wav  \n","  inflating: /content/data/private_test/private_17365.wav  \n","  inflating: /content/data/private_test/private_18056.wav  \n","  inflating: /content/data/private_test/private_00619.wav  \n","  inflating: /content/data/private_test/private_07176.wav  \n","  inflating: /content/data/private_test/private_08245.wav  \n","  inflating: /content/data/private_test/private_04457.wav  \n","  inflating: /content/data/private_test/private_14644.wav  \n","  inflating: /content/data/private_test/private_05991.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_05991.wav  \n","  inflating: /content/data/private_test/private_03338.wav  \n","  inflating: /content/data/private_test/private_02026.wav  \n","  inflating: /content/data/private_test/private_05749.wav  \n","  inflating: /content/data/private_test/private_12235.wav  \n","  inflating: /content/data/private_test/private_09173.wav  \n","  inflating: /content/data/private_test/private_06240.wav  \n","  inflating: /content/data/private_test/private_19360.wav  \n","  inflating: /content/data/private_test/private_16053.wav  \n","  inflating: /content/data/private_test/private_00631.wav  \n","  inflating: /content/data/private_test/private_10422.wav  \n","  inflating: /content/data/private_test/private_10344.wav  \n","  inflating: /content/data/private_test/private_07638.wav  \n","  inflating: /content/data/private_test/private_00157.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00157.wav  \n","  inflating: /content/data/private_test/private_18718.wav  \n","  inflating: /content/data/private_test/private_19406.wav  \n","  inflating: /content/data/private_test/private_01249.wav  \n","  inflating: /content/data/private_test/private_16735.wav  \n","  inflating: /content/data/private_test/private_09615.wav  \n","  inflating: /content/data/private_test/private_06526.wav  \n","  inflating: /content/data/private_test/private_12553.wav  \n","  inflating: /content/data/private_test/private_02740.wav  \n","  inflating: /content/data/private_test/private_13895.wav  \n","  inflating: /content/data/private_test/private_14122.wav  \n","  inflating: /content/data/private_test/private_04331.wav  \n","  inflating: /content/data/private_test/private_02998.wav  \n","  inflating: /content/data/private_test/private_12547.wav  \n","  inflating: /content/data/private_test/private_15228.wav  \n","  inflating: /content/data/private_test/private_13881.wav  \n","  inflating: /content/data/private_test/private_02754.wav  \n","  inflating: /content/data/private_test/private_14136.wav  \n","  inflating: /content/data/private_test/private_04325.wav  \n","  inflating: /content/data/private_test/private_13659.wav  \n","  inflating: /content/data/private_test/private_10350.wav  \n","  inflating: /content/data/private_test/private_00143.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00143.wav  \n","  inflating: /content/data/private_test/private_19412.wav  \n","  inflating: /content/data/private_test/private_16721.wav  \n","  inflating: /content/data/private_test/private_09601.wav  \n","  inflating: /content/data/private_test/private_06532.wav  \n","  inflating: /content/data/private_test/private_09167.wav  \n","  inflating: /content/data/private_test/private_11728.wav  \n","  inflating: /content/data/private_test/private_06254.wav  \n","  inflating: /content/data/private_test/private_19374.wav  \n","  inflating: /content/data/private_test/private_16047.wav  \n","  inflating: /content/data/private_test/private_00625.wav  \n","  inflating: /content/data/private_test/private_17359.wav  \n","  inflating: /content/data/private_test/private_10436.wav  \n","  inflating: /content/data/private_test/private_08279.wav  \n","  inflating: /content/data/private_test/private_04443.wav  \n","  inflating: /content/data/private_test/private_05985.wav  \n","  inflating: /content/data/private_test/private_14650.wav  \n","  inflating: /content/data/private_test/private_02032.wav  \n","  inflating: /content/data/private_test/private_14888.wav  \n","  inflating: /content/data/private_test/private_12221.wav  \n","  inflating: /content/data/private_test/private_18052.wav  \n","  inflating: /content/data/private_test/private_17361.wav  \n","  inflating: /content/data/private_test/private_08241.wav  \n","  inflating: /content/data/private_test/private_07172.wav  \n","  inflating: /content/data/private_test/private_11710.wav  \n","  inflating: /content/data/private_test/private_01503.wav  \n","  inflating: /content/data/private_test/private_15576.wav  \n","  inflating: /content/data/private_test/private_05765.wav  \n","  inflating: /content/data/private_test/private_12219.wav  \n","  inflating: /content/data/private_test/private_13107.wav  \n","  inflating: /content/data/private_test/private_14668.wav  \n","  inflating: /content/data/private_test/private_03314.wav  \n","  inflating: /content/data/private_test/private_03472.wav  \n","  inflating: /content/data/private_test/private_13661.wav  \n","  inflating: /content/data/private_test/private_05003.wav  \n","  inflating: /content/data/private_test/private_15210.wav  \n","  inflating: /content/data/private_test/private_01265.wav  \n","  inflating: /content/data/private_test/private_16719.wav  \n","  inflating: /content/data/private_test/private_11076.wav  \n","  inflating: /content/data/private_test/private_09639.wav  \n","  inflating: /content/data/private_test/private_08527.wav  \n","  inflating: /content/data/private_test/private_10368.wav  \n","  inflating: /content/data/private_test/private_07614.wav  \n","  inflating: /content/data/private_test/private_18734.wav  \n","  inflating: /content/data/private_test/private_17407.wav  \n","  inflating: /content/data/private_test/private_01271.wav  \n","  inflating: /content/data/private_test/private_11062.wav  \n","  inflating: /content/data/private_test/private_08533.wav  \n","  inflating: /content/data/private_test/private_07600.wav  \n","  inflating: /content/data/private_test/private_18720.wav  \n","  inflating: /content/data/private_test/private_17413.wav  \n","  inflating: /content/data/private_test/private_03466.wav  \n","  inflating: /content/data/private_test/private_04309.wav  \n","  inflating: /content/data/private_test/private_13675.wav  \n","  inflating: /content/data/private_test/private_05017.wav  \n","  inflating: /content/data/private_test/private_15204.wav  \n","  inflating: /content/data/private_test/private_02778.wav  \n","  inflating: /content/data/private_test/private_15562.wav  \n","  inflating: /content/data/private_test/private_05771.wav  \n","  inflating: /content/data/private_test/private_13113.wav  \n","  inflating: /content/data/private_test/private_03300.wav  \n","  inflating: /content/data/private_test/private_18046.wav  \n","  inflating: /content/data/private_test/private_00609.wav  \n","  inflating: /content/data/private_test/private_17375.wav  \n","  inflating: /content/data/private_test/private_08255.wav  \n","  inflating: /content/data/private_test/private_07166.wav  \n","  inflating: /content/data/private_test/private_11704.wav  \n","  inflating: /content/data/private_test/private_06278.wav  \n","  inflating: /content/data/private_test/private_01517.wav  \n","  inflating: /content/data/private_test/private_19358.wav  \n","  inflating: /content/data/private_test/private_02036.wav  \n","  inflating: /content/data/private_test/private_12225.wav  \n","  inflating: /content/data/private_test/private_05759.wav  \n","  inflating: /content/data/private_test/private_04447.wav  \n","  inflating: /content/data/private_test/private_03328.wav  \n","  inflating: /content/data/private_test/private_14654.wav  \n","  inflating: /content/data/private_test/private_05981.wav  \n","  inflating: /content/data/private_test/private_00621.wav  \n","  inflating: /content/data/private_test/private_10432.wav  \n","  inflating: /content/data/private_test/private_06250.wav  \n","  inflating: /content/data/private_test/private_09163.wav  \n","  inflating: /content/data/private_test/private_16043.wav  \n","  inflating: /content/data/private_test/private_19370.wav  \n","  inflating: /content/data/private_test/private_16725.wav  \n","  inflating: /content/data/private_test/private_19416.wav  \n","  inflating: /content/data/private_test/private_01259.wav  \n","  inflating: /content/data/private_test/private_06536.wav  \n","  inflating: /content/data/private_test/private_09605.wav  \n","  inflating: /content/data/private_test/private_07628.wav  \n","  inflating: /content/data/private_test/private_10354.wav  \n","  inflating: /content/data/private_test/private_00147.wav  \n","  inflating: /content/data/private_test/private_18708.wav  \n","  inflating: /content/data/private_test/private_14132.wav  \n","  inflating: /content/data/private_test/private_02988.wav  \n","  inflating: /content/data/private_test/private_04321.wav  \n","  inflating: /content/data/private_test/private_12543.wav  \n","  inflating: /content/data/private_test/private_02750.wav  \n","  inflating: /content/data/private_test/private_13885.wav  \n","  inflating: /content/data/private_test/private_14126.wav  \n","  inflating: /content/data/private_test/private_13649.wav  \n","  inflating: /content/data/private_test/private_04335.wav  \n","  inflating: /content/data/private_test/private_12557.wav  \n","  inflating: /content/data/private_test/private_13891.wav  \n","  inflating: /content/data/private_test/private_02744.wav  \n","  inflating: /content/data/private_test/private_15238.wav  \n","  inflating: /content/data/private_test/private_16731.wav  \n","  inflating: /content/data/private_test/private_19402.wav  \n","  inflating: /content/data/private_test/private_06522.wav  \n","  inflating: /content/data/private_test/private_09611.wav  \n","  inflating: /content/data/private_test/private_10340.wav  \n","  inflating: /content/data/private_test/private_00153.wav  \n","  inflating: /content/data/private_test/private_17349.wav  \n","  inflating: /content/data/private_test/private_00635.wav  \n","  inflating: /content/data/private_test/private_10426.wav  \n","  inflating: /content/data/private_test/private_08269.wav  \n","  inflating: /content/data/private_test/private_06244.wav  \n","  inflating: /content/data/private_test/private_09177.wav  \n","  inflating: /content/data/private_test/private_11738.wav  \n","  inflating: /content/data/private_test/private_16057.wav  \n","  inflating: /content/data/private_test/private_19364.wav  \n","  inflating: /content/data/private_test/private_02022.wav  \n","  inflating: /content/data/private_test/private_12231.wav  \n","  inflating: /content/data/private_test/private_14898.wav  \n","  inflating: /content/data/private_test/private_04453.wav  \n","  inflating: /content/data/private_test/private_05995.wav  \n","  inflating: /content/data/private_test/private_14640.wav  \n","  inflating: /content/data/private_test/private_14873.wav  \n","  inflating: /content/data/private_test/private_00806.wav  \n","  inflating: /content/data/private_test/private_18091.wav  \n","  inflating: /content/data/private_test/private_08282.wav  \n","  inflating: /content/data/private_test/private_16902.wav  \n","  inflating: /content/data/private_test/private_09822.wav  \n","  inflating: /content/data/private_test/private_02977.wav  \n","  inflating: /content/data/private_test/private_02963.wav  \n","  inflating: /content/data/private_test/private_16916.wav  \n","  inflating: /content/data/private_test/private_09836.wav  \n","  inflating: /content/data/private_test/private_00812.wav  \n","  inflating: /content/data/private_test/private_09188.wav  \n","  inflating: /content/data/private_test/private_18085.wav  \n","  inflating: /content/data/private_test/private_08296.wav  \n","  inflating: /content/data/private_test/private_14867.wav  \n","  inflating: /content/data/private_test/private_06293.wav  \n","  inflating: /content/data/private_test/private_16080.wav  \n","  inflating: /content/data/private_test/private_11937.wav  \n","  inflating: /content/data/private_test/private_04484.wav  \n","  inflating: /content/data/private_test/private_14697.wav  \n","  inflating: /content/data/private_test/private_05942.wav  \n","  inflating: /content/data/private_test/private_15589.wav  \n","  inflating: /content/data/private_test/private_12580.wav  \n","  inflating: /content/data/private_test/private_02793.wav  \n","  inflating: /content/data/private_test/private_13846.wav  \n","  inflating: /content/data/private_test/private_10397.wav  \n","  inflating: /content/data/private_test/private_00184.wav  \n","  inflating: /content/data/private_test/private_07833.wav  \n","  inflating: /content/data/private_test/private_18913.wav  \n","  inflating: /content/data/private_test/private_11089.wav  \n","  inflating: /content/data/private_test/private_10383.wav  \n","  inflating: /content/data/private_test/private_00190.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00190.wav  \n","  inflating: /content/data/private_test/private_07827.wav  \n","  inflating: /content/data/private_test/private_18907.wav  \n","  inflating: /content/data/private_test/private_12594.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_12594.wav  \n","  inflating: /content/data/private_test/private_13852.wav  \n","  inflating: /content/data/private_test/private_02787.wav  \n","  inflating: /content/data/private_test/private_03499.wav  \n","  inflating: /content/data/private_test/private_04490.wav  \n","  inflating: /content/data/private_test/private_05956.wav  \n","  inflating: /content/data/private_test/private_14683.wav  \n","  inflating: /content/data/private_test/private_06287.wav  \n","  inflating: /content/data/private_test/private_16094.wav  \n","  inflating: /content/data/private_test/private_11923.wav  \n","  inflating: /content/data/private_test/private_07199.wav  \n","  inflating: /content/data/private_test/private_17605.wav  \n","  inflating: /content/data/private_test/private_18536.wav  \n","  inflating: /content/data/private_test/private_00379.wav  \n","  inflating: /content/data/private_test/private_07416.wav  \n","  inflating: /content/data/private_test/private_08725.wav  \n","  inflating: /content/data/private_test/private_06708.wav  \n","  inflating: /content/data/private_test/private_11274.wav  \n","  inflating: /content/data/private_test/private_01067.wav  \n","  inflating: /content/data/private_test/private_19628.wav  \n","  inflating: /content/data/private_test/private_15012.wav  \n","  inflating: /content/data/private_test/private_05201.wav  \n","  inflating: /content/data/private_test/private_13463.wav  \n","  inflating: /content/data/private_test/private_03670.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03670.wav  \n","  inflating: /content/data/private_test/private_03116.wav  \n","  inflating: /content/data/private_test/private_13305.wav  \n","  inflating: /content/data/private_test/private_04679.wav  \n","  inflating: /content/data/private_test/private_05567.wav  \n","  inflating: /content/data/private_test/private_02208.wav  \n","  inflating: /content/data/private_test/private_15774.wav  \n","  inflating: /content/data/private_test/private_01701.wav  \n","  inflating: /content/data/private_test/private_11512.wav  \n","  inflating: /content/data/private_test/private_07370.wav  \n","  inflating: /content/data/private_test/private_08043.wav  \n","  inflating: /content/data/private_test/private_17163.wav  \n","  inflating: /content/data/private_test/private_18250.wav  \n","  inflating: /content/data/private_test/private_16269.wav  \n","  inflating: /content/data/private_test/private_01715.wav  \n","  inflating: /content/data/private_test/private_11506.wav  \n","  inflating: /content/data/private_test/private_09349.wav  \n","  inflating: /content/data/private_test/private_07364.wav  \n","  inflating: /content/data/private_test/private_08057.wav  \n","  inflating: /content/data/private_test/private_10618.wav  \n","  inflating: /content/data/private_test/private_17177.wav  \n","  inflating: /content/data/private_test/private_18244.wav  \n","  inflating: /content/data/private_test/private_03102.wav  \n","  inflating: /content/data/private_test/private_13311.wav  \n","  inflating: /content/data/private_test/private_05573.wav  \n","  inflating: /content/data/private_test/private_15760.wav  \n","  inflating: /content/data/private_test/private_15006.wav  \n","  inflating: /content/data/private_test/private_12769.wav  \n","  inflating: /content/data/private_test/private_05215.wav  \n","  inflating: /content/data/private_test/private_13477.wav  \n","  inflating: /content/data/private_test/private_03664.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03664.wav  \n","  inflating: /content/data/private_test/private_14318.wav  \n","  inflating: /content/data/private_test/private_17611.wav  \n","  inflating: /content/data/private_test/private_18522.wav  \n","  inflating: /content/data/private_test/private_07402.wav  \n","  inflating: /content/data/private_test/private_08731.wav  \n","  inflating: /content/data/private_test/private_11260.wav  \n","  inflating: /content/data/private_test/private_01073.wav  \n","  inflating: /content/data/private_test/private_02552.wav  \n","  inflating: /content/data/private_test/private_12741.wav  \n","  inflating: /content/data/private_test/private_03894.wav  \n","  inflating: /content/data/private_test/private_04123.wav  \n","  inflating: /content/data/private_test/private_14330.wav  \n","  inflating: /content/data/private_test/private_12999.wav  \n","  inflating: /content/data/private_test/private_00345.wav  \n","  inflating: /content/data/private_test/private_17639.wav  \n","  inflating: /content/data/private_test/private_10156.wav  \n","  inflating: /content/data/private_test/private_08719.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_08719.wav  \n","  inflating: /content/data/private_test/private_09407.wav  \n","  inflating: /content/data/private_test/private_11248.wav  \n","  inflating: /content/data/private_test/private_06734.wav  \n","  inflating: /content/data/private_test/private_19614.wav  \n","  inflating: /content/data/private_test/private_16527.wav  \n","  inflating: /content/data/private_test/private_19172.wav  \n","  inflating: /content/data/private_test/private_16241.wav  \n","  inflating: /content/data/private_test/private_09361.wav  \n","  inflating: /content/data/private_test/private_06052.wav  \n","  inflating: /content/data/private_test/private_10630.wav  \n","  inflating: /content/data/private_test/private_00423.wav  \n","  inflating: /content/data/private_test/private_14456.wav  \n","  inflating: /content/data/private_test/private_04645.wav  \n","  inflating: /content/data/private_test/private_15990.wav  \n","  inflating: /content/data/private_test/private_13339.wav  \n","  inflating: /content/data/private_test/private_12027.wav  \n","  inflating: /content/data/private_test/private_15748.wav  \n","  inflating: /content/data/private_test/private_02234.wav  \n","  inflating: /content/data/private_test/private_14442.wav  \n","  inflating: /content/data/private_test/private_15984.wav  \n","  inflating: /content/data/private_test/private_04651.wav  \n","  inflating: /content/data/private_test/private_12033.wav  \n","  inflating: /content/data/private_test/private_04889.wav  \n","  inflating: /content/data/private_test/private_02220.wav  \n","  inflating: /content/data/private_test/private_19166.wav  \n","  inflating: /content/data/private_test/private_01729.wav  \n","  inflating: /content/data/private_test/private_16255.wav  \n","  inflating: /content/data/private_test/private_09375.wav  \n","  inflating: /content/data/private_test/private_06046.wav  \n","  inflating: /content/data/private_test/private_10624.wav  \n","  inflating: /content/data/private_test/private_07358.wav  \n","  inflating: /content/data/private_test/private_00437.wav  \n","  inflating: /content/data/private_test/private_18278.wav  \n","  inflating: /content/data/private_test/private_00351.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00351.wav  \n","  inflating: /content/data/private_test/private_10142.wav  \n","  inflating: /content/data/private_test/private_09413.wav  \n","  inflating: /content/data/private_test/private_06720.wav  \n","  inflating: /content/data/private_test/private_19600.wav  \n","  inflating: /content/data/private_test/private_16533.wav  \n","  inflating: /content/data/private_test/private_02546.wav  \n","  inflating: /content/data/private_test/private_05229.wav  \n","  inflating: /content/data/private_test/private_03880.wav  \n","  inflating: /content/data/private_test/private_12755.wav  \n","  inflating: /content/data/private_test/private_04137.wav  \n","  inflating: /content/data/private_test/private_14324.wav  \n","  inflating: /content/data/private_test/private_03658.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03658.wav  \n","  inflating: /content/data/private_test/private_12966.wav  \n","  inflating: /content/data/private_test/private_06913.wav  \n","  inflating: /content/data/private_test/private_19833.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19833.wav  \n","  inflating: /content/data/private_test/private_08080.wav  \n","  inflating: /content/data/private_test/private_18293.wav  \n","  inflating: /content/data/private_test/private_10817.wav  \n","  inflating: /content/data/private_test/private_04862.wav  \n","  inflating: /content/data/private_test/private_04876.wav  \n","  inflating: /content/data/private_test/private_08094.wav  \n","  inflating: /content/data/private_test/private_18287.wav  \n","  inflating: /content/data/private_test/private_10803.wav  \n","  inflating: /content/data/private_test/private_19199.wav  \n","  inflating: /content/data/private_test/private_06907.wav  \n","  inflating: /content/data/private_test/private_19827.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19827.wav  \n","  inflating: /content/data/private_test/private_12972.wav  \n","  inflating: /content/data/private_test/private_17822.wav  \n","  inflating: /content/data/private_test/private_08902.wav  \n","  inflating: /content/data/private_test/private_01098.wav  \n","  inflating: /content/data/private_test/private_00386.wav  \n","  inflating: /content/data/private_test/private_10195.wav  \n","  inflating: /content/data/private_test/private_02591.wav  \n","  inflating: /content/data/private_test/private_12782.wav  \n","  inflating: /content/data/private_test/private_03857.wav  \n","  inflating: /content/data/private_test/private_05598.wav  \n","  inflating: /content/data/private_test/private_14495.wav  \n","  inflating: /content/data/private_test/private_04686.wav  \n","  inflating: /content/data/private_test/private_15953.wav  \n","  inflating: /content/data/private_test/private_01926.wav  \n","  inflating: /content/data/private_test/private_16282.wav  \n","  inflating: /content/data/private_test/private_06091.wav  \n","  inflating: /content/data/private_test/private_01932.wav  \n","  inflating: /content/data/private_test/private_17188.wav  \n","  inflating: /content/data/private_test/private_16296.wav  \n","  inflating: /content/data/private_test/private_06085.wav  \n","  inflating: /content/data/private_test/private_14481.wav  \n","  inflating: /content/data/private_test/private_15947.wav  \n","  inflating: /content/data/private_test/private_04692.wav  \n","  inflating: /content/data/private_test/private_13488.wav  \n","  inflating: /content/data/private_test/private_02585.wav  \n","  inflating: /content/data/private_test/private_03843.wav  \n","  inflating: /content/data/private_test/private_12796.wav  \n","  inflating: /content/data/private_test/private_17836.wav  \n","  inflating: /content/data/private_test/private_08916.wav  \n","  inflating: /content/data/private_test/private_00392.wav  \n","  inflating: /content/data/private_test/private_10181.wav  \n","  inflating: /content/data/private_test/private_08889.wav  \n","  inflating: /content/data/private_test/private_01113.wav  \n","  inflating: /content/data/private_test/private_11300.wav  \n","  inflating: /content/data/private_test/private_07562.wav  \n","  inflating: /content/data/private_test/private_19984.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19984.wav  \n","  inflating: /content/data/private_test/private_08651.wav  \n","  inflating: /content/data/private_test/private_17771.wav  \n","  inflating: /content/data/private_test/private_18442.wav  \n","  inflating: /content/data/private_test/private_03704.wav  \n","  inflating: /content/data/private_test/private_14278.wav  \n","  inflating: /content/data/private_test/private_13517.wav  \n","  inflating: /content/data/private_test/private_12609.wav  \n","  inflating: /content/data/private_test/private_05375.wav  \n","  inflating: /content/data/private_test/private_15166.wav  \n","  inflating: /content/data/private_test/private_15600.wav  \n","  inflating: /content/data/private_test/private_05413.wav  \n","  inflating: /content/data/private_test/private_13271.wav  \n","  inflating: /content/data/private_test/private_03062.wav  \n","  inflating: /content/data/private_test/private_17017.wav  \n","  inflating: /content/data/private_test/private_18324.wav  \n","  inflating: /content/data/private_test/private_07204.wav  \n","  inflating: /content/data/private_test/private_08137.wav  \n","  inflating: /content/data/private_test/private_10778.wav  \n","  inflating: /content/data/private_test/private_11466.wav  \n","  inflating: /content/data/private_test/private_09229.wav  \n","  inflating: /content/data/private_test/private_16309.wav  \n","  inflating: /content/data/private_test/private_01675.wav  \n","  inflating: /content/data/private_test/private_17003.wav  \n","  inflating: /content/data/private_test/private_18330.wav  \n","  inflating: /content/data/private_test/private_07210.wav  \n","  inflating: /content/data/private_test/private_08123.wav  \n","  inflating: /content/data/private_test/private_11472.wav  \n","  inflating: /content/data/private_test/private_01661.wav  \n","  inflating: /content/data/private_test/private_02368.wav  \n","  inflating: /content/data/private_test/private_15614.wav  \n","  inflating: /content/data/private_test/private_05407.wav  \n","  inflating: /content/data/private_test/private_13265.wav  \n","  inflating: /content/data/private_test/private_04719.wav  \n","  inflating: /content/data/private_test/private_03076.wav  \n","  inflating: /content/data/private_test/private_03710.wav  \n","  inflating: /content/data/private_test/private_13503.wav  \n","  inflating: /content/data/private_test/private_05361.wav  \n","  inflating: /content/data/private_test/private_15172.wav  \n","  inflating: /content/data/private_test/private_01107.wav  \n","  inflating: /content/data/private_test/private_19748.wav  \n","  inflating: /content/data/private_test/private_06668.wav  \n","  inflating: /content/data/private_test/private_11314.wav  \n","  inflating: /content/data/private_test/private_07576.wav  \n","  inflating: /content/data/private_test/private_08645.wav  \n","  inflating: /content/data/private_test/private_19990.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19990.wav  \n","  inflating: /content/data/private_test/private_17765.wav  \n","  inflating: /content/data/private_test/private_18456.wav  \n","  inflating: /content/data/private_test/private_00219.wav  \n","  inflating: /content/data/private_test/private_14244.wav  \n","  inflating: /content/data/private_test/private_03738.wav  \n","  inflating: /content/data/private_test/private_04057.wav  \n","  inflating: /content/data/private_test/private_05349.wav  \n","  inflating: /content/data/private_test/private_12635.wav  \n","  inflating: /content/data/private_test/private_02426.wav  \n","  inflating: /content/data/private_test/private_19760.wav  \n","  inflating: /content/data/private_test/private_16453.wav  \n","  inflating: /content/data/private_test/private_09573.wav  \n","  inflating: /content/data/private_test/private_06640.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06640.wav  \n","  inflating: /content/data/private_test/private_17995.wav  \n","  inflating: /content/data/private_test/private_10022.wav  \n","  inflating: /content/data/private_test/private_00231.wav  \n","  inflating: /content/data/private_test/private_06898.wav  \n","  inflating: /content/data/private_test/private_00557.wav  \n","  inflating: /content/data/private_test/private_18318.wav  \n","  inflating: /content/data/private_test/private_10744.wav  \n","  inflating: /content/data/private_test/private_01891.wav  \n","  inflating: /content/data/private_test/private_07238.wav  \n","  inflating: /content/data/private_test/private_09215.wav  \n","  inflating: /content/data/private_test/private_06126.wav  \n","  inflating: /content/data/private_test/private_19006.wav  \n","  inflating: /content/data/private_test/private_01649.wav  \n","  inflating: /content/data/private_test/private_16335.wav  \n","  inflating: /content/data/private_test/private_02340.wav  \n","  inflating: /content/data/private_test/private_12153.wav  \n","  inflating: /content/data/private_test/private_04731.wav  \n","  inflating: /content/data/private_test/private_14522.wav  \n","  inflating: /content/data/private_test/private_15628.wav  \n","  inflating: /content/data/private_test/private_02354.wav  \n","  inflating: /content/data/private_test/private_12147.wav  \n","  inflating: /content/data/private_test/private_04725.wav  \n","  inflating: /content/data/private_test/private_13259.wav  \n","  inflating: /content/data/private_test/private_14536.wav  \n","  inflating: /content/data/private_test/private_00543.wav  \n","  inflating: /content/data/private_test/private_01885.wav  \n","  inflating: /content/data/private_test/private_10750.wav  \n","  inflating: /content/data/private_test/private_09201.wav  \n","  inflating: /content/data/private_test/private_06132.wav  \n","  inflating: /content/data/private_test/private_10988.wav  \n","  inflating: /content/data/private_test/private_19012.wav  \n","  inflating: /content/data/private_test/private_16321.wav  \n","  inflating: /content/data/private_test/private_19774.wav  \n","  inflating: /content/data/private_test/private_16447.wav  \n","  inflating: /content/data/private_test/private_09567.wav  \n","  inflating: /content/data/private_test/private_11328.wav  \n","  inflating: /content/data/private_test/private_17981.wav  \n","  inflating: /content/data/private_test/private_06654.wav  \n","  inflating: /content/data/private_test/private_10036.wav  \n","  inflating: /content/data/private_test/private_08679.wav  \n","  inflating: /content/data/private_test/private_00225.wav  \n","  inflating: /content/data/private_test/private_17759.wav  \n","  inflating: /content/data/private_test/private_14250.wav  \n","  inflating: /content/data/private_test/private_04043.wav  \n","  inflating: /content/data/private_test/private_12621.wav  \n","  inflating: /content/data/private_test/private_02432.wav  \n","  inflating: /content/data/private_test/private_12812.wav  \n","  inflating: /content/data/private_test/private_19947.wav  \n","  inflating: /content/data/private_test/private_08692.wav  \n","  inflating: /content/data/private_test/private_06867.wav  \n","  inflating: /content/data/private_test/private_18481.wav  \n","  inflating: /content/data/private_test/private_10963.wav  \n","  inflating: /content/data/private_test/private_04916.wav  \n","  inflating: /content/data/private_test/private_04902.wav  \n","  inflating: /content/data/private_test/private_10977.wav  \n","  inflating: /content/data/private_test/private_08686.wav  \n","  inflating: /content/data/private_test/private_19953.wav  \n","  inflating: /content/data/private_test/private_06873.wav  \n","  inflating: /content/data/private_test/private_18495.wav  \n","  inflating: /content/data/private_test/private_09598.wav  \n","  inflating: /content/data/private_test/private_12806.wav  \n","  inflating: /content/data/private_test/private_08876.wav  \n","  inflating: /content/data/private_test/private_16490.wav  \n","  inflating: /content/data/private_test/private_06683.wav  \n","  inflating: /content/data/private_test/private_17956.wav  \n","  inflating: /content/data/private_test/private_03923.wav  \n","  inflating: /content/data/private_test/private_15199.wav  \n","  inflating: /content/data/private_test/private_14287.wav  \n","  inflating: /content/data/private_test/private_04094.wav  \n","  inflating: /content/data/private_test/private_15827.wav  \n","  inflating: /content/data/private_test/private_02383.wav  \n","  inflating: /content/data/private_test/private_12190.wav  \n","  inflating: /content/data/private_test/private_11499.wav  \n","  inflating: /content/data/private_test/private_00594.wav  \n","  inflating: /content/data/private_test/private_10787.wav  \n","  inflating: /content/data/private_test/private_01852.wav  \n","  inflating: /content/data/private_test/private_00580.wav  \n","  inflating: /content/data/private_test/private_01846.wav  \n","  inflating: /content/data/private_test/private_10793.wav  \n","  inflating: /content/data/private_test/private_15833.wav  \n","  inflating: /content/data/private_test/private_03089.wav  \n","  inflating: /content/data/private_test/private_02397.wav  \n","  inflating: /content/data/private_test/private_12184.wav  \n","  inflating: /content/data/private_test/private_03937.wav  \n","  inflating: /content/data/private_test/private_14293.wav  \n","  inflating: /content/data/private_test/private_04080.wav  \n","  inflating: /content/data/private_test/private_07589.wav  \n","  inflating: /content/data/private_test/private_08862.wav  \n","  inflating: /content/data/private_test/private_16484.wav  \n","  inflating: /content/data/private_test/private_17942.wav  \n","  inflating: /content/data/private_test/private_06697.wav  \n","  inflating: /content/data/private_test/private_01477.wav  \n","  inflating: /content/data/private_test/private_19238.wav  \n","  inflating: /content/data/private_test/private_11664.wav  \n","  inflating: /content/data/private_test/private_06318.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06318.wav  \n","  inflating: /content/data/private_test/private_08335.wav  \n","  inflating: /content/data/private_test/private_07006.wav  \n","  inflating: /content/data/private_test/private_18126.wav  \n","  inflating: /content/data/private_test/private_00769.wav  \n","  inflating: /content/data/private_test/private_17215.wav  \n","  inflating: /content/data/private_test/private_03260.wav  \n","  inflating: /content/data/private_test/private_13073.wav  \n","  inflating: /content/data/private_test/private_05611.wav  \n","  inflating: /content/data/private_test/private_15402.wav  \n","  inflating: /content/data/private_test/private_15364.wav  \n","  inflating: /content/data/private_test/private_02618.wav  \n","  inflating: /content/data/private_test/private_05177.wav  \n","  inflating: /content/data/private_test/private_04269.wav  \n","  inflating: /content/data/private_test/private_13715.wav  \n","  inflating: /content/data/private_test/private_03506.wav  \n","  inflating: /content/data/private_test/private_09995.wav  \n","  inflating: /content/data/private_test/private_18640.wav  \n","  inflating: /content/data/private_test/private_17573.wav  \n","  inflating: /content/data/private_test/private_08453.wav  \n","  inflating: /content/data/private_test/private_07760.wav  \n","  inflating: /content/data/private_test/private_18898.wav  \n","  inflating: /content/data/private_test/private_11102.wav  \n","  inflating: /content/data/private_test/private_01311.wav  \n","  inflating: /content/data/private_test/private_18654.wav  \n","  inflating: /content/data/private_test/private_09981.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_09981.wav  \n","  inflating: /content/data/private_test/private_17567.wav  \n","  inflating: /content/data/private_test/private_08447.wav  \n","  inflating: /content/data/private_test/private_10208.wav  \n","  inflating: /content/data/private_test/private_07774.wav  \n","  inflating: /content/data/private_test/private_11116.wav  \n","  inflating: /content/data/private_test/private_09759.wav  \n","  inflating: /content/data/private_test/private_01305.wav  \n","  inflating: /content/data/private_test/private_16679.wav  \n","  inflating: /content/data/private_test/private_15370.wav  \n","  inflating: /content/data/private_test/private_05163.wav  \n","  inflating: /content/data/private_test/private_13701.wav  \n","  inflating: /content/data/private_test/private_03512.wav  \n","  inflating: /content/data/private_test/private_14708.wav  \n","  inflating: /content/data/private_test/private_03274.wav  \n","  inflating: /content/data/private_test/private_13067.wav  \n","  inflating: /content/data/private_test/private_05605.wav  \n","  inflating: /content/data/private_test/private_12379.wav  \n","  inflating: /content/data/private_test/private_15416.wav  \n","  inflating: /content/data/private_test/private_01463.wav  \n","  inflating: /content/data/private_test/private_11670.wav  \n","  inflating: /content/data/private_test/private_08321.wav  \n","  inflating: /content/data/private_test/private_07012.wav  \n","  inflating: /content/data/private_test/private_18132.wav  \n","  inflating: /content/data/private_test/private_17201.wav  \n","  inflating: /content/data/private_test/private_14720.wav  \n","  inflating: /content/data/private_test/private_04533.wav  \n","  inflating: /content/data/private_test/private_12351.wav  \n","  inflating: /content/data/private_test/private_02142.wav  \n","  inflating: /content/data/private_test/private_16137.wav  \n","  inflating: /content/data/private_test/private_19204.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19204.wav  \n","  inflating: /content/data/private_test/private_06324.wav  \n","  inflating: /content/data/private_test/private_09017.wav  \n","  inflating: /content/data/private_test/private_11658.wav  \n","  inflating: /content/data/private_test/private_10546.wav  \n","  inflating: /content/data/private_test/private_08309.wav  \n","  inflating: /content/data/private_test/private_17229.wav  \n","  inflating: /content/data/private_test/private_00755.wav  \n","  inflating: /content/data/private_test/private_11880.wav  \n","  inflating: /content/data/private_test/private_00033.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00033.wav  \n","  inflating: /content/data/private_test/private_16889.wav  \n","  inflating: /content/data/private_test/private_10220.wav  \n","  inflating: /content/data/private_test/private_06442.wav  \n","  inflating: /content/data/private_test/private_09771.wav  \n","  inflating: /content/data/private_test/private_16651.wav  \n","  inflating: /content/data/private_test/private_07984.wav  \n","  inflating: /content/data/private_test/private_19562.wav  \n","  inflating: /content/data/private_test/private_02624.wav  \n","  inflating: /content/data/private_test/private_15358.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15358.wav  \n","  inflating: /content/data/private_test/private_12437.wav  \n","  inflating: /content/data/private_test/private_13729.wav  \n","  inflating: /content/data/private_test/private_04255.wav  \n","  inflating: /content/data/private_test/private_14046.wav  \n","  inflating: /content/data/private_test/private_02630.wav  \n","  inflating: /content/data/private_test/private_12423.wav  \n","  inflating: /content/data/private_test/private_04241.wav  \n","  inflating: /content/data/private_test/private_14052.wav  \n","  inflating: /content/data/private_test/private_00027.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00027.wav  \n","  inflating: /content/data/private_test/private_18668.wav  \n","  inflating: /content/data/private_test/private_07748.wav  \n","  inflating: /content/data/private_test/private_10234.wav  \n","  inflating: /content/data/private_test/private_06456.wav  \n","  inflating: /content/data/private_test/private_09765.wav  \n","  inflating: /content/data/private_test/private_07990.wav  \n","  inflating: /content/data/private_test/private_16645.wav  \n","  inflating: /content/data/private_test/private_19576.wav  \n","  inflating: /content/data/private_test/private_01339.wav  \n","  inflating: /content/data/private_test/private_16123.wav  \n","  inflating: /content/data/private_test/private_19210.wav  \n","  inflating: /content/data/private_test/private_06330.wav  \n","  inflating: /content/data/private_test/private_00999.wav  \n","  inflating: /content/data/private_test/private_09003.wav  \n","  inflating: /content/data/private_test/private_10552.wav  \n","  inflating: /content/data/private_test/private_11894.wav  \n","  inflating: /content/data/private_test/private_00741.wav  \n","  inflating: /content/data/private_test/private_03248.wav  \n","  inflating: /content/data/private_test/private_14734.wav  \n","  inflating: /content/data/private_test/private_04527.wav  \n","  inflating: /content/data/private_test/private_12345.wav  \n","  inflating: /content/data/private_test/private_05639.wav  \n","  inflating: /content/data/private_test/private_02156.wav  \n","  inflating: /content/data/private_test/private_14907.wav  \n","  inflating: /content/data/private_test/private_00972.wav  \n","  inflating: /content/data/private_test/private_09956.wav  \n","  inflating: /content/data/private_test/private_18683.wav  \n","  inflating: /content/data/private_test/private_08490.wav  \n","  inflating: /content/data/private_test/private_16876.wav  \n","  inflating: /content/data/private_test/private_02803.wav  \n","  inflating: /content/data/private_test/private_02817.wav  \n","  inflating: /content/data/private_test/private_19589.wav  \n","  inflating: /content/data/private_test/private_18697.wav  \n","  inflating: /content/data/private_test/private_09942.wav  \n","  inflating: /content/data/private_test/private_08484.wav  \n","  inflating: /content/data/private_test/private_16862.wav  \n","  inflating: /content/data/private_test/private_00966.wav  \n","  inflating: /content/data/private_test/private_14913.wav  \n","  inflating: /content/data/private_test/private_10585.wav  \n","  inflating: /content/data/private_test/private_00796.wav  \n","  inflating: /content/data/private_test/private_11843.wav  \n","  inflating: /content/data/private_test/private_01488.wav  \n","  inflating: /content/data/private_test/private_12392.wav  \n","  inflating: /content/data/private_test/private_02181.wav  \n","  inflating: /content/data/private_test/private_05836.wav  \n","  inflating: /content/data/private_test/private_04296.wav  \n","  inflating: /content/data/private_test/private_14085.wav  \n","  inflating: /content/data/private_test/private_13932.wav  \n","  inflating: /content/data/private_test/private_05188.wav  \n","  inflating: /content/data/private_test/private_06481.wav  \n","  inflating: /content/data/private_test/private_18867.wav  \n","  inflating: /content/data/private_test/private_16692.wav  \n","  inflating: /content/data/private_test/private_07947.wav  \n","  inflating: /content/data/private_test/private_06495.wav  \n","  inflating: /content/data/private_test/private_18873.wav  \n","  inflating: /content/data/private_test/private_07953.wav  \n","  inflating: /content/data/private_test/private_16686.wav  \n","  inflating: /content/data/private_test/private_17598.wav  \n","  inflating: /content/data/private_test/private_04282.wav  \n","  inflating: /content/data/private_test/private_14091.wav  \n","  inflating: /content/data/private_test/private_13926.wav  \n","  inflating: /content/data/private_test/private_12386.wav  \n","  inflating: /content/data/private_test/private_02195.wav  \n","  inflating: /content/data/private_test/private_05822.wav  \n","  inflating: /content/data/private_test/private_13098.wav  \n","  inflating: /content/data/private_test/private_10591.wav  \n","  inflating: /content/data/private_test/private_11857.wav  \n","  inflating: /content/data/private_test/private_00782.wav  \n","  inflating: /content/data/private_test/private_13927.wav  \n","  inflating: /content/data/private_test/private_14090.wav  \n","  inflating: /content/data/private_test/private_04283.wav  \n","  inflating: /content/data/private_test/private_17599.wav  \n","  inflating: /content/data/private_test/private_16687.wav  \n","  inflating: /content/data/private_test/private_07952.wav  \n","  inflating: /content/data/private_test/private_18872.wav  \n","  inflating: /content/data/private_test/private_06494.wav  \n","  inflating: /content/data/private_test/private_00783.wav  \n","  inflating: /content/data/private_test/private_11856.wav  \n","  inflating: /content/data/private_test/private_10590.wav  \n","  inflating: /content/data/private_test/private_13099.wav  \n","  inflating: /content/data/private_test/private_05823.wav  \n","  inflating: /content/data/private_test/private_02194.wav  \n","  inflating: /content/data/private_test/private_12387.wav  \n","  inflating: /content/data/private_test/private_05837.wav  \n","  inflating: /content/data/private_test/private_02180.wav  \n","  inflating: /content/data/private_test/private_12393.wav  \n","  inflating: /content/data/private_test/private_01489.wav  \n","  inflating: /content/data/private_test/private_11842.wav  \n","  inflating: /content/data/private_test/private_00797.wav  \n","  inflating: /content/data/private_test/private_10584.wav  \n","  inflating: /content/data/private_test/private_07946.wav  \n","  inflating: /content/data/private_test/private_16693.wav  \n","  inflating: /content/data/private_test/private_18866.wav  \n","  inflating: /content/data/private_test/private_06480.wav  \n","  inflating: /content/data/private_test/private_05189.wav  \n","  inflating: /content/data/private_test/private_13933.wav  \n","  inflating: /content/data/private_test/private_14084.wav  \n","  inflating: /content/data/private_test/private_04297.wav  \n","  inflating: /content/data/private_test/private_16863.wav  \n","  inflating: /content/data/private_test/private_08485.wav  \n","  inflating: /content/data/private_test/private_09943.wav  \n","  inflating: /content/data/private_test/private_18696.wav  \n","  inflating: /content/data/private_test/private_19588.wav  \n","  inflating: /content/data/private_test/private_02816.wav  \n","  inflating: /content/data/private_test/private_14912.wav  \n","  inflating: /content/data/private_test/private_00967.wav  \n","  inflating: /content/data/private_test/private_00973.wav  \n","  inflating: /content/data/private_test/private_14906.wav  \n","  inflating: /content/data/private_test/private_02802.wav  \n","  inflating: /content/data/private_test/private_16877.wav  \n","  inflating: /content/data/private_test/private_08491.wav  \n","  inflating: /content/data/private_test/private_18682.wav  \n","  inflating: /content/data/private_test/private_09957.wav  \n","  inflating: /content/data/private_test/private_19577.wav  \n","  inflating: /content/data/private_test/private_01338.wav  \n","  inflating: /content/data/private_test/private_16644.wav  \n","  inflating: /content/data/private_test/private_07991.wav  \n","  inflating: /content/data/private_test/private_09764.wav  \n","  inflating: /content/data/private_test/private_06457.wav  \n","  inflating: /content/data/private_test/private_10235.wav  \n","  inflating: /content/data/private_test/private_07749.wav  \n","  inflating: /content/data/private_test/private_00026.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00026.wav  \n","  inflating: /content/data/private_test/private_18669.wav  \n","  inflating: /content/data/private_test/private_14053.wav  \n","  inflating: /content/data/private_test/private_04240.wav  \n","  inflating: /content/data/private_test/private_12422.wav  \n","  inflating: /content/data/private_test/private_02631.wav  \n","  inflating: /content/data/private_test/private_02157.wav  \n","  inflating: /content/data/private_test/private_05638.wav  \n","  inflating: /content/data/private_test/private_12344.wav  \n","  inflating: /content/data/private_test/private_04526.wav  \n","  inflating: /content/data/private_test/private_14735.wav  \n","  inflating: /content/data/private_test/private_03249.wav  \n","  inflating: /content/data/private_test/private_00740.wav  \n","  inflating: /content/data/private_test/private_11895.wav  \n","  inflating: /content/data/private_test/private_10553.wav  \n","  inflating: /content/data/private_test/private_00998.wav  \n","  inflating: /content/data/private_test/private_09002.wav  \n","  inflating: /content/data/private_test/private_06331.wav  \n","  inflating: /content/data/private_test/private_19211.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19211.wav  \n","  inflating: /content/data/private_test/private_16122.wav  \n","  inflating: /content/data/private_test/private_11881.wav  \n","  inflating: /content/data/private_test/private_00754.wav  \n","  inflating: /content/data/private_test/private_17228.wav  \n","  inflating: /content/data/private_test/private_10547.wav  \n","  inflating: /content/data/private_test/private_08308.wav  \n","  inflating: /content/data/private_test/private_09016.wav  \n","  inflating: /content/data/private_test/private_11659.wav  \n","  inflating: /content/data/private_test/private_06325.wav  \n","  inflating: /content/data/private_test/private_19205.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19205.wav  \n","  inflating: /content/data/private_test/private_16136.wav  \n","  inflating: /content/data/private_test/private_02143.wav  \n","  inflating: /content/data/private_test/private_12350.wav  \n","  inflating: /content/data/private_test/private_04532.wav  \n","  inflating: /content/data/private_test/private_14721.wav  \n","  inflating: /content/data/private_test/private_14047.wav  \n","  inflating: /content/data/private_test/private_04254.wav  \n","  inflating: /content/data/private_test/private_13728.wav  \n","  inflating: /content/data/private_test/private_12436.wav  \n","  inflating: /content/data/private_test/private_15359.wav  \n","  inflating: /content/data/private_test/private_02625.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_02625.wav  \n","  inflating: /content/data/private_test/private_19563.wav  \n","  inflating: /content/data/private_test/private_07985.wav  \n","  inflating: /content/data/private_test/private_16650.wav  \n","  inflating: /content/data/private_test/private_09770.wav  \n","  inflating: /content/data/private_test/private_06443.wav  \n","  inflating: /content/data/private_test/private_10221.wav  \n","  inflating: /content/data/private_test/private_16888.wav  \n","  inflating: /content/data/private_test/private_00032.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00032.wav  \n","  inflating: /content/data/private_test/private_03513.wav  \n","  inflating: /content/data/private_test/private_13700.wav  \n","  inflating: /content/data/private_test/private_05162.wav  \n","  inflating: /content/data/private_test/private_15371.wav  \n","  inflating: /content/data/private_test/private_16678.wav  \n","  inflating: /content/data/private_test/private_01304.wav  \n","  inflating: /content/data/private_test/private_11117.wav  \n","  inflating: /content/data/private_test/private_09758.wav  \n","  inflating: /content/data/private_test/private_07775.wav  \n","  inflating: /content/data/private_test/private_08446.wav  \n","  inflating: /content/data/private_test/private_10209.wav  \n","  inflating: /content/data/private_test/private_17566.wav  \n","  inflating: /content/data/private_test/private_09980.wav  \n","  inflating: /content/data/private_test/private_18655.wav  \n","  inflating: /content/data/private_test/private_17200.wav  \n","  inflating: /content/data/private_test/private_18133.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18133.wav  \n","  inflating: /content/data/private_test/private_07013.wav  \n","  inflating: /content/data/private_test/private_08320.wav  \n","  inflating: /content/data/private_test/private_11671.wav  \n","  inflating: /content/data/private_test/private_01462.wav  \n","  inflating: /content/data/private_test/private_15417.wav  \n","  inflating: /content/data/private_test/private_12378.wav  \n","  inflating: /content/data/private_test/private_05604.wav  \n","  inflating: /content/data/private_test/private_13066.wav  \n","  inflating: /content/data/private_test/private_03275.wav  \n","  inflating: /content/data/private_test/private_14709.wav  \n","  inflating: /content/data/private_test/private_15403.wav  \n","  inflating: /content/data/private_test/private_05610.wav  \n","  inflating: /content/data/private_test/private_13072.wav  \n","  inflating: /content/data/private_test/private_03261.wav  \n","  inflating: /content/data/private_test/private_17214.wav  \n","  inflating: /content/data/private_test/private_18127.wav  \n","  inflating: /content/data/private_test/private_00768.wav  \n","  inflating: /content/data/private_test/private_07007.wav  \n","  inflating: /content/data/private_test/private_08334.wav  \n","  inflating: /content/data/private_test/private_06319.wav  \n","  inflating: /content/data/private_test/private_11665.wav  \n","  inflating: /content/data/private_test/private_01476.wav  \n","  inflating: /content/data/private_test/private_19239.wav  \n","  inflating: /content/data/private_test/private_01310.wav  \n","  inflating: /content/data/private_test/private_18899.wav  \n","  inflating: /content/data/private_test/private_11103.wav  \n","  inflating: /content/data/private_test/private_07761.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_07761.wav  \n","  inflating: /content/data/private_test/private_08452.wav  \n","  inflating: /content/data/private_test/private_17572.wav  \n","  inflating: /content/data/private_test/private_18641.wav  \n","  inflating: /content/data/private_test/private_09994.wav  \n","  inflating: /content/data/private_test/private_03507.wav  \n","  inflating: /content/data/private_test/private_13714.wav  \n","  inflating: /content/data/private_test/private_04268.wav  \n","  inflating: /content/data/private_test/private_05176.wav  \n","  inflating: /content/data/private_test/private_02619.wav  \n","  inflating: /content/data/private_test/private_15365.wav  \n","  inflating: /content/data/private_test/private_12185.wav  \n","  inflating: /content/data/private_test/private_02396.wav  \n","  inflating: /content/data/private_test/private_03088.wav  \n","  inflating: /content/data/private_test/private_15832.wav  \n","  inflating: /content/data/private_test/private_10792.wav  \n","  inflating: /content/data/private_test/private_01847.wav  \n","  inflating: /content/data/private_test/private_00581.wav  \n","  inflating: /content/data/private_test/private_06696.wav  \n","  inflating: /content/data/private_test/private_17943.wav  \n","  inflating: /content/data/private_test/private_16485.wav  \n","  inflating: /content/data/private_test/private_08863.wav  \n","  inflating: /content/data/private_test/private_07588.wav  \n","  inflating: /content/data/private_test/private_04081.wav  \n","  inflating: /content/data/private_test/private_14292.wav  \n","  inflating: /content/data/private_test/private_03936.wav  \n","  inflating: /content/data/private_test/private_04095.wav  \n","  inflating: /content/data/private_test/private_14286.wav  \n","  inflating: /content/data/private_test/private_15198.wav  \n","  inflating: /content/data/private_test/private_03922.wav  \n","  inflating: /content/data/private_test/private_17957.wav  \n","  inflating: /content/data/private_test/private_06682.wav  \n","  inflating: /content/data/private_test/private_16491.wav  \n","  inflating: /content/data/private_test/private_08877.wav  \n","  inflating: /content/data/private_test/private_01853.wav  \n","  inflating: /content/data/private_test/private_10786.wav  \n","  inflating: /content/data/private_test/private_00595.wav  \n","  inflating: /content/data/private_test/private_11498.wav  \n","  inflating: /content/data/private_test/private_12191.wav  \n","  inflating: /content/data/private_test/private_02382.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_02382.wav  \n","  inflating: /content/data/private_test/private_15826.wav  \n","  inflating: /content/data/private_test/private_10976.wav  \n","  inflating: /content/data/private_test/private_04903.wav  \n","  inflating: /content/data/private_test/private_12807.wav  \n","  inflating: /content/data/private_test/private_09599.wav  \n","  inflating: /content/data/private_test/private_18494.wav  \n","  inflating: /content/data/private_test/private_06872.wav  \n","  inflating: /content/data/private_test/private_19952.wav  \n","  inflating: /content/data/private_test/private_08687.wav  \n","  inflating: /content/data/private_test/private_18480.wav  \n","  inflating: /content/data/private_test/private_06866.wav  \n","  inflating: /content/data/private_test/private_08693.wav  \n","  inflating: /content/data/private_test/private_19946.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19946.wav  \n","  inflating: /content/data/private_test/private_12813.wav  \n","  inflating: /content/data/private_test/private_04917.wav  \n","  inflating: /content/data/private_test/private_10962.wav  \n","  inflating: /content/data/private_test/private_16320.wav  \n","  inflating: /content/data/private_test/private_10989.wav  \n","  inflating: /content/data/private_test/private_19013.wav  \n","  inflating: /content/data/private_test/private_06133.wav  \n","  inflating: /content/data/private_test/private_09200.wav  \n","  inflating: /content/data/private_test/private_10751.wav  \n","  inflating: /content/data/private_test/private_01884.wav  \n","  inflating: /content/data/private_test/private_00542.wav  \n","  inflating: /content/data/private_test/private_14537.wav  \n","  inflating: /content/data/private_test/private_13258.wav  \n","  inflating: /content/data/private_test/private_04724.wav  \n","  inflating: /content/data/private_test/private_12146.wav  \n","  inflating: /content/data/private_test/private_02355.wav  \n","  inflating: /content/data/private_test/private_15629.wav  \n","  inflating: /content/data/private_test/private_02433.wav  \n","  inflating: /content/data/private_test/private_12620.wav  \n","  inflating: /content/data/private_test/private_04042.wav  \n","  inflating: /content/data/private_test/private_14251.wav  \n","  inflating: /content/data/private_test/private_17758.wav  \n","  inflating: /content/data/private_test/private_00224.wav  \n","  inflating: /content/data/private_test/private_10037.wav  \n","  inflating: /content/data/private_test/private_08678.wav  \n","  inflating: /content/data/private_test/private_06655.wav  \n","  inflating: /content/data/private_test/private_17980.wav  \n","  inflating: /content/data/private_test/private_09566.wav  \n","  inflating: /content/data/private_test/private_11329.wav  \n","  inflating: /content/data/private_test/private_16446.wav  \n","  inflating: /content/data/private_test/private_19775.wav  \n","  inflating: /content/data/private_test/private_06899.wav  \n","  inflating: /content/data/private_test/private_00230.wav  \n","  inflating: /content/data/private_test/private_10023.wav  \n","  inflating: /content/data/private_test/private_17994.wav  \n","  inflating: /content/data/private_test/private_06641.wav  \n","  inflating: /content/data/private_test/private_09572.wav  \n","  inflating: /content/data/private_test/private_16452.wav  \n","  inflating: /content/data/private_test/private_19761.wav  \n","  inflating: /content/data/private_test/private_02427.wav  \n","  inflating: /content/data/private_test/private_12634.wav  \n","  inflating: /content/data/private_test/private_05348.wav  \n","  inflating: /content/data/private_test/private_04056.wav  \n","  inflating: /content/data/private_test/private_03739.wav  \n","  inflating: /content/data/private_test/private_14245.wav  \n","  inflating: /content/data/private_test/private_14523.wav  \n","  inflating: /content/data/private_test/private_04730.wav  \n","  inflating: /content/data/private_test/private_12152.wav  \n","  inflating: /content/data/private_test/private_02341.wav  \n","  inflating: /content/data/private_test/private_16334.wav  \n","  inflating: /content/data/private_test/private_19007.wav  \n","  inflating: /content/data/private_test/private_01648.wav  \n","  inflating: /content/data/private_test/private_06127.wav  \n","  inflating: /content/data/private_test/private_09214.wav  \n","  inflating: /content/data/private_test/private_07239.wav  \n","  inflating: /content/data/private_test/private_01890.wav  \n","  inflating: /content/data/private_test/private_10745.wav  \n","  inflating: /content/data/private_test/private_00556.wav  \n","  inflating: /content/data/private_test/private_18319.wav  \n","  inflating: /content/data/private_test/private_03077.wav  \n","  inflating: /content/data/private_test/private_04718.wav  \n","  inflating: /content/data/private_test/private_13264.wav  \n","  inflating: /content/data/private_test/private_05406.wav  \n","  inflating: /content/data/private_test/private_15615.wav  \n","  inflating: /content/data/private_test/private_02369.wav  \n","  inflating: /content/data/private_test/private_01660.wav  \n","  inflating: /content/data/private_test/private_11473.wav  \n","  inflating: /content/data/private_test/private_08122.wav  \n","  inflating: /content/data/private_test/private_07211.wav  \n","  inflating: /content/data/private_test/private_18331.wav  \n","  inflating: /content/data/private_test/private_17002.wav  \n","  inflating: /content/data/private_test/private_18457.wav  \n","  inflating: /content/data/private_test/private_00218.wav  \n","  inflating: /content/data/private_test/private_17764.wav  \n","  inflating: /content/data/private_test/private_19991.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19991.wav  \n","  inflating: /content/data/private_test/private_08644.wav  \n","  inflating: /content/data/private_test/private_07577.wav  \n","  inflating: /content/data/private_test/private_11315.wav  \n","  inflating: /content/data/private_test/private_06669.wav  \n","  inflating: /content/data/private_test/private_01106.wav  \n","  inflating: /content/data/private_test/private_19749.wav  \n","  inflating: /content/data/private_test/private_15173.wav  \n","  inflating: /content/data/private_test/private_05360.wav  \n","  inflating: /content/data/private_test/private_13502.wav  \n","  inflating: /content/data/private_test/private_03711.wav  \n","  inflating: /content/data/private_test/private_15167.wav  \n","  inflating: /content/data/private_test/private_05374.wav  \n","  inflating: /content/data/private_test/private_12608.wav  \n","  inflating: /content/data/private_test/private_13516.wav  \n","  inflating: /content/data/private_test/private_14279.wav  \n","  inflating: /content/data/private_test/private_03705.wav  \n","  inflating: /content/data/private_test/private_18443.wav  \n","  inflating: /content/data/private_test/private_17770.wav  \n","  inflating: /content/data/private_test/private_08650.wav  \n","  inflating: /content/data/private_test/private_19985.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19985.wav  \n","  inflating: /content/data/private_test/private_07563.wav  \n","  inflating: /content/data/private_test/private_11301.wav  \n","  inflating: /content/data/private_test/private_08888.wav  \n","  inflating: /content/data/private_test/private_01112.wav  \n","  inflating: /content/data/private_test/private_01674.wav  \n","  inflating: /content/data/private_test/private_16308.wav  \n","  inflating: /content/data/private_test/private_11467.wav  \n","  inflating: /content/data/private_test/private_09228.wav  \n","  inflating: /content/data/private_test/private_08136.wav  \n","  inflating: /content/data/private_test/private_10779.wav  \n","  inflating: /content/data/private_test/private_07205.wav  \n","  inflating: /content/data/private_test/private_18325.wav  \n","  inflating: /content/data/private_test/private_17016.wav  \n","  inflating: /content/data/private_test/private_03063.wav  \n","  inflating: /content/data/private_test/private_13270.wav  \n","  inflating: /content/data/private_test/private_05412.wav  \n","  inflating: /content/data/private_test/private_15601.wav  \n","  inflating: /content/data/private_test/private_04693.wav  \n","  inflating: /content/data/private_test/private_15946.wav  \n","  inflating: /content/data/private_test/private_14480.wav  \n","  inflating: /content/data/private_test/private_06084.wav  \n","  inflating: /content/data/private_test/private_16297.wav  \n","  inflating: /content/data/private_test/private_17189.wav  \n","  inflating: /content/data/private_test/private_01933.wav  \n","  inflating: /content/data/private_test/private_10180.wav  \n","  inflating: /content/data/private_test/private_00393.wav  \n","  inflating: /content/data/private_test/private_08917.wav  \n","  inflating: /content/data/private_test/private_17837.wav  \n","  inflating: /content/data/private_test/private_12797.wav  \n","  inflating: /content/data/private_test/private_03842.wav  \n","  inflating: /content/data/private_test/private_02584.wav  \n","  inflating: /content/data/private_test/private_13489.wav  \n","  inflating: /content/data/private_test/private_03856.wav  \n","  inflating: /content/data/private_test/private_12783.wav  \n","  inflating: /content/data/private_test/private_02590.wav  \n","  inflating: /content/data/private_test/private_10194.wav  \n","  inflating: /content/data/private_test/private_00387.wav  \n","  inflating: /content/data/private_test/private_08903.wav  \n","  inflating: /content/data/private_test/private_01099.wav  \n","  inflating: /content/data/private_test/private_17823.wav  \n","  inflating: /content/data/private_test/private_06090.wav  \n","  inflating: /content/data/private_test/private_16283.wav  \n","  inflating: /content/data/private_test/private_01927.wav  \n","  inflating: /content/data/private_test/private_15952.wav  \n","  inflating: /content/data/private_test/private_04687.wav  \n","  inflating: /content/data/private_test/private_14494.wav  \n","  inflating: /content/data/private_test/private_05599.wav  \n","  inflating: /content/data/private_test/private_10802.wav  \n","  inflating: /content/data/private_test/private_19198.wav  \n","  inflating: /content/data/private_test/private_18286.wav  \n","  inflating: /content/data/private_test/private_08095.wav  \n","  inflating: /content/data/private_test/private_04877.wav  \n","  inflating: /content/data/private_test/private_12973.wav  \n","  inflating: /content/data/private_test/private_19826.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19826.wav  \n","  inflating: /content/data/private_test/private_06906.wav  \n","  inflating: /content/data/private_test/private_19832.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19832.wav  \n","  inflating: /content/data/private_test/private_06912.wav  \n","  inflating: /content/data/private_test/private_12967.wav  \n","  inflating: /content/data/private_test/private_04863.wav  \n","  inflating: /content/data/private_test/private_10816.wav  \n","  inflating: /content/data/private_test/private_18292.wav  \n","  inflating: /content/data/private_test/private_08081.wav  \n","  inflating: /content/data/private_test/private_00436.wav  \n","  inflating: /content/data/private_test/private_18279.wav  \n","  inflating: /content/data/private_test/private_07359.wav  \n","  inflating: /content/data/private_test/private_10625.wav  \n","  inflating: /content/data/private_test/private_06047.wav  \n","  inflating: /content/data/private_test/private_09374.wav  \n","  inflating: /content/data/private_test/private_16254.wav  \n","  inflating: /content/data/private_test/private_19167.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19167.wav  \n","  inflating: /content/data/private_test/private_01728.wav  \n","  inflating: /content/data/private_test/private_02221.wav  \n","  inflating: /content/data/private_test/private_04888.wav  \n","  inflating: /content/data/private_test/private_12032.wav  \n","  inflating: /content/data/private_test/private_04650.wav  \n","  inflating: /content/data/private_test/private_15985.wav  \n","  inflating: /content/data/private_test/private_14443.wav  \n","  inflating: /content/data/private_test/private_03659.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03659.wav  \n","  inflating: /content/data/private_test/private_14325.wav  \n","  inflating: /content/data/private_test/private_04136.wav  \n","  inflating: /content/data/private_test/private_12754.wav  \n","  inflating: /content/data/private_test/private_03881.wav  \n","  inflating: /content/data/private_test/private_05228.wav  \n","  inflating: /content/data/private_test/private_02547.wav  \n","  inflating: /content/data/private_test/private_16532.wav  \n","  inflating: /content/data/private_test/private_19601.wav  \n","  inflating: /content/data/private_test/private_06721.wav  \n","  inflating: /content/data/private_test/private_09412.wav  \n","  inflating: /content/data/private_test/private_10143.wav  \n","  inflating: /content/data/private_test/private_00350.wav  \n","  inflating: /content/data/private_test/private_16526.wav  \n","  inflating: /content/data/private_test/private_19615.wav  \n","  inflating: /content/data/private_test/private_06735.wav  \n","  inflating: /content/data/private_test/private_09406.wav  \n","  inflating: /content/data/private_test/private_11249.wav  \n","  inflating: /content/data/private_test/private_10157.wav  \n","  inflating: /content/data/private_test/private_08718.wav  \n","  inflating: /content/data/private_test/private_17638.wav  \n","  inflating: /content/data/private_test/private_00344.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00344.wav  \n","  inflating: /content/data/private_test/private_12998.wav  \n","  inflating: /content/data/private_test/private_14331.wav  \n","  inflating: /content/data/private_test/private_04122.wav  \n","  inflating: /content/data/private_test/private_03895.wav  \n","  inflating: /content/data/private_test/private_12740.wav  \n","  inflating: /content/data/private_test/private_02553.wav  \n","  inflating: /content/data/private_test/private_02235.wav  \n","  inflating: /content/data/private_test/private_15749.wav  \n","  inflating: /content/data/private_test/private_12026.wav  \n","  inflating: /content/data/private_test/private_13338.wav  \n","  inflating: /content/data/private_test/private_15991.wav  \n","  inflating: /content/data/private_test/private_04644.wav  \n","  inflating: /content/data/private_test/private_14457.wav  \n","  inflating: /content/data/private_test/private_00422.wav  \n","  inflating: /content/data/private_test/private_10631.wav  \n","  inflating: /content/data/private_test/private_06053.wav  \n","  inflating: /content/data/private_test/private_09360.wav  \n","  inflating: /content/data/private_test/private_16240.wav  \n","  inflating: /content/data/private_test/private_19173.wav  \n","  inflating: /content/data/private_test/private_15761.wav  \n","  inflating: /content/data/private_test/private_05572.wav  \n","  inflating: /content/data/private_test/private_13310.wav  \n","  inflating: /content/data/private_test/private_03103.wav  \n","  inflating: /content/data/private_test/private_18245.wav  \n","  inflating: /content/data/private_test/private_17176.wav  \n","  inflating: /content/data/private_test/private_08056.wav  \n","  inflating: /content/data/private_test/private_10619.wav  \n","  inflating: /content/data/private_test/private_07365.wav  \n","  inflating: /content/data/private_test/private_11507.wav  \n","  inflating: /content/data/private_test/private_09348.wav  \n","  inflating: /content/data/private_test/private_01714.wav  \n","  inflating: /content/data/private_test/private_16268.wav  \n","  inflating: /content/data/private_test/private_01072.wav  \n","  inflating: /content/data/private_test/private_11261.wav  \n","  inflating: /content/data/private_test/private_08730.wav  \n","  inflating: /content/data/private_test/private_07403.wav  \n","  inflating: /content/data/private_test/private_18523.wav  \n","  inflating: /content/data/private_test/private_17610.wav  \n","  inflating: /content/data/private_test/private_14319.wav  \n","  inflating: /content/data/private_test/private_03665.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03665.wav  \n","  inflating: /content/data/private_test/private_13476.wav  \n","  inflating: /content/data/private_test/private_05214.wav  \n","  inflating: /content/data/private_test/private_12768.wav  \n","  inflating: /content/data/private_test/private_15007.wav  \n","  inflating: /content/data/private_test/private_03671.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03671.wav  \n","  inflating: /content/data/private_test/private_13462.wav  \n","  inflating: /content/data/private_test/private_05200.wav  \n","  inflating: /content/data/private_test/private_15013.wav  \n","  inflating: /content/data/private_test/private_01066.wav  \n","  inflating: /content/data/private_test/private_19629.wav  \n","  inflating: /content/data/private_test/private_11275.wav  \n","  inflating: /content/data/private_test/private_06709.wav  \n","  inflating: /content/data/private_test/private_08724.wav  \n","  inflating: /content/data/private_test/private_07417.wav  \n","  inflating: /content/data/private_test/private_18537.wav  \n","  inflating: /content/data/private_test/private_00378.wav  \n","  inflating: /content/data/private_test/private_17604.wav  \n","  inflating: /content/data/private_test/private_18251.wav  \n","  inflating: /content/data/private_test/private_17162.wav  \n","  inflating: /content/data/private_test/private_08042.wav  \n","  inflating: /content/data/private_test/private_07371.wav  \n","  inflating: /content/data/private_test/private_11513.wav  \n","  inflating: /content/data/private_test/private_01700.wav  \n","  inflating: /content/data/private_test/private_15775.wav  \n","  inflating: /content/data/private_test/private_02209.wav  \n","  inflating: /content/data/private_test/private_05566.wav  \n","  inflating: /content/data/private_test/private_04678.wav  \n","  inflating: /content/data/private_test/private_13304.wav  \n","  inflating: /content/data/private_test/private_03117.wav  \n","  inflating: /content/data/private_test/private_03498.wav  \n","  inflating: /content/data/private_test/private_02786.wav  \n","  inflating: /content/data/private_test/private_13853.wav  \n","  inflating: /content/data/private_test/private_12595.wav  \n","  inflating: /content/data/private_test/private_18906.wav  \n","  inflating: /content/data/private_test/private_07826.wav  \n","  inflating: /content/data/private_test/private_00191.wav  \n","  inflating: /content/data/private_test/private_10382.wav  \n","  inflating: /content/data/private_test/private_07198.wav  \n","  inflating: /content/data/private_test/private_11922.wav  \n","  inflating: /content/data/private_test/private_16095.wav  \n","  inflating: /content/data/private_test/private_06286.wav  \n","  inflating: /content/data/private_test/private_14682.wav  \n","  inflating: /content/data/private_test/private_05957.wav  \n","  inflating: /content/data/private_test/private_04491.wav  \n","  inflating: /content/data/private_test/private_15588.wav  \n","  inflating: /content/data/private_test/private_05943.wav  \n","  inflating: /content/data/private_test/private_14696.wav  \n","  inflating: /content/data/private_test/private_04485.wav  \n","  inflating: /content/data/private_test/private_11936.wav  \n","  inflating: /content/data/private_test/private_16081.wav  \n","  inflating: /content/data/private_test/private_06292.wav  \n","  inflating: /content/data/private_test/private_18912.wav  \n","  inflating: /content/data/private_test/private_11088.wav  \n","  inflating: /content/data/private_test/private_07832.wav  \n","  inflating: /content/data/private_test/private_00185.wav  \n","  inflating: /content/data/private_test/private_10396.wav  \n","  inflating: /content/data/private_test/private_13847.wav  \n","  inflating: /content/data/private_test/private_02792.wav  \n","  inflating: /content/data/private_test/private_12581.wav  \n","  inflating: /content/data/private_test/private_09837.wav  \n","  inflating: /content/data/private_test/private_16917.wav  \n","  inflating: /content/data/private_test/private_02962.wav  \n","  inflating: /content/data/private_test/private_14866.wav  \n","  inflating: /content/data/private_test/private_08297.wav  \n","  inflating: /content/data/private_test/private_18084.wav  \n","  inflating: /content/data/private_test/private_00813.wav  \n","  inflating: /content/data/private_test/private_09189.wav  \n","  inflating: /content/data/private_test/private_08283.wav  \n","  inflating: /content/data/private_test/private_18090.wav  \n","  inflating: /content/data/private_test/private_00807.wav  \n","  inflating: /content/data/private_test/private_14872.wav  \n","  inflating: /content/data/private_test/private_02976.wav  \n","  inflating: /content/data/private_test/private_09823.wav  \n","  inflating: /content/data/private_test/private_16903.wav  \n","  inflating: /content/data/private_test/private_00152.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00152.wav  \n","  inflating: /content/data/private_test/private_10341.wav  \n","  inflating: /content/data/private_test/private_09610.wav  \n","  inflating: /content/data/private_test/private_06523.wav  \n","  inflating: /content/data/private_test/private_19403.wav  \n","  inflating: /content/data/private_test/private_16730.wav  \n","  inflating: /content/data/private_test/private_15239.wav  \n","  inflating: /content/data/private_test/private_02745.wav  \n","  inflating: /content/data/private_test/private_13890.wav  \n","  inflating: /content/data/private_test/private_12556.wav  \n","  inflating: /content/data/private_test/private_04334.wav  \n","  inflating: /content/data/private_test/private_13648.wav  \n","  inflating: /content/data/private_test/private_14127.wav  \n","  inflating: /content/data/private_test/private_14641.wav  \n","  inflating: /content/data/private_test/private_05994.wav  \n","  inflating: /content/data/private_test/private_04452.wav  \n","  inflating: /content/data/private_test/private_14899.wav  \n","  inflating: /content/data/private_test/private_12230.wav  \n","  inflating: /content/data/private_test/private_02023.wav  \n","  inflating: /content/data/private_test/private_19365.wav  \n","  inflating: /content/data/private_test/private_16056.wav  \n","  inflating: /content/data/private_test/private_09176.wav  \n","  inflating: /content/data/private_test/private_11739.wav  \n","  inflating: /content/data/private_test/private_06245.wav  \n","  inflating: /content/data/private_test/private_10427.wav  \n","  inflating: /content/data/private_test/private_08268.wav  \n","  inflating: /content/data/private_test/private_00634.wav  \n","  inflating: /content/data/private_test/private_17348.wav  \n","  inflating: /content/data/private_test/private_19371.wav  \n","  inflating: /content/data/private_test/private_16042.wav  \n","  inflating: /content/data/private_test/private_09162.wav  \n","  inflating: /content/data/private_test/private_06251.wav  \n","  inflating: /content/data/private_test/private_10433.wav  \n","  inflating: /content/data/private_test/private_00620.wav  \n","  inflating: /content/data/private_test/private_05980.wav  \n","  inflating: /content/data/private_test/private_14655.wav  \n","  inflating: /content/data/private_test/private_03329.wav  \n","  inflating: /content/data/private_test/private_04446.wav  \n","  inflating: /content/data/private_test/private_05758.wav  \n","  inflating: /content/data/private_test/private_12224.wav  \n","  inflating: /content/data/private_test/private_02037.wav  \n","  inflating: /content/data/private_test/private_13884.wav  \n","  inflating: /content/data/private_test/private_02751.wav  \n","  inflating: /content/data/private_test/private_12542.wav  \n","  inflating: /content/data/private_test/private_04320.wav  \n","  inflating: /content/data/private_test/private_02989.wav  \n","  inflating: /content/data/private_test/private_14133.wav  \n","  inflating: /content/data/private_test/private_00146.wav  \n","  inflating: /content/data/private_test/private_18709.wav  \n","  inflating: /content/data/private_test/private_10355.wav  \n","  inflating: /content/data/private_test/private_07629.wav  \n","  inflating: /content/data/private_test/private_09604.wav  \n","  inflating: /content/data/private_test/private_06537.wav  \n","  inflating: /content/data/private_test/private_19417.wav  \n","  inflating: /content/data/private_test/private_01258.wav  \n","  inflating: /content/data/private_test/private_16724.wav  \n","  inflating: /content/data/private_test/private_02779.wav  \n","  inflating: /content/data/private_test/private_15205.wav  \n","  inflating: /content/data/private_test/private_05016.wav  \n","  inflating: /content/data/private_test/private_13674.wav  \n","  inflating: /content/data/private_test/private_04308.wav  \n","  inflating: /content/data/private_test/private_03467.wav  \n","  inflating: /content/data/private_test/private_17412.wav  \n","  inflating: /content/data/private_test/private_18721.wav  \n","  inflating: /content/data/private_test/private_07601.wav  \n","  inflating: /content/data/private_test/private_08532.wav  \n","  inflating: /content/data/private_test/private_11063.wav  \n","  inflating: /content/data/private_test/private_01270.wav  \n","  inflating: /content/data/private_test/private_01516.wav  \n","  inflating: /content/data/private_test/private_19359.wav  \n","  inflating: /content/data/private_test/private_06279.wav  \n","  inflating: /content/data/private_test/private_11705.wav  \n","  inflating: /content/data/private_test/private_07167.wav  \n","  inflating: /content/data/private_test/private_08254.wav  \n","  inflating: /content/data/private_test/private_17374.wav  \n","  inflating: /content/data/private_test/private_18047.wav  \n","  inflating: /content/data/private_test/private_00608.wav  \n","  inflating: /content/data/private_test/private_03301.wav  \n","  inflating: /content/data/private_test/private_13112.wav  \n","  inflating: /content/data/private_test/private_05770.wav  \n","  inflating: /content/data/private_test/private_15563.wav  \n","  inflating: /content/data/private_test/private_03315.wav  \n","  inflating: /content/data/private_test/private_14669.wav  \n","  inflating: /content/data/private_test/private_13106.wav  \n","  inflating: /content/data/private_test/private_12218.wav  \n","  inflating: /content/data/private_test/private_05764.wav  \n","  inflating: /content/data/private_test/private_15577.wav  \n","  inflating: /content/data/private_test/private_01502.wav  \n","  inflating: /content/data/private_test/private_11711.wav  \n","  inflating: /content/data/private_test/private_07173.wav  \n","  inflating: /content/data/private_test/private_08240.wav  \n","  inflating: /content/data/private_test/private_17360.wav  \n","  inflating: /content/data/private_test/private_18053.wav  \n","  inflating: /content/data/private_test/private_17406.wav  \n","  inflating: /content/data/private_test/private_18735.wav  \n","  inflating: /content/data/private_test/private_07615.wav  \n","  inflating: /content/data/private_test/private_08526.wav  \n","  inflating: /content/data/private_test/private_10369.wav  \n","  inflating: /content/data/private_test/private_11077.wav  \n","  inflating: /content/data/private_test/private_09638.wav  \n","  inflating: /content/data/private_test/private_16718.wav  \n","  inflating: /content/data/private_test/private_01264.wav  \n","  inflating: /content/data/private_test/private_15211.wav  \n","  inflating: /content/data/private_test/private_05002.wav  \n","  inflating: /content/data/private_test/private_13660.wav  \n","  inflating: /content/data/private_test/private_03473.wav  \n","  inflating: /content/data/private_test/private_15561.wav  \n","  inflating: /content/data/private_test/private_05772.wav  \n","  inflating: /content/data/private_test/private_13110.wav  \n","  inflating: /content/data/private_test/private_03303.wav  \n","  inflating: /content/data/private_test/private_17376.wav  \n","  inflating: /content/data/private_test/private_18045.wav  \n","  inflating: /content/data/private_test/private_07165.wav  \n","  inflating: /content/data/private_test/private_08256.wav  \n","  inflating: /content/data/private_test/private_10419.wav  \n","  inflating: /content/data/private_test/private_11707.wav  \n","  inflating: /content/data/private_test/private_09148.wav  \n","  inflating: /content/data/private_test/private_16068.wav  \n","  inflating: /content/data/private_test/private_01514.wav  \n","  inflating: /content/data/private_test/private_01272.wav  \n","  inflating: /content/data/private_test/private_11061.wav  \n","  inflating: /content/data/private_test/private_07603.wav  \n","  inflating: /content/data/private_test/private_08530.wav  \n","  inflating: /content/data/private_test/private_17410.wav  \n","  inflating: /content/data/private_test/private_18723.wav  \n","  inflating: /content/data/private_test/private_03465.wav  \n","  inflating: /content/data/private_test/private_14119.wav  \n","  inflating: /content/data/private_test/private_13676.wav  \n","  inflating: /content/data/private_test/private_12568.wav  \n","  inflating: /content/data/private_test/private_05014.wav  \n","  inflating: /content/data/private_test/private_15207.wav  \n","  inflating: /content/data/private_test/private_03471.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03471.wav  \n","  inflating: /content/data/private_test/private_13662.wav  \n","  inflating: /content/data/private_test/private_05000.wav  \n","  inflating: /content/data/private_test/private_15213.wav  \n","  inflating: /content/data/private_test/private_01266.wav  \n","  inflating: /content/data/private_test/private_19429.wav  \n","  inflating: /content/data/private_test/private_06509.wav  \n","  inflating: /content/data/private_test/private_11075.wav  \n","  inflating: /content/data/private_test/private_07617.wav  \n","  inflating: /content/data/private_test/private_08524.wav  \n","  inflating: /content/data/private_test/private_17404.wav  \n","  inflating: /content/data/private_test/private_18737.wav  \n","  inflating: /content/data/private_test/private_00178.wav  \n","  inflating: /content/data/private_test/private_17362.wav  \n","  inflating: /content/data/private_test/private_18051.wav  \n","  inflating: /content/data/private_test/private_07171.wav  \n","  inflating: /content/data/private_test/private_08242.wav  \n","  inflating: /content/data/private_test/private_11713.wav  \n","  inflating: /content/data/private_test/private_01500.wav  \n","  inflating: /content/data/private_test/private_02009.wav  \n","  inflating: /content/data/private_test/private_15575.wav  \n","  inflating: /content/data/private_test/private_05766.wav  \n","  inflating: /content/data/private_test/private_13104.wav  \n","  inflating: /content/data/private_test/private_04478.wav  \n","  inflating: /content/data/private_test/private_03317.wav  \n","  inflating: /content/data/private_test/private_00636.wav  \n","  inflating: /content/data/private_test/private_18079.wav  \n","  inflating: /content/data/private_test/private_10425.wav  \n","  inflating: /content/data/private_test/private_07159.wav  \n","  inflating: /content/data/private_test/private_09174.wav  \n","  inflating: /content/data/private_test/private_06247.wav  \n","  inflating: /content/data/private_test/private_19367.wav  \n","  inflating: /content/data/private_test/private_01528.wav  \n","  inflating: /content/data/private_test/private_16054.wav  \n","  inflating: /content/data/private_test/private_02021.wav  \n","  inflating: /content/data/private_test/private_12232.wav  \n","  inflating: /content/data/private_test/private_04450.wav  \n","  inflating: /content/data/private_test/private_05996.wav  \n","  inflating: /content/data/private_test/private_14643.wav  \n","  inflating: /content/data/private_test/private_14125.wav  \n","  inflating: /content/data/private_test/private_03459.wav  \n","  inflating: /content/data/private_test/private_04336.wav  \n","  inflating: /content/data/private_test/private_05028.wav  \n","  inflating: /content/data/private_test/private_12554.wav  \n","  inflating: /content/data/private_test/private_13892.wav  \n","  inflating: /content/data/private_test/private_02747.wav  \n","  inflating: /content/data/private_test/private_19401.wav  \n","  inflating: /content/data/private_test/private_16732.wav  \n","  inflating: /content/data/private_test/private_09612.wav  \n","  inflating: /content/data/private_test/private_06521.wav  \n","  inflating: /content/data/private_test/private_10343.wav  \n","  inflating: /content/data/private_test/private_00150.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00150.wav  \n","  inflating: /content/data/private_test/private_19415.wav  \n","  inflating: /content/data/private_test/private_16726.wav  \n","  inflating: /content/data/private_test/private_09606.wav  \n","  inflating: /content/data/private_test/private_11049.wav  \n","  inflating: /content/data/private_test/private_06535.wav  \n","  inflating: /content/data/private_test/private_10357.wav  \n","  inflating: /content/data/private_test/private_08518.wav  \n","  inflating: /content/data/private_test/private_00144.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00144.wav  \n","  inflating: /content/data/private_test/private_17438.wav  \n","  inflating: /content/data/private_test/private_14131.wav  \n","  inflating: /content/data/private_test/private_04322.wav  \n","  inflating: /content/data/private_test/private_12540.wav  \n","  inflating: /content/data/private_test/private_02753.wav  \n","  inflating: /content/data/private_test/private_13886.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13886.wav  \n","  inflating: /content/data/private_test/private_15549.wav  \n","  inflating: /content/data/private_test/private_02035.wav  \n","  inflating: /content/data/private_test/private_12226.wav  \n","  inflating: /content/data/private_test/private_04444.wav  \n","  inflating: /content/data/private_test/private_13138.wav  \n","  inflating: /content/data/private_test/private_14657.wav  \n","  inflating: /content/data/private_test/private_05982.wav  \n","  inflating: /content/data/private_test/private_00622.wav  \n","  inflating: /content/data/private_test/private_10431.wav  \n","  inflating: /content/data/private_test/private_09160.wav  \n","  inflating: /content/data/private_test/private_06253.wav  \n","  inflating: /content/data/private_test/private_19373.wav  \n","  inflating: /content/data/private_test/private_16040.wav  \n","  inflating: /content/data/private_test/private_00811.wav  \n","  inflating: /content/data/private_test/private_19398.wav  \n","  inflating: /content/data/private_test/private_18086.wav  \n","  inflating: /content/data/private_test/private_08295.wav  \n","  inflating: /content/data/private_test/private_05969.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_05969.wav  \n","  inflating: /content/data/private_test/private_14864.wav  \n","  inflating: /content/data/private_test/private_02960.wav  \n","  inflating: /content/data/private_test/private_16915.wav  \n","  inflating: /content/data/private_test/private_09835.wav  \n","  inflating: /content/data/private_test/private_07818.wav  \n","  inflating: /content/data/private_test/private_18938.wav  \n","  inflating: /content/data/private_test/private_16901.wav  \n","  inflating: /content/data/private_test/private_09821.wav  \n","  inflating: /content/data/private_test/private_13879.wav  \n","  inflating: /content/data/private_test/private_02974.wav  \n","  inflating: /content/data/private_test/private_14870.wav  \n","  inflating: /content/data/private_test/private_00805.wav  \n","  inflating: /content/data/private_test/private_11908.wav  \n","  inflating: /content/data/private_test/private_18092.wav  \n","  inflating: /content/data/private_test/private_08281.wav  \n","  inflating: /content/data/private_test/private_04493.wav  \n","  inflating: /content/data/private_test/private_05955.wav  \n","  inflating: /content/data/private_test/private_14680.wav  \n","  inflating: /content/data/private_test/private_14858.wav  \n","  inflating: /content/data/private_test/private_06284.wav  \n","  inflating: /content/data/private_test/private_16097.wav  \n","  inflating: /content/data/private_test/private_11920.wav  \n","  inflating: /content/data/private_test/private_17389.wav  \n","  inflating: /content/data/private_test/private_10380.wav  \n","  inflating: /content/data/private_test/private_16929.wav  \n","  inflating: /content/data/private_test/private_09809.wav  \n","  inflating: /content/data/private_test/private_00193.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00193.wav  \n","  inflating: /content/data/private_test/private_07824.wav  \n","  inflating: /content/data/private_test/private_18904.wav  \n","  inflating: /content/data/private_test/private_12597.wav  \n","  inflating: /content/data/private_test/private_13851.wav  \n","  inflating: /content/data/private_test/private_02784.wav  \n","  inflating: /content/data/private_test/private_13689.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_13689.wav  \n","  inflating: /content/data/private_test/private_12583.wav  \n","  inflating: /content/data/private_test/private_02790.wav  \n","  inflating: /content/data/private_test/private_13845.wav  \n","  inflating: /content/data/private_test/private_02948.wav  \n","  inflating: /content/data/private_test/private_10394.wav  \n","  inflating: /content/data/private_test/private_00187.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00187.wav  \n","  inflating: /content/data/private_test/private_01299.wav  \n","  inflating: /content/data/private_test/private_07830.wav  \n","  inflating: /content/data/private_test/private_18910.wav  \n","  inflating: /content/data/private_test/private_00839.wav  \n","  inflating: /content/data/private_test/private_06290.wav  \n","  inflating: /content/data/private_test/private_16083.wav  \n","  inflating: /content/data/private_test/private_11934.wav  \n","  inflating: /content/data/private_test/private_04487.wav  \n","  inflating: /content/data/private_test/private_14694.wav  \n","  inflating: /content/data/private_test/private_05941.wav  \n","  inflating: /content/data/private_test/private_05799.wav  \n","  inflating: /content/data/private_test/private_15005.wav  \n","  inflating: /content/data/private_test/private_02579.wav  \n","  inflating: /content/data/private_test/private_05216.wav  \n","  inflating: /content/data/private_test/private_04108.wav  \n","  inflating: /content/data/private_test/private_13474.wav  \n","  inflating: /content/data/private_test/private_03667.wav  \n","  inflating: /content/data/private_test/private_18521.wav  \n","  inflating: /content/data/private_test/private_17612.wav  \n","  inflating: /content/data/private_test/private_08732.wav  \n","  inflating: /content/data/private_test/private_07401.wav  \n","  inflating: /content/data/private_test/private_11263.wav  \n","  inflating: /content/data/private_test/private_01070.wav  \n","  inflating: /content/data/private_test/private_01716.wav  \n","  inflating: /content/data/private_test/private_19159.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19159.wav  \n","  inflating: /content/data/private_test/private_11505.wav  \n","  inflating: /content/data/private_test/private_06079.wav  \n","  inflating: /content/data/private_test/private_08054.wav  \n","  inflating: /content/data/private_test/private_07367.wav  \n","  inflating: /content/data/private_test/private_18247.wav  \n","  inflating: /content/data/private_test/private_00408.wav  \n","  inflating: /content/data/private_test/private_17174.wav  \n","  inflating: /content/data/private_test/private_03101.wav  \n","  inflating: /content/data/private_test/private_13312.wav  \n","  inflating: /content/data/private_test/private_05570.wav  \n","  inflating: /content/data/private_test/private_15763.wav  \n","  inflating: /content/data/private_test/private_14469.wav  \n","  inflating: /content/data/private_test/private_03115.wav  \n","  inflating: /content/data/private_test/private_13306.wav  \n","  inflating: /content/data/private_test/private_05564.wav  \n","  inflating: /content/data/private_test/private_12018.wav  \n","  inflating: /content/data/private_test/private_15777.wav  \n","  inflating: /content/data/private_test/private_01702.wav  \n","  inflating: /content/data/private_test/private_11511.wav  \n","  inflating: /content/data/private_test/private_08040.wav  \n","  inflating: /content/data/private_test/private_07373.wav  \n","  inflating: /content/data/private_test/private_18253.wav  \n","  inflating: /content/data/private_test/private_17160.wav  \n","  inflating: /content/data/private_test/private_18535.wav  \n","  inflating: /content/data/private_test/private_17606.wav  \n","  inflating: /content/data/private_test/private_08726.wav  \n","  inflating: /content/data/private_test/private_10169.wav  \n","  inflating: /content/data/private_test/private_07415.wav  \n","  inflating: /content/data/private_test/private_11277.wav  \n","  inflating: /content/data/private_test/private_09438.wav  \n","  inflating: /content/data/private_test/private_01064.wav  \n","  inflating: /content/data/private_test/private_16518.wav  \n","  inflating: /content/data/private_test/private_15011.wav  \n","  inflating: /content/data/private_test/private_05202.wav  \n","  inflating: /content/data/private_test/private_13460.wav  \n","  inflating: /content/data/private_test/private_03673.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03673.wav  \n","  inflating: /content/data/private_test/private_00352.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00352.wav  \n","  inflating: /content/data/private_test/private_10141.wav  \n","  inflating: /content/data/private_test/private_06723.wav  \n","  inflating: /content/data/private_test/private_09410.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_09410.wav  \n","  inflating: /content/data/private_test/private_16530.wav  \n","  inflating: /content/data/private_test/private_19603.wav  \n","  inflating: /content/data/private_test/private_02545.wav  \n","  inflating: /content/data/private_test/private_15039.wav  \n","  inflating: /content/data/private_test/private_03883.wav  \n","  inflating: /content/data/private_test/private_12756.wav  \n","  inflating: /content/data/private_test/private_13448.wav  \n","  inflating: /content/data/private_test/private_04134.wav  \n","  inflating: /content/data/private_test/private_14327.wav  \n","  inflating: /content/data/private_test/private_14441.wav  \n","  inflating: /content/data/private_test/private_15987.wav  \n","  inflating: /content/data/private_test/private_04652.wav  \n","  inflating: /content/data/private_test/private_12030.wav  \n","  inflating: /content/data/private_test/private_02223.wav  \n","  inflating: /content/data/private_test/private_16256.wav  \n","  inflating: /content/data/private_test/private_19165.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19165.wav  \n","  inflating: /content/data/private_test/private_06045.wav  \n","  inflating: /content/data/private_test/private_09376.wav  \n","  inflating: /content/data/private_test/private_11539.wav  \n","  inflating: /content/data/private_test/private_10627.wav  \n","  inflating: /content/data/private_test/private_08068.wav  \n","  inflating: /content/data/private_test/private_17148.wav  \n","  inflating: /content/data/private_test/private_00434.wav  \n","  inflating: /content/data/private_test/private_16242.wav  \n","  inflating: /content/data/private_test/private_19171.wav  \n","  inflating: /content/data/private_test/private_06051.wav  \n","  inflating: /content/data/private_test/private_09362.wav  \n","  inflating: /content/data/private_test/private_10633.wav  \n","  inflating: /content/data/private_test/private_00420.wav  \n","  inflating: /content/data/private_test/private_03129.wav  \n","  inflating: /content/data/private_test/private_14455.wav  \n","  inflating: /content/data/private_test/private_04646.wav  \n","  inflating: /content/data/private_test/private_15993.wav  \n","  inflating: /content/data/private_test/private_12024.wav  \n","  inflating: /content/data/private_test/private_05558.wav  \n","  inflating: /content/data/private_test/private_02237.wav  \n","  inflating: /content/data/private_test/private_02551.wav  \n","  inflating: /content/data/private_test/private_12742.wav  \n","  inflating: /content/data/private_test/private_03897.wav  \n","  inflating: /content/data/private_test/private_04120.wav  \n","  inflating: /content/data/private_test/private_14333.wav  \n","  inflating: /content/data/private_test/private_00346.wav  \n","  inflating: /content/data/private_test/private_18509.wav  \n","  inflating: /content/data/private_test/private_07429.wav  \n","  inflating: /content/data/private_test/private_10155.wav  \n","  inflating: /content/data/private_test/private_06737.wav  \n","  inflating: /content/data/private_test/private_09404.wav  \n","  inflating: /content/data/private_test/private_16524.wav  \n","  inflating: /content/data/private_test/private_19617.wav  \n","  inflating: /content/data/private_test/private_01058.wav  \n","  inflating: /content/data/private_test/private_17809.wav  \n","  inflating: /content/data/private_test/private_08929.wav  \n","  inflating: /content/data/private_test/private_06904.wav  \n","  inflating: /content/data/private_test/private_19824.wav  \n","  inflating: /content/data/private_test/private_12971.wav  \n","  inflating: /content/data/private_test/private_04875.wav  \n","  inflating: /content/data/private_test/private_15978.wav  \n","  inflating: /content/data/private_test/private_08097.wav  \n","  inflating: /content/data/private_test/private_18284.wav  \n","  inflating: /content/data/private_test/private_10800.wav  \n","  inflating: /content/data/private_test/private_09389.wav  \n","  inflating: /content/data/private_test/private_01919.wav  \n","  inflating: /content/data/private_test/private_08083.wav  \n","  inflating: /content/data/private_test/private_18290.wav  \n","  inflating: /content/data/private_test/private_10814.wav  \n","  inflating: /content/data/private_test/private_04861.wav  \n","  inflating: /content/data/private_test/private_12965.wav  \n","  inflating: /content/data/private_test/private_03868.wav  \n","  inflating: /content/data/private_test/private_06910.wav  \n","  inflating: /content/data/private_test/private_19830.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19830.wav  \n","  inflating: /content/data/private_test/private_03698.wav  \n","  inflating: /content/data/private_test/private_02586.wav  \n","  inflating: /content/data/private_test/private_03840.wav  \n","  inflating: /content/data/private_test/private_12795.wav  \n","  inflating: /content/data/private_test/private_17835.wav  \n","  inflating: /content/data/private_test/private_08915.wav  \n","  inflating: /content/data/private_test/private_06938.wav  \n","  inflating: /content/data/private_test/private_00391.wav  \n","  inflating: /content/data/private_test/private_19818.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19818.wav  \n","  inflating: /content/data/private_test/private_10182.wav  \n","  inflating: /content/data/private_test/private_07398.wav  \n","  inflating: /content/data/private_test/private_01931.wav  \n","  inflating: /content/data/private_test/private_16295.wav  \n","  inflating: /content/data/private_test/private_06086.wav  \n","  inflating: /content/data/private_test/private_04849.wav  \n","  inflating: /content/data/private_test/private_14482.wav  \n","  inflating: /content/data/private_test/private_15944.wav  \n","  inflating: /content/data/private_test/private_04691.wav  \n","  inflating: /content/data/private_test/private_15788.wav  \n","  inflating: /content/data/private_test/private_14496.wav  \n","  inflating: /content/data/private_test/private_04685.wav  \n","  inflating: /content/data/private_test/private_15950.wav  \n","  inflating: /content/data/private_test/private_01925.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_01925.wav  \n","  inflating: /content/data/private_test/private_16281.wav  \n","  inflating: /content/data/private_test/private_10828.wav  \n","  inflating: /content/data/private_test/private_06092.wav  \n","  inflating: /content/data/private_test/private_17821.wav  \n","  inflating: /content/data/private_test/private_11288.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_11288.wav  \n","  inflating: /content/data/private_test/private_08901.wav  \n","  inflating: /content/data/private_test/private_00385.wav  \n","  inflating: /content/data/private_test/private_10196.wav  \n","  inflating: /content/data/private_test/private_12959.wav  \n","  inflating: /content/data/private_test/private_02592.wav  \n","  inflating: /content/data/private_test/private_12781.wav  \n","  inflating: /content/data/private_test/private_03854.wav  \n","  inflating: /content/data/private_test/private_03713.wav  \n","  inflating: /content/data/private_test/private_13500.wav  \n","  inflating: /content/data/private_test/private_05362.wav  \n","  inflating: /content/data/private_test/private_15171.wav  \n","  inflating: /content/data/private_test/private_01104.wav  \n","  inflating: /content/data/private_test/private_16478.wav  \n","  inflating: /content/data/private_test/private_11317.wav  \n","  inflating: /content/data/private_test/private_09558.wav  \n","  inflating: /content/data/private_test/private_08646.wav  \n","  inflating: /content/data/private_test/private_19993.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19993.wav  \n","  inflating: /content/data/private_test/private_10009.wav  \n","  inflating: /content/data/private_test/private_07575.wav  \n","  inflating: /content/data/private_test/private_18455.wav  \n","  inflating: /content/data/private_test/private_17766.wav  \n","  inflating: /content/data/private_test/private_18333.wav  \n","  inflating: /content/data/private_test/private_17000.wav  \n","  inflating: /content/data/private_test/private_08120.wav  \n","  inflating: /content/data/private_test/private_07213.wav  \n","  inflating: /content/data/private_test/private_11471.wav  \n","  inflating: /content/data/private_test/private_01662.wav  \n","  inflating: /content/data/private_test/private_15617.wav  \n","  inflating: /content/data/private_test/private_05404.wav  \n","  inflating: /content/data/private_test/private_12178.wav  \n","  inflating: /content/data/private_test/private_13266.wav  \n","  inflating: /content/data/private_test/private_14509.wav  \n","  inflating: /content/data/private_test/private_03075.wav  \n","  inflating: /content/data/private_test/private_15603.wav  \n","  inflating: /content/data/private_test/private_05410.wav  \n","  inflating: /content/data/private_test/private_13272.wav  \n","  inflating: /content/data/private_test/private_03061.wav  \n","  inflating: /content/data/private_test/private_18327.wav  \n","  inflating: /content/data/private_test/private_00568.wav  \n","  inflating: /content/data/private_test/private_17014.wav  \n","  inflating: /content/data/private_test/private_08134.wav  \n","  inflating: /content/data/private_test/private_07207.wav  \n","  inflating: /content/data/private_test/private_11465.wav  \n","  inflating: /content/data/private_test/private_06119.wav  \n","  inflating: /content/data/private_test/private_01676.wav  \n","  inflating: /content/data/private_test/private_19039.wav  \n","  inflating: /content/data/private_test/private_01110.wav  \n","  inflating: /content/data/private_test/private_11303.wav  \n","  inflating: /content/data/private_test/private_19987.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19987.wav  \n","  inflating: /content/data/private_test/private_08652.wav  \n","  inflating: /content/data/private_test/private_07561.wav  \n","  inflating: /content/data/private_test/private_18441.wav  \n","  inflating: /content/data/private_test/private_17772.wav  \n","  inflating: /content/data/private_test/private_03707.wav  \n","  inflating: /content/data/private_test/private_04068.wav  \n","  inflating: /content/data/private_test/private_13514.wav  \n","  inflating: /content/data/private_test/private_05376.wav  \n","  inflating: /content/data/private_test/private_15165.wav  \n","  inflating: /content/data/private_test/private_02419.wav  \n","  inflating: /content/data/private_test/private_16444.wav  \n","  inflating: /content/data/private_test/private_19777.wav  \n","  inflating: /content/data/private_test/private_01138.wav  \n","  inflating: /content/data/private_test/private_17982.wav  \n","  inflating: /content/data/private_test/private_06657.wav  \n","  inflating: /content/data/private_test/private_09564.wav  \n","  inflating: /content/data/private_test/private_07549.wav  \n","  inflating: /content/data/private_test/private_10035.wav  \n","  inflating: /content/data/private_test/private_00226.wav  \n","  inflating: /content/data/private_test/private_18469.wav  \n","  inflating: /content/data/private_test/private_14253.wav  \n","  inflating: /content/data/private_test/private_04040.wav  \n","  inflating: /content/data/private_test/private_12622.wav  \n","  inflating: /content/data/private_test/private_02431.wav  \n","  inflating: /content/data/private_test/private_02357.wav  \n","  inflating: /content/data/private_test/private_12144.wav  \n","  inflating: /content/data/private_test/private_05438.wav  \n","  inflating: /content/data/private_test/private_04726.wav  \n","  inflating: /content/data/private_test/private_03049.wav  \n","  inflating: /content/data/private_test/private_14535.wav  \n","  inflating: /content/data/private_test/private_00540.wav  \n","  inflating: /content/data/private_test/private_01886.wav  \n","  inflating: /content/data/private_test/private_10753.wav  \n","  inflating: /content/data/private_test/private_06131.wav  \n","  inflating: /content/data/private_test/private_09202.wav  \n","  inflating: /content/data/private_test/private_16322.wav  \n","  inflating: /content/data/private_test/private_19011.wav  \n","  inflating: /content/data/private_test/private_17028.wav  \n","  inflating: /content/data/private_test/private_00554.wav  \n","  inflating: /content/data/private_test/private_10747.wav  \n","  inflating: /content/data/private_test/private_01892.wav  \n","  inflating: /content/data/private_test/private_08108.wav  \n","  inflating: /content/data/private_test/private_06125.wav  \n","  inflating: /content/data/private_test/private_09216.wav  \n","  inflating: /content/data/private_test/private_11459.wav  \n","  inflating: /content/data/private_test/private_16336.wav  \n","  inflating: /content/data/private_test/private_19005.wav  \n","  inflating: /content/data/private_test/private_02343.wav  \n","  inflating: /content/data/private_test/private_12150.wav  \n","  inflating: /content/data/private_test/private_04732.wav  \n","  inflating: /content/data/private_test/private_14521.wav  \n","  inflating: /content/data/private_test/private_14247.wav  \n","  inflating: /content/data/private_test/private_13528.wav  \n","  inflating: /content/data/private_test/private_04054.wav  \n","  inflating: /content/data/private_test/private_12636.wav  \n","  inflating: /content/data/private_test/private_02425.wav  \n","  inflating: /content/data/private_test/private_15159.wav  \n","  inflating: /content/data/private_test/private_16450.wav  \n","  inflating: /content/data/private_test/private_19763.wav  \n","  inflating: /content/data/private_test/private_06643.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06643.wav  \n","  inflating: /content/data/private_test/private_17996.wav  \n","  inflating: /content/data/private_test/private_09570.wav  \n","  inflating: /content/data/private_test/private_10021.wav  \n","  inflating: /content/data/private_test/private_00232.wav  \n","  inflating: /content/data/private_test/private_08685.wav  \n","  inflating: /content/data/private_test/private_19950.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19950.wav  \n","  inflating: /content/data/private_test/private_18496.wav  \n","  inflating: /content/data/private_test/private_06870.wav  \n","  inflating: /content/data/private_test/private_19788.wav  \n","  inflating: /content/data/private_test/private_03908.wav  \n","  inflating: /content/data/private_test/private_12805.wav  \n","  inflating: /content/data/private_test/private_04901.wav  \n","  inflating: /content/data/private_test/private_10974.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_10974.wav  \n","  inflating: /content/data/private_test/private_01879.wav  \n","  inflating: /content/data/private_test/private_10960.wav  \n","  inflating: /content/data/private_test/private_15818.wav  \n","  inflating: /content/data/private_test/private_04915.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_04915.wav  \n","  inflating: /content/data/private_test/private_12811.wav  \n","  inflating: /content/data/private_test/private_19944.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19944.wav  \n","  inflating: /content/data/private_test/private_08691.wav  \n","  inflating: /content/data/private_test/private_18482.wav  \n","  inflating: /content/data/private_test/private_06864.wav  \n","  inflating: /content/data/private_test/private_08849.wav  \n","  inflating: /content/data/private_test/private_17969.wav  \n","  inflating: /content/data/private_test/private_03934.wav  \n","  inflating: /content/data/private_test/private_12839.wav  \n","  inflating: /content/data/private_test/private_14290.wav  \n","  inflating: /content/data/private_test/private_04083.wav  \n","  inflating: /content/data/private_test/private_17799.wav  \n","  inflating: /content/data/private_test/private_16487.wav  \n","  inflating: /content/data/private_test/private_08861.wav  \n","  inflating: /content/data/private_test/private_17941.wav  \n","  inflating: /content/data/private_test/private_06694.wav  \n","  inflating: /content/data/private_test/private_10948.wav  \n","  inflating: /content/data/private_test/private_00583.wav  \n","  inflating: /content/data/private_test/private_01845.wav  \n","  inflating: /content/data/private_test/private_10790.wav  \n","  inflating: /content/data/private_test/private_13299.wav  \n","  inflating: /content/data/private_test/private_15830.wav  \n","  inflating: /content/data/private_test/private_02394.wav  \n","  inflating: /content/data/private_test/private_12187.wav  \n","  inflating: /content/data/private_test/private_15824.wav  \n","  inflating: /content/data/private_test/private_02380.wav  \n","  inflating: /content/data/private_test/private_04929.wav  \n","  inflating: /content/data/private_test/private_12193.wav  \n","  inflating: /content/data/private_test/private_01689.wav  \n","  inflating: /content/data/private_test/private_00597.wav  \n","  inflating: /content/data/private_test/private_10784.wav  \n","  inflating: /content/data/private_test/private_01851.wav  \n","  inflating: /content/data/private_test/private_19978.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19978.wav  \n","  inflating: /content/data/private_test/private_06858.wav  \n","  inflating: /content/data/private_test/private_16493.wav  \n","  inflating: /content/data/private_test/private_08875.wav  \n","  inflating: /content/data/private_test/private_06680.wav  \n","  inflating: /content/data/private_test/private_17955.wav  \n","  inflating: /content/data/private_test/private_03920.wav  \n","  inflating: /content/data/private_test/private_05389.wav  \n","  inflating: /content/data/private_test/private_14284.wav  \n","  inflating: /content/data/private_test/private_04097.wav  \n","  inflating: /content/data/private_test/private_03277.wav  \n","  inflating: /content/data/private_test/private_13064.wav  \n","  inflating: /content/data/private_test/private_04518.wav  \n","  inflating: /content/data/private_test/private_05606.wav  \n","  inflating: /content/data/private_test/private_02169.wav  \n","  inflating: /content/data/private_test/private_15415.wav  \n","  inflating: /content/data/private_test/private_01460.wav  \n","  inflating: /content/data/private_test/private_11673.wav  \n","  inflating: /content/data/private_test/private_07011.wav  \n","  inflating: /content/data/private_test/private_08322.wav  \n","  inflating: /content/data/private_test/private_17202.wav  \n","  inflating: /content/data/private_test/private_18131.wav  \n","  inflating: /content/data/private_test/private_17564.wav  \n","  inflating: /content/data/private_test/private_18657.wav  \n","  inflating: /content/data/private_test/private_09982.wav  \n","  inflating: /content/data/private_test/private_00018.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00018.wav  \n","  inflating: /content/data/private_test/private_07777.wav  \n","  inflating: /content/data/private_test/private_08444.wav  \n","  inflating: /content/data/private_test/private_06469.wav  \n","  inflating: /content/data/private_test/private_11115.wav  \n","  inflating: /content/data/private_test/private_01306.wav  \n","  inflating: /content/data/private_test/private_19549.wav  \n","  inflating: /content/data/private_test/private_15373.wav  \n","  inflating: /content/data/private_test/private_05160.wav  \n","  inflating: /content/data/private_test/private_13702.wav  \n","  inflating: /content/data/private_test/private_03511.wav  \n","  inflating: /content/data/private_test/private_15367.wav  \n","  inflating: /content/data/private_test/private_12408.wav  \n","  inflating: /content/data/private_test/private_05174.wav  \n","  inflating: /content/data/private_test/private_13716.wav  \n","  inflating: /content/data/private_test/private_03505.wav  \n","  inflating: /content/data/private_test/private_14079.wav  \n","  inflating: /content/data/private_test/private_17570.wav  \n","  inflating: /content/data/private_test/private_09996.wav  \n","  inflating: /content/data/private_test/private_18643.wav  \n","  inflating: /content/data/private_test/private_07763.wav  \n","  inflating: /content/data/private_test/private_08450.wav  \n","  inflating: /content/data/private_test/private_11101.wav  \n","  inflating: /content/data/private_test/private_01312.wav  \n","  inflating: /content/data/private_test/private_16108.wav  \n","  inflating: /content/data/private_test/private_01474.wav  \n","  inflating: /content/data/private_test/private_11667.wav  \n","  inflating: /content/data/private_test/private_09028.wav  \n","  inflating: /content/data/private_test/private_07005.wav  \n","  inflating: /content/data/private_test/private_08336.wav  \n","  inflating: /content/data/private_test/private_10579.wav  \n","  inflating: /content/data/private_test/private_17216.wav  \n","  inflating: /content/data/private_test/private_18125.wav  \n","  inflating: /content/data/private_test/private_03263.wav  \n","  inflating: /content/data/private_test/private_13070.wav  \n","  inflating: /content/data/private_test/private_05612.wav  \n","  inflating: /content/data/private_test/private_15401.wav  \n","  inflating: /content/data/private_test/private_19213.wav  \n","  inflating: /content/data/private_test/private_16120.wav  \n","  inflating: /content/data/private_test/private_09000.wav  \n","  inflating: /content/data/private_test/private_06333.wav  \n","  inflating: /content/data/private_test/private_10551.wav  \n","  inflating: /content/data/private_test/private_11897.wav  \n","  inflating: /content/data/private_test/private_00742.wav  \n","  inflating: /content/data/private_test/private_14737.wav  \n","  inflating: /content/data/private_test/private_04524.wav  \n","  inflating: /content/data/private_test/private_13058.wav  \n","  inflating: /content/data/private_test/private_12346.wav  \n","  inflating: /content/data/private_test/private_15429.wav  \n","  inflating: /content/data/private_test/private_02155.wav  \n","  inflating: /content/data/private_test/private_02633.wav  \n","  inflating: /content/data/private_test/private_12420.wav  \n","  inflating: /content/data/private_test/private_04242.wav  \n","  inflating: /content/data/private_test/private_14051.wav  \n","  inflating: /content/data/private_test/private_00024.wav  \n","  inflating: /content/data/private_test/private_17558.wav  \n","  inflating: /content/data/private_test/private_10237.wav  \n","  inflating: /content/data/private_test/private_08478.wav  \n","  inflating: /content/data/private_test/private_09766.wav  \n","  inflating: /content/data/private_test/private_11129.wav  \n","  inflating: /content/data/private_test/private_06455.wav  \n","  inflating: /content/data/private_test/private_19575.wav  \n","  inflating: /content/data/private_test/private_07993.wav  \n","  inflating: /content/data/private_test/private_16646.wav  \n","  inflating: /content/data/private_test/private_00030.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00030.wav  \n","  inflating: /content/data/private_test/private_10223.wav  \n","  inflating: /content/data/private_test/private_09772.wav  \n","  inflating: /content/data/private_test/private_06441.wav  \n","  inflating: /content/data/private_test/private_19561.wav  \n","  inflating: /content/data/private_test/private_16652.wav  \n","  inflating: /content/data/private_test/private_07987.wav  \n","  inflating: /content/data/private_test/private_02627.wav  \n","  inflating: /content/data/private_test/private_05148.wav  \n","  inflating: /content/data/private_test/private_12434.wav  \n","  inflating: /content/data/private_test/private_04256.wav  \n","  inflating: /content/data/private_test/private_14045.wav  \n","  inflating: /content/data/private_test/private_03539.wav  \n","  inflating: /content/data/private_test/private_14723.wav  \n","  inflating: /content/data/private_test/private_04530.wav  \n","  inflating: /content/data/private_test/private_12352.wav  \n","  inflating: /content/data/private_test/private_02141.wav  \n","  inflating: /content/data/private_test/private_19207.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19207.wav  \n","  inflating: /content/data/private_test/private_01448.wav  \n","  inflating: /content/data/private_test/private_16134.wav  \n","  inflating: /content/data/private_test/private_09014.wav  \n","  inflating: /content/data/private_test/private_06327.wav  \n","  inflating: /content/data/private_test/private_10545.wav  \n","  inflating: /content/data/private_test/private_07039.wav  \n","  inflating: /content/data/private_test/private_00756.wav  \n","  inflating: /content/data/private_test/private_11883.wav  \n","  inflating: /content/data/private_test/private_18119.wav  \n","  inflating: /content/data/private_test/private_11868.wav  \n","  inflating: /content/data/private_test/private_00965.wav  \n","  inflating: /content/data/private_test/private_14910.wav  \n","  inflating: /content/data/private_test/private_02814.wav  \n","  inflating: /content/data/private_test/private_13919.wav  \n","  inflating: /content/data/private_test/private_09799.wav  \n","  inflating: /content/data/private_test/private_18694.wav  \n","  inflating: /content/data/private_test/private_09941.wav  \n","  inflating: /content/data/private_test/private_16861.wav  \n","  inflating: /content/data/private_test/private_08487.wav  \n","  inflating: /content/data/private_test/private_18858.wav  \n","  inflating: /content/data/private_test/private_07978.wav  \n","  inflating: /content/data/private_test/private_09955.wav  \n","  inflating: /content/data/private_test/private_18680.wav  \n","  inflating: /content/data/private_test/private_16875.wav  \n","  inflating: /content/data/private_test/private_08493.wav  \n","  inflating: /content/data/private_test/private_02800.wav  \n","  inflating: /content/data/private_test/private_14904.wav  \n","  inflating: /content/data/private_test/private_05809.wav  \n","  inflating: /content/data/private_test/private_00971.wav  \n","  inflating: /content/data/private_test/private_12385.wav  \n","  inflating: /content/data/private_test/private_02196.wav  \n","  inflating: /content/data/private_test/private_05821.wav  \n","  inflating: /content/data/private_test/private_03288.wav  \n","  inflating: /content/data/private_test/private_10592.wav  \n","  inflating: /content/data/private_test/private_11854.wav  \n","  inflating: /content/data/private_test/private_00781.wav  \n","  inflating: /content/data/private_test/private_00959.wav  \n","  inflating: /content/data/private_test/private_18870.wav  \n","  inflating: /content/data/private_test/private_06496.wav  \n","  inflating: /content/data/private_test/private_07950.wav  \n","  inflating: /content/data/private_test/private_16685.wav  \n","  inflating: /content/data/private_test/private_07788.wav  \n","  inflating: /content/data/private_test/private_04281.wav  \n","  inflating: /content/data/private_test/private_02828.wav  \n","  inflating: /content/data/private_test/private_14092.wav  \n","  inflating: /content/data/private_test/private_13925.wav  \n","  inflating: /content/data/private_test/private_04295.wav  \n","  inflating: /content/data/private_test/private_14086.wav  \n","  inflating: /content/data/private_test/private_15398.wav  \n","  inflating: /content/data/private_test/private_13931.wav  \n","  inflating: /content/data/private_test/private_18864.wav  \n","  inflating: /content/data/private_test/private_06482.wav  \n","  inflating: /content/data/private_test/private_16691.wav  \n","  inflating: /content/data/private_test/private_07944.wav  \n","  inflating: /content/data/private_test/private_09969.wav  \n","  inflating: /content/data/private_test/private_16849.wav  \n","  inflating: /content/data/private_test/private_10586.wav  \n","  inflating: /content/data/private_test/private_00795.wav  \n","  inflating: /content/data/private_test/private_11840.wav  \n","  inflating: /content/data/private_test/private_11698.wav  \n","  inflating: /content/data/private_test/private_14938.wav  \n","  inflating: /content/data/private_test/private_12391.wav  \n","  inflating: /content/data/private_test/private_02182.wav  \n","  inflating: /content/data/private_test/private_05835.wav  \n","  inflating: /content/data/private_test/private_16848.wav  \n","  inflating: /content/data/private_test/private_09968.wav  \n","  inflating: /content/data/private_test/private_07945.wav  \n","  inflating: /content/data/private_test/private_16690.wav  \n","  inflating: /content/data/private_test/private_06483.wav  \n","  inflating: /content/data/private_test/private_18865.wav  \n","  inflating: /content/data/private_test/private_13930.wav  \n","  inflating: /content/data/private_test/private_15399.wav  \n","  inflating: /content/data/private_test/private_14087.wav  \n","  inflating: /content/data/private_test/private_04294.wav  \n","  inflating: /content/data/private_test/private_05834.wav  \n","  inflating: /content/data/private_test/private_02183.wav  \n","  inflating: /content/data/private_test/private_12390.wav  \n","  inflating: /content/data/private_test/private_14939.wav  \n","  inflating: /content/data/private_test/private_11699.wav  \n","  inflating: /content/data/private_test/private_11841.wav  \n","  inflating: /content/data/private_test/private_00794.wav  \n","  inflating: /content/data/private_test/private_10587.wav  \n","  inflating: /content/data/private_test/private_00958.wav  \n","  inflating: /content/data/private_test/private_00780.wav  \n","  inflating: /content/data/private_test/private_11855.wav  \n","  inflating: /content/data/private_test/private_10593.wav  \n","  inflating: /content/data/private_test/private_03289.wav  \n","  inflating: /content/data/private_test/private_05820.wav  \n","  inflating: /content/data/private_test/private_02197.wav  \n","  inflating: /content/data/private_test/private_12384.wav  \n","  inflating: /content/data/private_test/private_13924.wav  \n","  inflating: /content/data/private_test/private_14093.wav  \n","  inflating: /content/data/private_test/private_02829.wav  \n","  inflating: /content/data/private_test/private_04280.wav  \n","  inflating: /content/data/private_test/private_07789.wav  \n","  inflating: /content/data/private_test/private_16684.wav  \n","  inflating: /content/data/private_test/private_07951.wav  \n","  inflating: /content/data/private_test/private_06497.wav  \n","  inflating: /content/data/private_test/private_18871.wav  \n","  inflating: /content/data/private_test/private_02801.wav  \n","  inflating: /content/data/private_test/private_08492.wav  \n","  inflating: /content/data/private_test/private_16874.wav  \n","  inflating: /content/data/private_test/private_18681.wav  \n","  inflating: /content/data/private_test/private_09954.wav  \n","  inflating: /content/data/private_test/private_07979.wav  \n","  inflating: /content/data/private_test/private_18859.wav  \n","  inflating: /content/data/private_test/private_00970.wav  \n","  inflating: /content/data/private_test/private_05808.wav  \n","  inflating: /content/data/private_test/private_14905.wav  \n","  inflating: /content/data/private_test/private_14911.wav  \n","  inflating: /content/data/private_test/private_00964.wav  \n","  inflating: /content/data/private_test/private_11869.wav  \n","  inflating: /content/data/private_test/private_08486.wav  \n","  inflating: /content/data/private_test/private_16860.wav  \n","  inflating: /content/data/private_test/private_09940.wav  \n","  inflating: /content/data/private_test/private_18695.wav  \n","  inflating: /content/data/private_test/private_09798.wav  \n","  inflating: /content/data/private_test/private_13918.wav  \n","  inflating: /content/data/private_test/private_02815.wav  \n","  inflating: /content/data/private_test/private_03538.wav  \n","  inflating: /content/data/private_test/private_14044.wav  \n","  inflating: /content/data/private_test/private_04257.wav  \n","  inflating: /content/data/private_test/private_12435.wav  \n","  inflating: /content/data/private_test/private_05149.wav  \n","  inflating: /content/data/private_test/private_02626.wav  \n","  inflating: /content/data/private_test/private_07986.wav  \n","  inflating: /content/data/private_test/private_16653.wav  \n","  inflating: /content/data/private_test/private_19560.wav  \n","  inflating: /content/data/private_test/private_06440.wav  \n","  inflating: /content/data/private_test/private_09773.wav  \n","  inflating: /content/data/private_test/private_10222.wav  \n","  inflating: /content/data/private_test/private_00031.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00031.wav  \n","  inflating: /content/data/private_test/private_11882.wav  \n","  inflating: /content/data/private_test/private_00757.wav  \n","  inflating: /content/data/private_test/private_18118.wav  \n","  inflating: /content/data/private_test/private_07038.wav  \n","  inflating: /content/data/private_test/private_10544.wav  \n","  inflating: /content/data/private_test/private_06326.wav  \n","  inflating: /content/data/private_test/private_09015.wav  \n","  inflating: /content/data/private_test/private_16135.wav  \n","  inflating: /content/data/private_test/private_19206.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19206.wav  \n","  inflating: /content/data/private_test/private_01449.wav  \n","  inflating: /content/data/private_test/private_02140.wav  \n","  inflating: /content/data/private_test/private_12353.wav  \n","  inflating: /content/data/private_test/private_04531.wav  \n","  inflating: /content/data/private_test/private_14722.wav  \n","  inflating: /content/data/private_test/private_02154.wav  \n","  inflating: /content/data/private_test/private_15428.wav  \n","  inflating: /content/data/private_test/private_12347.wav  \n","  inflating: /content/data/private_test/private_13059.wav  \n","  inflating: /content/data/private_test/private_04525.wav  \n","  inflating: /content/data/private_test/private_14736.wav  \n","  inflating: /content/data/private_test/private_00743.wav  \n","  inflating: /content/data/private_test/private_11896.wav  \n","  inflating: /content/data/private_test/private_10550.wav  \n","  inflating: /content/data/private_test/private_06332.wav  \n","  inflating: /content/data/private_test/private_09001.wav  \n","  inflating: /content/data/private_test/private_16121.wav  \n","  inflating: /content/data/private_test/private_19212.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19212.wav  \n","  inflating: /content/data/private_test/private_16647.wav  \n","  inflating: /content/data/private_test/private_07992.wav  \n","  inflating: /content/data/private_test/private_19574.wav  \n","  inflating: /content/data/private_test/private_06454.wav  \n","  inflating: /content/data/private_test/private_09767.wav  \n","  inflating: /content/data/private_test/private_11128.wav  \n","  inflating: /content/data/private_test/private_10236.wav  \n","  inflating: /content/data/private_test/private_08479.wav  \n","  inflating: /content/data/private_test/private_17559.wav  \n","  inflating: /content/data/private_test/private_00025.wav  \n","  inflating: /content/data/private_test/private_14050.wav  \n","  inflating: /content/data/private_test/private_04243.wav  \n","  inflating: /content/data/private_test/private_12421.wav  \n","  inflating: /content/data/private_test/private_02632.wav  \n","  inflating: /content/data/private_test/private_01313.wav  \n","  inflating: /content/data/private_test/private_11100.wav  \n","  inflating: /content/data/private_test/private_08451.wav  \n","  inflating: /content/data/private_test/private_07762.wav  \n","  inflating: /content/data/private_test/private_18642.wav  \n","  inflating: /content/data/private_test/private_09997.wav  \n","  inflating: /content/data/private_test/private_17571.wav  \n","  inflating: /content/data/private_test/private_14078.wav  \n","  inflating: /content/data/private_test/private_03504.wav  \n","  inflating: /content/data/private_test/private_13717.wav  \n","  inflating: /content/data/private_test/private_05175.wav  \n","  inflating: /content/data/private_test/private_12409.wav  \n","  inflating: /content/data/private_test/private_15366.wav  \n","  inflating: /content/data/private_test/private_15400.wav  \n","  inflating: /content/data/private_test/private_05613.wav  \n","  inflating: /content/data/private_test/private_13071.wav  \n","  inflating: /content/data/private_test/private_03262.wav  \n","  inflating: /content/data/private_test/private_18124.wav  \n","  inflating: /content/data/private_test/private_17217.wav  \n","  inflating: /content/data/private_test/private_08337.wav  \n","  inflating: /content/data/private_test/private_10578.wav  \n","  inflating: /content/data/private_test/private_07004.wav  \n","  inflating: /content/data/private_test/private_11666.wav  \n","  inflating: /content/data/private_test/private_09029.wav  \n","  inflating: /content/data/private_test/private_01475.wav  \n","  inflating: /content/data/private_test/private_16109.wav  \n","  inflating: /content/data/private_test/private_18130.wav  \n","  inflating: /content/data/private_test/private_17203.wav  \n","  inflating: /content/data/private_test/private_08323.wav  \n","  inflating: /content/data/private_test/private_07010.wav  \n","  inflating: /content/data/private_test/private_11672.wav  \n","  inflating: /content/data/private_test/private_01461.wav  \n","  inflating: /content/data/private_test/private_15414.wav  \n","  inflating: /content/data/private_test/private_02168.wav  \n","  inflating: /content/data/private_test/private_05607.wav  \n","  inflating: /content/data/private_test/private_04519.wav  \n","  inflating: /content/data/private_test/private_13065.wav  \n","  inflating: /content/data/private_test/private_03276.wav  \n","  inflating: /content/data/private_test/private_03510.wav  \n","  inflating: /content/data/private_test/private_13703.wav  \n","  inflating: /content/data/private_test/private_05161.wav  \n","  inflating: /content/data/private_test/private_15372.wav  \n","  inflating: /content/data/private_test/private_01307.wav  \n","  inflating: /content/data/private_test/private_19548.wav  \n","  inflating: /content/data/private_test/private_11114.wav  \n","  inflating: /content/data/private_test/private_06468.wav  \n","  inflating: /content/data/private_test/private_08445.wav  \n","  inflating: /content/data/private_test/private_07776.wav  \n","  inflating: /content/data/private_test/private_09983.wav  \n","  inflating: /content/data/private_test/private_18656.wav  \n","  inflating: /content/data/private_test/private_00019.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00019.wav  \n","  inflating: /content/data/private_test/private_17565.wav  \n","  inflating: /content/data/private_test/private_01850.wav  \n","  inflating: /content/data/private_test/private_10785.wav  \n","  inflating: /content/data/private_test/private_00596.wav  \n","  inflating: /content/data/private_test/private_01688.wav  \n","  inflating: /content/data/private_test/private_12192.wav  \n","  inflating: /content/data/private_test/private_04928.wav  \n","  inflating: /content/data/private_test/private_02381.wav  \n","  inflating: /content/data/private_test/private_15825.wav  \n","  inflating: /content/data/private_test/private_04096.wav  \n","  inflating: /content/data/private_test/private_14285.wav  \n","  inflating: /content/data/private_test/private_05388.wav  \n","  inflating: /content/data/private_test/private_03921.wav  \n","  inflating: /content/data/private_test/private_17954.wav  \n","  inflating: /content/data/private_test/private_06681.wav  \n","  inflating: /content/data/private_test/private_08874.wav  \n","  inflating: /content/data/private_test/private_16492.wav  \n","  inflating: /content/data/private_test/private_06859.wav  \n","  inflating: /content/data/private_test/private_19979.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19979.wav  \n","  inflating: /content/data/private_test/private_06695.wav  \n","  inflating: /content/data/private_test/private_17940.wav  \n","  inflating: /content/data/private_test/private_08860.wav  \n","  inflating: /content/data/private_test/private_16486.wav  \n","  inflating: /content/data/private_test/private_17798.wav  \n","  inflating: /content/data/private_test/private_04082.wav  \n","  inflating: /content/data/private_test/private_14291.wav  \n","  inflating: /content/data/private_test/private_12838.wav  \n","  inflating: /content/data/private_test/private_03935.wav  \n","  inflating: /content/data/private_test/private_12186.wav  \n","  inflating: /content/data/private_test/private_02395.wav  \n","  inflating: /content/data/private_test/private_15831.wav  \n","  inflating: /content/data/private_test/private_13298.wav  \n","  inflating: /content/data/private_test/private_10791.wav  \n","  inflating: /content/data/private_test/private_01844.wav  \n","  inflating: /content/data/private_test/private_00582.wav  \n","  inflating: /content/data/private_test/private_10949.wav  \n","  inflating: /content/data/private_test/private_04914.wav  \n","  inflating: /content/data/private_test/private_15819.wav  \n","  inflating: /content/data/private_test/private_10961.wav  \n","  inflating: /content/data/private_test/private_17968.wav  \n","  inflating: /content/data/private_test/private_08848.wav  \n","  inflating: /content/data/private_test/private_06865.wav  \n","  inflating: /content/data/private_test/private_18483.wav  \n","  inflating: /content/data/private_test/private_08690.wav  \n","  inflating: /content/data/private_test/private_19945.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19945.wav  \n","  inflating: /content/data/private_test/private_12810.wav  \n","  inflating: /content/data/private_test/private_12804.wav  \n","  inflating: /content/data/private_test/private_03909.wav  \n","  inflating: /content/data/private_test/private_19789.wav  \n","  inflating: /content/data/private_test/private_06871.wav  \n","  inflating: /content/data/private_test/private_18497.wav  \n","  inflating: /content/data/private_test/private_19951.wav  \n","  inflating: /content/data/private_test/private_08684.wav  \n","  inflating: /content/data/private_test/private_01878.wav  \n","  inflating: /content/data/private_test/private_10975.wav  \n","  inflating: /content/data/private_test/private_04900.wav  \n","  inflating: /content/data/private_test/private_14520.wav  \n","  inflating: /content/data/private_test/private_04733.wav  \n","  inflating: /content/data/private_test/private_12151.wav  \n","  inflating: /content/data/private_test/private_02342.wav  \n","  inflating: /content/data/private_test/private_19004.wav  \n","  inflating: /content/data/private_test/private_16337.wav  \n","  inflating: /content/data/private_test/private_09217.wav  \n","  inflating: /content/data/private_test/private_11458.wav  \n","  inflating: /content/data/private_test/private_06124.wav  \n","  inflating: /content/data/private_test/private_01893.wav  \n","  inflating: /content/data/private_test/private_10746.wav  \n","  inflating: /content/data/private_test/private_08109.wav  \n","  inflating: /content/data/private_test/private_00555.wav  \n","  inflating: /content/data/private_test/private_17029.wav  \n","  inflating: /content/data/private_test/private_00233.wav  \n","  inflating: /content/data/private_test/private_10020.wav  \n","  inflating: /content/data/private_test/private_09571.wav  \n","  inflating: /content/data/private_test/private_17997.wav  \n","  inflating: /content/data/private_test/private_06642.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_06642.wav  \n","  inflating: /content/data/private_test/private_19762.wav  \n","  inflating: /content/data/private_test/private_16451.wav  \n","  inflating: /content/data/private_test/private_15158.wav  \n","  inflating: /content/data/private_test/private_02424.wav  \n","  inflating: /content/data/private_test/private_12637.wav  \n","  inflating: /content/data/private_test/private_04055.wav  \n","  inflating: /content/data/private_test/private_13529.wav  \n","  inflating: /content/data/private_test/private_14246.wav  \n","  inflating: /content/data/private_test/private_02430.wav  \n","  inflating: /content/data/private_test/private_12623.wav  \n","  inflating: /content/data/private_test/private_04041.wav  \n","  inflating: /content/data/private_test/private_14252.wav  \n","  inflating: /content/data/private_test/private_00227.wav  \n","  inflating: /content/data/private_test/private_18468.wav  \n","  inflating: /content/data/private_test/private_10034.wav  \n","  inflating: /content/data/private_test/private_07548.wav  \n","  inflating: /content/data/private_test/private_09565.wav  \n","  inflating: /content/data/private_test/private_06656.wav  \n","  inflating: /content/data/private_test/private_17983.wav  \n","  inflating: /content/data/private_test/private_19776.wav  \n","  inflating: /content/data/private_test/private_01139.wav  \n","  inflating: /content/data/private_test/private_16445.wav  \n","  inflating: /content/data/private_test/private_19010.wav  \n","  inflating: /content/data/private_test/private_16323.wav  \n","  inflating: /content/data/private_test/private_09203.wav  \n","  inflating: /content/data/private_test/private_06130.wav  \n","  inflating: /content/data/private_test/private_10752.wav  \n","  inflating: /content/data/private_test/private_01887.wav  \n","  inflating: /content/data/private_test/private_00541.wav  \n","  inflating: /content/data/private_test/private_14534.wav  \n","  inflating: /content/data/private_test/private_03048.wav  \n","  inflating: /content/data/private_test/private_04727.wav  \n","  inflating: /content/data/private_test/private_05439.wav  \n","  inflating: /content/data/private_test/private_12145.wav  \n","  inflating: /content/data/private_test/private_02356.wav  \n","  inflating: /content/data/private_test/private_01677.wav  \n","  inflating: /content/data/private_test/private_19038.wav  \n","  inflating: /content/data/private_test/private_06118.wav  \n","  inflating: /content/data/private_test/private_11464.wav  \n","  inflating: /content/data/private_test/private_07206.wav  \n","  inflating: /content/data/private_test/private_08135.wav  \n","  inflating: /content/data/private_test/private_17015.wav  \n","  inflating: /content/data/private_test/private_18326.wav  \n","  inflating: /content/data/private_test/private_00569.wav  \n","  inflating: /content/data/private_test/private_03060.wav  \n","  inflating: /content/data/private_test/private_13273.wav  \n","  inflating: /content/data/private_test/private_05411.wav  \n","  inflating: /content/data/private_test/private_15602.wav  \n","  inflating: /content/data/private_test/private_02418.wav  \n","  inflating: /content/data/private_test/private_15164.wav  \n","  inflating: /content/data/private_test/private_05377.wav  \n","  inflating: /content/data/private_test/private_13515.wav  \n","  inflating: /content/data/private_test/private_04069.wav  \n","  inflating: /content/data/private_test/private_03706.wav  \n","  inflating: /content/data/private_test/private_17773.wav  \n","  inflating: /content/data/private_test/private_18440.wav  \n","  inflating: /content/data/private_test/private_07560.wav  \n","  inflating: /content/data/private_test/private_08653.wav  \n","  inflating: /content/data/private_test/private_19986.wav  \n","  inflating: /content/data/private_test/private_11302.wav  \n","  inflating: /content/data/private_test/private_01111.wav  \n","  inflating: /content/data/private_test/private_17767.wav  \n","  inflating: /content/data/private_test/private_18454.wav  \n","  inflating: /content/data/private_test/private_07574.wav  \n","  inflating: /content/data/private_test/private_19992.wav  \n","  inflating: /content/data/private_test/private_08647.wav  \n","  inflating: /content/data/private_test/private_10008.wav  \n","  inflating: /content/data/private_test/private_11316.wav  \n","  inflating: /content/data/private_test/private_09559.wav  \n","  inflating: /content/data/private_test/private_16479.wav  \n","  inflating: /content/data/private_test/private_01105.wav  \n","  inflating: /content/data/private_test/private_15170.wav  \n","  inflating: /content/data/private_test/private_05363.wav  \n","  inflating: /content/data/private_test/private_13501.wav  \n","  inflating: /content/data/private_test/private_03712.wav  \n","  inflating: /content/data/private_test/private_03074.wav  \n","  inflating: /content/data/private_test/private_14508.wav  \n","  inflating: /content/data/private_test/private_13267.wav  \n","  inflating: /content/data/private_test/private_12179.wav  \n","  inflating: /content/data/private_test/private_05405.wav  \n","  inflating: /content/data/private_test/private_15616.wav  \n","  inflating: /content/data/private_test/private_01663.wav  \n","  inflating: /content/data/private_test/private_11470.wav  \n","  inflating: /content/data/private_test/private_07212.wav  \n","  inflating: /content/data/private_test/private_08121.wav  \n","  inflating: /content/data/private_test/private_17001.wav  \n","  inflating: /content/data/private_test/private_18332.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_18332.wav  \n","  inflating: /content/data/private_test/private_06093.wav  \n","  inflating: /content/data/private_test/private_10829.wav  \n","  inflating: /content/data/private_test/private_16280.wav  \n","  inflating: /content/data/private_test/private_01924.wav  \n","  inflating: /content/data/private_test/private_15951.wav  \n","  inflating: /content/data/private_test/private_04684.wav  \n","  inflating: /content/data/private_test/private_14497.wav  \n","  inflating: /content/data/private_test/private_15789.wav  \n","  inflating: /content/data/private_test/private_03855.wav  \n","  inflating: /content/data/private_test/private_12780.wav  \n","  inflating: /content/data/private_test/private_02593.wav  \n","  inflating: /content/data/private_test/private_12958.wav  \n","  inflating: /content/data/private_test/private_10197.wav  \n","  inflating: /content/data/private_test/private_00384.wav  \n","  inflating: /content/data/private_test/private_08900.wav  \n","  inflating: /content/data/private_test/private_11289.wav  \n","  inflating: /content/data/private_test/private_17820.wav  \n","  inflating: /content/data/private_test/private_19819.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19819.wav  \n","  inflating: /content/data/private_test/private_10183.wav  \n","  inflating: /content/data/private_test/private_00390.wav  \n","  inflating: /content/data/private_test/private_06939.wav  \n","  inflating: /content/data/private_test/private_08914.wav  \n","  inflating: /content/data/private_test/private_17834.wav  \n","  inflating: /content/data/private_test/private_12794.wav  \n","  inflating: /content/data/private_test/private_03841.wav  \n","  inflating: /content/data/private_test/private_02587.wav  \n","  inflating: /content/data/private_test/private_03699.wav  \n","  inflating: /content/data/private_test/private_04690.wav  \n","  inflating: /content/data/private_test/private_15945.wav  \n","  inflating: /content/data/private_test/private_14483.wav  \n","  inflating: /content/data/private_test/private_04848.wav  \n","  inflating: /content/data/private_test/private_06087.wav  \n","  inflating: /content/data/private_test/private_16294.wav  \n","  inflating: /content/data/private_test/private_01930.wav  \n","  inflating: /content/data/private_test/private_07399.wav  \n","  inflating: /content/data/private_test/private_04860.wav  \n","  inflating: /content/data/private_test/private_10815.wav  \n","  inflating: /content/data/private_test/private_18291.wav  \n","  inflating: /content/data/private_test/private_01918.wav  \n","  inflating: /content/data/private_test/private_08082.wav  \n","  inflating: /content/data/private_test/private_19831.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19831.wav  \n","  inflating: /content/data/private_test/private_06911.wav  \n","  inflating: /content/data/private_test/private_03869.wav  \n","  inflating: /content/data/private_test/private_12964.wav  \n","  inflating: /content/data/private_test/private_12970.wav  \n","  inflating: /content/data/private_test/private_19825.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19825.wav  \n","  inflating: /content/data/private_test/private_06905.wav  \n","  inflating: /content/data/private_test/private_08928.wav  \n","  inflating: /content/data/private_test/private_17808.wav  \n","  inflating: /content/data/private_test/private_09388.wav  \n","  inflating: /content/data/private_test/private_10801.wav  \n","  inflating: /content/data/private_test/private_18285.wav  \n","  inflating: /content/data/private_test/private_08096.wav  \n","  inflating: /content/data/private_test/private_15979.wav  \n","  inflating: /content/data/private_test/private_04874.wav  \n","  inflating: /content/data/private_test/private_02236.wav  \n","  inflating: /content/data/private_test/private_05559.wav  \n","  inflating: /content/data/private_test/private_12025.wav  \n","  inflating: /content/data/private_test/private_15992.wav  \n","  inflating: /content/data/private_test/private_04647.wav  \n","  inflating: /content/data/private_test/private_14454.wav  \n","  inflating: /content/data/private_test/private_03128.wav  \n","  inflating: /content/data/private_test/private_00421.wav  \n","  inflating: /content/data/private_test/private_10632.wav  \n","  inflating: /content/data/private_test/private_09363.wav  \n","  inflating: /content/data/private_test/private_06050.wav  \n","  inflating: /content/data/private_test/private_19170.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_19170.wav  \n","  inflating: /content/data/private_test/private_16243.wav  \n","  inflating: /content/data/private_test/private_19616.wav  \n","  inflating: /content/data/private_test/private_01059.wav  \n","  inflating: /content/data/private_test/private_16525.wav  \n","  inflating: /content/data/private_test/private_09405.wav  \n","  inflating: /content/data/private_test/private_06736.wav  \n","  inflating: /content/data/private_test/private_10154.wav  \n","  inflating: /content/data/private_test/private_07428.wav  \n","  inflating: /content/data/private_test/private_00347.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00347.wav  \n","  inflating: /content/data/private_test/private_18508.wav  \n","  inflating: /content/data/private_test/private_14332.wav  \n","  inflating: /content/data/private_test/private_04121.wav  \n","  inflating: /content/data/private_test/private_03896.wav  \n","  inflating: /content/data/private_test/private_12743.wav  \n","  inflating: /content/data/private_test/private_02550.wav  \n","  inflating: /content/data/private_test/private_14326.wav  \n","  inflating: /content/data/private_test/private_04135.wav  \n","  inflating: /content/data/private_test/private_13449.wav  \n","  inflating: /content/data/private_test/private_12757.wav  \n","  inflating: /content/data/private_test/private_03882.wav  \n","  inflating: /content/data/private_test/private_15038.wav  \n","  inflating: /content/data/private_test/private_02544.wav  \n","  inflating: /content/data/private_test/private_19602.wav  \n","  inflating: /content/data/private_test/private_16531.wav  \n","  inflating: /content/data/private_test/private_09411.wav  \n","  inflating: /content/data/private_test/private_06722.wav  \n","  inflating: /content/data/private_test/private_10140.wav  \n","  inflating: /content/data/private_test/private_00353.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00353.wav  \n","  inflating: /content/data/private_test/private_00435.wav  \n","  inflating: /content/data/private_test/private_17149.wav  \n","  inflating: /content/data/private_test/private_10626.wav  \n","  inflating: /content/data/private_test/private_08069.wav  \n","  inflating: /content/data/private_test/private_09377.wav  \n","  inflating: /content/data/private_test/private_11538.wav  \n","  inflating: /content/data/private_test/private_06044.wav  \n","  inflating: /content/data/private_test/private_19164.wav  \n","  inflating: /content/data/private_test/private_16257.wav  \n","  inflating: /content/data/private_test/private_02222.wav  \n","  inflating: /content/data/private_test/private_12031.wav  \n","  inflating: /content/data/private_test/private_04653.wav  \n","  inflating: /content/data/private_test/private_15986.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_15986.wav  \n","  inflating: /content/data/private_test/private_14440.wav  \n","  inflating: /content/data/private_test/private_17161.wav  \n","  inflating: /content/data/private_test/private_18252.wav  \n","  inflating: /content/data/private_test/private_07372.wav  \n","  inflating: /content/data/private_test/private_08041.wav  \n","  inflating: /content/data/private_test/private_11510.wav  \n","  inflating: /content/data/private_test/private_01703.wav  \n","  inflating: /content/data/private_test/private_15776.wav  \n","  inflating: /content/data/private_test/private_12019.wav  \n","  inflating: /content/data/private_test/private_05565.wav  \n","  inflating: /content/data/private_test/private_13307.wav  \n","  inflating: /content/data/private_test/private_03114.wav  \n","  inflating: /content/data/private_test/private_14468.wav  \n","  inflating: /content/data/private_test/private_03672.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03672.wav  \n","  inflating: /content/data/private_test/private_13461.wav  \n","  inflating: /content/data/private_test/private_05203.wav  \n","  inflating: /content/data/private_test/private_15010.wav  \n","  inflating: /content/data/private_test/private_16519.wav  \n","  inflating: /content/data/private_test/private_01065.wav  \n","  inflating: /content/data/private_test/private_11276.wav  \n","  inflating: /content/data/private_test/private_09439.wav  \n","  inflating: /content/data/private_test/private_07414.wav  \n","  inflating: /content/data/private_test/private_08727.wav  \n","  inflating: /content/data/private_test/private_10168.wav  \n","  inflating: /content/data/private_test/private_17607.wav  \n","  inflating: /content/data/private_test/private_18534.wav  \n","  inflating: /content/data/private_test/private_01071.wav  \n","  inflating: /content/data/private_test/private_11262.wav  \n","  inflating: /content/data/private_test/private_07400.wav  \n","  inflating: /content/data/private_test/private_08733.wav  \n","  inflating: /content/data/private_test/private_17613.wav  \n","  inflating: /content/data/private_test/private_18520.wav  \n","  inflating: /content/data/private_test/private_03666.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_03666.wav  \n","  inflating: /content/data/private_test/private_13475.wav  \n","  inflating: /content/data/private_test/private_04109.wav  \n","  inflating: /content/data/private_test/private_05217.wav  \n","  inflating: /content/data/private_test/private_02578.wav  \n","  inflating: /content/data/private_test/private_15004.wav  \n","  inflating: /content/data/private_test/private_15762.wav  \n","  inflating: /content/data/private_test/private_05571.wav  \n","  inflating: /content/data/private_test/private_13313.wav  \n","  inflating: /content/data/private_test/private_03100.wav  \n","  inflating: /content/data/private_test/private_17175.wav  \n","  inflating: /content/data/private_test/private_18246.wav  \n","  inflating: /content/data/private_test/private_00409.wav  \n","  inflating: /content/data/private_test/private_07366.wav  \n","  inflating: /content/data/private_test/private_08055.wav  \n","  inflating: /content/data/private_test/private_06078.wav  \n","  inflating: /content/data/private_test/private_11504.wav  \n","  inflating: /content/data/private_test/private_01717.wav  \n","  inflating: /content/data/private_test/private_19158.wav  \n","  inflating: /content/data/private_test/private_18911.wav  \n","  inflating: /content/data/private_test/private_07831.wav  \n","  inflating: /content/data/private_test/private_01298.wav  \n","  inflating: /content/data/private_test/private_00186.wav  \n","  inflating: /content/data/private_test/private_10395.wav  \n","  inflating: /content/data/private_test/private_02949.wav  \n","  inflating: /content/data/private_test/private_13844.wav  \n","  inflating: /content/data/private_test/private_02791.wav  \n","  inflating: /content/data/private_test/private_12582.wav  \n","  inflating: /content/data/private_test/private_05798.wav  \n","  inflating: /content/data/private_test/private_05940.wav  \n","  inflating: /content/data/private_test/private_14695.wav  \n","  inflating: /content/data/private_test/private_04486.wav  \n","  inflating: /content/data/private_test/private_11935.wav  \n","  inflating: /content/data/private_test/private_16082.wav  \n","  inflating: /content/data/private_test/private_06291.wav  \n","  inflating: /content/data/private_test/private_00838.wav  \n","  inflating: /content/data/private_test/private_17388.wav  \n","  inflating: /content/data/private_test/private_11921.wav  \n","  inflating: /content/data/private_test/private_16096.wav  \n","  inflating: /content/data/private_test/private_06285.wav  \n","  inflating: /content/data/private_test/private_14859.wav  \n","  inflating: /content/data/private_test/private_14681.wav  \n","  inflating: /content/data/private_test/private_05954.wav  \n","  inflating: /content/data/private_test/private_04492.wav  \n","  inflating: /content/data/private_test/private_13688.wav  \n","  inflating: /content/data/private_test/private_02785.wav  \n","  inflating: /content/data/private_test/private_13850.wav  \n","  inflating: /content/data/private_test/private_12596.wav  \n","  inflating: /content/data/private_test/private_18905.wav  \n","  inflating: /content/data/private_test/private_07825.wav  \n","  inflating: /content/data/private_test/private_09808.wav  \n","  inflating: /content/data/private_test/private_00192.wav  \n","  inflating: /content/data/private_test/private_16928.wav  \n","  inflating: /content/data/private_test/private_10381.wav  \n","  inflating: /content/data/private_test/private_02975.wav  \n","  inflating: /content/data/private_test/private_13878.wav  \n","  inflating: /content/data/private_test/private_09820.wav  \n","  inflating: /content/data/private_test/private_16900.wav  \n","  inflating: /content/data/private_test/private_08280.wav  \n","  inflating: /content/data/private_test/private_11909.wav  \n","  inflating: /content/data/private_test/private_18093.wav  \n","  inflating: /content/data/private_test/private_00804.wav  \n","  inflating: /content/data/private_test/private_14871.wav  \n","  inflating: /content/data/private_test/private_14865.wav  \n","  inflating: /content/data/private_test/private_05968.wav  \n","  inflating: /content/data/private_test/private_08294.wav  \n","  inflating: /content/data/private_test/private_18087.wav  \n","  inflating: /content/data/private_test/private_19399.wav  \n","  inflating: /content/data/private_test/private_00810.wav  \n","  inflating: /content/data/private_test/private_18939.wav  \n","  inflating: /content/data/private_test/private_07819.wav  \n","  inflating: /content/data/private_test/private_09834.wav  \n","  inflating: /content/data/private_test/private_16914.wav  \n","  inflating: /content/data/private_test/private_02961.wav  \n","  inflating: /content/data/private_test/private_13887.wav  \n","  inflating: /content/data/private_test/private_02752.wav  \n","  inflating: /content/data/private_test/private_12541.wav  \n","  inflating: /content/data/private_test/private_04323.wav  \n","  inflating: /content/data/private_test/private_14130.wav  \n","  inflating: /content/data/private_test/private_17439.wav  \n","  inflating: /content/data/private_test/private_00145.wav  \n","  inflating: /content/data/private_test/private_10356.wav  \n","  inflating: /content/data/private_test/private_08519.wav  \n","  inflating: /content/data/private_test/private_06534.wav  \n","  inflating: /content/data/private_test/private_09607.wav  \n","  inflating: /content/data/private_test/private_11048.wav  \n","  inflating: /content/data/private_test/private_16727.wav  \n","  inflating: /content/data/private_test/private_19414.wav  \n","  inflating: /content/data/private_test/private_16041.wav  \n","  inflating: /content/data/private_test/private_19372.wav  \n","  inflating: /content/data/private_test/private_06252.wav  \n","  inflating: /content/data/private_test/private_09161.wav  \n","  inflating: /content/data/private_test/private_10430.wav  \n","  inflating: /content/data/private_test/private_00623.wav  \n","  inflating: /content/data/private_test/private_05983.wav  \n","  inflating: /content/data/private_test/private_14656.wav  \n","  inflating: /content/data/private_test/private_13139.wav  \n","  inflating: /content/data/private_test/private_04445.wav  \n","  inflating: /content/data/private_test/private_12227.wav  \n","  inflating: /content/data/private_test/private_02034.wav  \n","  inflating: /content/data/private_test/private_15548.wav  \n","  inflating: /content/data/private_test/private_14642.wav  \n","  inflating: /content/data/private_test/private_05997.wav  \n","  inflating: /content/data/private_test/private_04451.wav  \n","  inflating: /content/data/private_test/private_12233.wav  \n","  inflating: /content/data/private_test/private_02020.wav  \n","  inflating: /content/data/private_test/private_16055.wav  \n","  inflating: /content/data/private_test/private_19366.wav  \n","  inflating: /content/data/private_test/private_01529.wav  \n","  inflating: /content/data/private_test/private_06246.wav  \n","  inflating: /content/data/private_test/private_09175.wav  \n","  inflating: /content/data/private_test/private_07158.wav  \n","  inflating: /content/data/private_test/private_10424.wav  \n","  inflating: /content/data/private_test/private_00637.wav  \n","  inflating: /content/data/private_test/private_18078.wav  \n","  inflating: /content/data/private_test/private_00151.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00151.wav  \n","  inflating: /content/data/private_test/private_10342.wav  \n","  inflating: /content/data/private_test/private_06520.wav  \n","  inflating: /content/data/private_test/private_09613.wav  \n","  inflating: /content/data/private_test/private_16733.wav  \n","  inflating: /content/data/private_test/private_19400.wav  \n","  inflating: /content/data/private_test/private_02746.wav  \n","  inflating: /content/data/private_test/private_13893.wav  \n","  inflating: /content/data/private_test/private_12555.wav  \n","  inflating: /content/data/private_test/private_05029.wav  \n","  inflating: /content/data/private_test/private_04337.wav  \n","  inflating: /content/data/private_test/private_03458.wav  \n","  inflating: /content/data/private_test/private_14124.wav  \n","  inflating: /content/data/private_test/private_18736.wav  \n","  inflating: /content/data/private_test/private_00179.wav  \n","  inflating: /content/data/__MACOSX/private_test/._private_00179.wav  \n","  inflating: /content/data/private_test/private_17405.wav  \n","  inflating: /content/data/private_test/private_08525.wav  \n","  inflating: /content/data/private_test/private_07616.wav  \n","  inflating: /content/data/private_test/private_11074.wav  \n","  inflating: /content/data/private_test/private_06508.wav  \n","  inflating: /content/data/private_test/private_01267.wav  \n","  inflating: /content/data/private_test/private_19428.wav  \n","  inflating: /content/data/private_test/private_15212.wav  \n","  inflating: /content/data/private_test/private_05001.wav  \n","  inflating: /content/data/private_test/private_13663.wav  \n","  inflating: /content/data/private_test/private_03470.wav  \n","  inflating: /content/data/private_test/private_03316.wav  \n","  inflating: /content/data/private_test/private_04479.wav  \n","  inflating: /content/data/private_test/private_13105.wav  \n","  inflating: /content/data/private_test/private_05767.wav  \n","  inflating: /content/data/private_test/private_15574.wav  \n","  inflating: /content/data/private_test/private_02008.wav  \n","  inflating: /content/data/private_test/private_01501.wav  \n","  inflating: /content/data/private_test/private_11712.wav  \n","  inflating: /content/data/private_test/private_08243.wav  \n","  inflating: /content/data/private_test/private_07170.wav  \n","  inflating: /content/data/private_test/private_18050.wav  \n","  inflating: /content/data/private_test/private_17363.wav  \n","  inflating: /content/data/private_test/private_01515.wav  \n","  inflating: /content/data/private_test/private_16069.wav  \n","  inflating: /content/data/private_test/private_11706.wav  \n","  inflating: /content/data/private_test/private_09149.wav  \n","  inflating: /content/data/private_test/private_08257.wav  \n","  inflating: /content/data/private_test/private_10418.wav  \n","  inflating: /content/data/private_test/private_07164.wav  \n","  inflating: /content/data/private_test/private_18044.wav  \n","  inflating: /content/data/private_test/private_17377.wav  \n","  inflating: /content/data/private_test/private_03302.wav  \n","  inflating: /content/data/private_test/private_13111.wav  \n","  inflating: /content/data/private_test/private_05773.wav  \n","  inflating: /content/data/private_test/private_15560.wav  \n","  inflating: /content/data/private_test/private_15206.wav  \n","  inflating: /content/data/private_test/private_05015.wav  \n","  inflating: /content/data/private_test/private_12569.wav  \n","  inflating: /content/data/private_test/private_13677.wav  \n","  inflating: /content/data/private_test/private_14118.wav  \n","  inflating: /content/data/private_test/private_03464.wav  \n","  inflating: /content/data/private_test/private_18722.wav  \n","  inflating: /content/data/private_test/private_17411.wav  \n","  inflating: /content/data/private_test/private_08531.wav  \n","  inflating: /content/data/private_test/private_07602.wav  \n","  inflating: /content/data/private_test/private_11060.wav  \n","  inflating: /content/data/private_test/private_01273.wav  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQlG4lBgsPV2","executionInfo":{"status":"ok","timestamp":1623286824120,"user_tz":-480,"elapsed":20169,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"671836d4-c4c6-4fde-8f68-d7f0b4262d21"},"source":["# !wget https://zenodo.org/record/3987831/files/Cnn14_DecisionLevelAtt_mAP%3D0.425.pth?download=1 -O /content/Cnn14_DecisionLevelAtt_mAP0.425.pth\n","!wget https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1 -O /content/Cnn14_mAP_0.431.pth"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-06-10 01:00:03--  https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1\n","Resolving zenodo.org (zenodo.org)... 137.138.76.77\n","Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 327428481 (312M) [application/octet-stream]\n","Saving to: /content/Cnn14_mAP_0.431.pth\n","\n","/content/Cnn14_mAP_ 100%[===================>] 312.26M  18.9MB/s    in 18s     \n","\n","2021-06-10 01:00:23 (17.0 MB/s) - /content/Cnn14_mAP_0.431.pth saved [327428481/327428481]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87sXCPsZGWNK","executionInfo":{"status":"ok","timestamp":1623286835361,"user_tz":-480,"elapsed":11253,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e0573b49-479a-452f-bf79-89f513984d61"},"source":["!pip install -U catalyst\n","!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n","!pip install audiomentations"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting catalyst\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/9b/cfcf2d617c11ab871e21abc10181c17ece5f2fd75436d33394e7b0f147e1/catalyst-21.5-py2.py3-none-any.whl (518kB)\n","\u001b[K     || 522kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (4.41.1)\n","Collecting PyYAML>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     || 645kB 17.2MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.8.1+cu101)\n","Requirement already satisfied, skipping upgrade: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.19.5)\n","Collecting tensorboardX>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n","\u001b[K     || 122kB 33.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->catalyst) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=2.1.0->catalyst) (3.12.4)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX>=2.1.0->catalyst) (57.0.0)\n","Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX>=2.1.0->catalyst) (1.15.0)\n","Installing collected packages: PyYAML, tensorboardX, catalyst\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.4.1 catalyst-21.5 tensorboardX-2.2\n","Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n","  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-sh8dnkni\n","  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-sh8dnkni\n","Building wheels for collected packages: warmup-scheduler\n","  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-cp37-none-any.whl size=3880 sha256=6d58816c766201609f4e2bb1acab4134edeba0082dee4ec1b7c5d3774b6567c2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-k1f9uqu3/wheels/b7/24/83/d30234cc013cff538805b14df916e79091f7cf9ee2c5bf3a64\n","Successfully built warmup-scheduler\n","Installing collected packages: warmup-scheduler\n","Successfully installed warmup-scheduler-0.3.2\n","Collecting audiomentations\n","  Downloading https://files.pythonhosted.org/packages/fb/e1/3078fe444be2a100d804ee1296115367c27fa1dfa6298bf4155f77345822/audiomentations-0.16.0-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.19.5)\n","Requirement already satisfied: librosa<=0.8.0,>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.0)\n","Requirement already satisfied: scipy<1.6.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.2.2)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.51.2)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.22.2.post1)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (4.4.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (1.0.1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (2.1.9)\n","Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.10.3.post1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (1.3.0)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<=0.8.0,>=0.6.1->audiomentations) (1.15.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations) (57.0.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.14.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.23.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.4.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.24.3)\n","Installing collected packages: audiomentations\n","Successfully installed audiomentations-0.16.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:31.262304Z","iopub.status.busy":"2020-08-14T10:26:31.261641Z","iopub.status.idle":"2020-08-14T10:26:43.679533Z","shell.execute_reply":"2020-08-14T10:26:43.678430Z"},"papermill":{"duration":12.43994,"end_time":"2020-08-14T10:26:43.679682","exception":false,"start_time":"2020-08-14T10:26:31.239742","status":"completed"},"tags":[],"id":"IEEebe8MEIAX"},"source":["import cv2\n","import audioread\n","import logging\n","import os\n","import random\n","import time\n","import warnings\n","\n","from tqdm import tqdm\n","\n","import librosa\n","import librosa.display as display\n","import numpy as np\n","import pandas as pd\n","import soundfile as sf\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","\n","from contextlib import contextmanager\n","from IPython.display import Audio\n","from pathlib import Path\n","from typing import Optional, List\n","\n","from catalyst.dl import SupervisedRunner, CallbackOrder, Callback, CheckpointCallback  # ,State \n","from fastprogress import progress_bar\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score, average_precision_score, roc_auc_score\n","\n","from warmup_scheduler import GradualWarmupScheduler\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tTXrngte18w"},"source":["from catalyst import dl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-08-14T10:26:43.721167Z","iopub.status.busy":"2020-08-14T10:26:43.720157Z","iopub.status.idle":"2020-08-14T10:26:43.726429Z","shell.execute_reply":"2020-08-14T10:26:43.725812Z"},"papermill":{"duration":0.033698,"end_time":"2020-08-14T10:26:43.726552","exception":false,"start_time":"2020-08-14T10:26:43.692854","status":"completed"},"tags":[],"id":"Hiklv0e0EIAY"},"source":["def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","    \n","    \n","def get_logger(out_file=None):\n","    logger = logging.getLogger()\n","    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n","    logger.handlers = []\n","    logger.setLevel(logging.INFO)\n","\n","    handler = logging.StreamHandler()\n","    handler.setFormatter(formatter)\n","    handler.setLevel(logging.INFO)\n","    logger.addHandler(handler)\n","\n","    if out_file is not None:\n","        fh = logging.FileHandler(out_file)\n","        fh.setFormatter(formatter)\n","        fh.setLevel(logging.INFO)\n","        logger.addHandler(fh)\n","    logger.info(\"logger set up\")\n","    return logger\n","    \n","    \n","@contextmanager\n","def timer(name: str, logger: Optional[logging.Logger] = None):\n","    t0 = time.time()\n","    msg = f\"[{name}] start\"\n","    if logger is None:\n","        print(msg)\n","    else:\n","        logger.info(msg)\n","    yield\n","\n","    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n","    if logger is None:\n","        print(msg)\n","    else:\n","        logger.info(msg)\n","    \n","    \n","set_seed(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-08-14T10:26:43.757897Z","iopub.status.busy":"2020-08-14T10:26:43.757128Z","iopub.status.idle":"2020-08-14T10:26:43.759607Z","shell.execute_reply":"2020-08-14T10:26:43.760169Z"},"papermill":{"duration":0.021548,"end_time":"2020-08-14T10:26:43.760287","exception":false,"start_time":"2020-08-14T10:26:43.738739","status":"completed"},"tags":[],"id":"-p-NMx2QEIAZ"},"source":["INPUT_ROOT = \"/content/\" \n","RAW_DATA = INPUT_ROOT + \"data/\"\n","TRAIN_AUDIO_DIR = RAW_DATA + \"train/\"\n","# TRAIN_RESAMPLED_AUDIO_DIRS = [\n","#   INPUT_ROOT / \"birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5)\n","# ]\n","TEST_AUDIO_DIR = RAW_DATA + \"public_test/\"\n","# TEST2_AUDIO_DIR = RAW_DATA + \"private_test/\"\n","\n","!mv /content/data/private_test/*.wav /content/data/public_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:43.794375Z","iopub.status.busy":"2020-08-14T10:26:43.793466Z","iopub.status.idle":"2020-08-14T10:26:44.076854Z","shell.execute_reply":"2020-08-14T10:26:44.075869Z"},"papermill":{"duration":0.30389,"end_time":"2020-08-14T10:26:44.076977","exception":false,"start_time":"2020-08-14T10:26:43.773087","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"7axhB8DBEIAa","executionInfo":{"status":"ok","timestamp":1623286844819,"user_tz":-480,"elapsed":973,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"04aefd87-e574-4f94-a1c2-e0581514eb4d"},"source":["import pandas as pd\n","\n","train = pd.read_csv(RAW_DATA + \"meta_train.csv\")\n","train['Filename'] = train['Filename'] + '.wav'\n","print(train.head())\n","test = pd.read_csv(\"/gdrive/My Drive/Colab Notebooks/esun_tbrain/dog_sound/data/sample_submission.csv\")\n","test['Filename'] = test['Filename'] + '.wav'\n","print(test.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["          Filename  Label   Remark\n","0  train_00001.wav      0  Barking\n","1  train_00002.wav      0  Barking\n","2  train_00003.wav      0  Barking\n","3  train_00004.wav      0  Barking\n","4  train_00005.wav      0  Barking\n","           Filename  Barking  Howling  Crying  COSmoke  GlassBreaking  Other\n","0  public_00001.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","1  public_00002.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","2  public_00003.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","3  public_00004.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n","4  public_00005.wav   0.1666   0.1666  0.1666   0.1666         0.1666  0.167\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012238,"end_time":"2020-08-14T10:26:44.102097","exception":false,"start_time":"2020-08-14T10:26:44.089859","status":"completed"},"tags":[],"id":"KLsj27zUEIAa"},"source":["### torchlibrosa\n","\n","\n","In PANNs, `torchlibrosa`, a PyTorch based implementation are used to replace some of the `librosa`'s functions. Here I use some functions of `torchlibrosa`.\n","\n","Ref: https://github.com/qiuqiangkong/torchlibrosa"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:44.186427Z","iopub.status.busy":"2020-08-14T10:26:44.160452Z","iopub.status.idle":"2020-08-14T10:26:44.202387Z","shell.execute_reply":"2020-08-14T10:26:44.201812Z"},"papermill":{"duration":0.062799,"end_time":"2020-08-14T10:26:44.202497","exception":false,"start_time":"2020-08-14T10:26:44.139698","status":"completed"},"tags":[],"id":"_69sQ63eEIAb"},"source":["class DFTBase(nn.Module):\n","    def __init__(self):\n","        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n","        super(DFTBase, self).__init__()\n","\n","    def dft_matrix(self, n):\n","        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n","        omega = np.exp(-2 * np.pi * 1j / n)\n","        W = np.power(omega, x * y)\n","        return W\n","\n","    def idft_matrix(self, n):\n","        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n","        omega = np.exp(2 * np.pi * 1j / n)\n","        W = np.power(omega, x * y)\n","        return W\n","    \n","    \n","class STFT(DFTBase):\n","    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n","        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n","        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n","        of librosa.core.stft\n","        \"\"\"\n","        super(STFT, self).__init__()\n","\n","        assert pad_mode in ['constant', 'reflect']\n","\n","        self.n_fft = n_fft\n","        self.center = center\n","        self.pad_mode = pad_mode\n","\n","        # By default, use the entire frame\n","        if win_length is None:\n","            win_length = n_fft\n","\n","        # Set the default hop, if it's not already specified\n","        if hop_length is None:\n","            hop_length = int(win_length // 4)\n","\n","        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n","\n","        # Pad the window out to n_fft size\n","        fft_window = librosa.util.pad_center(fft_window, n_fft)\n","\n","        # DFT & IDFT matrix\n","        self.W = self.dft_matrix(n_fft)\n","\n","        out_channels = n_fft // 2 + 1\n","\n","        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n","            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n","            groups=1, bias=False)\n","\n","        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n","            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n","            groups=1, bias=False)\n","\n","        self.conv_real.weight.data = torch.Tensor(\n","            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n","        # (n_fft // 2 + 1, 1, n_fft)\n","\n","        self.conv_imag.weight.data = torch.Tensor(\n","            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n","        # (n_fft // 2 + 1, 1, n_fft)\n","\n","        if freeze_parameters:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, data_length)\n","        Returns:\n","          real: (batch_size, n_fft // 2 + 1, time_steps)\n","          imag: (batch_size, n_fft // 2 + 1, time_steps)\n","        \"\"\"\n","\n","        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n","\n","        if self.center:\n","            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n","\n","        real = self.conv_real(x)\n","        imag = self.conv_imag(x)\n","        # (batch_size, n_fft // 2 + 1, time_steps)\n","\n","        real = real[:, None, :, :].transpose(2, 3)\n","        imag = imag[:, None, :, :].transpose(2, 3)\n","        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n","\n","        return real, imag\n","    \n","    \n","class Spectrogram(nn.Module):\n","    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n","        window='hann', center=True, pad_mode='reflect', power=2.0, \n","        freeze_parameters=True):\n","        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n","        Conv1d. The function has the same output of librosa.core.stft\n","        \"\"\"\n","        super(Spectrogram, self).__init__()\n","\n","        self.power = power\n","\n","        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n","            win_length=win_length, window=window, center=center, \n","            pad_mode=pad_mode, freeze_parameters=True)\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n","        Returns:\n","          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n","        \"\"\"\n","\n","        (real, imag) = self.stft.forward(input)\n","        # (batch_size, n_fft // 2 + 1, time_steps)\n","\n","        spectrogram = real ** 2 + imag ** 2\n","\n","        if self.power == 2.0:\n","            pass\n","        else:\n","            spectrogram = spectrogram ** (power / 2.0)\n","\n","        return spectrogram\n","\n","    \n","class LogmelFilterBank(nn.Module):\n","    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n","        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n","        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n","        the pytorch implementation of as librosa.filters.mel \n","        \"\"\"\n","        super(LogmelFilterBank, self).__init__()\n","\n","        self.is_log = is_log\n","        self.ref = ref\n","        self.amin = amin\n","        self.top_db = top_db\n","\n","        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n","            fmin=fmin, fmax=fmax).T\n","        # (n_fft // 2 + 1, mel_bins)\n","\n","        self.melW = nn.Parameter(torch.Tensor(self.melW))\n","\n","        if freeze_parameters:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, channels, time_steps)\n","        \n","        Output: (batch_size, time_steps, mel_bins)\n","        \"\"\"\n","\n","        # Mel spectrogram\n","        mel_spectrogram = torch.matmul(input, self.melW)\n","\n","        # Logmel spectrogram\n","        if self.is_log:\n","            output = self.power_to_db(mel_spectrogram)\n","        else:\n","            output = mel_spectrogram\n","\n","        return output\n","\n","\n","    def power_to_db(self, input):\n","        \"\"\"Power to db, this function is the pytorch implementation of \n","        librosa.core.power_to_lb\n","        \"\"\"\n","        ref_value = self.ref\n","        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n","        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n","\n","        if self.top_db is not None:\n","            if self.top_db < 0:\n","                raise ParameterError('top_db must be non-negative')\n","            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n","\n","        return log_spec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:44.246248Z","iopub.status.busy":"2020-08-14T10:26:44.240679Z","iopub.status.idle":"2020-08-14T10:26:44.249107Z","shell.execute_reply":"2020-08-14T10:26:44.248561Z"},"papermill":{"duration":0.03404,"end_time":"2020-08-14T10:26:44.249211","exception":false,"start_time":"2020-08-14T10:26:44.215171","status":"completed"},"tags":[],"id":"OCjDEDLVEIAd"},"source":["class DropStripes(nn.Module):\n","    def __init__(self, dim, drop_width, stripes_num):\n","        \"\"\"Drop stripes. \n","        Args:\n","          dim: int, dimension along which to drop\n","          drop_width: int, maximum width of stripes to drop\n","          stripes_num: int, how many stripes to drop\n","        \"\"\"\n","        super(DropStripes, self).__init__()\n","\n","        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n","\n","        self.dim = dim\n","        self.drop_width = drop_width\n","        self.stripes_num = stripes_num\n","\n","    def forward(self, input):\n","        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n","\n","        assert input.ndimension() == 4\n","\n","        if self.training is False:\n","            return input\n","\n","        else:\n","            batch_size = input.shape[0]\n","            total_width = input.shape[self.dim]\n","\n","            for n in range(batch_size):\n","                self.transform_slice(input[n], total_width)\n","\n","            return input\n","\n","\n","    def transform_slice(self, e, total_width):\n","        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n","\n","        for _ in range(self.stripes_num):\n","            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n","            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n","\n","            if self.dim == 2:\n","                e[:, bgn : bgn + distance, :] = 0\n","            elif self.dim == 3:\n","                e[:, :, bgn : bgn + distance] = 0\n","\n","\n","class SpecAugmentation(nn.Module):\n","    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n","        freq_stripes_num):\n","        \"\"\"Spec augmetation. \n","        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n","        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n","        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n","        Args:\n","          time_drop_width: int\n","          time_stripes_num: int\n","          freq_drop_width: int\n","          freq_stripes_num: int\n","        \"\"\"\n","\n","        super(SpecAugmentation, self).__init__()\n","\n","        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n","            stripes_num=time_stripes_num)\n","\n","        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n","            stripes_num=freq_stripes_num)\n","\n","    def forward(self, input):\n","        x = self.time_dropper(input)\n","        x = self.freq_dropper(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012766,"end_time":"2020-08-14T10:26:44.276141","exception":false,"start_time":"2020-08-14T10:26:44.263375","status":"completed"},"tags":[],"id":"6i-L7_hMEIAe"},"source":["### audioset_tagging_cnn\n","\n","I also use `Cnn14_DecisionLevelAtt` model from [PANNs models](https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/pytorch/models.py), which is a SED model."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012344,"end_time":"2020-08-14T10:26:44.325825","exception":false,"start_time":"2020-08-14T10:26:44.313481","status":"completed"},"tags":[],"id":"QTxn9Q0WEIAf"},"source":["### Building blocks"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:44.384942Z","iopub.status.busy":"2020-08-14T10:26:44.363459Z","iopub.status.idle":"2020-08-14T10:26:44.388830Z","shell.execute_reply":"2020-08-14T10:26:44.388352Z"},"papermill":{"duration":0.051099,"end_time":"2020-08-14T10:26:44.388925","exception":false,"start_time":"2020-08-14T10:26:44.337826","status":"completed"},"tags":[],"id":"AkrKwIXWEIAf"},"source":["def init_layer(layer):\n","    nn.init.xavier_uniform_(layer.weight)\n","\n","    if hasattr(layer, \"bias\"):\n","        if layer.bias is not None:\n","            layer.bias.data.fill_(0.)\n","\n","\n","def init_bn(bn):\n","    bn.bias.data.fill_(0.)\n","    bn.weight.data.fill_(1.0)\n","\n","\n","def interpolate(x: torch.Tensor, ratio: int):\n","    \"\"\"Interpolate data in time domain. This is used to compensate the\n","    resolution reduction in downsampling of a CNN.\n","\n","    Args:\n","      x: (batch_size, time_steps, classes_num)\n","      ratio: int, ratio to interpolate\n","    Returns:\n","      upsampled: (batch_size, time_steps * ratio, classes_num)\n","    \"\"\"\n","    (batch_size, time_steps, classes_num) = x.shape\n","    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n","    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n","    return upsampled\n","\n","\n","def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n","    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n","    is the same as the value of the last frame.\n","    Args:\n","      framewise_output: (batch_size, frames_num, classes_num)\n","      frames_num: int, number of frames to pad\n","    Outputs:\n","      output: (batch_size, frames_num, classes_num)\n","    \"\"\"\n","    pad = framewise_output[:, -1:, :].repeat(\n","        1, frames_num - framewise_output.shape[1], 1)\n","    \"\"\"tensor for padding\"\"\"\n","\n","    output = torch.cat((framewise_output, pad), dim=1)\n","    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n","\n","    return output\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=(3, 3),\n","            stride=(1, 1),\n","            padding=(1, 1),\n","            bias=False)\n","\n","        self.conv2 = nn.Conv2d(\n","            in_channels=out_channels,\n","            out_channels=out_channels,\n","            kernel_size=(3, 3),\n","            stride=(1, 1),\n","            padding=(1, 1),\n","            bias=False)\n","\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        init_layer(self.conv1)\n","        init_layer(self.conv2)\n","        init_bn(self.bn1)\n","        init_bn(self.bn2)\n","\n","    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n","\n","        x = input\n","        x = F.relu_(self.bn1(self.conv1(x)))\n","        x = F.relu_(self.bn2(self.conv2(x)))\n","        if pool_type == 'max':\n","            x = F.max_pool2d(x, kernel_size=pool_size)\n","        elif pool_type == 'avg':\n","            x = F.avg_pool2d(x, kernel_size=pool_size)\n","        elif pool_type == 'avg+max':\n","            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n","            x2 = F.max_pool2d(x, kernel_size=pool_size)\n","            x = x1 + x2\n","        else:\n","            raise Exception('Incorrect argument!')\n","\n","        return x\n","\n","\n","class AttBlock(nn.Module):\n","    def __init__(self,\n","                 in_features: int,\n","                 out_features: int,\n","                 activation=\"linear\",\n","                 temperature=1.0):\n","        super().__init__()\n","\n","        self.activation = activation\n","        self.temperature = temperature\n","        self.att = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","        self.cla = nn.Conv1d(\n","            in_channels=in_features,\n","            out_channels=out_features,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True)\n","\n","        self.bn_att = nn.BatchNorm1d(out_features)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_layer(self.att)\n","        init_layer(self.cla)\n","        init_bn(self.bn_att)\n","\n","    def forward(self, x):\n","        # x: (n_samples, n_in, n_time)\n","        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n","        cla = self.nonlinear_transform(self.cla(x))\n","        x = torch.sum(norm_att * cla, dim=2)\n","        return x, norm_att, cla\n","\n","    def nonlinear_transform(self, x):\n","        if self.activation == 'linear':\n","            return x\n","        elif self.activation == 'sigmoid':\n","            return torch.sigmoid(x)\n","        elif self.activation == 'softmax':\n","            return nn.LogSoftmax(dim=-1)(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EN6gM0upq2_a"},"source":["def do_mixup(x, mixup_lambda):\n","    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n","    (1, 3, 5, ...).\n","    Args:\n","      x: (batch_size * 2, ...)\n","      mixup_lambda: (batch_size * 2,)\n","    Returns:\n","      out: (batch_size, ...)\n","    \"\"\"\n","    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n","        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n","    return out\n","\n","\n","class Mixup(object):\n","    def __init__(self, mixup_alpha, random_seed=1234):\n","        \"\"\"Mixup coefficient generator.\n","        \"\"\"\n","        self.mixup_alpha = mixup_alpha\n","        self.random_state = np.random.RandomState(random_seed)\n","\n","    def get_lambda(self, batch_size):\n","        \"\"\"Get mixup random coefficients.\n","        Args:\n","          batch_size: int\n","        Returns:\n","          mixup_lambdas: (batch_size,)\n","        \"\"\"\n","        mixup_lambdas = []\n","        for n in range(0, batch_size, 2):\n","            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n","            mixup_lambdas.append(lam)\n","            mixup_lambdas.append(1. - lam)\n","\n","        return np.array(mixup_lambdas)\n","\n","class Cnn14(nn.Module):\n","    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n","        fmax, classes_num):\n","        \n","        super(Cnn14, self).__init__()\n","\n","        window = 'hann'\n","        center = True\n","        pad_mode = 'reflect'\n","        ref = 1.0\n","        amin = 1e-10\n","        top_db = None\n","\n","        # Spectrogram extractor\n","        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n","            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n","            freeze_parameters=True)\n","\n","        # Logmel feature extractor\n","        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n","            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n","            freeze_parameters=True)\n","\n","        # Spec augmenter\n","        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n","            freq_drop_width=8, freq_stripes_num=2)\n","\n","        self.bn0 = nn.BatchNorm2d(64)\n","\n","        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n","        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n","        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n","        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n","        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n","        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n","\n","        self.fc1 = nn.Linear(2048, 2048, bias=True)\n","        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n","        \n","        self.init_weight()\n","\n","    def init_weight(self):\n","        init_bn(self.bn0)\n","        init_layer(self.fc1)\n","        init_layer(self.fc_audioset)\n"," \n","    def forward(self, input, mixup_lambda=None):\n","        \"\"\"\n","        Input: (batch_size, data_length)\"\"\"\n","\n","        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n","        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n","\n","        x = x.transpose(1, 3)\n","        x = self.bn0(x)\n","        x = x.transpose(1, 3)\n","        \n","        if self.training:\n","            x = self.spec_augmenter(x)\n","\n","        # Mixup on spectrogram\n","        if self.training and mixup_lambda is not None:\n","            x = do_mixup(x, mixup_lambda)\n","\n","        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = torch.mean(x, dim=3)\n","        \n","        (x1, _) = torch.max(x, dim=2)\n","        x2 = torch.mean(x, dim=2)\n","        x = x1 + x2\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = F.relu_(self.fc1(x))\n","        embedding = F.dropout(x, p=0.5, training=self.training)\n","        # clipwise_output = torch.sigmoid(self.fc_audioset(x))\n","        clipwise_output = self.fc_audioset(x)\n","        \n","        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n","\n","        return output_dict\n","\n","class Transfer_Cnn14(nn.Module):\n","    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n","        fmax, classes_num, freeze_base):\n","        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n","        \"\"\"\n","        super(Transfer_Cnn14, self).__init__()\n","        audioset_classes_num = 527\n","        \n","        self.base = Cnn14(sample_rate, window_size, hop_size, mel_bins, fmin, \n","            fmax, audioset_classes_num)\n","\n","        # Transfer to another task layer\n","        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n","\n","        if freeze_base:\n","            # Freeze AudioSet pretrained layers\n","            for param in self.base.parameters():\n","                param.requires_grad = False\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init_layer(self.fc_transfer)\n","\n","    def load_from_pretrain(self, pretrained_checkpoint_path):\n","        checkpoint = torch.load(pretrained_checkpoint_path)\n","        self.base.load_state_dict(checkpoint['model'])\n","\n","    def forward(self, input, mixup_lambda=None):\n","        \"\"\"Input: (batch_size, data_length)\n","        \"\"\"\n","        output_dict = self.base(input, mixup_lambda)\n","        embedding = output_dict['embedding']\n","\n","        clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n","        output_dict['clipwise_output'] = clipwise_output\n"," \n","        return output_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012155,"end_time":"2020-08-14T10:26:44.478670","exception":false,"start_time":"2020-08-14T10:26:44.466515","status":"completed"},"tags":[],"id":"tUGg3XUpEIAg"},"source":["What is good in PANNs models is that they accept raw audio clip as input. Let's put a chunk into the CNN feature extractor of the model above."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:44.509078Z","iopub.status.busy":"2020-08-14T10:26:44.508213Z","iopub.status.idle":"2020-08-14T10:26:44.612574Z","shell.execute_reply":"2020-08-14T10:26:44.613080Z"},"papermill":{"duration":0.122259,"end_time":"2020-08-14T10:26:44.613225","exception":false,"start_time":"2020-08-14T10:26:44.490966","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"Pxmp98pcEIAg","executionInfo":{"status":"ok","timestamp":1623286845881,"user_tz":-480,"elapsed":17,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"9e46ea14-df61-4f7d-a37e-5355302df0b4"},"source":["path_train_data = '/content/data/train'\n","path_test_data = '/content/data/public_test'\n","path_meta = '/content/data/meta_train.csv'\n","path_submission = 'data/sample_submission.csv'\n","CLASSES = 6\n","SR = 8000  # 8000\n","sample_duration = 40000\n","hop_length = 256 # making it 128 in size\n","fmin = 0\n","fmax = 4000  # fmax <= sampling_rate\n","n_mels = 64  # 128\n","n_fft = 1024 # n_mels * 20\n","padmode = 'reflect'\n","res_type = 'kaiser_best'  # 'kaiser_fast', 'kaiser_best\n","\n","# (n+1)*n_fft-hop=40000\n","\n","train_ratio = 0.9\n","val_ratio = 1 - train_ratio\n","\n","warmup_epo = 20\n","cosine_epo = 180\n","n_epochs = warmup_epo + cosine_epo\n","warmup_factor = 10\n","batch_size = 128\n","LR = 5e-4\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(device)\n","\n","kernel_type = 'starter'  # experiment_name\n","use_fold = 0  # no use"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:44.961633Z","iopub.status.busy":"2020-08-14T10:26:44.960605Z","iopub.status.idle":"2020-08-14T10:26:46.429944Z","shell.execute_reply":"2020-08-14T10:26:46.431123Z"},"papermill":{"duration":1.508482,"end_time":"2020-08-14T10:26:46.431338","exception":false,"start_time":"2020-08-14T10:26:44.922856","status":"completed"},"tags":[],"id":"CAFhRJpDEIAj"},"source":["model_config = {\n","    \"sample_rate\": SR,\n","    \"window_size\": n_fft,\n","    \"hop_size\": hop_length,\n","    \"mel_bins\": n_mels,\n","    \"fmin\": fmin,\n","    \"fmax\": fmax,\n","    \"classes_num\": CLASSES,\n","    \"freeze_base\": False\n","}\n","\n","# model = PANNsCNN14Att(**model_config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031231,"end_time":"2020-08-14T10:26:48.347603","exception":false,"start_time":"2020-08-14T10:26:48.316372","status":"completed"},"tags":[],"id":"oYSXOnCNEIAl"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:48.540845Z","iopub.status.busy":"2020-08-14T10:26:48.540238Z","iopub.status.idle":"2020-08-14T10:26:48.544594Z","shell.execute_reply":"2020-08-14T10:26:48.544102Z"},"papermill":{"duration":0.050901,"end_time":"2020-08-14T10:26:48.544697","exception":false,"start_time":"2020-08-14T10:26:48.493796","status":"completed"},"tags":[],"id":"qByzHHHlEIAl"},"source":["PERIOD = 5\n","\n","class PANNsDataset(data.Dataset):\n","    def __init__(\n","            self,\n","            file_list: List[List[str]],\n","            waveform_transforms=None,\n","            training=True):\n","        self.file_list = file_list  # list of list: [file_path, code]\n","        self.waveform_transforms = waveform_transforms\n","        self.training = training\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx: int):\n","        if self.training:\n","            wav_path, code = self.file_list[idx]\n","\n","            y, sr = sf.read(TRAIN_AUDIO_DIR + wav_path)\n","\n","            if self.waveform_transforms:\n","                # y = self.waveform_transforms(y)\n","                y = self.waveform_transforms(samples=y, sample_rate=8000)\n","            \n","            # else:\n","\n","            len_y = len(y)\n","            effective_length = sr * PERIOD\n","            if len_y < effective_length:\n","                new_y = np.zeros(effective_length, dtype=y.dtype)\n","                start = np.random.randint(effective_length - len_y)\n","                new_y[start:start + len_y] = y\n","                y = new_y.astype(np.float32)\n","            elif len_y > effective_length:\n","                start = np.random.randint(len_y - effective_length)\n","                y = y[start:start + effective_length].astype(np.float32)\n","            else:\n","                y = y.astype(np.float32)\n","\n","            labels = np.zeros(CLASSES, dtype=\"f\")\n","            labels[code] = 1\n","\n","            return {\"waveform\": y, \"targets\": labels}\n","        else:\n","            wav_path = self.file_list[idx]\n","\n","            y, sr = sf.read(TEST_AUDIO_DIR + wav_path)\n","\n","            len_y = len(y)\n","            effective_length = sr * PERIOD\n","            if len_y < effective_length:\n","                new_y = np.zeros(effective_length, dtype=y.dtype)\n","                start = np.random.randint(effective_length - len_y)\n","                new_y[start:start + len_y] = y\n","                y = new_y.astype(np.float32)\n","            elif len_y > effective_length:\n","                start = np.random.randint(len_y - effective_length)\n","                y = y[start:start + effective_length].astype(np.float32)\n","            else:\n","                y = y.astype(np.float32)\n","\n","            return {\"waveform\": y}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031604,"end_time":"2020-08-14T10:26:48.609308","exception":false,"start_time":"2020-08-14T10:26:48.577704","status":"completed"},"tags":[],"id":"LFJXP9EDEIAm"},"source":["### Criterion"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:48.692139Z","iopub.status.busy":"2020-08-14T10:26:48.691461Z","iopub.status.idle":"2020-08-14T10:26:48.695848Z","shell.execute_reply":"2020-08-14T10:26:48.695373Z"},"papermill":{"duration":0.054385,"end_time":"2020-08-14T10:26:48.695948","exception":false,"start_time":"2020-08-14T10:26:48.641563","status":"completed"},"tags":[],"id":"GeL5KP6fEIAm"},"source":["class PANNsLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # self.bce = nn.BCELoss()\n","        # self.bce = nn.BCEWithLogitsLoss()\n","        self.bce = nn.NLLLoss()\n","\n","    def forward(self, input, target):\n","        input_ = input[\"clipwise_output\"]\n","        input_ = torch.where(torch.isnan(input_),\n","                             torch.zeros_like(input_),\n","                             input_)\n","        input_ = torch.where(torch.isinf(input_),\n","                             torch.zeros_like(input_),\n","                             input_)\n","\n","        # target = target.float()\n","\n","        return self.bce(input_, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCiEcsOCcIs2"},"source":["class CutMixCrossEntropyLoss(nn.Module):\n","    def __init__(self, size_average=True):\n","        super().__init__()\n","        self.size_average = size_average\n","\n","    def forward(self, input, target):\n","        if len(target.size()) == 1:\n","            target = F.one_hot(target, num_classes=input.size(-1))\n","            target = target.float().cuda()\n","        return cross_entropy(input, target, self.size_average)\n","\n","\n","def cross_entropy(input, target, size_average=True):\n","    \"\"\" Cross entropy that accepts soft targets\n","    Args:\n","         pred: predictions for neural network\n","         targets: targets, can be soft\n","         size_average: if false, sum is returned instead of mean\n","    Examples::\n","        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n","        input = torch.autograd.Variable(out, requires_grad=True)\n","        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n","        target = torch.autograd.Variable(y1)\n","        loss = cross_entropy(input, target)\n","        loss.backward()\n","    \"\"\"\n","    # logsoftmax = torch.nn.LogSoftmax(dim=1)\n","    # if size_average:\n","    #     return torch.mean(torch.sum(-target * logsoftmax(input), dim=1))\n","    # else:\n","    #     return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))\n","    \n","    if size_average:\n","        return torch.mean(torch.sum(-target * (input), dim=1))\n","    else:\n","        return torch.sum(torch.sum(-target * (input), dim=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.032181,"end_time":"2020-08-14T10:26:48.760978","exception":false,"start_time":"2020-08-14T10:26:48.728797","status":"completed"},"tags":[],"id":"qrUskYXWEIAm"},"source":["### Callbacks"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:48.854976Z","iopub.status.busy":"2020-08-14T10:26:48.849889Z","iopub.status.idle":"2020-08-14T10:26:48.858082Z","shell.execute_reply":"2020-08-14T10:26:48.857544Z"},"papermill":{"duration":0.065321,"end_time":"2020-08-14T10:26:48.858189","exception":false,"start_time":"2020-08-14T10:26:48.792868","status":"completed"},"tags":[],"id":"1TB3whHKEIAm"},"source":["# class F1Callback(Callback):\n","#     def __init__(self,\n","#                  input_key: str = \"targets\",\n","#                  output_key: str = \"logits\",\n","#                  model_output_key: str = \"clipwise_output\",\n","#                  prefix: str = \"f1\"):\n","#         super().__init__(CallbackOrder.Metric)\n","\n","#         self.input_key = input_key\n","#         self.output_key = output_key\n","#         self.model_output_key = model_output_key\n","#         self.prefix = prefix\n","\n","#     def on_loader_start(self, state: State):\n","#         self.prediction: List[np.ndarray] = []\n","#         self.target: List[np.ndarray] = []\n","\n","#     def on_batch_end(self, state: State):\n","#         targ = state.input[self.input_key].detach().cpu().numpy()\n","#         out = state.output[self.output_key]\n","\n","#         clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n","\n","#         self.prediction.append(clipwise_output)\n","#         self.target.append(targ)\n","\n","#         y_pred = clipwise_output.argmax(axis=1)\n","#         y_true = targ.argmax(axis=1)\n","\n","#         score = f1_score(y_true, y_pred, average=\"macro\")\n","#         state.batch_metrics[self.prefix] = score\n","\n","#     def on_loader_end(self, state: State):\n","#         y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n","#         y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n","#         score = f1_score(y_true, y_pred, average=\"macro\")\n","#         state.loader_metrics[self.prefix] = score\n","#         if state.is_valid_loader:\n","#             state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n","#                                 self.prefix] = score\n","#         else:\n","#             state.epoch_metrics[\"train_epoch_\" + self.prefix] = score\n","\n","# class AUCCallback(Callback):\n","#     def __init__(self,\n","#                  input_key: str = \"targets\",\n","#                  output_key: str = \"logits\",\n","#                  model_output_key: str = \"clipwise_output\",\n","#                  prefix: str = \"auc\"):\n","#         super().__init__(CallbackOrder.Metric)\n","\n","#         self.input_key = input_key\n","#         self.output_key = output_key\n","#         self.model_output_key = model_output_key\n","#         self.prefix = prefix\n","\n","#     def on_loader_start(self, runner):\n","#         self.prediction: List[np.ndarray] = []\n","#         self.target: List[np.ndarray] = []\n","\n","#     def on_batch_end(self, runner):\n","#         targ = runner.input[self.input_key].detach().cpu().numpy()\n","#         out = runner.output[self.output_key]\n","\n","#         clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n","\n","#         self.prediction.append(clipwise_output)\n","#         self.target.append(targ)\n","\n","#         y_pred = clipwise_output.argmax(axis=1)\n","#         y_true = targ.argmax(axis=1)\n","\n","#         score = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","#         runner.batch_metrics[self.prefix] = score\n","\n","#     def on_loader_end(self, runner):\n","#         y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n","#         y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n","#         score = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","#         runner.loader_metrics[self.prefix] = score\n","#         if runner.is_valid_loader:\n","#             runner.epoch_metrics[runner.valid_loader + \"_epoch_\" +\n","#                                 self.prefix] = score\n","#         else:\n","#             runner.epoch_metrics[\"train_epoch_\" + self.prefix] = score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.033622,"end_time":"2020-08-14T10:26:48.924696","exception":false,"start_time":"2020-08-14T10:26:48.891074","status":"completed"},"tags":[],"id":"JPjbSmM_EIAm"},"source":["### Train\n","\n","Some code are taken from https://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast .\n","Thanks @ttahara!"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:49.001106Z","iopub.status.busy":"2020-08-14T10:26:49.000144Z","iopub.status.idle":"2020-08-14T10:26:49.708541Z","shell.execute_reply":"2020-08-14T10:26:49.707629Z"},"papermill":{"duration":0.749533,"end_time":"2020-08-14T10:26:49.708664","exception":false,"start_time":"2020-08-14T10:26:48.959131","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"YkG-3zh-EIAn","executionInfo":{"status":"ok","timestamp":1623286845885,"user_tz":-480,"elapsed":17,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"caadbcf4-84cb-4d54-ee12-f288a797a0e3"},"source":["print(train.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1200, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:49.806182Z","iopub.status.busy":"2020-08-14T10:26:49.804681Z","iopub.status.idle":"2020-08-14T10:26:49.875178Z","shell.execute_reply":"2020-08-14T10:26:49.875766Z"},"papermill":{"duration":0.133113,"end_time":"2020-08-14T10:26:49.875942","exception":false,"start_time":"2020-08-14T10:26:49.742829","status":"completed"},"tags":[],"id":"f5XhzpdSEIAn"},"source":["skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","train[\"fold\"] = -1\n","for fold_id, (train_index, val_index) in enumerate(skf.split(train, train['Label'])):\n","    train.iloc[val_index, -1] = fold_id"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:49.960106Z","iopub.status.busy":"2020-08-14T10:26:49.959257Z","iopub.status.idle":"2020-08-14T10:26:50.006856Z","shell.execute_reply":"2020-08-14T10:26:50.007827Z"},"papermill":{"duration":0.094264,"end_time":"2020-08-14T10:26:50.007988","exception":false,"start_time":"2020-08-14T10:26:49.913724","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"TTptf0tlEIAn","executionInfo":{"status":"ok","timestamp":1623286845886,"user_tz":-480,"elapsed":13,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"0a54121d-1fd6-49e9-d0b7-894859a19ab2"},"source":["train_file_list = train.query(\"fold != @use_fold\")[[\"Filename\", \"Label\"]].values.tolist()\n","val_file_list = train.query(\"fold == @use_fold\")[[\"Filename\", \"Label\"]].values.tolist()\n","\n","print(\"[fold {}] train: {}, val: {}\".format(use_fold, len(train_file_list), len(val_file_list)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[fold 0] train: 960, val: 240\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vk2mW0UFOwAj"},"source":["# Fix Warmup Bug\n","class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T10:26:50.093491Z","iopub.status.busy":"2020-08-14T10:26:50.092046Z","iopub.status.idle":"2020-08-14T10:26:59.767467Z","shell.execute_reply":"2020-08-14T10:26:59.766328Z"},"papermill":{"duration":9.724143,"end_time":"2020-08-14T10:26:59.767606","exception":false,"start_time":"2020-08-14T10:26:50.043463","status":"completed"},"tags":[],"id":"pzdMf1ghEIAn"},"source":["from audiomentations import Compose, AddGaussianNoise, Gain, TimeStretch, PitchShift, Shift, TimeMask, AddGaussianSNR\n","\n","train_transform = Compose([\n","    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),\n","    # AddGaussianSNR(p=0.3),\n","    Gain(min_gain_in_db=-12, max_gain_in_db=12, p=0.3),\n","    # PitchShift(min_semitones=-4, max_semitones=4, p=0.3),\n","    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.3),\n","    Shift(min_fraction=-0.5, max_fraction=0.5, rollover=False, p=0.3),\n","    # TimeMask(min_band_part=0.0, max_band_part=0.2, p=0.5),\n","    # FrequencyMask(min_frequency_band=0.0, max_frequency_band=0.2, p=0.5)\n","])\n","\n","# loaders\n","loaders = {\n","    \"train\": data.DataLoader(PANNsDataset(train_file_list, None), \n","                             batch_size=batch_size, \n","                             shuffle=True, \n","                             num_workers=2, \n","                             pin_memory=True, \n","                             drop_last=True),\n","    \"valid\": data.DataLoader(PANNsDataset(val_file_list, None), \n","                             batch_size=batch_size, \n","                             shuffle=False,\n","                             num_workers=2,\n","                             pin_memory=True,\n","                             drop_last=False)\n","}\n","\n","train_loader = data.DataLoader(PANNsDataset(train_file_list, train_transform), \n","                             batch_size=batch_size, \n","                             shuffle=True, \n","                             num_workers=2, \n","                             pin_memory=True, \n","                             drop_last=False)\n","val_loader = data.DataLoader(PANNsDataset(val_file_list, None), \n","                             batch_size=batch_size, \n","                             shuffle=False,\n","                             num_workers=2,\n","                             pin_memory=True,\n","                             drop_last=False)\n","\n","# model\n","# model_config[\"classes_num\"] = 527  # for load weights only \n","# model = PANNsCNN14Att(**model_config)\n","# weights = torch.load(INPUT_ROOT + \"Cnn14_DecisionLevelAtt_mAP0.425.pth\")\n","# # Fixed in V3\n","# model.load_state_dict(weights[\"model\"])\n","# model.att_block = AttBlock(2048, CLASSES, activation='softmax')\n","# model.att_block.init_weights()\n","# model.to(device)\n","\n","# TODO\n","model = Transfer_Cnn14(**model_config)\n","model.load_from_pretrain(INPUT_ROOT + \"Cnn14_mAP_0.431.pth\")\n","model.to(device)\n","\n","# Optimizer\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","\n","# Scheduler\n","# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n","scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs-warmup_epo)\n","scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=warmup_factor, total_epoch=warmup_epo, after_scheduler=scheduler_cosine)\n","\n","# Loss\n","# criterion = PANNsLoss().to(device)\n","criterion = CutMixCrossEntropyLoss().to(device)\n","\n","# callbacks\n","# callbacks = [\n","#     AUCCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"auc\"),\n","#     # CheckpointCallback(save_n_best=0)\n","#     # F1Callback(input_key=\"targets\", output_key=\"logits\", prefix=\"f1\"),\n","#     # mAPCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"mAP\"),\n","    \n","# ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34y6eVBSd77i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623288953709,"user_tz":-480,"elapsed":2046967,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"5802294a-9bce-4d89-9466-02343368f006"},"source":["def train_epoch(loader, optimizer):\n","    model.train()\n","    train_loss = []\n","    LOGITS = []\n","    PREDS = []\n","    TARGETS = []\n","\n","    bar = tqdm(loader)\n","    for d in bar:\n","        data, target = d['waveform'], d['targets']\n","        optimizer.zero_grad()\n","        scheduler.step()\n","        \n","        data, target = data.to(device), target.to(device)\n","        \n","        # mixup_augmenter = Mixup(mixup_alpha=1.)\n","        # mixup_lambda = mixup_augmenter.get_lambda(batch_size=len(data))\n","        # mixup_lambda = torch.tensor(mixup_lambda, dtype=torch.float32).to(device)\n","        # target_mix = do_mixup(target, mixup_lambda)\n","       \n","        logits = model(data, mixup_lambda=None)\n","        # print(logits['framewise_output'].shape)    # shape (batch_size, frame_size, classes)\n","        # print(logits['clipwise_output'].shape)    # shape (batch_size, classes)\n","        logits_clip = logits['clipwise_output']\n","\n","        # loss = criterion(logits, target)\n","        ### hard cross entropy\n","        loss = criterion(logits_clip, (target==1).nonzero(as_tuple=True)[1].to(torch.long))\n","        ### soft cross entropy\n","        # loss = criterion(logits_clip, target_mix)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_np = loss.detach().cpu().numpy()\n","        train_loss.append(loss_np)\n","        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n","        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n","\n","        with torch.no_grad():\n","            # pred = logits_clip.argmax(dim=1).detach()\n","            logits_org = model(data)\n","            LOGITS.append(logits_org['clipwise_output'])\n","            PREDS.append(logits_org['clipwise_output'])\n","\n","            # to one hot\n","            # target = F.one_hot(target, num_classes=CLASSES)\n","            TARGETS.append(target)\n","\n","    PREDS = torch.cat(PREDS).detach().cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","    auc = roc_auc_score(TARGETS, PREDS, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","    return train_loss, auc\n","\n","\n","def val_epoch(loader, get_output=False):\n","\n","    model.eval()\n","    val_loss = []\n","    LOGITS = []\n","    PREDS = []\n","    TARGETS = []\n","\n","    with torch.no_grad():\n","        for d in tqdm(loader):\n","            data, target = d['waveform'], d['targets']\n","            data, target = data.to(device), target.to(device)\n","            logits = model(data)\n","            logits_clip = logits['clipwise_output']\n","\n","            # loss = criterion(logits, target)\n","            loss = criterion(logits_clip, (target==1).nonzero(as_tuple=True)[1].to(torch.long))\n","\n","            pred = logits_clip.argmax(dim=1).detach()\n","            LOGITS.append(logits_clip)\n","            PREDS.append(logits_clip)\n","            \n","            # to one hot\n","            # target = F.one_hot(target, num_classes=CLASSES)\n","            TARGETS.append(target)\n","\n","            val_loss.append(loss.detach().cpu().numpy())\n","        val_loss = np.mean(val_loss)\n","\n","    LOGITS = torch.cat(LOGITS).cpu().numpy()\n","    PREDS = torch.cat(PREDS).detach().cpu().numpy()\n","    TARGETS = torch.cat(TARGETS).cpu().numpy()\n","    auc = roc_auc_score(TARGETS, PREDS, average='weighted', multi_class='ovr', labels=list(range(CLASSES)))\n","\n","    if get_output:\n","        return LOGITS\n","    else:\n","        return val_loss, auc\n","\n","auc_max = 0\n","best_file = f'{kernel_type}_best_fold{use_fold}.pth'\n","for epoch in range(1, n_epochs+1):\n","    print(time.ctime(), 'Epoch:', epoch)\n","    scheduler.step(epoch-1)\n","\n","    train_loss, tr_auc = train_epoch(train_loader, optimizer)\n","    val_loss, val_auc = val_epoch(val_loader)\n","\n","    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, train auc: {(tr_auc):.5f}, val loss: {np.mean(val_loss):.5f}, val auc: {(val_auc):.5f}'\n","    print(content)\n","    with open(f'log_{kernel_type}.txt', 'a') as appender:\n","        appender.write(content + '\\n')\n","\n","    if val_auc > auc_max:\n","        print('score2 ({:.6f} --> {:.6f}).  Saving model ...'.format(auc_max, val_auc))\n","        torch.save(model.state_dict(), best_file)\n","        auc_max = val_auc\n","\n","torch.save(model.state_dict(), os.path.join(f'{kernel_type}_final_fold{use_fold}.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:01:46 2021 Epoch: 1\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.85442, smth: 1.85442:   0%|          | 0/8 [00:08<?, ?it/s]\u001b[A\n","loss: 1.85442, smth: 1.85442:  12%|        | 1/8 [00:08<00:58,  8.30s/it]\u001b[A\n","loss: 1.81677, smth: 1.83559:  12%|        | 1/8 [00:08<00:58,  8.30s/it]\u001b[A\n","loss: 1.81677, smth: 1.83559:  25%|       | 2/8 [00:08<00:36,  6.00s/it]\u001b[A\n","loss: 1.85474, smth: 1.84198:  25%|       | 2/8 [00:09<00:36,  6.00s/it]\u001b[A\n","loss: 1.85474, smth: 1.84198:  38%|      | 3/8 [00:09<00:22,  4.42s/it]\u001b[A\n","loss: 1.72455, smth: 1.81262:  38%|      | 3/8 [00:10<00:22,  4.42s/it]\u001b[A\n","loss: 1.72455, smth: 1.81262:  50%|     | 4/8 [00:10<00:13,  3.31s/it]\u001b[A\n","loss: 1.73398, smth: 1.79689:  50%|     | 4/8 [00:10<00:13,  3.31s/it]\u001b[A\n","loss: 1.73398, smth: 1.79689:  62%|   | 5/8 [00:11<00:07,  2.52s/it]\u001b[A\n","loss: 1.67622, smth: 1.77678:  62%|   | 5/8 [00:11<00:07,  2.52s/it]\u001b[A\n","loss: 1.67622, smth: 1.77678:  75%|  | 6/8 [00:11<00:03,  1.98s/it]\u001b[A\n","loss: 1.69309, smth: 1.76482:  75%|  | 6/8 [00:12<00:03,  1.98s/it]\u001b[A\n","loss: 1.69309, smth: 1.76482:  88%| | 7/8 [00:12<00:01,  1.58s/it]\u001b[A\n","loss: 1.45127, smth: 1.72563:  88%| | 7/8 [00:17<00:01,  1.58s/it]\u001b[A\n","loss: 1.45127, smth: 1.72563: 100%|| 8/8 [00:17<00:00,  2.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  1.17it/s]\u001b[A\n","100%|| 2/2 [00:03<00:00,  1.66s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:07 2021 Epoch 1, lr: 0.0023000, train loss: 1.72563, train auc: 0.61963, val loss: 1.00917, val auc: 0.93760\n","score2 (0.000000 --> 0.937604).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:08 2021 Epoch: 2\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.38608, smth: 1.38608:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 1.38608, smth: 1.38608:  12%|        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 1.15968, smth: 1.27288:  12%|        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 1.15968, smth: 1.27288:  25%|       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 1.10371, smth: 1.21649:  25%|       | 2/8 [00:05<00:14,  2.40s/it]\u001b[A\n","loss: 1.10371, smth: 1.21649:  38%|      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 1.13177, smth: 1.19531:  38%|      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 1.13177, smth: 1.19531:  50%|     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 1.06556, smth: 1.16936:  50%|     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 1.06556, smth: 1.16936:  62%|   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.96249, smth: 1.13488:  62%|   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.96249, smth: 1.13488:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 1.11051, smth: 1.13140:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 1.11051, smth: 1.13140:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 1.17383, smth: 1.13670:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 1.17383, smth: 1.13670: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:19 2021 Epoch 2, lr: 0.0025250, train loss: 1.13670, train auc: 0.86957, val loss: 0.80580, val auc: 0.94940\n","score2 (0.937604 --> 0.949396).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:20 2021 Epoch: 3\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.15091, smth: 1.15091:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.15091, smth: 1.15091:  12%|        | 1/8 [00:02<00:16,  2.41s/it]\u001b[A\n","loss: 0.92139, smth: 1.03615:  12%|        | 1/8 [00:03<00:16,  2.41s/it]\u001b[A\n","loss: 0.92139, smth: 1.03615:  25%|       | 2/8 [00:03<00:11,  1.91s/it]\u001b[A\n","loss: 0.84774, smth: 0.97335:  25%|       | 2/8 [00:04<00:11,  1.91s/it]\u001b[A\n","loss: 0.84774, smth: 0.97335:  38%|      | 3/8 [00:04<00:08,  1.64s/it]\u001b[A\n","loss: 1.00178, smth: 0.98046:  38%|      | 3/8 [00:05<00:08,  1.64s/it]\u001b[A\n","loss: 1.00178, smth: 0.98046:  50%|     | 4/8 [00:05<00:05,  1.44s/it]\u001b[A\n","loss: 0.81792, smth: 0.94795:  50%|     | 4/8 [00:06<00:05,  1.44s/it]\u001b[A\n","loss: 0.81792, smth: 0.94795:  62%|   | 5/8 [00:06<00:04,  1.36s/it]\u001b[A\n","loss: 0.72244, smth: 0.91036:  62%|   | 5/8 [00:07<00:04,  1.36s/it]\u001b[A\n","loss: 0.72244, smth: 0.91036:  75%|  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 1.01908, smth: 0.92590:  75%|  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 1.01908, smth: 0.92590:  88%| | 7/8 [00:07<00:01,  1.09s/it]\u001b[A\n","loss: 0.85947, smth: 0.91759:  88%| | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n","loss: 0.85947, smth: 0.91759: 100%|| 8/8 [00:08<00:00,  1.06s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.25it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:29 2021 Epoch 3, lr: 0.0027500, train loss: 0.91759, train auc: 0.93619, val loss: 0.63942, val auc: 0.96346\n","score2 (0.949396 --> 0.963458).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:30 2021 Epoch: 4\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.83254, smth: 0.83254:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.83254, smth: 0.83254:  12%|        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.80906, smth: 0.82080:  12%|        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.80906, smth: 0.82080:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.83037, smth: 0.82399:  25%|       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.83037, smth: 0.82399:  38%|      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.66744, smth: 0.78485:  38%|      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.66744, smth: 0.78485:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.62307, smth: 0.75250:  50%|     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.62307, smth: 0.75250:  62%|   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.51512, smth: 0.71293:  62%|   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.51512, smth: 0.71293:  75%|  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.55345, smth: 0.69015:  75%|  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.55345, smth: 0.69015:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.50414, smth: 0.66690:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.50414, smth: 0.66690: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.84it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.52it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:40 2021 Epoch 4, lr: 0.0029750, train loss: 0.66690, train auc: 0.96245, val loss: 0.77613, val auc: 0.95485\n","Thu Jun 10 01:02:40 2021 Epoch: 5\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48700, smth: 0.48700:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.48700, smth: 0.48700:  12%|        | 1/8 [00:03<00:22,  3.21s/it]\u001b[A\n","loss: 0.43448, smth: 0.46074:  12%|        | 1/8 [00:04<00:22,  3.21s/it]\u001b[A\n","loss: 0.43448, smth: 0.46074:  25%|       | 2/8 [00:04<00:15,  2.51s/it]\u001b[A\n","loss: 0.68850, smth: 0.53666:  25%|       | 2/8 [00:05<00:15,  2.51s/it]\u001b[A\n","loss: 0.68850, smth: 0.53666:  38%|      | 3/8 [00:05<00:10,  2.17s/it]\u001b[A\n","loss: 0.80894, smth: 0.60473:  38%|      | 3/8 [00:06<00:10,  2.17s/it]\u001b[A\n","loss: 0.80894, smth: 0.60473:  50%|     | 4/8 [00:06<00:07,  1.85s/it]\u001b[A\n","loss: 0.49556, smth: 0.58290:  50%|     | 4/8 [00:07<00:07,  1.85s/it]\u001b[A\n","loss: 0.49556, smth: 0.58290:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.54938, smth: 0.57731:  62%|   | 5/8 [00:08<00:04,  1.52s/it]\u001b[A\n","loss: 0.54938, smth: 0.57731:  75%|  | 6/8 [00:08<00:02,  1.47s/it]\u001b[A\n","loss: 0.76756, smth: 0.60449:  75%|  | 6/8 [00:09<00:02,  1.47s/it]\u001b[A\n","loss: 0.76756, smth: 0.60449:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.57820, smth: 0.60120:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.57820, smth: 0.60120: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  4.03it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:50 2021 Epoch 5, lr: 0.0032000, train loss: 0.60120, train auc: 0.96984, val loss: 0.61952, val auc: 0.97215\n","score2 (0.963458 --> 0.972146).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:02:52 2021 Epoch: 6\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59530, smth: 0.59530:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59530, smth: 0.59530:  12%|        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.46294, smth: 0.52912:  12%|        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.46294, smth: 0.52912:  25%|       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.61850, smth: 0.55891:  25%|       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.61850, smth: 0.55891:  38%|      | 3/8 [00:04<00:09,  1.95s/it]\u001b[A\n","loss: 0.44162, smth: 0.52959:  38%|      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.44162, smth: 0.52959:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.48667, smth: 0.52100:  50%|     | 4/8 [00:06<00:06,  1.60s/it]\u001b[A\n","loss: 0.48667, smth: 0.52100:  62%|   | 5/8 [00:06<00:04,  1.45s/it]\u001b[A\n","loss: 0.72897, smth: 0.55566:  62%|   | 5/8 [00:07<00:04,  1.45s/it]\u001b[A\n","loss: 0.72897, smth: 0.55566:  75%|  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.51500, smth: 0.54986:  75%|  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.51500, smth: 0.54986:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.33096, smth: 0.52249:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.33096, smth: 0.52249: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.75it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:02 2021 Epoch 6, lr: 0.0034250, train loss: 0.52249, train auc: 0.96954, val loss: 0.49305, val auc: 0.97660\n","score2 (0.972146 --> 0.976604).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:03 2021 Epoch: 7\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.46693, smth: 0.46693:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.46693, smth: 0.46693:  12%|        | 1/8 [00:02<00:17,  2.46s/it]\u001b[A\n","loss: 0.62007, smth: 0.54350:  12%|        | 1/8 [00:03<00:17,  2.46s/it]\u001b[A\n","loss: 0.62007, smth: 0.54350:  25%|       | 2/8 [00:03<00:11,  1.95s/it]\u001b[A\n","loss: 0.86328, smth: 0.65009:  25%|       | 2/8 [00:04<00:11,  1.95s/it]\u001b[A\n","loss: 0.86328, smth: 0.65009:  38%|      | 3/8 [00:04<00:08,  1.79s/it]\u001b[A\n","loss: 0.67372, smth: 0.65600:  38%|      | 3/8 [00:05<00:08,  1.79s/it]\u001b[A\n","loss: 0.67372, smth: 0.65600:  50%|     | 4/8 [00:05<00:05,  1.47s/it]\u001b[A\n","loss: 0.55663, smth: 0.63613:  50%|     | 4/8 [00:07<00:05,  1.47s/it]\u001b[A\n","loss: 0.55663, smth: 0.63613:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.40313, smth: 0.59729:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.40313, smth: 0.59729:  75%|  | 6/8 [00:07<00:02,  1.33s/it]\u001b[A\n","loss: 0.65392, smth: 0.60538:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.65392, smth: 0.60538:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.33880, smth: 0.57206:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.33880, smth: 0.57206: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.90it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:13 2021 Epoch 7, lr: 0.0036500, train loss: 0.57206, train auc: 0.96694, val loss: 0.77462, val auc: 0.96185\n","Thu Jun 10 01:03:13 2021 Epoch: 8\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.57412, smth: 0.57412:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.57412, smth: 0.57412:  12%|        | 1/8 [00:03<00:24,  3.55s/it]\u001b[A\n","loss: 0.45798, smth: 0.51605:  12%|        | 1/8 [00:04<00:24,  3.55s/it]\u001b[A\n","loss: 0.45798, smth: 0.51605:  25%|       | 2/8 [00:04<00:16,  2.70s/it]\u001b[A\n","loss: 0.50549, smth: 0.51253:  25%|       | 2/8 [00:05<00:16,  2.70s/it]\u001b[A\n","loss: 0.50549, smth: 0.51253:  38%|      | 3/8 [00:05<00:11,  2.37s/it]\u001b[A\n","loss: 0.54082, smth: 0.51960:  38%|      | 3/8 [00:06<00:11,  2.37s/it]\u001b[A\n","loss: 0.54082, smth: 0.51960:  50%|     | 4/8 [00:06<00:07,  1.92s/it]\u001b[A\n","loss: 0.58616, smth: 0.53291:  50%|     | 4/8 [00:07<00:07,  1.92s/it]\u001b[A\n","loss: 0.58616, smth: 0.53291:  62%|   | 5/8 [00:07<00:05,  1.69s/it]\u001b[A\n","loss: 0.41711, smth: 0.51361:  62%|   | 5/8 [00:08<00:05,  1.69s/it]\u001b[A\n","loss: 0.41711, smth: 0.51361:  75%|  | 6/8 [00:08<00:02,  1.47s/it]\u001b[A\n","loss: 0.51410, smth: 0.51368:  75%|  | 6/8 [00:10<00:02,  1.47s/it]\u001b[A\n","loss: 0.51410, smth: 0.51368:  88%| | 7/8 [00:10<00:01,  1.41s/it]\u001b[A\n","loss: 0.83069, smth: 0.55331:  88%| | 7/8 [00:10<00:01,  1.41s/it]\u001b[A\n","loss: 0.83069, smth: 0.55331: 100%|| 8/8 [00:10<00:00,  1.32s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.36it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  4.08it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:24 2021 Epoch 8, lr: 0.0038750, train loss: 0.55331, train auc: 0.97342, val loss: 0.99307, val auc: 0.96292\n","Thu Jun 10 01:03:24 2021 Epoch: 9\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59457, smth: 0.59457:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59457, smth: 0.59457:  12%|        | 1/8 [00:02<00:19,  2.76s/it]\u001b[A\n","loss: 0.36774, smth: 0.48116:  12%|        | 1/8 [00:03<00:19,  2.76s/it]\u001b[A\n","loss: 0.36774, smth: 0.48116:  25%|       | 2/8 [00:03<00:12,  2.16s/it]\u001b[A\n","loss: 0.43057, smth: 0.46430:  25%|       | 2/8 [00:05<00:12,  2.16s/it]\u001b[A\n","loss: 0.43057, smth: 0.46430:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.58863, smth: 0.49538:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.58863, smth: 0.49538:  50%|     | 4/8 [00:06<00:06,  1.65s/it]\u001b[A\n","loss: 0.78695, smth: 0.55369:  50%|     | 4/8 [00:07<00:06,  1.65s/it]\u001b[A\n","loss: 0.78695, smth: 0.55369:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.72497, smth: 0.58224:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.72497, smth: 0.58224:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.71348, smth: 0.60099:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.71348, smth: 0.60099:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.51424, smth: 0.59014:  88%| | 7/8 [00:09<00:01,  1.10s/it]\u001b[A\n","loss: 0.51424, smth: 0.59014: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.98it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:34 2021 Epoch 9, lr: 0.0041000, train loss: 0.59014, train auc: 0.96939, val loss: 0.68119, val auc: 0.97322\n","Thu Jun 10 01:03:34 2021 Epoch: 10\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.69259, smth: 0.69259:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.69259, smth: 0.69259:  12%|        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.90948, smth: 0.80104:  12%|        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.90948, smth: 0.80104:  25%|       | 2/8 [00:04<00:15,  2.51s/it]\u001b[A\n","loss: 0.45597, smth: 0.68601:  25%|       | 2/8 [00:05<00:15,  2.51s/it]\u001b[A\n","loss: 0.45597, smth: 0.68601:  38%|      | 3/8 [00:05<00:10,  2.19s/it]\u001b[A\n","loss: 0.44144, smth: 0.62487:  38%|      | 3/8 [00:06<00:10,  2.19s/it]\u001b[A\n","loss: 0.44144, smth: 0.62487:  50%|     | 4/8 [00:06<00:06,  1.75s/it]\u001b[A\n","loss: 1.03797, smth: 0.70749:  50%|     | 4/8 [00:07<00:06,  1.75s/it]\u001b[A\n","loss: 1.03797, smth: 0.70749:  62%|   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.67638, smth: 0.70230:  62%|   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.67638, smth: 0.70230:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.39480, smth: 0.65837:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.39480, smth: 0.65837:  88%| | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.66543, smth: 0.65926:  88%| | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.66543, smth: 0.65926: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.30it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  4.02it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:44 2021 Epoch 10, lr: 0.0043250, train loss: 0.65926, train auc: 0.97140, val loss: 0.66924, val auc: 0.97214\n","Thu Jun 10 01:03:44 2021 Epoch: 11\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.53183, smth: 0.53183:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.53183, smth: 0.53183:  12%|        | 1/8 [00:02<00:18,  2.63s/it]\u001b[A\n","loss: 0.81047, smth: 0.67115:  12%|        | 1/8 [00:03<00:18,  2.63s/it]\u001b[A\n","loss: 0.81047, smth: 0.67115:  25%|       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.61005, smth: 0.65079:  25%|       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.61005, smth: 0.65079:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.82511, smth: 0.69437:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.82511, smth: 0.69437:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.47226, smth: 0.64995:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.47226, smth: 0.64995:  62%|   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.63443, smth: 0.64736:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.63443, smth: 0.64736:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.45816, smth: 0.62033:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.45816, smth: 0.62033:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.43810, smth: 0.59755:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.43810, smth: 0.59755: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.24it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.99it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:03:53 2021 Epoch 11, lr: 0.0045500, train loss: 0.59755, train auc: 0.97359, val loss: 0.77577, val auc: 0.97439\n","Thu Jun 10 01:03:53 2021 Epoch: 12\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.61797, smth: 0.61797:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.61797, smth: 0.61797:  12%|        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.59983, smth: 0.60890:  12%|        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.59983, smth: 0.60890:  25%|       | 2/8 [00:03<00:13,  2.27s/it]\u001b[A\n","loss: 0.53437, smth: 0.58406:  25%|       | 2/8 [00:04<00:13,  2.27s/it]\u001b[A\n","loss: 0.53437, smth: 0.58406:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.54804, smth: 0.57505:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.54804, smth: 0.57505:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.94870, smth: 0.64978:  50%|     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.94870, smth: 0.64978:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.87158, smth: 0.68675:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.87158, smth: 0.68675:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.63518, smth: 0.67938:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.63518, smth: 0.67938:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.92282, smth: 0.70981:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.92282, smth: 0.70981: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  4.00it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:03 2021 Epoch 12, lr: 0.0047750, train loss: 0.70981, train auc: 0.96498, val loss: 1.03174, val auc: 0.96960\n","Thu Jun 10 01:04:03 2021 Epoch: 13\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.62070, smth: 0.62070:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.62070, smth: 0.62070:  12%|        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 1.06041, smth: 0.84056:  12%|        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 1.06041, smth: 0.84056:  25%|       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.57550, smth: 0.75220:  25%|       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.57550, smth: 0.75220:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.85053, smth: 0.77679:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.85053, smth: 0.77679:  50%|     | 4/8 [00:05<00:05,  1.49s/it]\u001b[A\n","loss: 0.80302, smth: 0.78203:  50%|     | 4/8 [00:06<00:05,  1.49s/it]\u001b[A\n","loss: 0.80302, smth: 0.78203:  62%|   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.64461, smth: 0.75913:  62%|   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.64461, smth: 0.75913:  75%|  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 1.05283, smth: 0.80109:  75%|  | 6/8 [00:08<00:02,  1.22s/it]\u001b[A\n","loss: 1.05283, smth: 0.80109:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.25037, smth: 0.73225:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.25037, smth: 0.73225: 100%|| 8/8 [00:08<00:00,  1.10s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.22it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.96it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:12 2021 Epoch 13, lr: 0.0050000, train loss: 0.73225, train auc: 0.95104, val loss: 8.15957, val auc: 0.79714\n","Thu Jun 10 01:04:12 2021 Epoch: 14\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.64485, smth: 0.64485:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.64485, smth: 0.64485:  12%|        | 1/8 [00:03<00:20,  3.00s/it]\u001b[A\n","loss: 0.48115, smth: 0.56300:  12%|        | 1/8 [00:03<00:20,  3.00s/it]\u001b[A\n","loss: 0.48115, smth: 0.56300:  25%|       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.62838, smth: 0.58479:  25%|       | 2/8 [00:05<00:14,  2.34s/it]\u001b[A\n","loss: 0.62838, smth: 0.58479:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.69743, smth: 0.61295:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.69743, smth: 0.61295:  50%|     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 1.00829, smth: 0.69202:  50%|     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 1.00829, smth: 0.69202:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.92567, smth: 0.73096:  62%|   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.92567, smth: 0.73096:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.69690, smth: 0.72610:  75%|  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.69690, smth: 0.72610:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n","\n","loss: 0.68355, smth: 0.72078:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.68355, smth: 0.72078: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.19it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.96it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:23 2021 Epoch 14, lr: 0.0050000, train loss: 0.72078, train auc: 0.95693, val loss: 5.91686, val auc: 0.86220\n","Thu Jun 10 01:04:23 2021 Epoch: 15\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.65834, smth: 0.65834:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.65834, smth: 0.65834:  12%|        | 1/8 [00:02<00:19,  2.80s/it]\u001b[A\n","loss: 0.57534, smth: 0.61684:  12%|        | 1/8 [00:03<00:19,  2.80s/it]\u001b[A\n","loss: 0.57534, smth: 0.61684:  25%|       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.78426, smth: 0.67265:  25%|       | 2/8 [00:04<00:13,  2.18s/it]\u001b[A\n","loss: 0.78426, smth: 0.67265:  38%|      | 3/8 [00:04<00:09,  1.96s/it]\u001b[A\n","loss: 0.88895, smth: 0.72672:  38%|      | 3/8 [00:05<00:09,  1.96s/it]\u001b[A\n","loss: 0.88895, smth: 0.72672:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.80375, smth: 0.74213:  50%|     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.80375, smth: 0.74213:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.64549, smth: 0.72602:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.64549, smth: 0.72602:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.59957, smth: 0.70796:  75%|  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.59957, smth: 0.70796:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.71804, smth: 0.70922:  88%| | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.71804, smth: 0.70922: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.21it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.97it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:32 2021 Epoch 15, lr: 0.0049985, train loss: 0.70922, train auc: 0.96025, val loss: 5.74044, val auc: 0.86356\n","Thu Jun 10 01:04:32 2021 Epoch: 16\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41030, smth: 0.41030:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41030, smth: 0.41030:  12%|        | 1/8 [00:02<00:19,  2.72s/it]\u001b[A\n","loss: 0.58773, smth: 0.49902:  12%|        | 1/8 [00:03<00:19,  2.72s/it]\u001b[A\n","loss: 0.58773, smth: 0.49902:  25%|       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 1.00416, smth: 0.66740:  25%|       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 1.00416, smth: 0.66740:  38%|      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 1.41490, smth: 0.85427:  38%|      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 1.41490, smth: 0.85427:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.77959, smth: 0.83934:  50%|     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.77959, smth: 0.83934:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.99615, smth: 0.86547:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.99615, smth: 0.86547:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 1.01265, smth: 0.88650:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 1.01265, smth: 0.88650:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 1.40597, smth: 0.95143:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 1.40597, smth: 0.95143: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.32it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.98it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:43 2021 Epoch 16, lr: 0.0049966, train loss: 0.95143, train auc: 0.95334, val loss: 1.50027, val auc: 0.94986\n","Thu Jun 10 01:04:43 2021 Epoch: 17\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.05981, smth: 1.05981:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.05981, smth: 1.05981:  12%|        | 1/8 [00:02<00:20,  2.86s/it]\u001b[A\n","loss: 0.84214, smth: 0.95098:  12%|        | 1/8 [00:03<00:20,  2.86s/it]\u001b[A\n","loss: 0.84214, smth: 0.95098:  25%|       | 2/8 [00:03<00:13,  2.24s/it]\u001b[A\n","loss: 0.74591, smth: 0.88262:  25%|       | 2/8 [00:05<00:13,  2.24s/it]\u001b[A\n","loss: 0.74591, smth: 0.88262:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.58753, smth: 0.80885:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.58753, smth: 0.80885:  50%|     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.72751, smth: 0.79258:  50%|     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.72751, smth: 0.79258:  62%|   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.60803, smth: 0.76182:  62%|   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.60803, smth: 0.76182:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.87757, smth: 0.77836:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.87757, smth: 0.77836:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.88345, smth: 0.79149:  88%| | 7/8 [00:09<00:01,  1.13s/it]\u001b[A\n","loss: 0.88345, smth: 0.79149: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.82it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:04:52 2021 Epoch 17, lr: 0.0049939, train loss: 0.79149, train auc: 0.96404, val loss: 0.96668, val auc: 0.94975\n","Thu Jun 10 01:04:52 2021 Epoch: 18\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.70969, smth: 0.70969:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.70969, smth: 0.70969:  12%|        | 1/8 [00:03<00:21,  3.14s/it]\u001b[A\n","loss: 0.66427, smth: 0.68698:  12%|        | 1/8 [00:03<00:21,  3.14s/it]\u001b[A\n","loss: 0.66427, smth: 0.68698:  25%|       | 2/8 [00:03<00:14,  2.41s/it]\u001b[A\n","loss: 1.05702, smth: 0.81033:  25%|       | 2/8 [00:05<00:14,  2.41s/it]\u001b[A\n","loss: 1.05702, smth: 0.81033:  38%|      | 3/8 [00:05<00:11,  2.20s/it]\u001b[A\n","loss: 0.93764, smth: 0.84216:  38%|      | 3/8 [00:06<00:11,  2.20s/it]\u001b[A\n","loss: 0.93764, smth: 0.84216:  50%|     | 4/8 [00:06<00:07,  1.77s/it]\u001b[A\n","loss: 0.69837, smth: 0.81340:  50%|     | 4/8 [00:07<00:07,  1.77s/it]\u001b[A\n","loss: 0.69837, smth: 0.81340:  62%|   | 5/8 [00:07<00:05,  1.67s/it]\u001b[A\n","loss: 0.61742, smth: 0.78074:  62%|   | 5/8 [00:08<00:05,  1.67s/it]\u001b[A\n","loss: 0.61742, smth: 0.78074:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.90524, smth: 0.79852:  75%|  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.90524, smth: 0.79852:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.93680, smth: 0.81581:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.93680, smth: 0.81581: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:03 2021 Epoch 18, lr: 0.0049905, train loss: 0.81581, train auc: 0.95060, val loss: 9.92424, val auc: 0.86247\n","Thu Jun 10 01:05:03 2021 Epoch: 19\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.01120, smth: 1.01120:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.01120, smth: 1.01120:  12%|        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.62598, smth: 0.81859:  12%|        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.62598, smth: 0.81859:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.81636, smth: 0.81785:  25%|       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.81636, smth: 0.81785:  38%|      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.69231, smth: 0.78646:  38%|      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.69231, smth: 0.78646:  50%|     | 4/8 [00:05<00:06,  1.54s/it]\u001b[A\n","loss: 0.72103, smth: 0.77337:  50%|     | 4/8 [00:07<00:06,  1.54s/it]\u001b[A\n","loss: 0.72103, smth: 0.77337:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 1.13018, smth: 0.83284:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 1.13018, smth: 0.83284:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 1.00076, smth: 0.85683:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 1.00076, smth: 0.85683:  88%| | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 1.15204, smth: 0.89373:  88%| | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 1.15204, smth: 0.89373: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.96it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:12 2021 Epoch 19, lr: 0.0049863, train loss: 0.89373, train auc: 0.95589, val loss: 1.05282, val auc: 0.93817\n","Thu Jun 10 01:05:12 2021 Epoch: 20\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.79186, smth: 0.79186:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.79186, smth: 0.79186:  12%|        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.57217, smth: 0.68202:  12%|        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.57217, smth: 0.68202:  25%|       | 2/8 [00:03<00:13,  2.17s/it]\u001b[A\n","loss: 0.69258, smth: 0.68554:  25%|       | 2/8 [00:05<00:13,  2.17s/it]\u001b[A\n","loss: 0.69258, smth: 0.68554:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.72626, smth: 0.69572:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.72626, smth: 0.69572:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.64613, smth: 0.68580:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.64613, smth: 0.68580:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.54329, smth: 0.66205:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.54329, smth: 0.66205:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.96462, smth: 0.70527:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.96462, smth: 0.70527:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.68279, smth: 0.70246:  88%| | 7/8 [00:09<00:01,  1.11s/it]\u001b[A\n","loss: 0.68279, smth: 0.70246: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:22 2021 Epoch 20, lr: 0.0049814, train loss: 0.70246, train auc: 0.95123, val loss: 0.64336, val auc: 0.96596\n","Thu Jun 10 01:05:22 2021 Epoch: 21\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59275, smth: 0.59275:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59275, smth: 0.59275:  12%|        | 1/8 [00:02<00:17,  2.51s/it]\u001b[A\n","loss: 0.51678, smth: 0.55477:  12%|        | 1/8 [00:03<00:17,  2.51s/it]\u001b[A\n","loss: 0.51678, smth: 0.55477:  25%|       | 2/8 [00:03<00:11,  1.97s/it]\u001b[A\n","loss: 0.62369, smth: 0.57774:  25%|       | 2/8 [00:04<00:11,  1.97s/it]\u001b[A\n","loss: 0.62369, smth: 0.57774:  38%|      | 3/8 [00:04<00:08,  1.74s/it]\u001b[A\n","loss: 0.94090, smth: 0.66853:  38%|      | 3/8 [00:05<00:08,  1.74s/it]\u001b[A\n","loss: 0.94090, smth: 0.66853:  50%|     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.45736, smth: 0.62630:  50%|     | 4/8 [00:06<00:05,  1.43s/it]\u001b[A\n","loss: 0.45736, smth: 0.62630:  62%|   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.62714, smth: 0.62644:  62%|   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.62714, smth: 0.62644:  75%|  | 6/8 [00:07<00:02,  1.19s/it]\u001b[A\n","loss: 0.69770, smth: 0.63662:  75%|  | 6/8 [00:08<00:02,  1.19s/it]\u001b[A\n","loss: 0.69770, smth: 0.63662:  88%| | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 2.65275, smth: 0.88863:  88%| | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 2.65275, smth: 0.88863: 100%|| 8/8 [00:08<00:00,  1.08s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.25it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.94it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:31 2021 Epoch 21, lr: 0.0049757, train loss: 0.88863, train auc: 0.95313, val loss: 1.86420, val auc: 0.94827\n","Thu Jun 10 01:05:31 2021 Epoch: 22\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.40282, smth: 1.40282:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.40282, smth: 1.40282:  12%|        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.77409, smth: 1.08846:  12%|        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.77409, smth: 1.08846:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 1.08674, smth: 1.08788:  25%|       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 1.08674, smth: 1.08788:  38%|      | 3/8 [00:04<00:08,  1.74s/it]\u001b[A\n","loss: 2.28033, smth: 1.38600:  38%|      | 3/8 [00:05<00:08,  1.74s/it]\u001b[A\n","loss: 2.28033, smth: 1.38600:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 1.84629, smth: 1.47806:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 1.84629, smth: 1.47806:  62%|   | 5/8 [00:06<00:04,  1.34s/it]\u001b[A\n","loss: 1.06994, smth: 1.41004:  62%|   | 5/8 [00:07<00:04,  1.34s/it]\u001b[A\n","loss: 1.06994, smth: 1.41004:  75%|  | 6/8 [00:07<00:02,  1.42s/it]\u001b[A\n","loss: 0.91058, smth: 1.33869:  75%|  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.91058, smth: 1.33869:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 1.84471, smth: 1.40194:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 1.84471, smth: 1.40194: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.87it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:41 2021 Epoch 22, lr: 0.0049692, train loss: 1.40194, train auc: 0.91230, val loss: 11.83332, val auc: 0.79329\n","Thu Jun 10 01:05:41 2021 Epoch: 23\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.47198, smth: 1.47198:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 1.47198, smth: 1.47198:  12%|        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 1.09394, smth: 1.28296:  12%|        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 1.09394, smth: 1.28296:  25%|       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 1.07596, smth: 1.21396:  25%|       | 2/8 [00:05<00:13,  2.26s/it]\u001b[A\n","loss: 1.07596, smth: 1.21396:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 1.04525, smth: 1.17178:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 1.04525, smth: 1.17178:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 1.21603, smth: 1.18063:  50%|     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 1.21603, smth: 1.18063:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.90728, smth: 1.13507:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.90728, smth: 1.13507:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.76042, smth: 1.08155:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.76042, smth: 1.08155:  88%| | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 1.02207, smth: 1.07412:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 1.02207, smth: 1.07412: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.29it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  4.01it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:05:51 2021 Epoch 23, lr: 0.0049620, train loss: 1.07412, train auc: 0.93030, val loss: 0.89682, val auc: 0.93692\n","Thu Jun 10 01:05:51 2021 Epoch: 24\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.98188, smth: 0.98188:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.98188, smth: 0.98188:  12%|        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 0.74438, smth: 0.86313:  12%|        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 0.74438, smth: 0.86313:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.82990, smth: 0.85205:  25%|       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.82990, smth: 0.85205:  38%|      | 3/8 [00:04<00:09,  1.81s/it]\u001b[A\n","loss: 0.83544, smth: 0.84790:  38%|      | 3/8 [00:05<00:09,  1.81s/it]\u001b[A\n","loss: 0.83544, smth: 0.84790:  50%|     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.96941, smth: 0.87220:  50%|     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.96941, smth: 0.87220:  62%|   | 5/8 [00:06<00:04,  1.41s/it]\u001b[A\n","loss: 1.02980, smth: 0.89847:  62%|   | 5/8 [00:07<00:04,  1.41s/it]\u001b[A\n","loss: 1.02980, smth: 0.89847:  75%|  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.64395, smth: 0.86211:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.64395, smth: 0.86211:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.88485, smth: 0.86495:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.88485, smth: 0.86495: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.24it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:00 2021 Epoch 24, lr: 0.0049541, train loss: 0.86495, train auc: 0.92997, val loss: 0.59958, val auc: 0.96698\n","Thu Jun 10 01:06:00 2021 Epoch: 25\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.56553, smth: 0.56553:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.56553, smth: 0.56553:  12%|        | 1/8 [00:02<00:19,  2.77s/it]\u001b[A\n","loss: 0.72634, smth: 0.64594:  12%|        | 1/8 [00:03<00:19,  2.77s/it]\u001b[A\n","loss: 0.72634, smth: 0.64594:  25%|       | 2/8 [00:03<00:12,  2.17s/it]\u001b[A\n","loss: 0.72172, smth: 0.67120:  25%|       | 2/8 [00:05<00:12,  2.17s/it]\u001b[A\n","loss: 0.72172, smth: 0.67120:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.72308, smth: 0.68417:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.72308, smth: 0.68417:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.97751, smth: 0.74284:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.97751, smth: 0.74284:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.65783, smth: 0.72867:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.65783, smth: 0.72867:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.65356, smth: 0.71794:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.65356, smth: 0.71794:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.78194, smth: 0.72594:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.78194, smth: 0.72594: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:10 2021 Epoch 25, lr: 0.0049454, train loss: 0.72594, train auc: 0.95243, val loss: 0.52566, val auc: 0.97652\n","Thu Jun 10 01:06:10 2021 Epoch: 26\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.66352, smth: 0.66352:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.66352, smth: 0.66352:  12%|        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 1.27738, smth: 0.97045:  12%|        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 1.27738, smth: 0.97045:  25%|       | 2/8 [00:03<00:12,  2.13s/it]\u001b[A\n","loss: 0.68798, smth: 0.87629:  25%|       | 2/8 [00:04<00:12,  2.13s/it]\u001b[A\n","loss: 0.68798, smth: 0.87629:  38%|      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.67430, smth: 0.82580:  38%|      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.67430, smth: 0.82580:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.75840, smth: 0.81232:  50%|     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.75840, smth: 0.81232:  62%|   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.73878, smth: 0.80006:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.73878, smth: 0.80006:  75%|  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.69042, smth: 0.78440:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.69042, smth: 0.78440:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.64476, smth: 0.76694:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.64476, smth: 0.76694: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.23it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.95it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:20 2021 Epoch 26, lr: 0.0049359, train loss: 0.76694, train auc: 0.94970, val loss: 0.45816, val auc: 0.97648\n","Thu Jun 10 01:06:20 2021 Epoch: 27\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48637, smth: 0.48637:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.48637, smth: 0.48637:  12%|        | 1/8 [00:02<00:19,  2.71s/it]\u001b[A\n","loss: 2.61476, smth: 1.55056:  12%|        | 1/8 [00:03<00:19,  2.71s/it]\u001b[A\n","loss: 2.61476, smth: 1.55056:  25%|       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.56730, smth: 1.22281:  25%|       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.56730, smth: 1.22281:  38%|      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.60030, smth: 1.06718:  38%|      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.60030, smth: 1.06718:  50%|     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 1.02708, smth: 1.05916:  50%|     | 4/8 [00:06<00:06,  1.57s/it]\u001b[A\n","loss: 1.02708, smth: 1.05916:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.73976, smth: 1.00593:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.73976, smth: 1.00593:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 1.11915, smth: 1.02210:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 1.11915, smth: 1.02210:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.93001, smth: 1.01059:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.93001, smth: 1.01059: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:30 2021 Epoch 27, lr: 0.0049257, train loss: 1.01059, train auc: 0.94613, val loss: 1.29746, val auc: 0.93975\n","Thu Jun 10 01:06:30 2021 Epoch: 28\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.83990, smth: 0.83990:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.83990, smth: 0.83990:  12%|        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 0.64743, smth: 0.74366:  12%|        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 0.64743, smth: 0.74366:  25%|       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.58769, smth: 0.69167:  25%|       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.58769, smth: 0.69167:  38%|      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.88267, smth: 0.73942:  38%|      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.88267, smth: 0.73942:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.80287, smth: 0.75211:  50%|     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.80287, smth: 0.75211:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.86701, smth: 0.77126:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.86701, smth: 0.77126:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.67263, smth: 0.75717:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.67263, smth: 0.75717:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 1.04107, smth: 0.79266:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 1.04107, smth: 0.79266: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.92it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:40 2021 Epoch 28, lr: 0.0049148, train loss: 0.79266, train auc: 0.94695, val loss: 0.61770, val auc: 0.96598\n","Thu Jun 10 01:06:40 2021 Epoch: 29\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.96258, smth: 0.96258:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.96258, smth: 0.96258:  12%|        | 1/8 [00:02<00:17,  2.56s/it]\u001b[A\n","loss: 0.84412, smth: 0.90335:  12%|        | 1/8 [00:03<00:17,  2.56s/it]\u001b[A\n","loss: 0.84412, smth: 0.90335:  25%|       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.88251, smth: 0.89640:  25%|       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.88251, smth: 0.89640:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.67355, smth: 0.84069:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.67355, smth: 0.84069:  50%|     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.84851, smth: 0.84225:  50%|     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.84851, smth: 0.84225:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.96418, smth: 0.86258:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.96418, smth: 0.86258:  75%|  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 1.08302, smth: 0.89407:  75%|  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 1.08302, smth: 0.89407:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.61592, smth: 0.85930:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.61592, smth: 0.85930: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.26it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:50 2021 Epoch 29, lr: 0.0049032, train loss: 0.85930, train auc: 0.94340, val loss: 0.41609, val auc: 0.98237\n","score2 (0.976604 --> 0.982375).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:06:51 2021 Epoch: 30\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.77179, smth: 0.77179:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.77179, smth: 0.77179:  12%|        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.78742, smth: 0.77961:  12%|        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.78742, smth: 0.77961:  25%|       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.75158, smth: 0.77026:  25%|       | 2/8 [00:05<00:13,  2.22s/it]\u001b[A\n","loss: 0.75158, smth: 0.77026:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.66800, smth: 0.74470:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.66800, smth: 0.74470:  50%|     | 4/8 [00:06<00:06,  1.66s/it]\u001b[A\n","loss: 0.82789, smth: 0.76134:  50%|     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.82789, smth: 0.76134:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.75598, smth: 0.76044:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.75598, smth: 0.76044:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.48049, smth: 0.72045:  75%|  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.48049, smth: 0.72045:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.55699, smth: 0.70002:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.55699, smth: 0.70002: 100%|| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.85it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.60it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:01 2021 Epoch 30, lr: 0.0048908, train loss: 0.70002, train auc: 0.95395, val loss: 0.58365, val auc: 0.97433\n","Thu Jun 10 01:07:01 2021 Epoch: 31\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.43496, smth: 0.43496:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.43496, smth: 0.43496:  12%|        | 1/8 [00:03<00:23,  3.38s/it]\u001b[A\n","loss: 0.53036, smth: 0.48266:  12%|        | 1/8 [00:04<00:23,  3.38s/it]\u001b[A\n","loss: 0.53036, smth: 0.48266:  25%|       | 2/8 [00:04<00:15,  2.60s/it]\u001b[A\n","loss: 0.62310, smth: 0.52947:  25%|       | 2/8 [00:05<00:15,  2.60s/it]\u001b[A\n","loss: 0.62310, smth: 0.52947:  38%|      | 3/8 [00:05<00:11,  2.33s/it]\u001b[A\n","loss: 0.62320, smth: 0.55290:  38%|      | 3/8 [00:06<00:11,  2.33s/it]\u001b[A\n","loss: 0.62320, smth: 0.55290:  50%|     | 4/8 [00:06<00:07,  1.86s/it]\u001b[A\n","loss: 0.64727, smth: 0.57178:  50%|     | 4/8 [00:08<00:07,  1.86s/it]\u001b[A\n","loss: 0.64727, smth: 0.57178:  62%|   | 5/8 [00:08<00:05,  1.74s/it]\u001b[A\n","loss: 0.60657, smth: 0.57758:  62%|   | 5/8 [00:08<00:05,  1.74s/it]\u001b[A\n","loss: 0.60657, smth: 0.57758:  75%|  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.56161, smth: 0.57530:  75%|  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.56161, smth: 0.57530:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.70261, smth: 0.59121:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.70261, smth: 0.59121: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.86it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:12 2021 Epoch 31, lr: 0.0048776, train loss: 0.59121, train auc: 0.96249, val loss: 0.41474, val auc: 0.98365\n","score2 (0.982375 --> 0.983646).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:13 2021 Epoch: 32\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.51781, smth: 0.51781:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.51781, smth: 0.51781:  12%|        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.64156, smth: 0.57969:  12%|        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.64156, smth: 0.57969:  25%|       | 2/8 [00:03<00:14,  2.41s/it]\u001b[A\n","loss: 0.78209, smth: 0.64715:  25%|       | 2/8 [00:05<00:14,  2.41s/it]\u001b[A\n","loss: 0.78209, smth: 0.64715:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.72508, smth: 0.66663:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.72508, smth: 0.66663:  50%|     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.59966, smth: 0.65324:  50%|     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.59966, smth: 0.65324:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.48271, smth: 0.62482:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.48271, smth: 0.62482:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.49350, smth: 0.60606:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.49350, smth: 0.60606:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.44333, smth: 0.58572:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.44333, smth: 0.58572: 100%|| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.72it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:23 2021 Epoch 32, lr: 0.0048638, train loss: 0.58572, train auc: 0.96093, val loss: 0.50449, val auc: 0.98377\n","score2 (0.983646 --> 0.983771).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:24 2021 Epoch: 33\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.73825, smth: 0.73825:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.73825, smth: 0.73825:  12%|        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.62407, smth: 0.68116:  12%|        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.62407, smth: 0.68116:  25%|       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.48113, smth: 0.61448:  25%|       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.48113, smth: 0.61448:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.46714, smth: 0.57765:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.46714, smth: 0.57765:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.49527, smth: 0.56117:  50%|     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.49527, smth: 0.56117:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.57784, smth: 0.56395:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.57784, smth: 0.56395:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.53299, smth: 0.55953:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.53299, smth: 0.55953:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.72356, smth: 0.58003:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.72356, smth: 0.58003: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.88it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.60it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:35 2021 Epoch 33, lr: 0.0048492, train loss: 0.58003, train auc: 0.96542, val loss: 0.40897, val auc: 0.98317\n","Thu Jun 10 01:07:35 2021 Epoch: 34\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41279, smth: 0.41279:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41279, smth: 0.41279:  12%|        | 1/8 [00:03<00:21,  3.09s/it]\u001b[A\n","loss: 0.59800, smth: 0.50540:  12%|        | 1/8 [00:03<00:21,  3.09s/it]\u001b[A\n","loss: 0.59800, smth: 0.50540:  25%|       | 2/8 [00:03<00:14,  2.39s/it]\u001b[A\n","loss: 0.60925, smth: 0.54002:  25%|       | 2/8 [00:05<00:14,  2.39s/it]\u001b[A\n","loss: 0.60925, smth: 0.54002:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.48601, smth: 0.52652:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.48601, smth: 0.52652:  50%|     | 4/8 [00:06<00:06,  1.72s/it]\u001b[A\n","loss: 0.48438, smth: 0.51809:  50%|     | 4/8 [00:07<00:06,  1.72s/it]\u001b[A\n","loss: 0.48438, smth: 0.51809:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.53409, smth: 0.52076:  62%|   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.53409, smth: 0.52076:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.43698, smth: 0.50879:  75%|  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.43698, smth: 0.50879:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.59570, smth: 0.51965:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.59570, smth: 0.51965: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:45 2021 Epoch 34, lr: 0.0048340, train loss: 0.51965, train auc: 0.97087, val loss: 0.39675, val auc: 0.98621\n","score2 (0.983771 --> 0.986208).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:46 2021 Epoch: 35\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.59830, smth: 0.59830:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.59830, smth: 0.59830:  12%|        | 1/8 [00:02<00:17,  2.43s/it]\u001b[A\n","loss: 0.51440, smth: 0.55635:  12%|        | 1/8 [00:03<00:17,  2.43s/it]\u001b[A\n","loss: 0.51440, smth: 0.55635:  25%|       | 2/8 [00:03<00:11,  1.93s/it]\u001b[A\n","loss: 0.50610, smth: 0.53960:  25%|       | 2/8 [00:04<00:11,  1.93s/it]\u001b[A\n","loss: 0.50610, smth: 0.53960:  38%|      | 3/8 [00:04<00:08,  1.72s/it]\u001b[A\n","loss: 0.41026, smth: 0.50726:  38%|      | 3/8 [00:05<00:08,  1.72s/it]\u001b[A\n","loss: 0.41026, smth: 0.50726:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.63563, smth: 0.53294:  50%|     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.63563, smth: 0.53294:  62%|   | 5/8 [00:06<00:04,  1.35s/it]\u001b[A\n","loss: 0.54791, smth: 0.53543:  62%|   | 5/8 [00:07<00:04,  1.35s/it]\u001b[A\n","loss: 0.54791, smth: 0.53543:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.50802, smth: 0.53152:  75%|  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.50802, smth: 0.53152:  88%| | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.63494, smth: 0.54444:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.63494, smth: 0.54444: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.92it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:56 2021 Epoch 35, lr: 0.0048180, train loss: 0.54444, train auc: 0.96835, val loss: 0.30754, val auc: 0.98740\n","score2 (0.986208 --> 0.987396).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:07:57 2021 Epoch: 36\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48584, smth: 0.48584:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.48584, smth: 0.48584:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.48177, smth: 0.48380:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.48177, smth: 0.48380:  25%|       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.59893, smth: 0.52218:  25%|       | 2/8 [00:05<00:14,  2.34s/it]\u001b[A\n","loss: 0.59893, smth: 0.52218:  38%|      | 3/8 [00:05<00:10,  2.12s/it]\u001b[A\n","loss: 0.48122, smth: 0.51194:  38%|      | 3/8 [00:06<00:10,  2.12s/it]\u001b[A\n","loss: 0.48122, smth: 0.51194:  50%|     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.71943, smth: 0.55344:  50%|     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.71943, smth: 0.55344:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.42839, smth: 0.53260:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.42839, smth: 0.53260:  75%|  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.62710, smth: 0.54610:  75%|  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.62710, smth: 0.54610:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.43553, smth: 0.53228:  88%| | 7/8 [00:09<00:01,  1.11s/it]\u001b[A\n","loss: 0.43553, smth: 0.53228: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.80it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.48it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:07 2021 Epoch 36, lr: 0.0048013, train loss: 0.53228, train auc: 0.97550, val loss: 0.34623, val auc: 0.98560\n","Thu Jun 10 01:08:07 2021 Epoch: 37\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.33461, smth: 0.33461:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.33461, smth: 0.33461:  12%|        | 1/8 [00:03<00:26,  3.85s/it]\u001b[A\n","loss: 0.52457, smth: 0.42959:  12%|        | 1/8 [00:04<00:26,  3.85s/it]\u001b[A\n","loss: 0.52457, smth: 0.42959:  25%|       | 2/8 [00:04<00:17,  2.90s/it]\u001b[A\n","loss: 0.46537, smth: 0.44152:  25%|       | 2/8 [00:05<00:17,  2.90s/it]\u001b[A\n","loss: 0.46537, smth: 0.44152:  38%|      | 3/8 [00:05<00:12,  2.41s/it]\u001b[A\n","loss: 0.50294, smth: 0.45687:  38%|      | 3/8 [00:06<00:12,  2.41s/it]\u001b[A\n","loss: 0.50294, smth: 0.45687:  50%|     | 4/8 [00:06<00:07,  1.91s/it]\u001b[A\n","loss: 0.52020, smth: 0.46954:  50%|     | 4/8 [00:08<00:07,  1.91s/it]\u001b[A\n","loss: 0.52020, smth: 0.46954:  62%|   | 5/8 [00:08<00:05,  1.81s/it]\u001b[A\n","loss: 0.50431, smth: 0.47533:  62%|   | 5/8 [00:08<00:05,  1.81s/it]\u001b[A\n","loss: 0.50431, smth: 0.47533:  75%|  | 6/8 [00:08<00:02,  1.48s/it]\u001b[A\n","loss: 0.47179, smth: 0.47483:  75%|  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.47179, smth: 0.47483:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.40252, smth: 0.46579:  88%| | 7/8 [00:10<00:01,  1.30s/it]\u001b[A\n","loss: 0.40252, smth: 0.46579: 100%|| 8/8 [00:10<00:00,  1.28s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.95it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:18 2021 Epoch 37, lr: 0.0047839, train loss: 0.46579, train auc: 0.97810, val loss: 0.30624, val auc: 0.98813\n","score2 (0.987396 --> 0.988125).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:19 2021 Epoch: 38\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.30732, smth: 0.30732:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.30732, smth: 0.30732:  12%|        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.66945, smth: 0.48838:  12%|        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.66945, smth: 0.48838:  25%|       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.46780, smth: 0.48152:  25%|       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.46780, smth: 0.48152:  38%|      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.48148, smth: 0.48151:  38%|      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.48148, smth: 0.48151:  50%|     | 4/8 [00:05<00:05,  1.45s/it]\u001b[A\n","loss: 0.43065, smth: 0.47134:  50%|     | 4/8 [00:06<00:05,  1.45s/it]\u001b[A\n","loss: 0.43065, smth: 0.47134:  62%|   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.41751, smth: 0.46237:  62%|   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.41751, smth: 0.46237:  75%|  | 6/8 [00:07<00:02,  1.42s/it]\u001b[A\n","loss: 0.42570, smth: 0.45713:  75%|  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.42570, smth: 0.45713:  88%| | 7/8 [00:08<00:01,  1.31s/it]\u001b[A\n","loss: 0.45270, smth: 0.45657:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.45270, smth: 0.45657: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.83it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:29 2021 Epoch 38, lr: 0.0047658, train loss: 0.45657, train auc: 0.97865, val loss: 0.30934, val auc: 0.98954\n","score2 (0.988125 --> 0.989542).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:31 2021 Epoch: 39\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.42306, smth: 0.42306:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.42306, smth: 0.42306:  12%|        | 1/8 [00:03<00:23,  3.36s/it]\u001b[A\n","loss: 0.40848, smth: 0.41577:  12%|        | 1/8 [00:04<00:23,  3.36s/it]\u001b[A\n","loss: 0.40848, smth: 0.41577:  25%|       | 2/8 [00:04<00:15,  2.58s/it]\u001b[A\n","loss: 0.35620, smth: 0.39592:  25%|       | 2/8 [00:05<00:15,  2.58s/it]\u001b[A\n","loss: 0.35620, smth: 0.39592:  38%|      | 3/8 [00:05<00:11,  2.23s/it]\u001b[A\n","loss: 0.36169, smth: 0.38736:  38%|      | 3/8 [00:06<00:11,  2.23s/it]\u001b[A\n","loss: 0.36169, smth: 0.38736:  50%|     | 4/8 [00:06<00:07,  1.78s/it]\u001b[A\n","loss: 0.38276, smth: 0.38644:  50%|     | 4/8 [00:07<00:07,  1.78s/it]\u001b[A\n","loss: 0.38276, smth: 0.38644:  62%|   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.34750, smth: 0.37995:  62%|   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.34750, smth: 0.37995:  75%|  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.43022, smth: 0.38713:  75%|  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.43022, smth: 0.38713:  88%| | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 1.10696, smth: 0.47711:  88%| | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 1.10696, smth: 0.47711: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:41 2021 Epoch 39, lr: 0.0047470, train loss: 0.47711, train auc: 0.98044, val loss: 0.29876, val auc: 0.98854\n","Thu Jun 10 01:08:41 2021 Epoch: 40\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.61816, smth: 0.61816:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.61816, smth: 0.61816:  12%|        | 1/8 [00:03<00:26,  3.76s/it]\u001b[A\n","loss: 0.41083, smth: 0.51449:  12%|        | 1/8 [00:04<00:26,  3.76s/it]\u001b[A\n","loss: 0.41083, smth: 0.51449:  25%|       | 2/8 [00:04<00:17,  2.87s/it]\u001b[A\n","loss: 0.46014, smth: 0.49638:  25%|       | 2/8 [00:06<00:17,  2.87s/it]\u001b[A\n","loss: 0.46014, smth: 0.49638:  38%|      | 3/8 [00:06<00:12,  2.49s/it]\u001b[A\n","loss: 0.53825, smth: 0.50685:  38%|      | 3/8 [00:06<00:12,  2.49s/it]\u001b[A\n","loss: 0.53825, smth: 0.50685:  50%|     | 4/8 [00:06<00:07,  1.97s/it]\u001b[A\n","loss: 0.37709, smth: 0.48089:  50%|     | 4/8 [00:08<00:07,  1.97s/it]\u001b[A\n","loss: 0.37709, smth: 0.48089:  62%|   | 5/8 [00:08<00:05,  1.82s/it]\u001b[A\n","loss: 0.57779, smth: 0.49704:  62%|   | 5/8 [00:09<00:05,  1.82s/it]\u001b[A\n","loss: 0.57779, smth: 0.49704:  75%|  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.37372, smth: 0.47943:  75%|  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.37372, smth: 0.47943:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.68510, smth: 0.50513:  88%| | 7/8 [00:10<00:01,  1.26s/it]\u001b[A\n","loss: 0.68510, smth: 0.50513: 100%|| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:52 2021 Epoch 40, lr: 0.0047275, train loss: 0.50513, train auc: 0.97793, val loss: 0.27275, val auc: 0.99033\n","score2 (0.989542 --> 0.990333).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:08:53 2021 Epoch: 41\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.44453, smth: 0.44453:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.44453, smth: 0.44453:  12%|        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.38781, smth: 0.41617:  12%|        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.38781, smth: 0.41617:  25%|       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.47249, smth: 0.43494:  25%|       | 2/8 [00:05<00:14,  2.34s/it]\u001b[A\n","loss: 0.47249, smth: 0.43494:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.39773, smth: 0.42564:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.39773, smth: 0.42564:  50%|     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.41126, smth: 0.42276:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.41126, smth: 0.42276:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.38674, smth: 0.41676:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.38674, smth: 0.41676:  75%|  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.55520, smth: 0.43654:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.55520, smth: 0.43654:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.43261, smth: 0.43605:  88%| | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 0.43261, smth: 0.43605: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.89it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.56it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:03 2021 Epoch 41, lr: 0.0047074, train loss: 0.43605, train auc: 0.98161, val loss: 0.33325, val auc: 0.98592\n","Thu Jun 10 01:09:03 2021 Epoch: 42\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.44554, smth: 0.44554:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.44554, smth: 0.44554:  12%|        | 1/8 [00:03<00:23,  3.41s/it]\u001b[A\n","loss: 0.44482, smth: 0.44518:  12%|        | 1/8 [00:04<00:23,  3.41s/it]\u001b[A\n","loss: 0.44482, smth: 0.44518:  25%|       | 2/8 [00:04<00:16,  2.67s/it]\u001b[A\n","loss: 0.40943, smth: 0.43326:  25%|       | 2/8 [00:05<00:16,  2.67s/it]\u001b[A\n","loss: 0.40943, smth: 0.43326:  38%|      | 3/8 [00:05<00:11,  2.31s/it]\u001b[A\n","loss: 0.29198, smth: 0.39794:  38%|      | 3/8 [00:06<00:11,  2.31s/it]\u001b[A\n","loss: 0.29198, smth: 0.39794:  50%|     | 4/8 [00:06<00:07,  1.86s/it]\u001b[A\n","loss: 0.44847, smth: 0.40805:  50%|     | 4/8 [00:07<00:07,  1.86s/it]\u001b[A\n","loss: 0.44847, smth: 0.40805:  62%|   | 5/8 [00:07<00:05,  1.68s/it]\u001b[A\n","loss: 0.48083, smth: 0.42018:  62%|   | 5/8 [00:08<00:05,  1.68s/it]\u001b[A\n","loss: 0.48083, smth: 0.42018:  75%|  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.37020, smth: 0.41304:  75%|  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.37020, smth: 0.41304:  88%| | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.42597, smth: 0.41466:  88%| | 7/8 [00:10<00:01,  1.37s/it]\u001b[A\n","loss: 0.42597, smth: 0.41466: 100%|| 8/8 [00:10<00:00,  1.31s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.86it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:14 2021 Epoch 42, lr: 0.0046865, train loss: 0.41466, train auc: 0.97929, val loss: 0.36345, val auc: 0.98785\n","Thu Jun 10 01:09:14 2021 Epoch: 43\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.52100, smth: 0.52100:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.52100, smth: 0.52100:  12%|        | 1/8 [00:02<00:20,  2.98s/it]\u001b[A\n","loss: 0.34568, smth: 0.43334:  12%|        | 1/8 [00:03<00:20,  2.98s/it]\u001b[A\n","loss: 0.34568, smth: 0.43334:  25%|       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.32810, smth: 0.39826:  25%|       | 2/8 [00:04<00:13,  2.32s/it]\u001b[A\n","loss: 0.32810, smth: 0.39826:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.48845, smth: 0.42081:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.48845, smth: 0.42081:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.54971, smth: 0.44659:  50%|     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.54971, smth: 0.44659:  62%|   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.44420, smth: 0.44619:  62%|   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.44420, smth: 0.44619:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.35903, smth: 0.43374:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.35903, smth: 0.43374:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.59107, smth: 0.45340:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.59107, smth: 0.45340: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.02it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:24 2021 Epoch 43, lr: 0.0046651, train loss: 0.45340, train auc: 0.98025, val loss: 0.24884, val auc: 0.99142\n","score2 (0.990333 --> 0.991417).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:26 2021 Epoch: 44\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.48248, smth: 0.48248:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.48248, smth: 0.48248:  12%|        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.28369, smth: 0.38309:  12%|        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.28369, smth: 0.38309:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.40463, smth: 0.39027:  25%|       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.40463, smth: 0.39027:  38%|      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.38106, smth: 0.38797:  38%|      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.38106, smth: 0.38797:  50%|     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.38660, smth: 0.38769:  50%|     | 4/8 [00:07<00:06,  1.58s/it]\u001b[A\n","loss: 0.38660, smth: 0.38769:  62%|   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.48363, smth: 0.40368:  62%|   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.48363, smth: 0.40368:  75%|  | 6/8 [00:07<00:02,  1.33s/it]\u001b[A\n","loss: 0.50081, smth: 0.41756:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.50081, smth: 0.41756:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.63085, smth: 0.44422:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.63085, smth: 0.44422: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.87it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.56it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:36 2021 Epoch 44, lr: 0.0046429, train loss: 0.44422, train auc: 0.98068, val loss: 0.27114, val auc: 0.98948\n","Thu Jun 10 01:09:36 2021 Epoch: 45\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.30057, smth: 0.30057:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.30057, smth: 0.30057:  12%|        | 1/8 [00:03<00:25,  3.70s/it]\u001b[A\n","loss: 0.42453, smth: 0.36255:  12%|        | 1/8 [00:04<00:25,  3.70s/it]\u001b[A\n","loss: 0.42453, smth: 0.36255:  25%|       | 2/8 [00:04<00:16,  2.83s/it]\u001b[A\n","loss: 0.37875, smth: 0.36795:  25%|       | 2/8 [00:05<00:16,  2.83s/it]\u001b[A\n","loss: 0.37875, smth: 0.36795:  38%|      | 3/8 [00:05<00:12,  2.41s/it]\u001b[A\n","loss: 0.56304, smth: 0.41673:  38%|      | 3/8 [00:06<00:12,  2.41s/it]\u001b[A\n","loss: 0.56304, smth: 0.41673:  50%|     | 4/8 [00:06<00:07,  1.94s/it]\u001b[A\n","loss: 0.34387, smth: 0.40215:  50%|     | 4/8 [00:08<00:07,  1.94s/it]\u001b[A\n","loss: 0.34387, smth: 0.40215:  62%|   | 5/8 [00:08<00:05,  1.75s/it]\u001b[A\n","loss: 0.48432, smth: 0.41585:  62%|   | 5/8 [00:08<00:05,  1.75s/it]\u001b[A\n","loss: 0.48432, smth: 0.41585:  75%|  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.56274, smth: 0.43683:  75%|  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.56274, smth: 0.43683:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.39902, smth: 0.43211:  88%| | 7/8 [00:10<00:01,  1.30s/it]\u001b[A\n","loss: 0.39902, smth: 0.43211: 100%|| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:47 2021 Epoch 45, lr: 0.0046201, train loss: 0.43211, train auc: 0.98563, val loss: 0.31331, val auc: 0.98635\n","Thu Jun 10 01:09:47 2021 Epoch: 46\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41725, smth: 0.41725:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41725, smth: 0.41725:  12%|        | 1/8 [00:02<00:20,  2.99s/it]\u001b[A\n","loss: 0.36070, smth: 0.38897:  12%|        | 1/8 [00:03<00:20,  2.99s/it]\u001b[A\n","loss: 0.36070, smth: 0.38897:  25%|       | 2/8 [00:03<00:14,  2.34s/it]\u001b[A\n","loss: 0.44601, smth: 0.40798:  25%|       | 2/8 [00:04<00:14,  2.34s/it]\u001b[A\n","loss: 0.44601, smth: 0.40798:  38%|      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.40476, smth: 0.40718:  38%|      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.40476, smth: 0.40718:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.38935, smth: 0.40361:  50%|     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.38935, smth: 0.40361:  62%|   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.39384, smth: 0.40198:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.39384, smth: 0.40198:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.30870, smth: 0.38866:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.30870, smth: 0.38866:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.48405, smth: 0.40058:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.48405, smth: 0.40058: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.27it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.91it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:09:56 2021 Epoch 46, lr: 0.0045967, train loss: 0.40058, train auc: 0.98223, val loss: 0.39283, val auc: 0.98227\n","Thu Jun 10 01:09:56 2021 Epoch: 47\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.37610, smth: 0.37610:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.37610, smth: 0.37610:  12%|        | 1/8 [00:02<00:20,  2.86s/it]\u001b[A\n","loss: 0.32197, smth: 0.34903:  12%|        | 1/8 [00:03<00:20,  2.86s/it]\u001b[A\n","loss: 0.32197, smth: 0.34903:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.35649, smth: 0.35152:  25%|       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.35649, smth: 0.35152:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.52965, smth: 0.39605:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.52965, smth: 0.39605:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.50587, smth: 0.41801:  50%|     | 4/8 [00:07<00:06,  1.61s/it]\u001b[A\n","loss: 0.50587, smth: 0.41801:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.38997, smth: 0.41334:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.38997, smth: 0.41334:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.44859, smth: 0.41838:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.44859, smth: 0.41838:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.57761, smth: 0.43828:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.57761, smth: 0.43828: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:06 2021 Epoch 47, lr: 0.0045726, train loss: 0.43828, train auc: 0.97940, val loss: 0.61005, val auc: 0.98397\n","Thu Jun 10 01:10:06 2021 Epoch: 48\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.42707, smth: 0.42707:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.42707, smth: 0.42707:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.31022, smth: 0.36865:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.31022, smth: 0.36865:  25%|       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.45934, smth: 0.39888:  25%|       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.45934, smth: 0.39888:  38%|      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.44391, smth: 0.41014:  38%|      | 3/8 [00:06<00:09,  1.91s/it]\u001b[A\n","loss: 0.44391, smth: 0.41014:  50%|     | 4/8 [00:06<00:07,  1.82s/it]\u001b[A\n","loss: 0.42391, smth: 0.41289:  50%|     | 4/8 [00:07<00:07,  1.82s/it]\u001b[A\n","loss: 0.42391, smth: 0.41289:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.42511, smth: 0.41493:  62%|   | 5/8 [00:08<00:04,  1.50s/it]\u001b[A\n","loss: 0.42511, smth: 0.41493:  75%|  | 6/8 [00:08<00:03,  1.57s/it]\u001b[A\n","loss: 0.47938, smth: 0.42414:  75%|  | 6/8 [00:09<00:03,  1.57s/it]\u001b[A\n","loss: 0.47938, smth: 0.42414:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.69519, smth: 0.45802:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.69519, smth: 0.45802: 100%|| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:17 2021 Epoch 48, lr: 0.0045479, train loss: 0.45802, train auc: 0.98478, val loss: 0.41478, val auc: 0.98371\n","Thu Jun 10 01:10:17 2021 Epoch: 49\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41023, smth: 0.41023:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41023, smth: 0.41023:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.28452, smth: 0.34738:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.28452, smth: 0.34738:  25%|       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.62837, smth: 0.44104:  25%|       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.62837, smth: 0.44104:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.48653, smth: 0.45241:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.48653, smth: 0.45241:  50%|     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.39521, smth: 0.44097:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.39521, smth: 0.44097:  62%|   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.47725, smth: 0.44702:  62%|   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.47725, smth: 0.44702:  75%|  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.43763, smth: 0.44568:  75%|  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.43763, smth: 0.44568:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.44915, smth: 0.44611:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.44915, smth: 0.44611: 100%|| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:27 2021 Epoch 49, lr: 0.0045225, train loss: 0.44611, train auc: 0.98579, val loss: 0.32182, val auc: 0.98827\n","Thu Jun 10 01:10:27 2021 Epoch: 50\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35260, smth: 0.35260:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35260, smth: 0.35260:  12%|        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.46032, smth: 0.40646:  12%|        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.46032, smth: 0.40646:  25%|       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.36312, smth: 0.39202:  25%|       | 2/8 [00:05<00:13,  2.26s/it]\u001b[A\n","loss: 0.36312, smth: 0.39202:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.31467, smth: 0.37268:  38%|      | 3/8 [00:06<00:10,  2.11s/it]\u001b[A\n","loss: 0.31467, smth: 0.37268:  50%|     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.70601, smth: 0.43934:  50%|     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.70601, smth: 0.43934:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.48598, smth: 0.44712:  62%|   | 5/8 [00:08<00:04,  1.57s/it]\u001b[A\n","loss: 0.48598, smth: 0.44712:  75%|  | 6/8 [00:08<00:02,  1.45s/it]\u001b[A\n","loss: 0.32002, smth: 0.42896:  75%|  | 6/8 [00:09<00:02,  1.45s/it]\u001b[A\n","loss: 0.32002, smth: 0.42896:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 1.08231, smth: 0.51063:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 1.08231, smth: 0.51063: 100%|| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:38 2021 Epoch 50, lr: 0.0044966, train loss: 0.51063, train auc: 0.98685, val loss: 0.47187, val auc: 0.98069\n","Thu Jun 10 01:10:38 2021 Epoch: 51\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.41488, smth: 0.41488:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.41488, smth: 0.41488:  12%|        | 1/8 [00:02<00:17,  2.57s/it]\u001b[A\n","loss: 0.43223, smth: 0.42355:  12%|        | 1/8 [00:03<00:17,  2.57s/it]\u001b[A\n","loss: 0.43223, smth: 0.42355:  25%|       | 2/8 [00:03<00:12,  2.02s/it]\u001b[A\n","loss: 0.45458, smth: 0.43390:  25%|       | 2/8 [00:04<00:12,  2.02s/it]\u001b[A\n","loss: 0.45458, smth: 0.43390:  38%|      | 3/8 [00:04<00:09,  1.80s/it]\u001b[A\n","loss: 0.47550, smth: 0.44430:  38%|      | 3/8 [00:05<00:09,  1.80s/it]\u001b[A\n","loss: 0.47550, smth: 0.44430:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.75071, smth: 0.50558:  50%|     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.75071, smth: 0.50558:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.43639, smth: 0.49405:  62%|   | 5/8 [00:08<00:04,  1.55s/it]\u001b[A\n","loss: 0.43639, smth: 0.49405:  75%|  | 6/8 [00:08<00:02,  1.46s/it]\u001b[A\n","loss: 1.16308, smth: 0.58963:  75%|  | 6/8 [00:09<00:02,  1.46s/it]\u001b[A\n","loss: 1.16308, smth: 0.58963:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.35417, smth: 0.56019:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.35417, smth: 0.56019: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.83it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:48 2021 Epoch 51, lr: 0.0044700, train loss: 0.56019, train auc: 0.97159, val loss: 0.46546, val auc: 0.97663\n","Thu Jun 10 01:10:48 2021 Epoch: 52\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 1.08114, smth: 1.08114:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 1.08114, smth: 1.08114:  12%|        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.79922, smth: 0.94018:  12%|        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.79922, smth: 0.94018:  25%|       | 2/8 [00:04<00:14,  2.50s/it]\u001b[A\n","loss: 0.82479, smth: 0.90172:  25%|       | 2/8 [00:05<00:14,  2.50s/it]\u001b[A\n","loss: 0.82479, smth: 0.90172:  38%|      | 3/8 [00:05<00:11,  2.30s/it]\u001b[A\n","loss: 0.70118, smth: 0.85158:  38%|      | 3/8 [00:06<00:11,  2.30s/it]\u001b[A\n","loss: 0.70118, smth: 0.85158:  50%|     | 4/8 [00:06<00:07,  1.85s/it]\u001b[A\n","loss: 0.60968, smth: 0.80320:  50%|     | 4/8 [00:08<00:07,  1.85s/it]\u001b[A\n","loss: 0.60968, smth: 0.80320:  62%|   | 5/8 [00:08<00:05,  1.87s/it]\u001b[A\n","loss: 0.37916, smth: 0.73253:  62%|   | 5/8 [00:09<00:05,  1.87s/it]\u001b[A\n","loss: 0.37916, smth: 0.73253:  75%|  | 6/8 [00:09<00:03,  1.51s/it]\u001b[A\n","loss: 0.59672, smth: 0.71313:  75%|  | 6/8 [00:09<00:03,  1.51s/it]\u001b[A\n","loss: 0.59672, smth: 0.71313:  88%| | 7/8 [00:10<00:01,  1.28s/it]\u001b[A\n","loss: 0.59224, smth: 0.69802:  88%| | 7/8 [00:10<00:01,  1.28s/it]\u001b[A\n","loss: 0.59224, smth: 0.69802: 100%|| 8/8 [00:10<00:00,  1.31s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.88it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:10:59 2021 Epoch 52, lr: 0.0044429, train loss: 0.69802, train auc: 0.97311, val loss: 1.17827, val auc: 0.96225\n","Thu Jun 10 01:10:59 2021 Epoch: 53\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35566, smth: 0.35566:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35566, smth: 0.35566:  12%|        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.56477, smth: 0.46021:  12%|        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.56477, smth: 0.46021:  25%|       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.42288, smth: 0.44777:  25%|       | 2/8 [00:04<00:13,  2.30s/it]\u001b[A\n","loss: 0.42288, smth: 0.44777:  38%|      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.45602, smth: 0.44983:  38%|      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.45602, smth: 0.44983:  50%|     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.43656, smth: 0.44718:  50%|     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.43656, smth: 0.44718:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.54249, smth: 0.46306:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.54249, smth: 0.46306:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.51072, smth: 0.46987:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.51072, smth: 0.46987:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.55465, smth: 0.48047:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.55465, smth: 0.48047: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:09 2021 Epoch 53, lr: 0.0044151, train loss: 0.48047, train auc: 0.97955, val loss: 0.36064, val auc: 0.98842\n","Thu Jun 10 01:11:09 2021 Epoch: 54\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35618, smth: 0.35618:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35618, smth: 0.35618:  12%|        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.35401, smth: 0.35509:  12%|        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.35401, smth: 0.35509:  25%|       | 2/8 [00:03<00:14,  2.37s/it]\u001b[A\n","loss: 0.74772, smth: 0.48597:  25%|       | 2/8 [00:05<00:14,  2.37s/it]\u001b[A\n","loss: 0.74772, smth: 0.48597:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.42305, smth: 0.47024:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.42305, smth: 0.47024:  50%|     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.44703, smth: 0.46560:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.44703, smth: 0.46560:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.44817, smth: 0.46269:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.44817, smth: 0.46269:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.40239, smth: 0.45408:  75%|  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.40239, smth: 0.45408:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.33413, smth: 0.43908:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.33413, smth: 0.43908: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:19 2021 Epoch 54, lr: 0.0043868, train loss: 0.43908, train auc: 0.97883, val loss: 0.30868, val auc: 0.98852\n","Thu Jun 10 01:11:19 2021 Epoch: 55\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.45369, smth: 0.45369:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.45369, smth: 0.45369:  12%|        | 1/8 [00:02<00:20,  2.87s/it]\u001b[A\n","loss: 0.34131, smth: 0.39750:  12%|        | 1/8 [00:03<00:20,  2.87s/it]\u001b[A\n","loss: 0.34131, smth: 0.39750:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.29795, smth: 0.36432:  25%|       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.29795, smth: 0.36432:  38%|      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.35227, smth: 0.36131:  38%|      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.35227, smth: 0.36131:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.27857, smth: 0.34476:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.27857, smth: 0.34476:  62%|   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.49207, smth: 0.36931:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.49207, smth: 0.36931:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.54158, smth: 0.39392:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.54158, smth: 0.39392:  88%| | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.57412, smth: 0.41645:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.57412, smth: 0.41645: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.23it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.90it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:29 2021 Epoch 55, lr: 0.0043579, train loss: 0.41645, train auc: 0.97957, val loss: 0.63003, val auc: 0.98146\n","Thu Jun 10 01:11:29 2021 Epoch: 56\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.34370, smth: 0.34370:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.34370, smth: 0.34370:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.38052, smth: 0.36211:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.38052, smth: 0.36211:  25%|       | 2/8 [00:03<00:14,  2.48s/it]\u001b[A\n","loss: 0.34465, smth: 0.35629:  25%|       | 2/8 [00:05<00:14,  2.48s/it]\u001b[A\n","loss: 0.34465, smth: 0.35629:  38%|      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.38465, smth: 0.36338:  38%|      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.38465, smth: 0.36338:  50%|     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.38750, smth: 0.36820:  50%|     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.38750, smth: 0.36820:  62%|   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.42048, smth: 0.37692:  62%|   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.42048, smth: 0.37692:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.38534, smth: 0.37812:  75%|  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.38534, smth: 0.37812:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.41672, smth: 0.38294:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.41672, smth: 0.38294: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.86it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:39 2021 Epoch 56, lr: 0.0043284, train loss: 0.38294, train auc: 0.98471, val loss: 0.77396, val auc: 0.98202\n","Thu Jun 10 01:11:39 2021 Epoch: 57\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.36262, smth: 0.36262:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.36262, smth: 0.36262:  12%|        | 1/8 [00:02<00:20,  2.97s/it]\u001b[A\n","loss: 0.28325, smth: 0.32293:  12%|        | 1/8 [00:03<00:20,  2.97s/it]\u001b[A\n","loss: 0.28325, smth: 0.32293:  25%|       | 2/8 [00:03<00:13,  2.31s/it]\u001b[A\n","loss: 0.39231, smth: 0.34606:  25%|       | 2/8 [00:05<00:13,  2.31s/it]\u001b[A\n","loss: 0.39231, smth: 0.34606:  38%|      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.43354, smth: 0.36793:  38%|      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.43354, smth: 0.36793:  50%|     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.38575, smth: 0.37149:  50%|     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.38575, smth: 0.37149:  62%|   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.41630, smth: 0.37896:  62%|   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.41630, smth: 0.37896:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.37985, smth: 0.37909:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.37985, smth: 0.37909:  88%| | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.47972, smth: 0.39167:  88%| | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.47972, smth: 0.39167: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.28it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.88it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:49 2021 Epoch 57, lr: 0.0042983, train loss: 0.39167, train auc: 0.98404, val loss: 0.54262, val auc: 0.98141\n","Thu Jun 10 01:11:49 2021 Epoch: 58\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32854, smth: 0.32854:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32854, smth: 0.32854:  12%|        | 1/8 [00:02<00:20,  2.93s/it]\u001b[A\n","loss: 0.39828, smth: 0.36341:  12%|        | 1/8 [00:03<00:20,  2.93s/it]\u001b[A\n","loss: 0.39828, smth: 0.36341:  25%|       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.27161, smth: 0.33281:  25%|       | 2/8 [00:04<00:13,  2.30s/it]\u001b[A\n","loss: 0.27161, smth: 0.33281:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.37382, smth: 0.34306:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.37382, smth: 0.34306:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.40443, smth: 0.35534:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.40443, smth: 0.35534:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.62073, smth: 0.39957:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.62073, smth: 0.39957:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.34839, smth: 0.39226:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.34839, smth: 0.39226:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.38244, smth: 0.39103:  88%| | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 0.38244, smth: 0.39103: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.87it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:11:59 2021 Epoch 58, lr: 0.0042678, train loss: 0.39103, train auc: 0.98464, val loss: 0.42086, val auc: 0.98311\n","Thu Jun 10 01:11:59 2021 Epoch: 59\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35786, smth: 0.35786:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35786, smth: 0.35786:  12%|        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.38031, smth: 0.36909:  12%|        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.38031, smth: 0.36909:  25%|       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.35052, smth: 0.36290:  25%|       | 2/8 [00:04<00:12,  2.15s/it]\u001b[A\n","loss: 0.35052, smth: 0.36290:  38%|      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.32333, smth: 0.35301:  38%|      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.32333, smth: 0.35301:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.49073, smth: 0.38055:  50%|     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.49073, smth: 0.38055:  62%|   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.45310, smth: 0.39264:  62%|   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.45310, smth: 0.39264:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.33517, smth: 0.38443:  75%|  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.33517, smth: 0.38443:  88%| | 7/8 [00:09<00:01,  1.38s/it]\u001b[A\n","loss: 0.40925, smth: 0.38753:  88%| | 7/8 [00:09<00:01,  1.38s/it]\u001b[A\n","loss: 0.40925, smth: 0.38753: 100%|| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:10 2021 Epoch 59, lr: 0.0042366, train loss: 0.38753, train auc: 0.98353, val loss: 0.48069, val auc: 0.98381\n","Thu Jun 10 01:12:10 2021 Epoch: 60\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.33926, smth: 0.33926:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.33926, smth: 0.33926:  12%|        | 1/8 [00:02<00:20,  2.87s/it]\u001b[A\n","loss: 0.37131, smth: 0.35528:  12%|        | 1/8 [00:03<00:20,  2.87s/it]\u001b[A\n","loss: 0.37131, smth: 0.35528:  25%|       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.39996, smth: 0.37017:  25%|       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.39996, smth: 0.37017:  38%|      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.43791, smth: 0.38711:  38%|      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.43791, smth: 0.38711:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.33771, smth: 0.37723:  50%|     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.33771, smth: 0.37723:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.30336, smth: 0.36492:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.30336, smth: 0.36492:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.42329, smth: 0.37326:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.42329, smth: 0.37326:  88%| | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.63144, smth: 0.40553:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.63144, smth: 0.40553: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:20 2021 Epoch 60, lr: 0.0042050, train loss: 0.40553, train auc: 0.97838, val loss: 0.32776, val auc: 0.98821\n","Thu Jun 10 01:12:20 2021 Epoch: 61\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.58762, smth: 0.58762:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.58762, smth: 0.58762:  12%|        | 1/8 [00:02<00:20,  2.94s/it]\u001b[A\n","loss: 0.48206, smth: 0.53484:  12%|        | 1/8 [00:03<00:20,  2.94s/it]\u001b[A\n","loss: 0.48206, smth: 0.53484:  25%|       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.50358, smth: 0.52442:  25%|       | 2/8 [00:05<00:13,  2.30s/it]\u001b[A\n","loss: 0.50358, smth: 0.52442:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.38509, smth: 0.48959:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.38509, smth: 0.48959:  50%|     | 4/8 [00:05<00:06,  1.66s/it]\u001b[A\n","loss: 0.37829, smth: 0.46733:  50%|     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.37829, smth: 0.46733:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.61461, smth: 0.49188:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.61461, smth: 0.49188:  75%|  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.39839, smth: 0.47852:  75%|  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.39839, smth: 0.47852:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.44957, smth: 0.47490:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.44957, smth: 0.47490: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:30 2021 Epoch 61, lr: 0.0041728, train loss: 0.47490, train auc: 0.98307, val loss: 0.36773, val auc: 0.98679\n","Thu Jun 10 01:12:30 2021 Epoch: 62\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31787, smth: 0.31787:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31787, smth: 0.31787:  12%|        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.55577, smth: 0.43682:  12%|        | 1/8 [00:03<00:21,  3.07s/it]\u001b[A\n","loss: 0.55577, smth: 0.43682:  25%|       | 2/8 [00:03<00:14,  2.38s/it]\u001b[A\n","loss: 0.40543, smth: 0.42635:  25%|       | 2/8 [00:05<00:14,  2.38s/it]\u001b[A\n","loss: 0.40543, smth: 0.42635:  38%|      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.39612, smth: 0.41879:  38%|      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.39612, smth: 0.41879:  50%|     | 4/8 [00:06<00:06,  1.72s/it]\u001b[A\n","loss: 0.45569, smth: 0.42617:  50%|     | 4/8 [00:07<00:06,  1.72s/it]\u001b[A\n","loss: 0.45569, smth: 0.42617:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.29656, smth: 0.40457:  62%|   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.29656, smth: 0.40457:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.44004, smth: 0.40964:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.44004, smth: 0.40964:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.43396, smth: 0.41268:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.43396, smth: 0.41268: 100%|| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:40 2021 Epoch 62, lr: 0.0041401, train loss: 0.41268, train auc: 0.98585, val loss: 0.25020, val auc: 0.99023\n","Thu Jun 10 01:12:40 2021 Epoch: 63\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.38927, smth: 0.38927:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.38927, smth: 0.38927:  12%|        | 1/8 [00:03<00:21,  3.10s/it]\u001b[A\n","loss: 0.22422, smth: 0.30675:  12%|        | 1/8 [00:03<00:21,  3.10s/it]\u001b[A\n","loss: 0.22422, smth: 0.30675:  25%|       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.43279, smth: 0.34876:  25%|       | 2/8 [00:05<00:14,  2.40s/it]\u001b[A\n","loss: 0.43279, smth: 0.34876:  38%|      | 3/8 [00:05<00:10,  2.16s/it]\u001b[A\n","loss: 0.31866, smth: 0.34124:  38%|      | 3/8 [00:06<00:10,  2.16s/it]\u001b[A\n","loss: 0.31866, smth: 0.34124:  50%|     | 4/8 [00:06<00:06,  1.74s/it]\u001b[A\n","loss: 0.50533, smth: 0.37406:  50%|     | 4/8 [00:07<00:06,  1.74s/it]\u001b[A\n","loss: 0.50533, smth: 0.37406:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.37552, smth: 0.37430:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.37552, smth: 0.37430:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.55740, smth: 0.40046:  75%|  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.55740, smth: 0.40046:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.27109, smth: 0.38429:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.27109, smth: 0.38429: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.82it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:12:50 2021 Epoch 63, lr: 0.0041070, train loss: 0.38429, train auc: 0.98857, val loss: 0.31024, val auc: 0.98815\n","Thu Jun 10 01:12:50 2021 Epoch: 64\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29140, smth: 0.29140:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29140, smth: 0.29140:  12%|        | 1/8 [00:02<00:20,  2.93s/it]\u001b[A\n","loss: 0.29271, smth: 0.29206:  12%|        | 1/8 [00:03<00:20,  2.93s/it]\u001b[A\n","loss: 0.29271, smth: 0.29206:  25%|       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.54786, smth: 0.37732:  25%|       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.54786, smth: 0.37732:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.43329, smth: 0.39132:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.43329, smth: 0.39132:  50%|     | 4/8 [00:05<00:06,  1.66s/it]\u001b[A\n","loss: 0.40352, smth: 0.39376:  50%|     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.40352, smth: 0.39376:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.34358, smth: 0.38539:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.34358, smth: 0.38539:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.41097, smth: 0.38905:  75%|  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.41097, smth: 0.38905:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.60652, smth: 0.41623:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.60652, smth: 0.41623: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.89it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:00 2021 Epoch 64, lr: 0.0040733, train loss: 0.41623, train auc: 0.98596, val loss: 0.26519, val auc: 0.99027\n","Thu Jun 10 01:13:00 2021 Epoch: 65\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.38637, smth: 0.38637:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.38637, smth: 0.38637:  12%|        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.51473, smth: 0.45055:  12%|        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.51473, smth: 0.45055:  25%|       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.23113, smth: 0.37741:  25%|       | 2/8 [00:04<00:12,  2.07s/it]\u001b[A\n","loss: 0.23113, smth: 0.37741:  38%|      | 3/8 [00:04<00:08,  1.80s/it]\u001b[A\n","loss: 0.40215, smth: 0.38360:  38%|      | 3/8 [00:05<00:08,  1.80s/it]\u001b[A\n","loss: 0.40215, smth: 0.38360:  50%|     | 4/8 [00:05<00:06,  1.65s/it]\u001b[A\n","loss: 0.27604, smth: 0.36209:  50%|     | 4/8 [00:06<00:06,  1.65s/it]\u001b[A\n","loss: 0.27604, smth: 0.36209:  62%|   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.40562, smth: 0.36934:  62%|   | 5/8 [00:08<00:04,  1.39s/it]\u001b[A\n","loss: 0.40562, smth: 0.36934:  75%|  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.38972, smth: 0.37225:  75%|  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.38972, smth: 0.37225:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.25996, smth: 0.35822:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.25996, smth: 0.35822: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.85it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:10 2021 Epoch 65, lr: 0.0040392, train loss: 0.35822, train auc: 0.98708, val loss: 0.33476, val auc: 0.98923\n","Thu Jun 10 01:13:10 2021 Epoch: 66\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.42147, smth: 0.42147:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.42147, smth: 0.42147:  12%|        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.30152, smth: 0.36150:  12%|        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.30152, smth: 0.36150:  25%|       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.20899, smth: 0.31066:  25%|       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.20899, smth: 0.31066:  38%|      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.20342, smth: 0.28385:  38%|      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.20342, smth: 0.28385:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.28310, smth: 0.28370:  50%|     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.28310, smth: 0.28370:  62%|   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.53975, smth: 0.32637:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.53975, smth: 0.32637:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.51716, smth: 0.35363:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.51716, smth: 0.35363:  88%| | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.35432, smth: 0.35372:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.35432, smth: 0.35372: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:20 2021 Epoch 66, lr: 0.0040045, train loss: 0.35372, train auc: 0.98562, val loss: 0.33359, val auc: 0.98802\n","Thu Jun 10 01:13:20 2021 Epoch: 67\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32693, smth: 0.32693:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32693, smth: 0.32693:  12%|        | 1/8 [00:02<00:19,  2.83s/it]\u001b[A\n","loss: 0.45221, smth: 0.38957:  12%|        | 1/8 [00:03<00:19,  2.83s/it]\u001b[A\n","loss: 0.45221, smth: 0.38957:  25%|       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.31738, smth: 0.36551:  25%|       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.31738, smth: 0.36551:  38%|      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.39472, smth: 0.37281:  38%|      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.39472, smth: 0.37281:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.27039, smth: 0.35233:  50%|     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.27039, smth: 0.35233:  62%|   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.32161, smth: 0.34721:  62%|   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.32161, smth: 0.34721:  75%|  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.32059, smth: 0.34340:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.32059, smth: 0.34340:  88%| | 7/8 [00:08<00:01,  1.24s/it]\u001b[A\n","loss: 0.39724, smth: 0.35013:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.39724, smth: 0.35013: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.19it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.90it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:30 2021 Epoch 67, lr: 0.0039695, train loss: 0.35013, train auc: 0.98743, val loss: 0.34121, val auc: 0.98830\n","Thu Jun 10 01:13:30 2021 Epoch: 68\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35040, smth: 0.35040:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35040, smth: 0.35040:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.23769, smth: 0.29404:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.23769, smth: 0.29404:  25%|       | 2/8 [00:03<00:13,  2.33s/it]\u001b[A\n","loss: 0.38234, smth: 0.32348:  25%|       | 2/8 [00:04<00:13,  2.33s/it]\u001b[A\n","loss: 0.38234, smth: 0.32348:  38%|      | 3/8 [00:04<00:09,  2.00s/it]\u001b[A\n","loss: 0.29261, smth: 0.31576:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.29261, smth: 0.31576:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.23325, smth: 0.29926:  50%|     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.23325, smth: 0.29926:  62%|   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.38526, smth: 0.31359:  62%|   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.38526, smth: 0.31359:  75%|  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.29838, smth: 0.31142:  75%|  | 6/8 [00:09<00:02,  1.41s/it]\u001b[A\n","loss: 0.29838, smth: 0.31142:  88%| | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.24716, smth: 0.30339:  88%| | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.24716, smth: 0.30339: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:41 2021 Epoch 68, lr: 0.0039339, train loss: 0.30339, train auc: 0.99137, val loss: 0.41275, val auc: 0.98483\n","Thu Jun 10 01:13:41 2021 Epoch: 69\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.38381, smth: 0.38381:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.38381, smth: 0.38381:  12%|        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.31847, smth: 0.35114:  12%|        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.31847, smth: 0.35114:  25%|       | 2/8 [00:03<00:14,  2.47s/it]\u001b[A\n","loss: 0.35914, smth: 0.35381:  25%|       | 2/8 [00:05<00:14,  2.47s/it]\u001b[A\n","loss: 0.35914, smth: 0.35381:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.28823, smth: 0.33741:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.28823, smth: 0.33741:  50%|     | 4/8 [00:05<00:06,  1.71s/it]\u001b[A\n","loss: 0.31162, smth: 0.33225:  50%|     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.31162, smth: 0.33225:  62%|   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.31219, smth: 0.32891:  62%|   | 5/8 [00:08<00:04,  1.61s/it]\u001b[A\n","loss: 0.31219, smth: 0.32891:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.25189, smth: 0.31791:  75%|  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.25189, smth: 0.31791:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.24397, smth: 0.30867:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.24397, smth: 0.30867: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:13:51 2021 Epoch 69, lr: 0.0038980, train loss: 0.30867, train auc: 0.98983, val loss: 0.36368, val auc: 0.98815\n","Thu Jun 10 01:13:51 2021 Epoch: 70\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26413, smth: 0.26413:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26413, smth: 0.26413:  12%|        | 1/8 [00:03<00:21,  3.05s/it]\u001b[A\n","loss: 0.35278, smth: 0.30846:  12%|        | 1/8 [00:03<00:21,  3.05s/it]\u001b[A\n","loss: 0.35278, smth: 0.30846:  25%|       | 2/8 [00:03<00:14,  2.36s/it]\u001b[A\n","loss: 0.28114, smth: 0.29935:  25%|       | 2/8 [00:05<00:14,  2.36s/it]\u001b[A\n","loss: 0.28114, smth: 0.29935:  38%|      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 0.39077, smth: 0.32220:  38%|      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 0.39077, smth: 0.32220:  50%|     | 4/8 [00:06<00:06,  1.75s/it]\u001b[A\n","loss: 0.31098, smth: 0.31996:  50%|     | 4/8 [00:07<00:06,  1.75s/it]\u001b[A\n","loss: 0.31098, smth: 0.31996:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.30100, smth: 0.31680:  62%|   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.30100, smth: 0.31680:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.25725, smth: 0.30829:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.25725, smth: 0.30829:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.26630, smth: 0.30304:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.26630, smth: 0.30304: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:01 2021 Epoch 70, lr: 0.0038616, train loss: 0.30304, train auc: 0.99229, val loss: 0.31160, val auc: 0.98948\n","Thu Jun 10 01:14:01 2021 Epoch: 71\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.37746, smth: 0.37746:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.37746, smth: 0.37746:  12%|        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.27732, smth: 0.32739:  12%|        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.27732, smth: 0.32739:  25%|       | 2/8 [00:03<00:13,  2.28s/it]\u001b[A\n","loss: 0.24409, smth: 0.29963:  25%|       | 2/8 [00:05<00:13,  2.28s/it]\u001b[A\n","loss: 0.24409, smth: 0.29963:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.30925, smth: 0.30203:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.30925, smth: 0.30203:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.49795, smth: 0.34121:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.49795, smth: 0.34121:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.29435, smth: 0.33340:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.29435, smth: 0.33340:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37816, smth: 0.33980:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37816, smth: 0.33980:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.29144, smth: 0.33375:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.29144, smth: 0.33375: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.93it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:11 2021 Epoch 71, lr: 0.0038248, train loss: 0.33375, train auc: 0.99154, val loss: 0.33253, val auc: 0.98935\n","Thu Jun 10 01:14:11 2021 Epoch: 72\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.34061, smth: 0.34061:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.34061, smth: 0.34061:  12%|        | 1/8 [00:02<00:15,  2.25s/it]\u001b[A\n","loss: 0.34800, smth: 0.34430:  12%|        | 1/8 [00:02<00:15,  2.25s/it]\u001b[A\n","loss: 0.34800, smth: 0.34430:  25%|       | 2/8 [00:02<00:10,  1.80s/it]\u001b[A\n","loss: 0.33895, smth: 0.34252:  25%|       | 2/8 [00:04<00:10,  1.80s/it]\u001b[A\n","loss: 0.33895, smth: 0.34252:  38%|      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.35556, smth: 0.34578:  38%|      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.35556, smth: 0.34578:  50%|     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.38217, smth: 0.35306:  50%|     | 4/8 [00:06<00:05,  1.43s/it]\u001b[A\n","loss: 0.38217, smth: 0.35306:  62%|   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.22246, smth: 0.33129:  62%|   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.22246, smth: 0.33129:  75%|  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.22781, smth: 0.31651:  75%|  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.22781, smth: 0.31651:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.57944, smth: 0.34937:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.57944, smth: 0.34937: 100%|| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:21 2021 Epoch 72, lr: 0.0037876, train loss: 0.34937, train auc: 0.98839, val loss: 0.34760, val auc: 0.98815\n","Thu Jun 10 01:14:21 2021 Epoch: 73\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28011, smth: 0.28011:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.28011, smth: 0.28011:  12%|        | 1/8 [00:02<00:17,  2.47s/it]\u001b[A\n","loss: 0.30987, smth: 0.29499:  12%|        | 1/8 [00:03<00:17,  2.47s/it]\u001b[A\n","loss: 0.30987, smth: 0.29499:  25%|       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.23332, smth: 0.27443:  25%|       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.23332, smth: 0.27443:  38%|      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.30956, smth: 0.28321:  38%|      | 3/8 [00:06<00:09,  1.82s/it]\u001b[A\n","loss: 0.30956, smth: 0.28321:  50%|     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.32713, smth: 0.29200:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.32713, smth: 0.29200:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.30099, smth: 0.29350:  62%|   | 5/8 [00:08<00:04,  1.47s/it]\u001b[A\n","loss: 0.30099, smth: 0.29350:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.28628, smth: 0.29247:  75%|  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.28628, smth: 0.29247:  88%| | 7/8 [00:09<00:01,  1.38s/it]\u001b[A\n","loss: 0.35742, smth: 0.30059:  88%| | 7/8 [00:10<00:01,  1.38s/it]\u001b[A\n","loss: 0.35742, smth: 0.30059: 100%|| 8/8 [00:10<00:00,  1.27s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.61it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:32 2021 Epoch 73, lr: 0.0037500, train loss: 0.30059, train auc: 0.98977, val loss: 0.46137, val auc: 0.98320\n","Thu Jun 10 01:14:32 2021 Epoch: 74\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.27149, smth: 0.27149:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.27149, smth: 0.27149:  12%|        | 1/8 [00:02<00:19,  2.79s/it]\u001b[A\n","loss: 0.25071, smth: 0.26110:  12%|        | 1/8 [00:03<00:19,  2.79s/it]\u001b[A\n","loss: 0.25071, smth: 0.26110:  25%|       | 2/8 [00:03<00:12,  2.16s/it]\u001b[A\n","loss: 0.26708, smth: 0.26309:  25%|       | 2/8 [00:04<00:12,  2.16s/it]\u001b[A\n","loss: 0.26708, smth: 0.26309:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.28246, smth: 0.26793:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.28246, smth: 0.26793:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.36044, smth: 0.28643:  50%|     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.36044, smth: 0.28643:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.20114, smth: 0.27222:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.20114, smth: 0.27222:  75%|  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.21448, smth: 0.26397:  75%|  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.21448, smth: 0.26397:  88%| | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.50536, smth: 0.29414:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.50536, smth: 0.29414: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:42 2021 Epoch 74, lr: 0.0037120, train loss: 0.29414, train auc: 0.99306, val loss: 0.42634, val auc: 0.98384\n","Thu Jun 10 01:14:42 2021 Epoch: 75\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.43287, smth: 0.43287:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.43287, smth: 0.43287:  12%|        | 1/8 [00:02<00:18,  2.69s/it]\u001b[A\n","loss: 0.27501, smth: 0.35394:  12%|        | 1/8 [00:03<00:18,  2.69s/it]\u001b[A\n","loss: 0.27501, smth: 0.35394:  25%|       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.34316, smth: 0.35035:  25%|       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.34316, smth: 0.35035:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.33205, smth: 0.34578:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.33205, smth: 0.34578:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.28351, smth: 0.33332:  50%|     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.28351, smth: 0.33332:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.38134, smth: 0.34133:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.38134, smth: 0.34133:  75%|  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.26063, smth: 0.32980:  75%|  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.26063, smth: 0.32980:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.44396, smth: 0.34407:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.44396, smth: 0.34407: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:14:52 2021 Epoch 75, lr: 0.0036737, train loss: 0.34407, train auc: 0.99131, val loss: 0.44194, val auc: 0.98610\n","Thu Jun 10 01:14:52 2021 Epoch: 76\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29371, smth: 0.29371:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29371, smth: 0.29371:  12%|        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.40221, smth: 0.34796:  12%|        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.40221, smth: 0.34796:  25%|       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.26166, smth: 0.31919:  25%|       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.26166, smth: 0.31919:  38%|      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.26966, smth: 0.30681:  38%|      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.26966, smth: 0.30681:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.29332, smth: 0.30411:  50%|     | 4/8 [00:07<00:06,  1.59s/it]\u001b[A\n","loss: 0.29332, smth: 0.30411:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.44793, smth: 0.32808:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.44793, smth: 0.32808:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.29597, smth: 0.32350:  75%|  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.29597, smth: 0.32350:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.37687, smth: 0.33017:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.37687, smth: 0.33017: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:02 2021 Epoch 76, lr: 0.0036350, train loss: 0.33017, train auc: 0.98872, val loss: 0.52347, val auc: 0.98264\n","Thu Jun 10 01:15:02 2021 Epoch: 77\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.53455, smth: 0.53455:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.53455, smth: 0.53455:  12%|        | 1/8 [00:02<00:18,  2.58s/it]\u001b[A\n","loss: 0.43580, smth: 0.48518:  12%|        | 1/8 [00:03<00:18,  2.58s/it]\u001b[A\n","loss: 0.43580, smth: 0.48518:  25%|       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.22926, smth: 0.39987:  25%|       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.22926, smth: 0.39987:  38%|      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.31479, smth: 0.37860:  38%|      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.31479, smth: 0.37860:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.33688, smth: 0.37026:  50%|     | 4/8 [00:07<00:06,  1.53s/it]\u001b[A\n","loss: 0.33688, smth: 0.37026:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.42202, smth: 0.37888:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.42202, smth: 0.37888:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.35410, smth: 0.37534:  75%|  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.35410, smth: 0.37534:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.30371, smth: 0.36639:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.30371, smth: 0.36639: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:12 2021 Epoch 77, lr: 0.0035959, train loss: 0.36639, train auc: 0.98808, val loss: 0.61472, val auc: 0.98158\n","Thu Jun 10 01:15:12 2021 Epoch: 78\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29130, smth: 0.29130:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29130, smth: 0.29130:  12%|        | 1/8 [00:02<00:18,  2.66s/it]\u001b[A\n","loss: 0.48550, smth: 0.38840:  12%|        | 1/8 [00:03<00:18,  2.66s/it]\u001b[A\n","loss: 0.48550, smth: 0.38840:  25%|       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.32698, smth: 0.36793:  25%|       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.32698, smth: 0.36793:  38%|      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.41750, smth: 0.38032:  38%|      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.41750, smth: 0.38032:  50%|     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.22379, smth: 0.34901:  50%|     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A\n","loss: 0.22379, smth: 0.34901:  62%|   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.35587, smth: 0.35016:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.35587, smth: 0.35016:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.27561, smth: 0.33951:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.27561, smth: 0.33951:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.42737, smth: 0.35049:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.42737, smth: 0.35049: 100%|| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:22 2021 Epoch 78, lr: 0.0035565, train loss: 0.35049, train auc: 0.99002, val loss: 0.31348, val auc: 0.98881\n","Thu Jun 10 01:15:22 2021 Epoch: 79\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31862, smth: 0.31862:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31862, smth: 0.31862:  12%|        | 1/8 [00:02<00:19,  2.80s/it]\u001b[A\n","loss: 0.28818, smth: 0.30340:  12%|        | 1/8 [00:03<00:19,  2.80s/it]\u001b[A\n","loss: 0.28818, smth: 0.30340:  25%|       | 2/8 [00:03<00:13,  2.17s/it]\u001b[A\n","loss: 0.22643, smth: 0.27774:  25%|       | 2/8 [00:04<00:13,  2.17s/it]\u001b[A\n","loss: 0.22643, smth: 0.27774:  38%|      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.41944, smth: 0.31317:  38%|      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.41944, smth: 0.31317:  50%|     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.49187, smth: 0.34891:  50%|     | 4/8 [00:07<00:06,  1.57s/it]\u001b[A\n","loss: 0.49187, smth: 0.34891:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.36792, smth: 0.35208:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.36792, smth: 0.35208:  75%|  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.35941, smth: 0.35312:  75%|  | 6/8 [00:09<00:02,  1.30s/it]\u001b[A\n","loss: 0.35941, smth: 0.35312:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.53464, smth: 0.37581:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.53464, smth: 0.37581: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:32 2021 Epoch 79, lr: 0.0035168, train loss: 0.37581, train auc: 0.99080, val loss: 0.32500, val auc: 0.98848\n","Thu Jun 10 01:15:32 2021 Epoch: 80\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.33413, smth: 0.33413:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.33413, smth: 0.33413:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.35492, smth: 0.34453:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.35492, smth: 0.34453:  25%|       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.35412, smth: 0.34772:  25%|       | 2/8 [00:05<00:13,  2.22s/it]\u001b[A\n","loss: 0.35412, smth: 0.34772:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.27602, smth: 0.32980:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.27602, smth: 0.32980:  50%|     | 4/8 [00:06<00:06,  1.66s/it]\u001b[A\n","loss: 0.26942, smth: 0.31772:  50%|     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.26942, smth: 0.31772:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.30309, smth: 0.31528:  62%|   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.30309, smth: 0.31528:  75%|  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.24586, smth: 0.30537:  75%|  | 6/8 [00:09<00:02,  1.31s/it]\u001b[A\n","loss: 0.24586, smth: 0.30537:  88%| | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.41226, smth: 0.31873:  88%| | 7/8 [00:09<00:01,  1.32s/it]\u001b[A\n","loss: 0.41226, smth: 0.31873: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:43 2021 Epoch 80, lr: 0.0034768, train loss: 0.31873, train auc: 0.98943, val loss: 0.44224, val auc: 0.98552\n","Thu Jun 10 01:15:43 2021 Epoch: 81\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.37552, smth: 0.37552:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.37552, smth: 0.37552:  12%|        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.28602, smth: 0.33077:  12%|        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.28602, smth: 0.33077:  25%|       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.32894, smth: 0.33016:  25%|       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.32894, smth: 0.33016:  38%|      | 3/8 [00:05<00:10,  2.16s/it]\u001b[A\n","loss: 0.28884, smth: 0.31983:  38%|      | 3/8 [00:06<00:10,  2.16s/it]\u001b[A\n","loss: 0.28884, smth: 0.31983:  50%|     | 4/8 [00:06<00:07,  1.75s/it]\u001b[A\n","loss: 0.36707, smth: 0.32928:  50%|     | 4/8 [00:07<00:07,  1.75s/it]\u001b[A\n","loss: 0.36707, smth: 0.32928:  62%|   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.23285, smth: 0.31321:  62%|   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.23285, smth: 0.31321:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.20439, smth: 0.29766:  75%|  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.20439, smth: 0.29766:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.17448, smth: 0.28226:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.17448, smth: 0.28226: 100%|| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:15:53 2021 Epoch 81, lr: 0.0034365, train loss: 0.28226, train auc: 0.98964, val loss: 0.35889, val auc: 0.98781\n","Thu Jun 10 01:15:53 2021 Epoch: 82\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29103, smth: 0.29103:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.29103, smth: 0.29103:  12%|        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.36249, smth: 0.32676:  12%|        | 1/8 [00:03<00:22,  3.22s/it]\u001b[A\n","loss: 0.36249, smth: 0.32676:  25%|       | 2/8 [00:04<00:15,  2.50s/it]\u001b[A\n","loss: 0.30334, smth: 0.31895:  25%|       | 2/8 [00:05<00:15,  2.50s/it]\u001b[A\n","loss: 0.30334, smth: 0.31895:  38%|      | 3/8 [00:05<00:10,  2.16s/it]\u001b[A\n","loss: 0.25322, smth: 0.30252:  38%|      | 3/8 [00:06<00:10,  2.16s/it]\u001b[A\n","loss: 0.25322, smth: 0.30252:  50%|     | 4/8 [00:06<00:06,  1.74s/it]\u001b[A\n","loss: 0.24961, smth: 0.29194:  50%|     | 4/8 [00:07<00:06,  1.74s/it]\u001b[A\n","loss: 0.24961, smth: 0.29194:  62%|   | 5/8 [00:07<00:05,  1.68s/it]\u001b[A\n","loss: 0.33063, smth: 0.29839:  62%|   | 5/8 [00:08<00:05,  1.68s/it]\u001b[A\n","loss: 0.33063, smth: 0.29839:  75%|  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.28402, smth: 0.29633:  75%|  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.28402, smth: 0.29633:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.44466, smth: 0.31488:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.44466, smth: 0.31488: 100%|| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.21it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.88it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:03 2021 Epoch 82, lr: 0.0033959, train loss: 0.31488, train auc: 0.98898, val loss: 0.36431, val auc: 0.98727\n","Thu Jun 10 01:16:03 2021 Epoch: 83\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21920, smth: 0.21920:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21920, smth: 0.21920:  12%|        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.39845, smth: 0.30882:  12%|        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.39845, smth: 0.30882:  25%|       | 2/8 [00:03<00:13,  2.17s/it]\u001b[A\n","loss: 0.30090, smth: 0.30618:  25%|       | 2/8 [00:04<00:13,  2.17s/it]\u001b[A\n","loss: 0.30090, smth: 0.30618:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.25973, smth: 0.29457:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.25973, smth: 0.29457:  50%|     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.25628, smth: 0.28691:  50%|     | 4/8 [00:07<00:06,  1.51s/it]\u001b[A\n","loss: 0.25628, smth: 0.28691:  62%|   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.29579, smth: 0.28839:  62%|   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.29579, smth: 0.28839:  75%|  | 6/8 [00:08<00:02,  1.40s/it]\u001b[A\n","loss: 0.30673, smth: 0.29101:  75%|  | 6/8 [00:09<00:02,  1.40s/it]\u001b[A\n","loss: 0.30673, smth: 0.29101:  88%| | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.27585, smth: 0.28912:  88%| | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.27585, smth: 0.28912: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.83it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:14 2021 Epoch 83, lr: 0.0033551, train loss: 0.28912, train auc: 0.98953, val loss: 0.37633, val auc: 0.98600\n","Thu Jun 10 01:16:14 2021 Epoch: 84\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32016, smth: 0.32016:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.32016, smth: 0.32016:  12%|        | 1/8 [00:03<00:23,  3.38s/it]\u001b[A\n","loss: 0.18367, smth: 0.25191:  12%|        | 1/8 [00:04<00:23,  3.38s/it]\u001b[A\n","loss: 0.18367, smth: 0.25191:  25%|       | 2/8 [00:04<00:15,  2.61s/it]\u001b[A\n","loss: 0.22327, smth: 0.24237:  25%|       | 2/8 [00:05<00:15,  2.61s/it]\u001b[A\n","loss: 0.22327, smth: 0.24237:  38%|      | 3/8 [00:05<00:10,  2.19s/it]\u001b[A\n","loss: 0.27176, smth: 0.24972:  38%|      | 3/8 [00:06<00:10,  2.19s/it]\u001b[A\n","loss: 0.27176, smth: 0.24972:  50%|     | 4/8 [00:06<00:07,  1.76s/it]\u001b[A\n","loss: 0.25313, smth: 0.25040:  50%|     | 4/8 [00:07<00:07,  1.76s/it]\u001b[A\n","loss: 0.25313, smth: 0.25040:  62%|   | 5/8 [00:07<00:05,  1.73s/it]\u001b[A\n","loss: 0.32461, smth: 0.26277:  62%|   | 5/8 [00:08<00:05,  1.73s/it]\u001b[A\n","loss: 0.32461, smth: 0.26277:  75%|  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.37197, smth: 0.27837:  75%|  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.37197, smth: 0.27837:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.41121, smth: 0.29497:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.41121, smth: 0.29497: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:24 2021 Epoch 84, lr: 0.0033139, train loss: 0.29497, train auc: 0.99252, val loss: 0.43774, val auc: 0.98476\n","Thu Jun 10 01:16:24 2021 Epoch: 85\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.27250, smth: 0.27250:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.27250, smth: 0.27250:  12%|        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.33062, smth: 0.30156:  12%|        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.33062, smth: 0.30156:  25%|       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.22325, smth: 0.27546:  25%|       | 2/8 [00:05<00:13,  2.30s/it]\u001b[A\n","loss: 0.22325, smth: 0.27546:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.29528, smth: 0.28041:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.29528, smth: 0.28041:  50%|     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.28557, smth: 0.28144:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.28557, smth: 0.28144:  62%|   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.90058, smth: 0.38463:  62%|   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.90058, smth: 0.38463:  75%|  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.22061, smth: 0.36120:  75%|  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.22061, smth: 0.36120:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.42338, smth: 0.36897:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.42338, smth: 0.36897: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:35 2021 Epoch 85, lr: 0.0032725, train loss: 0.36897, train auc: 0.99061, val loss: 0.46371, val auc: 0.98377\n","Thu Jun 10 01:16:35 2021 Epoch: 86\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.20548, smth: 0.20548:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.20548, smth: 0.20548:  12%|        | 1/8 [00:02<00:20,  2.97s/it]\u001b[A\n","loss: 0.40021, smth: 0.30285:  12%|        | 1/8 [00:03<00:20,  2.97s/it]\u001b[A\n","loss: 0.40021, smth: 0.30285:  25%|       | 2/8 [00:03<00:13,  2.33s/it]\u001b[A\n","loss: 0.41511, smth: 0.34027:  25%|       | 2/8 [00:05<00:13,  2.33s/it]\u001b[A\n","loss: 0.41511, smth: 0.34027:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.30897, smth: 0.33244:  38%|      | 3/8 [00:06<00:10,  2.11s/it]\u001b[A\n","loss: 0.30897, smth: 0.33244:  50%|     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.32566, smth: 0.33109:  50%|     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.32566, smth: 0.33109:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.33775, smth: 0.33220:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.33775, smth: 0.33220:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.29837, smth: 0.32736:  75%|  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.29837, smth: 0.32736:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.54015, smth: 0.35396:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.54015, smth: 0.35396: 100%|| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:45 2021 Epoch 86, lr: 0.0032309, train loss: 0.35396, train auc: 0.98764, val loss: 0.34002, val auc: 0.98673\n","Thu Jun 10 01:16:45 2021 Epoch: 87\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31966, smth: 0.31966:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31966, smth: 0.31966:  12%|        | 1/8 [00:02<00:18,  2.70s/it]\u001b[A\n","loss: 0.49758, smth: 0.40862:  12%|        | 1/8 [00:03<00:18,  2.70s/it]\u001b[A\n","loss: 0.49758, smth: 0.40862:  25%|       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.42661, smth: 0.41461:  25%|       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.42661, smth: 0.41461:  38%|      | 3/8 [00:04<00:09,  1.84s/it]\u001b[A\n","loss: 0.35562, smth: 0.39987:  38%|      | 3/8 [00:05<00:09,  1.84s/it]\u001b[A\n","loss: 0.35562, smth: 0.39987:  50%|     | 4/8 [00:05<00:06,  1.50s/it]\u001b[A\n","loss: 0.34685, smth: 0.38926:  50%|     | 4/8 [00:06<00:06,  1.50s/it]\u001b[A\n","loss: 0.34685, smth: 0.38926:  62%|   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.44754, smth: 0.39898:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.44754, smth: 0.39898:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.40574, smth: 0.39994:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.40574, smth: 0.39994:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.30199, smth: 0.38770:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.30199, smth: 0.38770: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:16:55 2021 Epoch 87, lr: 0.0031891, train loss: 0.38770, train auc: 0.98297, val loss: 0.44487, val auc: 0.98340\n","Thu Jun 10 01:16:55 2021 Epoch: 88\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.36232, smth: 0.36232:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.36232, smth: 0.36232:  12%|        | 1/8 [00:02<00:20,  2.86s/it]\u001b[A\n","loss: 0.35418, smth: 0.35825:  12%|        | 1/8 [00:03<00:20,  2.86s/it]\u001b[A\n","loss: 0.35418, smth: 0.35825:  25%|       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.26022, smth: 0.32557:  25%|       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.26022, smth: 0.32557:  38%|      | 3/8 [00:04<00:09,  1.97s/it]\u001b[A\n","loss: 0.35425, smth: 0.33274:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.35425, smth: 0.33274:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.45363, smth: 0.35692:  50%|     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.45363, smth: 0.35692:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.51508, smth: 0.38328:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.51508, smth: 0.38328:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.32114, smth: 0.37440:  75%|  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.32114, smth: 0.37440:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.39343, smth: 0.37678:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.39343, smth: 0.37678: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.89it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:05 2021 Epoch 88, lr: 0.0031470, train loss: 0.37678, train auc: 0.98493, val loss: 0.38338, val auc: 0.98529\n","Thu Jun 10 01:17:05 2021 Epoch: 89\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26417, smth: 0.26417:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26417, smth: 0.26417:  12%|        | 1/8 [00:02<00:20,  2.93s/it]\u001b[A\n","loss: 0.36304, smth: 0.31361:  12%|        | 1/8 [00:03<00:20,  2.93s/it]\u001b[A\n","loss: 0.36304, smth: 0.31361:  25%|       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.47045, smth: 0.36589:  25%|       | 2/8 [00:04<00:13,  2.29s/it]\u001b[A\n","loss: 0.47045, smth: 0.36589:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32201, smth: 0.35492:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32201, smth: 0.35492:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.36350, smth: 0.35664:  50%|     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 0.36350, smth: 0.35664:  62%|   | 5/8 [00:06<00:04,  1.44s/it]\u001b[A\n","loss: 0.31581, smth: 0.34983:  62%|   | 5/8 [00:08<00:04,  1.44s/it]\u001b[A\n","loss: 0.31581, smth: 0.34983:  75%|  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.24503, smth: 0.33486:  75%|  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.24503, smth: 0.33486:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.32469, smth: 0.33359:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.32469, smth: 0.33359: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:15 2021 Epoch 89, lr: 0.0031048, train loss: 0.33359, train auc: 0.99094, val loss: 0.44288, val auc: 0.98394\n","Thu Jun 10 01:17:15 2021 Epoch: 90\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18949, smth: 0.18949:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18949, smth: 0.18949:  12%|        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.31447, smth: 0.25198:  12%|        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.31447, smth: 0.25198:  25%|       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.34252, smth: 0.28216:  25%|       | 2/8 [00:05<00:12,  2.07s/it]\u001b[A\n","loss: 0.34252, smth: 0.28216:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.34623, smth: 0.29818:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.34623, smth: 0.29818:  50%|     | 4/8 [00:06<00:06,  1.63s/it]\u001b[A\n","loss: 0.33756, smth: 0.30605:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.33756, smth: 0.30605:  62%|   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.32944, smth: 0.30995:  62%|   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.32944, smth: 0.30995:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.27152, smth: 0.30446:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.27152, smth: 0.30446:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.18043, smth: 0.28896:  88%| | 7/8 [00:09<00:01,  1.17s/it]\u001b[A\n","loss: 0.18043, smth: 0.28896: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:25 2021 Epoch 90, lr: 0.0030624, train loss: 0.28896, train auc: 0.99185, val loss: 0.38933, val auc: 0.98596\n","Thu Jun 10 01:17:25 2021 Epoch: 91\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.35741, smth: 0.35741:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.35741, smth: 0.35741:  12%|        | 1/8 [00:02<00:19,  2.83s/it]\u001b[A\n","loss: 0.40308, smth: 0.38025:  12%|        | 1/8 [00:03<00:19,  2.83s/it]\u001b[A\n","loss: 0.40308, smth: 0.38025:  25%|       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.29537, smth: 0.35195:  25%|       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.29537, smth: 0.35195:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.25016, smth: 0.32651:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.25016, smth: 0.32651:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.24197, smth: 0.30960:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.24197, smth: 0.30960:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.28166, smth: 0.30494:  62%|   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.28166, smth: 0.30494:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.31095, smth: 0.30580:  75%|  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.31095, smth: 0.30580:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26206, smth: 0.30033:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26206, smth: 0.30033: 100%|| 8/8 [00:09<00:00,  1.23s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:35 2021 Epoch 91, lr: 0.0030198, train loss: 0.30033, train auc: 0.99085, val loss: 0.42208, val auc: 0.98433\n","Thu Jun 10 01:17:35 2021 Epoch: 92\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19387, smth: 0.19387:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19387, smth: 0.19387:  12%|        | 1/8 [00:02<00:19,  2.72s/it]\u001b[A\n","loss: 0.31380, smth: 0.25384:  12%|        | 1/8 [00:03<00:19,  2.72s/it]\u001b[A\n","loss: 0.31380, smth: 0.25384:  25%|       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.22517, smth: 0.24428:  25%|       | 2/8 [00:04<00:12,  2.15s/it]\u001b[A\n","loss: 0.22517, smth: 0.24428:  38%|      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.25499, smth: 0.24696:  38%|      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.25499, smth: 0.24696:  50%|     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.30050, smth: 0.25767:  50%|     | 4/8 [00:07<00:06,  1.57s/it]\u001b[A\n","loss: 0.30050, smth: 0.25767:  62%|   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.23727, smth: 0.25427:  62%|   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.23727, smth: 0.25427:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.33067, smth: 0.26518:  75%|  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.33067, smth: 0.26518:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.30580, smth: 0.27026:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.30580, smth: 0.27026: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.00it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:45 2021 Epoch 92, lr: 0.0029770, train loss: 0.27026, train auc: 0.99110, val loss: 0.44196, val auc: 0.98221\n","Thu Jun 10 01:17:45 2021 Epoch: 93\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31093, smth: 0.31093:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.31093, smth: 0.31093:  12%|        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.23565, smth: 0.27329:  12%|        | 1/8 [00:03<00:21,  3.11s/it]\u001b[A\n","loss: 0.23565, smth: 0.27329:  25%|       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.18704, smth: 0.24454:  25%|       | 2/8 [00:05<00:14,  2.40s/it]\u001b[A\n","loss: 0.18704, smth: 0.24454:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.17868, smth: 0.22807:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.17868, smth: 0.22807:  50%|     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.34880, smth: 0.25222:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.34880, smth: 0.25222:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.43361, smth: 0.28245:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.43361, smth: 0.28245:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.13404, smth: 0.26125:  75%|  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.13404, smth: 0.26125:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.45876, smth: 0.28594:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.45876, smth: 0.28594: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:17:55 2021 Epoch 93, lr: 0.0029341, train loss: 0.28594, train auc: 0.99389, val loss: 0.39647, val auc: 0.98469\n","Thu Jun 10 01:17:55 2021 Epoch: 94\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.24723, smth: 0.24723:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.24723, smth: 0.24723:  12%|        | 1/8 [00:02<00:16,  2.40s/it]\u001b[A\n","loss: 0.23008, smth: 0.23865:  12%|        | 1/8 [00:03<00:16,  2.40s/it]\u001b[A\n","loss: 0.23008, smth: 0.23865:  25%|       | 2/8 [00:03<00:11,  1.90s/it]\u001b[A\n","loss: 0.34568, smth: 0.27433:  25%|       | 2/8 [00:04<00:11,  1.90s/it]\u001b[A\n","loss: 0.34568, smth: 0.27433:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.24908, smth: 0.26801:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.24908, smth: 0.26801:  50%|     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.30509, smth: 0.27543:  50%|     | 4/8 [00:07<00:06,  1.52s/it]\u001b[A\n","loss: 0.30509, smth: 0.27543:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.41342, smth: 0.29843:  62%|   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.41342, smth: 0.29843:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.34876, smth: 0.30562:  75%|  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.34876, smth: 0.30562:  88%| | 7/8 [00:09<00:01,  1.36s/it]\u001b[A\n","loss: 0.19393, smth: 0.29166:  88%| | 7/8 [00:09<00:01,  1.36s/it]\u001b[A\n","loss: 0.19393, smth: 0.29166: 100%|| 8/8 [00:10<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:06 2021 Epoch 94, lr: 0.0028911, train loss: 0.29166, train auc: 0.99280, val loss: 0.56298, val auc: 0.97906\n","Thu Jun 10 01:18:06 2021 Epoch: 95\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29330, smth: 0.29330:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.29330, smth: 0.29330:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.30422, smth: 0.29876:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.30422, smth: 0.29876:  25%|       | 2/8 [00:04<00:14,  2.48s/it]\u001b[A\n","loss: 0.35214, smth: 0.31655:  25%|       | 2/8 [00:05<00:14,  2.48s/it]\u001b[A\n","loss: 0.35214, smth: 0.31655:  38%|      | 3/8 [00:05<00:10,  2.18s/it]\u001b[A\n","loss: 0.35110, smth: 0.32519:  38%|      | 3/8 [00:06<00:10,  2.18s/it]\u001b[A\n","loss: 0.35110, smth: 0.32519:  50%|     | 4/8 [00:06<00:06,  1.74s/it]\u001b[A\n","loss: 0.23851, smth: 0.30785:  50%|     | 4/8 [00:07<00:06,  1.74s/it]\u001b[A\n","loss: 0.23851, smth: 0.30785:  62%|   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.34212, smth: 0.31356:  62%|   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.34212, smth: 0.31356:  75%|  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.25888, smth: 0.30575:  75%|  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.25888, smth: 0.30575:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.35641, smth: 0.31208:  88%| | 7/8 [00:10<00:01,  1.31s/it]\u001b[A\n","loss: 0.35641, smth: 0.31208: 100%|| 8/8 [00:10<00:00,  1.28s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:17 2021 Epoch 95, lr: 0.0028479, train loss: 0.31208, train auc: 0.99141, val loss: 0.43549, val auc: 0.98219\n","Thu Jun 10 01:18:17 2021 Epoch: 96\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22743, smth: 0.22743:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22743, smth: 0.22743:  12%|        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.17585, smth: 0.20164:  12%|        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.17585, smth: 0.20164:  25%|       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.26992, smth: 0.22440:  25%|       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.26992, smth: 0.22440:  38%|      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 0.34485, smth: 0.25451:  38%|      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 0.34485, smth: 0.25451:  50%|     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.27376, smth: 0.25836:  50%|     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.27376, smth: 0.25836:  62%|   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.25182, smth: 0.25727:  62%|   | 5/8 [00:08<00:04,  1.60s/it]\u001b[A\n","loss: 0.25182, smth: 0.25727:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.30242, smth: 0.26372:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.30242, smth: 0.26372:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.45220, smth: 0.28728:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.45220, smth: 0.28728: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:27 2021 Epoch 96, lr: 0.0028047, train loss: 0.28728, train auc: 0.99270, val loss: 0.43166, val auc: 0.98360\n","Thu Jun 10 01:18:27 2021 Epoch: 97\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28366, smth: 0.28366:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.28366, smth: 0.28366:  12%|        | 1/8 [00:02<00:17,  2.55s/it]\u001b[A\n","loss: 0.24950, smth: 0.26658:  12%|        | 1/8 [00:03<00:17,  2.55s/it]\u001b[A\n","loss: 0.24950, smth: 0.26658:  25%|       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.31764, smth: 0.28360:  25%|       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.31764, smth: 0.28360:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.28921, smth: 0.28500:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.28921, smth: 0.28500:  50%|     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.18074, smth: 0.26415:  50%|     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A\n","loss: 0.18074, smth: 0.26415:  62%|   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.30751, smth: 0.27138:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.30751, smth: 0.27138:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.35666, smth: 0.28356:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.35666, smth: 0.28356:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.16396, smth: 0.26861:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.16396, smth: 0.26861: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:37 2021 Epoch 97, lr: 0.0027613, train loss: 0.26861, train auc: 0.99425, val loss: 0.39885, val auc: 0.98511\n","Thu Jun 10 01:18:37 2021 Epoch: 98\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22981, smth: 0.22981:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22981, smth: 0.22981:  12%|        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.35497, smth: 0.29239:  12%|        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.35497, smth: 0.29239:  25%|       | 2/8 [00:03<00:12,  2.05s/it]\u001b[A\n","loss: 0.30069, smth: 0.29515:  25%|       | 2/8 [00:04<00:12,  2.05s/it]\u001b[A\n","loss: 0.30069, smth: 0.29515:  38%|      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.32851, smth: 0.30349:  38%|      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.32851, smth: 0.30349:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.35404, smth: 0.31360:  50%|     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.35404, smth: 0.31360:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.26647, smth: 0.30575:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.26647, smth: 0.30575:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.22343, smth: 0.29399:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.22343, smth: 0.29399:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26765, smth: 0.29069:  88%| | 7/8 [00:09<00:01,  1.31s/it]\u001b[A\n","loss: 0.26765, smth: 0.29069: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.99it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:47 2021 Epoch 98, lr: 0.0027179, train loss: 0.29069, train auc: 0.99444, val loss: 0.44949, val auc: 0.98308\n","Thu Jun 10 01:18:47 2021 Epoch: 99\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31167, smth: 0.31167:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31167, smth: 0.31167:  12%|        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.18623, smth: 0.24895:  12%|        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.18623, smth: 0.24895:  25%|       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.26198, smth: 0.25330:  25%|       | 2/8 [00:04<00:14,  2.40s/it]\u001b[A\n","loss: 0.26198, smth: 0.25330:  38%|      | 3/8 [00:04<00:09,  1.99s/it]\u001b[A\n","loss: 0.24065, smth: 0.25013:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.24065, smth: 0.25013:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.18870, smth: 0.23785:  50%|     | 4/8 [00:07<00:06,  1.61s/it]\u001b[A\n","loss: 0.18870, smth: 0.23785:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.14564, smth: 0.22248:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.14564, smth: 0.22248:  75%|  | 6/8 [00:07<00:02,  1.35s/it]\u001b[A\n","loss: 0.28153, smth: 0.23091:  75%|  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.28153, smth: 0.23091:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.17412, smth: 0.22381:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.17412, smth: 0.22381: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:18:58 2021 Epoch 99, lr: 0.0026744, train loss: 0.22381, train auc: 0.99441, val loss: 0.45270, val auc: 0.98357\n","Thu Jun 10 01:18:58 2021 Epoch: 100\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25846, smth: 0.25846:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25846, smth: 0.25846:  12%|        | 1/8 [00:02<00:18,  2.60s/it]\u001b[A\n","loss: 0.21154, smth: 0.23500:  12%|        | 1/8 [00:03<00:18,  2.60s/it]\u001b[A\n","loss: 0.21154, smth: 0.23500:  25%|       | 2/8 [00:03<00:12,  2.05s/it]\u001b[A\n","loss: 0.34600, smth: 0.27200:  25%|       | 2/8 [00:04<00:12,  2.05s/it]\u001b[A\n","loss: 0.34600, smth: 0.27200:  38%|      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.26129, smth: 0.26932:  38%|      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.26129, smth: 0.26932:  50%|     | 4/8 [00:05<00:06,  1.50s/it]\u001b[A\n","loss: 0.14933, smth: 0.24532:  50%|     | 4/8 [00:07<00:06,  1.50s/it]\u001b[A\n","loss: 0.14933, smth: 0.24532:  62%|   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.23300, smth: 0.24327:  62%|   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.23300, smth: 0.24327:  75%|  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.33406, smth: 0.25624:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.33406, smth: 0.25624:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.23457, smth: 0.25353:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.23457, smth: 0.25353: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:08 2021 Epoch 100, lr: 0.0026308, train loss: 0.25353, train auc: 0.99523, val loss: 0.40947, val auc: 0.98557\n","Thu Jun 10 01:19:08 2021 Epoch: 101\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.36275, smth: 0.36275:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.36275, smth: 0.36275:  12%|        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.20573, smth: 0.28424:  12%|        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.20573, smth: 0.28424:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.28265, smth: 0.28371:  25%|       | 2/8 [00:05<00:12,  2.14s/it]\u001b[A\n","loss: 0.28265, smth: 0.28371:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32875, smth: 0.29497:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.32875, smth: 0.29497:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.32514, smth: 0.30101:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.32514, smth: 0.30101:  62%|   | 5/8 [00:07<00:05,  1.69s/it]\u001b[A\n","loss: 0.16602, smth: 0.27851:  62%|   | 5/8 [00:08<00:05,  1.69s/it]\u001b[A\n","loss: 0.16602, smth: 0.27851:  75%|  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.18412, smth: 0.26503:  75%|  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.18412, smth: 0.26503:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.11381, smth: 0.24612:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.11381, smth: 0.24612: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:18 2021 Epoch 101, lr: 0.0025872, train loss: 0.24612, train auc: 0.99403, val loss: 0.42697, val auc: 0.98421\n","Thu Jun 10 01:19:18 2021 Epoch: 102\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25703, smth: 0.25703:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25703, smth: 0.25703:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.19822, smth: 0.22763:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.19822, smth: 0.22763:  25%|       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.22159, smth: 0.22562:  25%|       | 2/8 [00:05<00:13,  2.23s/it]\u001b[A\n","loss: 0.22159, smth: 0.22562:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.15511, smth: 0.20799:  38%|      | 3/8 [00:05<00:10,  2.02s/it]\u001b[A\n","loss: 0.15511, smth: 0.20799:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.24341, smth: 0.21507:  50%|     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.24341, smth: 0.21507:  62%|   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.27529, smth: 0.22511:  62%|   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.27529, smth: 0.22511:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.19073, smth: 0.22020:  75%|  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.19073, smth: 0.22020:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.24660, smth: 0.22350:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.24660, smth: 0.22350: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:28 2021 Epoch 102, lr: 0.0025436, train loss: 0.22350, train auc: 0.99402, val loss: 0.45656, val auc: 0.98432\n","Thu Jun 10 01:19:28 2021 Epoch: 103\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15465, smth: 0.15465:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15465, smth: 0.15465:  12%|        | 1/8 [00:02<00:18,  2.63s/it]\u001b[A\n","loss: 0.17242, smth: 0.16353:  12%|        | 1/8 [00:03<00:18,  2.63s/it]\u001b[A\n","loss: 0.17242, smth: 0.16353:  25%|       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.19943, smth: 0.17550:  25%|       | 2/8 [00:04<00:12,  2.07s/it]\u001b[A\n","loss: 0.19943, smth: 0.17550:  38%|      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.17238, smth: 0.17472:  38%|      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.17238, smth: 0.17472:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.13641, smth: 0.16705:  50%|     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.13641, smth: 0.16705:  62%|   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.12657, smth: 0.16031:  62%|   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.12657, smth: 0.16031:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.16707, smth: 0.16127:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.16707, smth: 0.16127:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.14954, smth: 0.15981:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.14954, smth: 0.15981: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:38 2021 Epoch 103, lr: 0.0025000, train loss: 0.15981, train auc: 0.99573, val loss: 0.49147, val auc: 0.98466\n","Thu Jun 10 01:19:38 2021 Epoch: 104\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25284, smth: 0.25284:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25284, smth: 0.25284:  12%|        | 1/8 [00:02<00:17,  2.56s/it]\u001b[A\n","loss: 0.17743, smth: 0.21513:  12%|        | 1/8 [00:03<00:17,  2.56s/it]\u001b[A\n","loss: 0.17743, smth: 0.21513:  25%|       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.19812, smth: 0.20946:  25%|       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.19812, smth: 0.20946:  38%|      | 3/8 [00:04<00:09,  1.81s/it]\u001b[A\n","loss: 0.22521, smth: 0.21340:  38%|      | 3/8 [00:05<00:09,  1.81s/it]\u001b[A\n","loss: 0.22521, smth: 0.21340:  50%|     | 4/8 [00:05<00:05,  1.50s/it]\u001b[A\n","loss: 0.24211, smth: 0.21914:  50%|     | 4/8 [00:06<00:05,  1.50s/it]\u001b[A\n","loss: 0.24211, smth: 0.21914:  62%|   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.27368, smth: 0.22823:  62%|   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.27368, smth: 0.22823:  75%|  | 6/8 [00:07<00:02,  1.19s/it]\u001b[A\n","loss: 0.25964, smth: 0.23272:  75%|  | 6/8 [00:08<00:02,  1.19s/it]\u001b[A\n","loss: 0.25964, smth: 0.23272:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.14464, smth: 0.22171:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.14464, smth: 0.22171: 100%|| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:48 2021 Epoch 104, lr: 0.0024564, train loss: 0.22171, train auc: 0.99390, val loss: 0.46420, val auc: 0.98469\n","Thu Jun 10 01:19:48 2021 Epoch: 105\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22315, smth: 0.22315:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22315, smth: 0.22315:  12%|        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.22965, smth: 0.22640:  12%|        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.22965, smth: 0.22640:  25%|       | 2/8 [00:03<00:12,  2.07s/it]\u001b[A\n","loss: 0.17968, smth: 0.21083:  25%|       | 2/8 [00:05<00:12,  2.07s/it]\u001b[A\n","loss: 0.17968, smth: 0.21083:  38%|      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.18364, smth: 0.20403:  38%|      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.18364, smth: 0.20403:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.34648, smth: 0.23252:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.34648, smth: 0.23252:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.20776, smth: 0.22839:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.20776, smth: 0.22839:  75%|  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.32311, smth: 0.24192:  75%|  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.32311, smth: 0.24192:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.15674, smth: 0.23128:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.15674, smth: 0.23128: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:19:58 2021 Epoch 105, lr: 0.0024128, train loss: 0.23128, train auc: 0.99312, val loss: 0.42929, val auc: 0.98650\n","Thu Jun 10 01:19:58 2021 Epoch: 106\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22694, smth: 0.22694:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22694, smth: 0.22694:  12%|        | 1/8 [00:02<00:18,  2.58s/it]\u001b[A\n","loss: 0.21216, smth: 0.21955:  12%|        | 1/8 [00:03<00:18,  2.58s/it]\u001b[A\n","loss: 0.21216, smth: 0.21955:  25%|       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.21486, smth: 0.21799:  25%|       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.21486, smth: 0.21799:  38%|      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.18675, smth: 0.21018:  38%|      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.18675, smth: 0.21018:  50%|     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.18581, smth: 0.20530:  50%|     | 4/8 [00:06<00:06,  1.57s/it]\u001b[A\n","loss: 0.18581, smth: 0.20530:  62%|   | 5/8 [00:06<00:04,  1.43s/it]\u001b[A\n","loss: 0.11983, smth: 0.19106:  62%|   | 5/8 [00:07<00:04,  1.43s/it]\u001b[A\n","loss: 0.11983, smth: 0.19106:  75%|  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 0.27882, smth: 0.20360:  75%|  | 6/8 [00:08<00:02,  1.22s/it]\u001b[A\n","loss: 0.27882, smth: 0.20360:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.28226, smth: 0.21343:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.28226, smth: 0.21343: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:07 2021 Epoch 106, lr: 0.0023692, train loss: 0.21343, train auc: 0.99351, val loss: 0.38702, val auc: 0.98744\n","Thu Jun 10 01:20:07 2021 Epoch: 107\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22848, smth: 0.22848:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22848, smth: 0.22848:  12%|        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 0.20065, smth: 0.21456:  12%|        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 0.20065, smth: 0.21456:  25%|       | 2/8 [00:03<00:12,  2.12s/it]\u001b[A\n","loss: 0.28140, smth: 0.23684:  25%|       | 2/8 [00:04<00:12,  2.12s/it]\u001b[A\n","loss: 0.28140, smth: 0.23684:  38%|      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.23167, smth: 0.23555:  38%|      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.23167, smth: 0.23555:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.17749, smth: 0.22394:  50%|     | 4/8 [00:07<00:06,  1.59s/it]\u001b[A\n","loss: 0.17749, smth: 0.22394:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.28625, smth: 0.23432:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.28625, smth: 0.23432:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.38487, smth: 0.25583:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.38487, smth: 0.25583:  88%| | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.18317, smth: 0.24675:  88%| | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.18317, smth: 0.24675: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.85it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:17 2021 Epoch 107, lr: 0.0023256, train loss: 0.24675, train auc: 0.99578, val loss: 0.48940, val auc: 0.98342\n","Thu Jun 10 01:20:17 2021 Epoch: 108\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21614, smth: 0.21614:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21614, smth: 0.21614:  12%|        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.22353, smth: 0.21984:  12%|        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.22353, smth: 0.21984:  25%|       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.28777, smth: 0.24248:  25%|       | 2/8 [00:05<00:13,  2.26s/it]\u001b[A\n","loss: 0.28777, smth: 0.24248:  38%|      | 3/8 [00:05<00:10,  2.15s/it]\u001b[A\n","loss: 0.27196, smth: 0.24985:  38%|      | 3/8 [00:06<00:10,  2.15s/it]\u001b[A\n","loss: 0.27196, smth: 0.24985:  50%|     | 4/8 [00:06<00:06,  1.73s/it]\u001b[A\n","loss: 0.21024, smth: 0.24193:  50%|     | 4/8 [00:07<00:06,  1.73s/it]\u001b[A\n","loss: 0.21024, smth: 0.24193:  62%|   | 5/8 [00:07<00:05,  1.67s/it]\u001b[A\n","loss: 0.25489, smth: 0.24409:  62%|   | 5/8 [00:08<00:05,  1.67s/it]\u001b[A\n","loss: 0.25489, smth: 0.24409:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.22697, smth: 0.24164:  75%|  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.22697, smth: 0.24164:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.25890, smth: 0.24380:  88%| | 7/8 [00:10<00:01,  1.30s/it]\u001b[A\n","loss: 0.25890, smth: 0.24380: 100%|| 8/8 [00:10<00:00,  1.27s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.02it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:28 2021 Epoch 108, lr: 0.0022821, train loss: 0.24380, train auc: 0.99407, val loss: 0.40146, val auc: 0.98654\n","Thu Jun 10 01:20:28 2021 Epoch: 109\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16176, smth: 0.16176:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16176, smth: 0.16176:  12%|        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.22814, smth: 0.19495:  12%|        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.22814, smth: 0.19495:  25%|       | 2/8 [00:03<00:13,  2.24s/it]\u001b[A\n","loss: 0.26622, smth: 0.21871:  25%|       | 2/8 [00:05<00:13,  2.24s/it]\u001b[A\n","loss: 0.26622, smth: 0.21871:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.30024, smth: 0.23909:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.30024, smth: 0.23909:  50%|     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.22953, smth: 0.23718:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.22953, smth: 0.23718:  62%|   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.33802, smth: 0.25399:  62%|   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.33802, smth: 0.25399:  75%|  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.28578, smth: 0.25853:  75%|  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.28578, smth: 0.25853:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.24141, smth: 0.25639:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.24141, smth: 0.25639: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:38 2021 Epoch 109, lr: 0.0022387, train loss: 0.25639, train auc: 0.99296, val loss: 0.36063, val auc: 0.98781\n","Thu Jun 10 01:20:38 2021 Epoch: 110\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.30881, smth: 0.30881:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.30881, smth: 0.30881:  12%|        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.23340, smth: 0.27111:  12%|        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.23340, smth: 0.27111:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.11285, smth: 0.21835:  25%|       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.11285, smth: 0.21835:  38%|      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.24338, smth: 0.22461:  38%|      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.24338, smth: 0.22461:  50%|     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.21939, smth: 0.22357:  50%|     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.21939, smth: 0.22357:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.22197, smth: 0.22330:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.22197, smth: 0.22330:  75%|  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.19603, smth: 0.21940:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.19603, smth: 0.21940:  88%| | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.29076, smth: 0.22832:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.29076, smth: 0.22832: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:48 2021 Epoch 110, lr: 0.0021953, train loss: 0.22832, train auc: 0.99434, val loss: 0.34178, val auc: 0.98735\n","Thu Jun 10 01:20:48 2021 Epoch: 111\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16376, smth: 0.16376:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16376, smth: 0.16376:  12%|        | 1/8 [00:02<00:17,  2.46s/it]\u001b[A\n","loss: 0.21851, smth: 0.19114:  12%|        | 1/8 [00:03<00:17,  2.46s/it]\u001b[A\n","loss: 0.21851, smth: 0.19114:  25%|       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.21368, smth: 0.19865:  25%|       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.21368, smth: 0.19865:  38%|      | 3/8 [00:04<00:08,  1.69s/it]\u001b[A\n","loss: 0.20479, smth: 0.20019:  38%|      | 3/8 [00:05<00:08,  1.69s/it]\u001b[A\n","loss: 0.20479, smth: 0.20019:  50%|     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.19246, smth: 0.19864:  50%|     | 4/8 [00:05<00:05,  1.43s/it]\u001b[A\n","loss: 0.19246, smth: 0.19864:  62%|   | 5/8 [00:06<00:03,  1.26s/it]\u001b[A\n","loss: 0.23772, smth: 0.20515:  62%|   | 5/8 [00:06<00:03,  1.26s/it]\u001b[A\n","loss: 0.23772, smth: 0.20515:  75%|  | 6/8 [00:06<00:02,  1.13s/it]\u001b[A\n","loss: 0.23881, smth: 0.20996:  75%|  | 6/8 [00:07<00:02,  1.13s/it]\u001b[A\n","loss: 0.23881, smth: 0.20996:  88%| | 7/8 [00:07<00:01,  1.12s/it]\u001b[A\n","loss: 0.30437, smth: 0.22176:  88%| | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 0.30437, smth: 0.22176: 100%|| 8/8 [00:08<00:00,  1.05s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:20:57 2021 Epoch 111, lr: 0.0021521, train loss: 0.22176, train auc: 0.99568, val loss: 0.39411, val auc: 0.98633\n","Thu Jun 10 01:20:57 2021 Epoch: 112\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.23403, smth: 0.23403:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.23403, smth: 0.23403:  12%|        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.16341, smth: 0.19872:  12%|        | 1/8 [00:03<00:21,  3.03s/it]\u001b[A\n","loss: 0.16341, smth: 0.19872:  25%|       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.24568, smth: 0.21437:  25%|       | 2/8 [00:05<00:14,  2.35s/it]\u001b[A\n","loss: 0.24568, smth: 0.21437:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.24846, smth: 0.22289:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.24846, smth: 0.22289:  50%|     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.26018, smth: 0.23035:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.26018, smth: 0.23035:  62%|   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.15525, smth: 0.21783:  62%|   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.15525, smth: 0.21783:  75%|  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.16995, smth: 0.21099:  75%|  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.16995, smth: 0.21099:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.12509, smth: 0.20025:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.12509, smth: 0.20025: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:07 2021 Epoch 112, lr: 0.0021089, train loss: 0.20025, train auc: 0.99612, val loss: 0.38276, val auc: 0.98717\n","Thu Jun 10 01:21:07 2021 Epoch: 113\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19584, smth: 0.19584:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19584, smth: 0.19584:  12%|        | 1/8 [00:02<00:17,  2.50s/it]\u001b[A\n","loss: 0.15328, smth: 0.17456:  12%|        | 1/8 [00:03<00:17,  2.50s/it]\u001b[A\n","loss: 0.15328, smth: 0.17456:  25%|       | 2/8 [00:03<00:11,  1.97s/it]\u001b[A\n","loss: 0.17760, smth: 0.17557:  25%|       | 2/8 [00:04<00:11,  1.97s/it]\u001b[A\n","loss: 0.17760, smth: 0.17557:  38%|      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.31776, smth: 0.21112:  38%|      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.31776, smth: 0.21112:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.19176, smth: 0.20725:  50%|     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.19176, smth: 0.20725:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.16488, smth: 0.20019:  62%|   | 5/8 [00:08<00:04,  1.58s/it]\u001b[A\n","loss: 0.16488, smth: 0.20019:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.14426, smth: 0.19220:  75%|  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.14426, smth: 0.19220:  88%| | 7/8 [00:09<00:01,  1.34s/it]\u001b[A\n","loss: 0.37103, smth: 0.21455:  88%| | 7/8 [00:09<00:01,  1.34s/it]\u001b[A\n","loss: 0.37103, smth: 0.21455: 100%|| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:17 2021 Epoch 113, lr: 0.0020659, train loss: 0.21455, train auc: 0.99508, val loss: 0.42515, val auc: 0.98714\n","Thu Jun 10 01:21:17 2021 Epoch: 114\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.23674, smth: 0.23674:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.23674, smth: 0.23674:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.24925, smth: 0.24299:  12%|        | 1/8 [00:03<00:21,  3.02s/it]\u001b[A\n","loss: 0.24925, smth: 0.24299:  25%|       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.22937, smth: 0.23845:  25%|       | 2/8 [00:04<00:14,  2.35s/it]\u001b[A\n","loss: 0.22937, smth: 0.23845:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.14974, smth: 0.21628:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.14974, smth: 0.21628:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.18030, smth: 0.20908:  50%|     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.18030, smth: 0.20908:  62%|   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.16519, smth: 0.20177:  62%|   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.16519, smth: 0.20177:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.23865, smth: 0.20704:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.23865, smth: 0.20704:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.21808, smth: 0.20842:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.21808, smth: 0.20842: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:27 2021 Epoch 114, lr: 0.0020230, train loss: 0.20842, train auc: 0.99394, val loss: 0.41503, val auc: 0.98662\n","Thu Jun 10 01:21:27 2021 Epoch: 115\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17171, smth: 0.17171:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.17171, smth: 0.17171:  12%|        | 1/8 [00:03<00:23,  3.33s/it]\u001b[A\n","loss: 0.21665, smth: 0.19418:  12%|        | 1/8 [00:04<00:23,  3.33s/it]\u001b[A\n","loss: 0.21665, smth: 0.19418:  25%|       | 2/8 [00:04<00:15,  2.56s/it]\u001b[A\n","loss: 0.14142, smth: 0.17659:  25%|       | 2/8 [00:05<00:15,  2.56s/it]\u001b[A\n","loss: 0.14142, smth: 0.17659:  38%|      | 3/8 [00:05<00:11,  2.30s/it]\u001b[A\n","loss: 0.15126, smth: 0.17026:  38%|      | 3/8 [00:06<00:11,  2.30s/it]\u001b[A\n","loss: 0.15126, smth: 0.17026:  50%|     | 4/8 [00:06<00:07,  1.86s/it]\u001b[A\n","loss: 0.24394, smth: 0.18500:  50%|     | 4/8 [00:07<00:07,  1.86s/it]\u001b[A\n","loss: 0.24394, smth: 0.18500:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.21535, smth: 0.19006:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.21535, smth: 0.19006:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.24036, smth: 0.19724:  75%|  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.24036, smth: 0.19724:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.20312, smth: 0.19798:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.20312, smth: 0.19798: 100%|| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:38 2021 Epoch 115, lr: 0.0019802, train loss: 0.19798, train auc: 0.99661, val loss: 0.46790, val auc: 0.98461\n","Thu Jun 10 01:21:38 2021 Epoch: 116\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.29430, smth: 0.29430:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.29430, smth: 0.29430:  12%|        | 1/8 [00:02<00:20,  2.99s/it]\u001b[A\n","loss: 0.19633, smth: 0.24532:  12%|        | 1/8 [00:03<00:20,  2.99s/it]\u001b[A\n","loss: 0.19633, smth: 0.24532:  25%|       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.24717, smth: 0.24593:  25%|       | 2/8 [00:05<00:13,  2.32s/it]\u001b[A\n","loss: 0.24717, smth: 0.24593:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.19078, smth: 0.23215:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.19078, smth: 0.23215:  50%|     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.25299, smth: 0.23631:  50%|     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.25299, smth: 0.23631:  62%|   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.24208, smth: 0.23728:  62%|   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.24208, smth: 0.23728:  75%|  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.10906, smth: 0.21896:  75%|  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.10906, smth: 0.21896:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23021, smth: 0.22037:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23021, smth: 0.22037: 100%|| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:48 2021 Epoch 116, lr: 0.0019376, train loss: 0.22037, train auc: 0.99530, val loss: 0.59413, val auc: 0.98149\n","Thu Jun 10 01:21:48 2021 Epoch: 117\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32001, smth: 0.32001:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32001, smth: 0.32001:  12%|        | 1/8 [00:02<00:17,  2.57s/it]\u001b[A\n","loss: 0.30060, smth: 0.31030:  12%|        | 1/8 [00:03<00:17,  2.57s/it]\u001b[A\n","loss: 0.30060, smth: 0.31030:  25%|       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.19835, smth: 0.27298:  25%|       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.19835, smth: 0.27298:  38%|      | 3/8 [00:04<00:09,  1.85s/it]\u001b[A\n","loss: 0.15962, smth: 0.24464:  38%|      | 3/8 [00:05<00:09,  1.85s/it]\u001b[A\n","loss: 0.15962, smth: 0.24464:  50%|     | 4/8 [00:05<00:06,  1.52s/it]\u001b[A\n","loss: 0.15066, smth: 0.22585:  50%|     | 4/8 [00:06<00:06,  1.52s/it]\u001b[A\n","loss: 0.15066, smth: 0.22585:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.13076, smth: 0.21000:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.13076, smth: 0.21000:  75%|  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.22310, smth: 0.21187:  75%|  | 6/8 [00:09<00:02,  1.30s/it]\u001b[A\n","loss: 0.22310, smth: 0.21187:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.12458, smth: 0.20096:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.12458, smth: 0.20096: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:21:58 2021 Epoch 117, lr: 0.0018952, train loss: 0.20096, train auc: 0.99314, val loss: 0.59656, val auc: 0.98339\n","Thu Jun 10 01:21:59 2021 Epoch: 118\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19436, smth: 0.19436:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19436, smth: 0.19436:  12%|        | 1/8 [00:02<00:17,  2.47s/it]\u001b[A\n","loss: 0.18819, smth: 0.19127:  12%|        | 1/8 [00:03<00:17,  2.47s/it]\u001b[A\n","loss: 0.18819, smth: 0.19127:  25%|       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.23639, smth: 0.20631:  25%|       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.23639, smth: 0.20631:  38%|      | 3/8 [00:04<00:08,  1.69s/it]\u001b[A\n","loss: 0.32644, smth: 0.23634:  38%|      | 3/8 [00:04<00:08,  1.69s/it]\u001b[A\n","loss: 0.32644, smth: 0.23634:  50%|     | 4/8 [00:05<00:05,  1.42s/it]\u001b[A\n","loss: 0.18200, smth: 0.22548:  50%|     | 4/8 [00:06<00:05,  1.42s/it]\u001b[A\n","loss: 0.18200, smth: 0.22548:  62%|   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.29972, smth: 0.23785:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.29972, smth: 0.23785:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.26063, smth: 0.24110:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.26063, smth: 0.24110:  88%| | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.13275, smth: 0.22756:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.13275, smth: 0.22756: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:08 2021 Epoch 118, lr: 0.0018530, train loss: 0.22756, train auc: 0.99684, val loss: 0.46676, val auc: 0.98586\n","Thu Jun 10 01:22:08 2021 Epoch: 119\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21024, smth: 0.21024:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21024, smth: 0.21024:  12%|        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.24161, smth: 0.22593:  12%|        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.24161, smth: 0.22593:  25%|       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.15937, smth: 0.20374:  25%|       | 2/8 [00:05<00:13,  2.21s/it]\u001b[A\n","loss: 0.15937, smth: 0.20374:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.11788, smth: 0.18228:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.11788, smth: 0.18228:  50%|     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.27582, smth: 0.20099:  50%|     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.27582, smth: 0.20099:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.22047, smth: 0.20423:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.22047, smth: 0.20423:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.21095, smth: 0.20519:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.21095, smth: 0.20519:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.21924, smth: 0.20695:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.21924, smth: 0.20695: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:18 2021 Epoch 119, lr: 0.0018109, train loss: 0.20695, train auc: 0.99450, val loss: 0.43858, val auc: 0.98663\n","Thu Jun 10 01:22:18 2021 Epoch: 120\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22260, smth: 0.22260:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22260, smth: 0.22260:  12%|        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.12109, smth: 0.17184:  12%|        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.12109, smth: 0.17184:  25%|       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.15102, smth: 0.16490:  25%|       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.15102, smth: 0.16490:  38%|      | 3/8 [00:04<00:08,  1.78s/it]\u001b[A\n","loss: 0.17292, smth: 0.16691:  38%|      | 3/8 [00:05<00:08,  1.78s/it]\u001b[A\n","loss: 0.17292, smth: 0.16691:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.27075, smth: 0.18768:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.27075, smth: 0.18768:  62%|   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.23190, smth: 0.19505:  62%|   | 5/8 [00:08<00:04,  1.42s/it]\u001b[A\n","loss: 0.23190, smth: 0.19505:  75%|  | 6/8 [00:08<00:02,  1.46s/it]\u001b[A\n","loss: 0.14446, smth: 0.18782:  75%|  | 6/8 [00:08<00:02,  1.46s/it]\u001b[A\n","loss: 0.14446, smth: 0.18782:  88%| | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.14384, smth: 0.18232:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.14384, smth: 0.18232: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:28 2021 Epoch 120, lr: 0.0017691, train loss: 0.18232, train auc: 0.99673, val loss: 0.42079, val auc: 0.98715\n","Thu Jun 10 01:22:28 2021 Epoch: 121\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16990, smth: 0.16990:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16990, smth: 0.16990:  12%|        | 1/8 [00:02<00:17,  2.48s/it]\u001b[A\n","loss: 0.19348, smth: 0.18169:  12%|        | 1/8 [00:03<00:17,  2.48s/it]\u001b[A\n","loss: 0.19348, smth: 0.18169:  25%|       | 2/8 [00:03<00:11,  1.98s/it]\u001b[A\n","loss: 0.19599, smth: 0.18646:  25%|       | 2/8 [00:04<00:11,  1.98s/it]\u001b[A\n","loss: 0.19599, smth: 0.18646:  38%|      | 3/8 [00:04<00:08,  1.80s/it]\u001b[A\n","loss: 0.23542, smth: 0.19870:  38%|      | 3/8 [00:05<00:08,  1.80s/it]\u001b[A\n","loss: 0.23542, smth: 0.19870:  50%|     | 4/8 [00:05<00:05,  1.48s/it]\u001b[A\n","loss: 0.12672, smth: 0.18430:  50%|     | 4/8 [00:06<00:05,  1.48s/it]\u001b[A\n","loss: 0.12672, smth: 0.18430:  62%|   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.13979, smth: 0.17688:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.13979, smth: 0.17688:  75%|  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.12908, smth: 0.17006:  75%|  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.12908, smth: 0.17006:  88%| | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.20636, smth: 0.17459:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.20636, smth: 0.17459: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:38 2021 Epoch 121, lr: 0.0017275, train loss: 0.17459, train auc: 0.99694, val loss: 0.47540, val auc: 0.98499\n","Thu Jun 10 01:22:38 2021 Epoch: 122\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18439, smth: 0.18439:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18439, smth: 0.18439:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.12994, smth: 0.15717:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.12994, smth: 0.15717:  25%|       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.20241, smth: 0.17225:  25%|       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.20241, smth: 0.17225:  38%|      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.18586, smth: 0.17565:  38%|      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.18586, smth: 0.17565:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.13001, smth: 0.16652:  50%|     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.13001, smth: 0.16652:  62%|   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.16417, smth: 0.16613:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.16417, smth: 0.16613:  75%|  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.29213, smth: 0.18413:  75%|  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.29213, smth: 0.18413:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.38408, smth: 0.20912:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.38408, smth: 0.20912: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:48 2021 Epoch 122, lr: 0.0016861, train loss: 0.20912, train auc: 0.99532, val loss: 0.55646, val auc: 0.98325\n","Thu Jun 10 01:22:48 2021 Epoch: 123\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.32407, smth: 0.32407:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.32407, smth: 0.32407:  12%|        | 1/8 [00:02<00:19,  2.79s/it]\u001b[A\n","loss: 0.21489, smth: 0.26948:  12%|        | 1/8 [00:03<00:19,  2.79s/it]\u001b[A\n","loss: 0.21489, smth: 0.26948:  25%|       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.19380, smth: 0.24426:  25%|       | 2/8 [00:04<00:13,  2.18s/it]\u001b[A\n","loss: 0.19380, smth: 0.24426:  38%|      | 3/8 [00:04<00:09,  1.85s/it]\u001b[A\n","loss: 0.23418, smth: 0.24174:  38%|      | 3/8 [00:05<00:09,  1.85s/it]\u001b[A\n","loss: 0.23418, smth: 0.24174:  50%|     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.10643, smth: 0.21468:  50%|     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.10643, smth: 0.21468:  62%|   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.24772, smth: 0.22018:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.24772, smth: 0.22018:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.10488, smth: 0.20371:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.10488, smth: 0.20371:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.32875, smth: 0.21934:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.32875, smth: 0.21934: 100%|| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:22:58 2021 Epoch 123, lr: 0.0016449, train loss: 0.21934, train auc: 0.99474, val loss: 0.58633, val auc: 0.98164\n","Thu Jun 10 01:22:58 2021 Epoch: 124\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19060, smth: 0.19060:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19060, smth: 0.19060:  12%|        | 1/8 [00:02<00:17,  2.53s/it]\u001b[A\n","loss: 0.17416, smth: 0.18238:  12%|        | 1/8 [00:03<00:17,  2.53s/it]\u001b[A\n","loss: 0.17416, smth: 0.18238:  25%|       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.09839, smth: 0.15438:  25%|       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.09839, smth: 0.15438:  38%|      | 3/8 [00:04<00:09,  1.84s/it]\u001b[A\n","loss: 0.34528, smth: 0.20211:  38%|      | 3/8 [00:05<00:09,  1.84s/it]\u001b[A\n","loss: 0.34528, smth: 0.20211:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.15366, smth: 0.19242:  50%|     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.15366, smth: 0.19242:  62%|   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.27784, smth: 0.20666:  62%|   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.27784, smth: 0.20666:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.17992, smth: 0.20284:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.17992, smth: 0.20284:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.40327, smth: 0.22789:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.40327, smth: 0.22789: 100%|| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.99it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:07 2021 Epoch 124, lr: 0.0016041, train loss: 0.22789, train auc: 0.99449, val loss: 0.52510, val auc: 0.98350\n","Thu Jun 10 01:23:07 2021 Epoch: 125\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21638, smth: 0.21638:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21638, smth: 0.21638:  12%|        | 1/8 [00:02<00:18,  2.61s/it]\u001b[A\n","loss: 0.22241, smth: 0.21939:  12%|        | 1/8 [00:03<00:18,  2.61s/it]\u001b[A\n","loss: 0.22241, smth: 0.21939:  25%|       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.19604, smth: 0.21161:  25%|       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.19604, smth: 0.21161:  38%|      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.18724, smth: 0.20552:  38%|      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.18724, smth: 0.20552:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.23297, smth: 0.21101:  50%|     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 0.23297, smth: 0.21101:  62%|   | 5/8 [00:06<00:04,  1.38s/it]\u001b[A\n","loss: 0.30742, smth: 0.22708:  62%|   | 5/8 [00:07<00:04,  1.38s/it]\u001b[A\n","loss: 0.30742, smth: 0.22708:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.14118, smth: 0.21480:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.14118, smth: 0.21480:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.24792, smth: 0.21894:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.24792, smth: 0.21894: 100%|| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:17 2021 Epoch 125, lr: 0.0015635, train loss: 0.21894, train auc: 0.99645, val loss: 0.41478, val auc: 0.98642\n","Thu Jun 10 01:23:17 2021 Epoch: 126\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28358, smth: 0.28358:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.28358, smth: 0.28358:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.19916, smth: 0.24137:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.19916, smth: 0.24137:  25%|       | 2/8 [00:03<00:14,  2.47s/it]\u001b[A\n","loss: 0.19098, smth: 0.22457:  25%|       | 2/8 [00:05<00:14,  2.47s/it]\u001b[A\n","loss: 0.19098, smth: 0.22457:  38%|      | 3/8 [00:05<00:11,  2.27s/it]\u001b[A\n","loss: 0.20144, smth: 0.21879:  38%|      | 3/8 [00:06<00:11,  2.27s/it]\u001b[A\n","loss: 0.20144, smth: 0.21879:  50%|     | 4/8 [00:06<00:07,  1.82s/it]\u001b[A\n","loss: 0.29160, smth: 0.23335:  50%|     | 4/8 [00:08<00:07,  1.82s/it]\u001b[A\n","loss: 0.29160, smth: 0.23335:  62%|   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.24746, smth: 0.23570:  62%|   | 5/8 [00:08<00:05,  1.76s/it]\u001b[A\n","loss: 0.24746, smth: 0.23570:  75%|  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.14993, smth: 0.22345:  75%|  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.14993, smth: 0.22345:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.13896, smth: 0.21289:  88%| | 7/8 [00:10<00:01,  1.25s/it]\u001b[A\n","loss: 0.13896, smth: 0.21289: 100%|| 8/8 [00:10<00:00,  1.27s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:27 2021 Epoch 126, lr: 0.0015232, train loss: 0.21289, train auc: 0.99570, val loss: 0.38780, val auc: 0.98658\n","Thu Jun 10 01:23:27 2021 Epoch: 127\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19671, smth: 0.19671:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19671, smth: 0.19671:  12%|        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.28885, smth: 0.24278:  12%|        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.28885, smth: 0.24278:  25%|       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.28696, smth: 0.25751:  25%|       | 2/8 [00:05<00:13,  2.18s/it]\u001b[A\n","loss: 0.28696, smth: 0.25751:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.14910, smth: 0.23040:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.14910, smth: 0.23040:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.22486, smth: 0.22930:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.22486, smth: 0.22930:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.26295, smth: 0.23490:  62%|   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.26295, smth: 0.23490:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.12025, smth: 0.21853:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.12025, smth: 0.21853:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.16779, smth: 0.21218:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.16779, smth: 0.21218: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:37 2021 Epoch 127, lr: 0.0014832, train loss: 0.21218, train auc: 0.99548, val loss: 0.43956, val auc: 0.98479\n","Thu Jun 10 01:23:37 2021 Epoch: 128\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12391, smth: 0.12391:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12391, smth: 0.12391:  12%|        | 1/8 [00:02<00:16,  2.42s/it]\u001b[A\n","loss: 0.19165, smth: 0.15778:  12%|        | 1/8 [00:03<00:16,  2.42s/it]\u001b[A\n","loss: 0.19165, smth: 0.15778:  25%|       | 2/8 [00:03<00:11,  1.93s/it]\u001b[A\n","loss: 0.19508, smth: 0.17021:  25%|       | 2/8 [00:04<00:11,  1.93s/it]\u001b[A\n","loss: 0.19508, smth: 0.17021:  38%|      | 3/8 [00:04<00:09,  1.84s/it]\u001b[A\n","loss: 0.12670, smth: 0.15934:  38%|      | 3/8 [00:05<00:09,  1.84s/it]\u001b[A\n","loss: 0.12670, smth: 0.15934:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.15665, smth: 0.15880:  50%|     | 4/8 [00:07<00:06,  1.53s/it]\u001b[A\n","loss: 0.15665, smth: 0.15880:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.24264, smth: 0.17277:  62%|   | 5/8 [00:07<00:04,  1.58s/it]\u001b[A\n","loss: 0.24264, smth: 0.17277:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.11753, smth: 0.16488:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.11753, smth: 0.16488:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.28066, smth: 0.17935:  88%| | 7/8 [00:09<00:01,  1.13s/it]\u001b[A\n","loss: 0.28066, smth: 0.17935: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:47 2021 Epoch 128, lr: 0.0014435, train loss: 0.17935, train auc: 0.99494, val loss: 0.54825, val auc: 0.98259\n","Thu Jun 10 01:23:47 2021 Epoch: 129\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.27442, smth: 0.27442:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.27442, smth: 0.27442:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.12960, smth: 0.20201:  12%|        | 1/8 [00:03<00:22,  3.20s/it]\u001b[A\n","loss: 0.12960, smth: 0.20201:  25%|       | 2/8 [00:03<00:14,  2.45s/it]\u001b[A\n","loss: 0.19620, smth: 0.20007:  25%|       | 2/8 [00:05<00:14,  2.45s/it]\u001b[A\n","loss: 0.19620, smth: 0.20007:  38%|      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.06969, smth: 0.16748:  38%|      | 3/8 [00:05<00:10,  2.13s/it]\u001b[A\n","loss: 0.06969, smth: 0.16748:  50%|     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.16027, smth: 0.16604:  50%|     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.16027, smth: 0.16604:  62%|   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.27039, smth: 0.18343:  62%|   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.27039, smth: 0.18343:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.15567, smth: 0.17946:  75%|  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.15567, smth: 0.17946:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.04837, smth: 0.16308:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.04837, smth: 0.16308: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:23:57 2021 Epoch 129, lr: 0.0014041, train loss: 0.16308, train auc: 0.99692, val loss: 0.54291, val auc: 0.98378\n","Thu Jun 10 01:23:57 2021 Epoch: 130\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10956, smth: 0.10956:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10956, smth: 0.10956:  12%|        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.09433, smth: 0.10195:  12%|        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.09433, smth: 0.10195:  25%|       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.16503, smth: 0.12298:  25%|       | 2/8 [00:05<00:12,  2.15s/it]\u001b[A\n","loss: 0.16503, smth: 0.12298:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.17448, smth: 0.13585:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.17448, smth: 0.13585:  50%|     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.23349, smth: 0.15538:  50%|     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.23349, smth: 0.15538:  62%|   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.12475, smth: 0.15027:  62%|   | 5/8 [00:08<00:04,  1.60s/it]\u001b[A\n","loss: 0.12475, smth: 0.15027:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.23343, smth: 0.16215:  75%|  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.23343, smth: 0.16215:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.17754, smth: 0.16408:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.17754, smth: 0.16408: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.98it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.63it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:08 2021 Epoch 130, lr: 0.0013650, train loss: 0.16408, train auc: 0.99628, val loss: 0.51342, val auc: 0.98434\n","Thu Jun 10 01:24:08 2021 Epoch: 131\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16418, smth: 0.16418:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16418, smth: 0.16418:  12%|        | 1/8 [00:02<00:20,  2.95s/it]\u001b[A\n","loss: 0.08542, smth: 0.12480:  12%|        | 1/8 [00:03<00:20,  2.95s/it]\u001b[A\n","loss: 0.08542, smth: 0.12480:  25%|       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.32237, smth: 0.19066:  25%|       | 2/8 [00:04<00:13,  2.29s/it]\u001b[A\n","loss: 0.32237, smth: 0.19066:  38%|      | 3/8 [00:04<00:09,  1.98s/it]\u001b[A\n","loss: 0.19259, smth: 0.19114:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.19259, smth: 0.19114:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.16570, smth: 0.18605:  50%|     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.16570, smth: 0.18605:  62%|   | 5/8 [00:07<00:05,  1.68s/it]\u001b[A\n","loss: 0.17511, smth: 0.18423:  62%|   | 5/8 [00:08<00:05,  1.68s/it]\u001b[A\n","loss: 0.17511, smth: 0.18423:  75%|  | 6/8 [00:08<00:02,  1.49s/it]\u001b[A\n","loss: 0.14180, smth: 0.17817:  75%|  | 6/8 [00:09<00:02,  1.49s/it]\u001b[A\n","loss: 0.14180, smth: 0.17817:  88%| | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.08867, smth: 0.16698:  88%| | 7/8 [00:10<00:01,  1.35s/it]\u001b[A\n","loss: 0.08867, smth: 0.16698: 100%|| 8/8 [00:10<00:00,  1.26s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:18 2021 Epoch 131, lr: 0.0013263, train loss: 0.16698, train auc: 0.99563, val loss: 0.40554, val auc: 0.98834\n","Thu Jun 10 01:24:18 2021 Epoch: 132\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13046, smth: 0.13046:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13046, smth: 0.13046:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.15615, smth: 0.14330:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.15615, smth: 0.14330:  25%|       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.14422, smth: 0.14361:  25%|       | 2/8 [00:04<00:13,  2.23s/it]\u001b[A\n","loss: 0.14422, smth: 0.14361:  38%|      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.18762, smth: 0.15461:  38%|      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.18762, smth: 0.15461:  50%|     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.12214, smth: 0.14812:  50%|     | 4/8 [00:07<00:06,  1.58s/it]\u001b[A\n","loss: 0.12214, smth: 0.14812:  62%|   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.29318, smth: 0.17229:  62%|   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.29318, smth: 0.17229:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.16834, smth: 0.17173:  75%|  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.16834, smth: 0.17173:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.08643, smth: 0.16107:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.08643, smth: 0.16107: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.78it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:29 2021 Epoch 132, lr: 0.0012880, train loss: 0.16107, train auc: 0.99629, val loss: 0.38921, val auc: 0.98914\n","Thu Jun 10 01:24:29 2021 Epoch: 133\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14472, smth: 0.14472:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14472, smth: 0.14472:  12%|        | 1/8 [00:02<00:20,  2.94s/it]\u001b[A\n","loss: 0.12632, smth: 0.13552:  12%|        | 1/8 [00:03<00:20,  2.94s/it]\u001b[A\n","loss: 0.12632, smth: 0.13552:  25%|       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.09870, smth: 0.12325:  25%|       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.09870, smth: 0.12325:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.16998, smth: 0.13493:  38%|      | 3/8 [00:06<00:10,  2.11s/it]\u001b[A\n","loss: 0.16998, smth: 0.13493:  50%|     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.14739, smth: 0.13742:  50%|     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.14739, smth: 0.13742:  62%|   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.14199, smth: 0.13818:  62%|   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.14199, smth: 0.13818:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.18481, smth: 0.14485:  75%|  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.18481, smth: 0.14485:  88%| | 7/8 [00:09<00:01,  1.39s/it]\u001b[A\n","loss: 0.24457, smth: 0.15731:  88%| | 7/8 [00:10<00:01,  1.39s/it]\u001b[A\n","loss: 0.24457, smth: 0.15731: 100%|| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:40 2021 Epoch 133, lr: 0.0012500, train loss: 0.15731, train auc: 0.99615, val loss: 0.48225, val auc: 0.98675\n","Thu Jun 10 01:24:40 2021 Epoch: 134\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.09781, smth: 0.09781:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.09781, smth: 0.09781:  12%|        | 1/8 [00:03<00:22,  3.16s/it]\u001b[A\n","loss: 0.18882, smth: 0.14332:  12%|        | 1/8 [00:03<00:22,  3.16s/it]\u001b[A\n","loss: 0.18882, smth: 0.14332:  25%|       | 2/8 [00:03<00:14,  2.44s/it]\u001b[A\n","loss: 0.12962, smth: 0.13875:  25%|       | 2/8 [00:05<00:14,  2.44s/it]\u001b[A\n","loss: 0.12962, smth: 0.13875:  38%|      | 3/8 [00:05<00:10,  2.12s/it]\u001b[A\n","loss: 0.25494, smth: 0.16780:  38%|      | 3/8 [00:05<00:10,  2.12s/it]\u001b[A\n","loss: 0.25494, smth: 0.16780:  50%|     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.21005, smth: 0.17625:  50%|     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.21005, smth: 0.17625:  62%|   | 5/8 [00:07<00:05,  1.70s/it]\u001b[A\n","loss: 0.21241, smth: 0.18228:  62%|   | 5/8 [00:08<00:05,  1.70s/it]\u001b[A\n","loss: 0.21241, smth: 0.18228:  75%|  | 6/8 [00:08<00:02,  1.40s/it]\u001b[A\n","loss: 0.16340, smth: 0.17958:  75%|  | 6/8 [00:09<00:02,  1.40s/it]\u001b[A\n","loss: 0.16340, smth: 0.17958:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.19016, smth: 0.18090:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.19016, smth: 0.18090: 100%|| 8/8 [00:10<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.18it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:24:50 2021 Epoch 134, lr: 0.0012124, train loss: 0.18090, train auc: 0.99655, val loss: 0.41290, val auc: 0.98801\n","Thu Jun 10 01:24:50 2021 Epoch: 135\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21972, smth: 0.21972:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21972, smth: 0.21972:  12%|        | 1/8 [00:02<00:20,  2.89s/it]\u001b[A\n","loss: 0.17729, smth: 0.19851:  12%|        | 1/8 [00:03<00:20,  2.89s/it]\u001b[A\n","loss: 0.17729, smth: 0.19851:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.21238, smth: 0.20313:  25%|       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.21238, smth: 0.20313:  38%|      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.15598, smth: 0.19134:  38%|      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.15598, smth: 0.19134:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.29888, smth: 0.21285:  50%|     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.29888, smth: 0.21285:  62%|   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.21078, smth: 0.21250:  62%|   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.21078, smth: 0.21250:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.24024, smth: 0.21647:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.24024, smth: 0.21647:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.34265, smth: 0.23224:  88%| | 7/8 [00:09<00:01,  1.26s/it]\u001b[A\n","loss: 0.34265, smth: 0.23224: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:00 2021 Epoch 135, lr: 0.0011752, train loss: 0.23224, train auc: 0.99432, val loss: 0.40404, val auc: 0.98791\n","Thu Jun 10 01:25:00 2021 Epoch: 136\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22106, smth: 0.22106:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22106, smth: 0.22106:  12%|        | 1/8 [00:02<00:20,  2.90s/it]\u001b[A\n","loss: 0.21344, smth: 0.21725:  12%|        | 1/8 [00:03<00:20,  2.90s/it]\u001b[A\n","loss: 0.21344, smth: 0.21725:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.14921, smth: 0.19457:  25%|       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.14921, smth: 0.19457:  38%|      | 3/8 [00:04<00:09,  1.94s/it]\u001b[A\n","loss: 0.18169, smth: 0.19135:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.18169, smth: 0.19135:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.13754, smth: 0.18059:  50%|     | 4/8 [00:07<00:06,  1.59s/it]\u001b[A\n","loss: 0.13754, smth: 0.18059:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.20477, smth: 0.18462:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.20477, smth: 0.18462:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.17493, smth: 0.18324:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.17493, smth: 0.18324:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.14061, smth: 0.17791:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.14061, smth: 0.17791: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:10 2021 Epoch 136, lr: 0.0011384, train loss: 0.17791, train auc: 0.99698, val loss: 0.36919, val auc: 0.98844\n","Thu Jun 10 01:25:10 2021 Epoch: 137\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25101, smth: 0.25101:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25101, smth: 0.25101:  12%|        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.21847, smth: 0.23474:  12%|        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.21847, smth: 0.23474:  25%|       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.17345, smth: 0.21431:  25%|       | 2/8 [00:04<00:13,  2.29s/it]\u001b[A\n","loss: 0.17345, smth: 0.21431:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.17412, smth: 0.20426:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.17412, smth: 0.20426:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.24073, smth: 0.21156:  50%|     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.24073, smth: 0.21156:  62%|   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.26692, smth: 0.22079:  62%|   | 5/8 [00:08<00:04,  1.61s/it]\u001b[A\n","loss: 0.26692, smth: 0.22079:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.10152, smth: 0.20375:  75%|  | 6/8 [00:09<00:02,  1.35s/it]\u001b[A\n","loss: 0.10152, smth: 0.20375:  88%| | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.24450, smth: 0.20884:  88%| | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.24450, smth: 0.20884: 100%|| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.81it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:21 2021 Epoch 137, lr: 0.0011020, train loss: 0.20884, train auc: 0.99559, val loss: 0.42842, val auc: 0.98765\n","Thu Jun 10 01:25:21 2021 Epoch: 138\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15826, smth: 0.15826:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15826, smth: 0.15826:  12%|        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.25728, smth: 0.20777:  12%|        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.25728, smth: 0.20777:  25%|       | 2/8 [00:03<00:12,  2.06s/it]\u001b[A\n","loss: 0.22042, smth: 0.21199:  25%|       | 2/8 [00:04<00:12,  2.06s/it]\u001b[A\n","loss: 0.22042, smth: 0.21199:  38%|      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.16940, smth: 0.20134:  38%|      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.16940, smth: 0.20134:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.20690, smth: 0.20245:  50%|     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.20690, smth: 0.20245:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.10728, smth: 0.18659:  62%|   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.10728, smth: 0.18659:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.19275, smth: 0.18747:  75%|  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.19275, smth: 0.18747:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.21902, smth: 0.19141:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.21902, smth: 0.19141: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:31 2021 Epoch 138, lr: 0.0010661, train loss: 0.19141, train auc: 0.99658, val loss: 0.42001, val auc: 0.98695\n","Thu Jun 10 01:25:31 2021 Epoch: 139\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26328, smth: 0.26328:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26328, smth: 0.26328:  12%|        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.25817, smth: 0.26073:  12%|        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.25817, smth: 0.26073:  25%|       | 2/8 [00:03<00:13,  2.28s/it]\u001b[A\n","loss: 0.18886, smth: 0.23677:  25%|       | 2/8 [00:04<00:13,  2.28s/it]\u001b[A\n","loss: 0.18886, smth: 0.23677:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.20653, smth: 0.22921:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.20653, smth: 0.22921:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.16228, smth: 0.21582:  50%|     | 4/8 [00:07<00:06,  1.64s/it]\u001b[A\n","loss: 0.16228, smth: 0.21582:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.12726, smth: 0.20106:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.12726, smth: 0.20106:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.17614, smth: 0.19750:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.17614, smth: 0.19750:  88%| | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.16678, smth: 0.19366:  88%| | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.16678, smth: 0.19366: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:42 2021 Epoch 139, lr: 0.0010305, train loss: 0.19366, train auc: 0.99532, val loss: 0.47707, val auc: 0.98679\n","Thu Jun 10 01:25:42 2021 Epoch: 140\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26568, smth: 0.26568:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.26568, smth: 0.26568:  12%|        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.19004, smth: 0.22786:  12%|        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.19004, smth: 0.22786:  25%|       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.19515, smth: 0.21696:  25%|       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.19515, smth: 0.21696:  38%|      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.21131, smth: 0.21555:  38%|      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.21131, smth: 0.21555:  50%|     | 4/8 [00:06<00:06,  1.71s/it]\u001b[A\n","loss: 0.08907, smth: 0.19025:  50%|     | 4/8 [00:07<00:06,  1.71s/it]\u001b[A\n","loss: 0.08907, smth: 0.19025:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.20670, smth: 0.19299:  62%|   | 5/8 [00:08<00:04,  1.59s/it]\u001b[A\n","loss: 0.20670, smth: 0.19299:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.21464, smth: 0.19608:  75%|  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.21464, smth: 0.19608:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.18233, smth: 0.19436:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.18233, smth: 0.19436: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:25:52 2021 Epoch 140, lr: 0.0009955, train loss: 0.19436, train auc: 0.99647, val loss: 0.42942, val auc: 0.98599\n","Thu Jun 10 01:25:52 2021 Epoch: 141\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21119, smth: 0.21119:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21119, smth: 0.21119:  12%|        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.19771, smth: 0.20445:  12%|        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.19771, smth: 0.20445:  25%|       | 2/8 [00:03<00:13,  2.24s/it]\u001b[A\n","loss: 0.12508, smth: 0.17799:  25%|       | 2/8 [00:04<00:13,  2.24s/it]\u001b[A\n","loss: 0.12508, smth: 0.17799:  38%|      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.08335, smth: 0.15433:  38%|      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.08335, smth: 0.15433:  50%|     | 4/8 [00:05<00:06,  1.50s/it]\u001b[A\n","loss: 0.12205, smth: 0.14788:  50%|     | 4/8 [00:06<00:06,  1.50s/it]\u001b[A\n","loss: 0.12205, smth: 0.14788:  62%|   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.16035, smth: 0.14996:  62%|   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.16035, smth: 0.14996:  75%|  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 0.19764, smth: 0.15677:  75%|  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 0.19764, smth: 0.15677:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.12160, smth: 0.15237:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.12160, smth: 0.15237: 100%|| 8/8 [00:08<00:00,  1.10s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:02 2021 Epoch 141, lr: 0.0009608, train loss: 0.15237, train auc: 0.99643, val loss: 0.40550, val auc: 0.98620\n","Thu Jun 10 01:26:02 2021 Epoch: 142\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.09275, smth: 0.09275:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.09275, smth: 0.09275:  12%|        | 1/8 [00:02<00:19,  2.83s/it]\u001b[A\n","loss: 0.10690, smth: 0.09983:  12%|        | 1/8 [00:03<00:19,  2.83s/it]\u001b[A\n","loss: 0.10690, smth: 0.09983:  25%|       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.20480, smth: 0.13482:  25%|       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.20480, smth: 0.13482:  38%|      | 3/8 [00:04<00:09,  1.95s/it]\u001b[A\n","loss: 0.26460, smth: 0.16726:  38%|      | 3/8 [00:05<00:09,  1.95s/it]\u001b[A\n","loss: 0.26460, smth: 0.16726:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.24206, smth: 0.18222:  50%|     | 4/8 [00:07<00:06,  1.61s/it]\u001b[A\n","loss: 0.24206, smth: 0.18222:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.17065, smth: 0.18029:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.17065, smth: 0.18029:  75%|  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.23666, smth: 0.18834:  75%|  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.23666, smth: 0.18834:  88%| | 7/8 [00:09<00:01,  1.39s/it]\u001b[A\n","loss: 0.24436, smth: 0.19535:  88%| | 7/8 [00:10<00:01,  1.39s/it]\u001b[A\n","loss: 0.24436, smth: 0.19535: 100%|| 8/8 [00:10<00:00,  1.28s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.00it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.63it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:12 2021 Epoch 142, lr: 0.0009267, train loss: 0.19535, train auc: 0.99617, val loss: 0.41284, val auc: 0.98605\n","Thu Jun 10 01:26:12 2021 Epoch: 143\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11698, smth: 0.11698:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.11698, smth: 0.11698:  12%|        | 1/8 [00:03<00:22,  3.17s/it]\u001b[A\n","loss: 0.12477, smth: 0.12088:  12%|        | 1/8 [00:03<00:22,  3.17s/it]\u001b[A\n","loss: 0.12477, smth: 0.12088:  25%|       | 2/8 [00:03<00:14,  2.44s/it]\u001b[A\n","loss: 0.17312, smth: 0.13829:  25%|       | 2/8 [00:05<00:14,  2.44s/it]\u001b[A\n","loss: 0.17312, smth: 0.13829:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.14521, smth: 0.14002:  38%|      | 3/8 [00:05<00:10,  2.11s/it]\u001b[A\n","loss: 0.14521, smth: 0.14002:  50%|     | 4/8 [00:06<00:06,  1.72s/it]\u001b[A\n","loss: 0.12792, smth: 0.13760:  50%|     | 4/8 [00:08<00:06,  1.72s/it]\u001b[A\n","loss: 0.12792, smth: 0.13760:  62%|   | 5/8 [00:08<00:05,  1.82s/it]\u001b[A\n","loss: 0.12467, smth: 0.13545:  62%|   | 5/8 [00:08<00:05,  1.82s/it]\u001b[A\n","loss: 0.12467, smth: 0.13545:  75%|  | 6/8 [00:08<00:02,  1.48s/it]\u001b[A\n","loss: 0.15826, smth: 0.13871:  75%|  | 6/8 [00:09<00:02,  1.48s/it]\u001b[A\n","loss: 0.15826, smth: 0.13871:  88%| | 7/8 [00:09<00:01,  1.33s/it]\u001b[A\n","loss: 0.21671, smth: 0.14846:  88%| | 7/8 [00:10<00:01,  1.33s/it]\u001b[A\n","loss: 0.21671, smth: 0.14846: 100%|| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:23 2021 Epoch 143, lr: 0.0008930, train loss: 0.14846, train auc: 0.99712, val loss: 0.39224, val auc: 0.98844\n","Thu Jun 10 01:26:23 2021 Epoch: 144\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.22539, smth: 0.22539:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.22539, smth: 0.22539:  12%|        | 1/8 [00:02<00:19,  2.85s/it]\u001b[A\n","loss: 0.11380, smth: 0.16959:  12%|        | 1/8 [00:03<00:19,  2.85s/it]\u001b[A\n","loss: 0.11380, smth: 0.16959:  25%|       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.14463, smth: 0.16127:  25%|       | 2/8 [00:05<00:13,  2.23s/it]\u001b[A\n","loss: 0.14463, smth: 0.16127:  38%|      | 3/8 [00:05<00:10,  2.10s/it]\u001b[A\n","loss: 0.17018, smth: 0.16350:  38%|      | 3/8 [00:06<00:10,  2.10s/it]\u001b[A\n","loss: 0.17018, smth: 0.16350:  50%|     | 4/8 [00:06<00:06,  1.70s/it]\u001b[A\n","loss: 0.11902, smth: 0.15460:  50%|     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.11902, smth: 0.15460:  62%|   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.05156, smth: 0.13743:  62%|   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.05156, smth: 0.13743:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.17463, smth: 0.14274:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.17463, smth: 0.14274:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.25899, smth: 0.15727:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.25899, smth: 0.15727: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:33 2021 Epoch 144, lr: 0.0008599, train loss: 0.15727, train auc: 0.99690, val loss: 0.39985, val auc: 0.98840\n","Thu Jun 10 01:26:33 2021 Epoch: 145\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15824, smth: 0.15824:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15824, smth: 0.15824:  12%|        | 1/8 [00:03<00:21,  3.01s/it]\u001b[A\n","loss: 0.19263, smth: 0.17544:  12%|        | 1/8 [00:03<00:21,  3.01s/it]\u001b[A\n","loss: 0.19263, smth: 0.17544:  25%|       | 2/8 [00:03<00:14,  2.35s/it]\u001b[A\n","loss: 0.14012, smth: 0.16366:  25%|       | 2/8 [00:04<00:14,  2.35s/it]\u001b[A\n","loss: 0.14012, smth: 0.16366:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.17496, smth: 0.16649:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.17496, smth: 0.16649:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.11302, smth: 0.15579:  50%|     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.11302, smth: 0.15579:  62%|   | 5/8 [00:07<00:05,  1.75s/it]\u001b[A\n","loss: 0.17607, smth: 0.15917:  62%|   | 5/8 [00:08<00:05,  1.75s/it]\u001b[A\n","loss: 0.17607, smth: 0.15917:  75%|  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.09735, smth: 0.15034:  75%|  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.09735, smth: 0.15034:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.15013, smth: 0.15031:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.15013, smth: 0.15031: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.69it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:44 2021 Epoch 145, lr: 0.0008272, train loss: 0.15031, train auc: 0.99762, val loss: 0.41492, val auc: 0.98806\n","Thu Jun 10 01:26:44 2021 Epoch: 146\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10220, smth: 0.10220:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10220, smth: 0.10220:  12%|        | 1/8 [00:02<00:17,  2.45s/it]\u001b[A\n","loss: 0.20888, smth: 0.15554:  12%|        | 1/8 [00:03<00:17,  2.45s/it]\u001b[A\n","loss: 0.20888, smth: 0.15554:  25%|       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.17271, smth: 0.16127:  25%|       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.17271, smth: 0.16127:  38%|      | 3/8 [00:04<00:08,  1.74s/it]\u001b[A\n","loss: 0.11571, smth: 0.14988:  38%|      | 3/8 [00:05<00:08,  1.74s/it]\u001b[A\n","loss: 0.11571, smth: 0.14988:  50%|     | 4/8 [00:05<00:05,  1.46s/it]\u001b[A\n","loss: 0.19417, smth: 0.15874:  50%|     | 4/8 [00:07<00:05,  1.46s/it]\u001b[A\n","loss: 0.19417, smth: 0.15874:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25336, smth: 0.17451:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25336, smth: 0.17451:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.08291, smth: 0.16142:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.08291, smth: 0.16142:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.44277, smth: 0.19659:  88%| | 7/8 [00:09<00:01,  1.30s/it]\u001b[A\n","loss: 0.44277, smth: 0.19659: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:26:54 2021 Epoch 146, lr: 0.0007950, train loss: 0.19659, train auc: 0.99719, val loss: 0.42819, val auc: 0.98745\n","Thu Jun 10 01:26:54 2021 Epoch: 147\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.34373, smth: 0.34373:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.34373, smth: 0.34373:  12%|        | 1/8 [00:02<00:18,  2.66s/it]\u001b[A\n","loss: 0.12376, smth: 0.23374:  12%|        | 1/8 [00:03<00:18,  2.66s/it]\u001b[A\n","loss: 0.12376, smth: 0.23374:  25%|       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.09720, smth: 0.18823:  25%|       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.09720, smth: 0.18823:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.14639, smth: 0.17777:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.14639, smth: 0.17777:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.07580, smth: 0.15737:  50%|     | 4/8 [00:06<00:06,  1.63s/it]\u001b[A\n","loss: 0.07580, smth: 0.15737:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.16474, smth: 0.15860:  62%|   | 5/8 [00:08<00:04,  1.52s/it]\u001b[A\n","loss: 0.16474, smth: 0.15860:  75%|  | 6/8 [00:08<00:02,  1.42s/it]\u001b[A\n","loss: 0.20102, smth: 0.16466:  75%|  | 6/8 [00:09<00:02,  1.42s/it]\u001b[A\n","loss: 0.20102, smth: 0.16466:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23212, smth: 0.17309:  88%| | 7/8 [00:09<00:01,  1.29s/it]\u001b[A\n","loss: 0.23212, smth: 0.17309: 100%|| 8/8 [00:09<00:00,  1.22s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:04 2021 Epoch 147, lr: 0.0007634, train loss: 0.17309, train auc: 0.99682, val loss: 0.38738, val auc: 0.98775\n","Thu Jun 10 01:27:04 2021 Epoch: 148\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.25674, smth: 0.25674:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.25674, smth: 0.25674:  12%|        | 1/8 [00:02<00:20,  2.96s/it]\u001b[A\n","loss: 0.12467, smth: 0.19070:  12%|        | 1/8 [00:03<00:20,  2.96s/it]\u001b[A\n","loss: 0.12467, smth: 0.19070:  25%|       | 2/8 [00:03<00:13,  2.31s/it]\u001b[A\n","loss: 0.20261, smth: 0.19467:  25%|       | 2/8 [00:04<00:13,  2.31s/it]\u001b[A\n","loss: 0.20261, smth: 0.19467:  38%|      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.17202, smth: 0.18901:  38%|      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.17202, smth: 0.18901:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.09184, smth: 0.16957:  50%|     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\n","loss: 0.09184, smth: 0.16957:  62%|   | 5/8 [00:06<00:04,  1.49s/it]\u001b[A\n","loss: 0.20696, smth: 0.17580:  62%|   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.20696, smth: 0.17580:  75%|  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.18760, smth: 0.17749:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.18760, smth: 0.17749:  88%| | 7/8 [00:08<00:01,  1.28s/it]\u001b[A\n","loss: 0.07054, smth: 0.16412:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.07054, smth: 0.16412: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:14 2021 Epoch 148, lr: 0.0007322, train loss: 0.16412, train auc: 0.99677, val loss: 0.39121, val auc: 0.98626\n","Thu Jun 10 01:27:14 2021 Epoch: 149\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18152, smth: 0.18152:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18152, smth: 0.18152:  12%|        | 1/8 [00:02<00:20,  2.94s/it]\u001b[A\n","loss: 0.15417, smth: 0.16784:  12%|        | 1/8 [00:03<00:20,  2.94s/it]\u001b[A\n","loss: 0.15417, smth: 0.16784:  25%|       | 2/8 [00:03<00:13,  2.29s/it]\u001b[A\n","loss: 0.13157, smth: 0.15575:  25%|       | 2/8 [00:05<00:13,  2.29s/it]\u001b[A\n","loss: 0.13157, smth: 0.15575:  38%|      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.20636, smth: 0.16840:  38%|      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.20636, smth: 0.16840:  50%|     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.16244, smth: 0.16721:  50%|     | 4/8 [00:07<00:06,  1.68s/it]\u001b[A\n","loss: 0.16244, smth: 0.16721:  62%|   | 5/8 [00:07<00:05,  1.73s/it]\u001b[A\n","loss: 0.27918, smth: 0.18587:  62%|   | 5/8 [00:08<00:05,  1.73s/it]\u001b[A\n","loss: 0.27918, smth: 0.18587:  75%|  | 6/8 [00:08<00:02,  1.44s/it]\u001b[A\n","loss: 0.14854, smth: 0.18054:  75%|  | 6/8 [00:09<00:02,  1.44s/it]\u001b[A\n","loss: 0.14854, smth: 0.18054:  88%| | 7/8 [00:09<00:01,  1.36s/it]\u001b[A\n","loss: 0.22020, smth: 0.18549:  88%| | 7/8 [00:10<00:01,  1.36s/it]\u001b[A\n","loss: 0.22020, smth: 0.18549: 100%|| 8/8 [00:10<00:00,  1.29s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:25 2021 Epoch 149, lr: 0.0007017, train loss: 0.18549, train auc: 0.99659, val loss: 0.38954, val auc: 0.98653\n","Thu Jun 10 01:27:25 2021 Epoch: 150\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12492, smth: 0.12492:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12492, smth: 0.12492:  12%|        | 1/8 [00:02<00:15,  2.28s/it]\u001b[A\n","loss: 0.18185, smth: 0.15339:  12%|        | 1/8 [00:02<00:15,  2.28s/it]\u001b[A\n","loss: 0.18185, smth: 0.15339:  25%|       | 2/8 [00:03<00:11,  1.83s/it]\u001b[A\n","loss: 0.24930, smth: 0.18536:  25%|       | 2/8 [00:04<00:11,  1.83s/it]\u001b[A\n","loss: 0.24930, smth: 0.18536:  38%|      | 3/8 [00:04<00:08,  1.71s/it]\u001b[A\n","loss: 0.08679, smth: 0.16072:  38%|      | 3/8 [00:05<00:08,  1.71s/it]\u001b[A\n","loss: 0.08679, smth: 0.16072:  50%|     | 4/8 [00:05<00:05,  1.44s/it]\u001b[A\n","loss: 0.13094, smth: 0.15476:  50%|     | 4/8 [00:06<00:05,  1.44s/it]\u001b[A\n","loss: 0.13094, smth: 0.15476:  62%|   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.10279, smth: 0.14610:  62%|   | 5/8 [00:07<00:04,  1.39s/it]\u001b[A\n","loss: 0.10279, smth: 0.14610:  75%|  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.13058, smth: 0.14388:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.13058, smth: 0.14388:  88%| | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.43128, smth: 0.17981:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.43128, smth: 0.17981: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.54it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:35 2021 Epoch 150, lr: 0.0006716, train loss: 0.17981, train auc: 0.99592, val loss: 0.37742, val auc: 0.98789\n","Thu Jun 10 01:27:35 2021 Epoch: 151\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.23775, smth: 0.23775:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.23775, smth: 0.23775:  12%|        | 1/8 [00:03<00:21,  3.13s/it]\u001b[A\n","loss: 0.08748, smth: 0.16262:  12%|        | 1/8 [00:03<00:21,  3.13s/it]\u001b[A\n","loss: 0.08748, smth: 0.16262:  25%|       | 2/8 [00:03<00:14,  2.43s/it]\u001b[A\n","loss: 0.22493, smth: 0.18339:  25%|       | 2/8 [00:05<00:14,  2.43s/it]\u001b[A\n","loss: 0.22493, smth: 0.18339:  38%|      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.12755, smth: 0.16943:  38%|      | 3/8 [00:05<00:10,  2.08s/it]\u001b[A\n","loss: 0.12755, smth: 0.16943:  50%|     | 4/8 [00:05<00:06,  1.69s/it]\u001b[A\n","loss: 0.15555, smth: 0.16665:  50%|     | 4/8 [00:07<00:06,  1.69s/it]\u001b[A\n","loss: 0.15555, smth: 0.16665:  62%|   | 5/8 [00:07<00:04,  1.64s/it]\u001b[A\n","loss: 0.16481, smth: 0.16635:  62%|   | 5/8 [00:08<00:04,  1.64s/it]\u001b[A\n","loss: 0.16481, smth: 0.16635:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.13228, smth: 0.16148:  75%|  | 6/8 [00:09<00:02,  1.36s/it]\u001b[A\n","loss: 0.13228, smth: 0.16148:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.17020, smth: 0.16257:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.17020, smth: 0.16257: 100%|| 8/8 [00:09<00:00,  1.20s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.65it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:45 2021 Epoch 151, lr: 0.0006421, train loss: 0.16257, train auc: 0.99774, val loss: 0.42239, val auc: 0.98572\n","Thu Jun 10 01:27:46 2021 Epoch: 152\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.31209, smth: 0.31209:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.31209, smth: 0.31209:  12%|        | 1/8 [00:02<00:19,  2.73s/it]\u001b[A\n","loss: 0.18052, smth: 0.24630:  12%|        | 1/8 [00:03<00:19,  2.73s/it]\u001b[A\n","loss: 0.18052, smth: 0.24630:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.16876, smth: 0.22045:  25%|       | 2/8 [00:05<00:12,  2.14s/it]\u001b[A\n","loss: 0.16876, smth: 0.22045:  38%|      | 3/8 [00:05<00:10,  2.04s/it]\u001b[A\n","loss: 0.17460, smth: 0.20899:  38%|      | 3/8 [00:05<00:10,  2.04s/it]\u001b[A\n","loss: 0.17460, smth: 0.20899:  50%|     | 4/8 [00:06<00:06,  1.66s/it]\u001b[A\n","loss: 0.15700, smth: 0.19859:  50%|     | 4/8 [00:08<00:06,  1.66s/it]\u001b[A\n","loss: 0.15700, smth: 0.19859:  62%|   | 5/8 [00:08<00:05,  1.79s/it]\u001b[A\n","loss: 0.10349, smth: 0.18274:  62%|   | 5/8 [00:08<00:05,  1.79s/it]\u001b[A\n","loss: 0.10349, smth: 0.18274:  75%|  | 6/8 [00:08<00:02,  1.47s/it]\u001b[A\n","loss: 0.11430, smth: 0.17296:  75%|  | 6/8 [00:09<00:02,  1.47s/it]\u001b[A\n","loss: 0.11430, smth: 0.17296:  88%| | 7/8 [00:10<00:01,  1.37s/it]\u001b[A\n","loss: 0.13661, smth: 0.16842:  88%| | 7/8 [00:10<00:01,  1.37s/it]\u001b[A\n","loss: 0.13661, smth: 0.16842: 100%|| 8/8 [00:10<00:00,  1.32s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.02it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:27:57 2021 Epoch 152, lr: 0.0006132, train loss: 0.16842, train auc: 0.99698, val loss: 0.42443, val auc: 0.98604\n","Thu Jun 10 01:27:57 2021 Epoch: 153\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13147, smth: 0.13147:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13147, smth: 0.13147:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.07546, smth: 0.10347:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.07546, smth: 0.10347:  25%|       | 2/8 [00:03<00:13,  2.23s/it]\u001b[A\n","loss: 0.13826, smth: 0.11506:  25%|       | 2/8 [00:04<00:13,  2.23s/it]\u001b[A\n","loss: 0.13826, smth: 0.11506:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.16947, smth: 0.12867:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.16947, smth: 0.12867:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.16262, smth: 0.13546:  50%|     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.16262, smth: 0.13546:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.18648, smth: 0.14396:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.18648, smth: 0.14396:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.12006, smth: 0.14055:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.12006, smth: 0.14055:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.12013, smth: 0.13799:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.12013, smth: 0.13799: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:07 2021 Epoch 153, lr: 0.0005849, train loss: 0.13799, train auc: 0.99748, val loss: 0.40105, val auc: 0.98739\n","Thu Jun 10 01:28:07 2021 Epoch: 154\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14928, smth: 0.14928:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14928, smth: 0.14928:  12%|        | 1/8 [00:02<00:20,  2.97s/it]\u001b[A\n","loss: 0.09169, smth: 0.12049:  12%|        | 1/8 [00:03<00:20,  2.97s/it]\u001b[A\n","loss: 0.09169, smth: 0.12049:  25%|       | 2/8 [00:03<00:13,  2.30s/it]\u001b[A\n","loss: 0.18555, smth: 0.14217:  25%|       | 2/8 [00:05<00:13,  2.30s/it]\u001b[A\n","loss: 0.18555, smth: 0.14217:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.23067, smth: 0.16430:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.23067, smth: 0.16430:  50%|     | 4/8 [00:05<00:06,  1.66s/it]\u001b[A\n","loss: 0.12723, smth: 0.15688:  50%|     | 4/8 [00:07<00:06,  1.66s/it]\u001b[A\n","loss: 0.12723, smth: 0.15688:  62%|   | 5/8 [00:07<00:04,  1.53s/it]\u001b[A\n","loss: 0.13577, smth: 0.15336:  62%|   | 5/8 [00:08<00:04,  1.53s/it]\u001b[A\n","loss: 0.13577, smth: 0.15336:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.12851, smth: 0.14981:  75%|  | 6/8 [00:09<00:02,  1.38s/it]\u001b[A\n","loss: 0.12851, smth: 0.14981:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.16048, smth: 0.15115:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.16048, smth: 0.15115: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:17 2021 Epoch 154, lr: 0.0005571, train loss: 0.15115, train auc: 0.99768, val loss: 0.39047, val auc: 0.98818\n","Thu Jun 10 01:28:17 2021 Epoch: 155\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.20465, smth: 0.20465:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.20465, smth: 0.20465:  12%|        | 1/8 [00:03<00:23,  3.34s/it]\u001b[A\n","loss: 0.18594, smth: 0.19530:  12%|        | 1/8 [00:03<00:23,  3.34s/it]\u001b[A\n","loss: 0.18594, smth: 0.19530:  25%|       | 2/8 [00:04<00:15,  2.57s/it]\u001b[A\n","loss: 0.13071, smth: 0.17377:  25%|       | 2/8 [00:05<00:15,  2.57s/it]\u001b[A\n","loss: 0.13071, smth: 0.17377:  38%|      | 3/8 [00:05<00:11,  2.23s/it]\u001b[A\n","loss: 0.13545, smth: 0.16419:  38%|      | 3/8 [00:06<00:11,  2.23s/it]\u001b[A\n","loss: 0.13545, smth: 0.16419:  50%|     | 4/8 [00:06<00:07,  1.80s/it]\u001b[A\n","loss: 0.14287, smth: 0.15992:  50%|     | 4/8 [00:07<00:07,  1.80s/it]\u001b[A\n","loss: 0.14287, smth: 0.15992:  62%|   | 5/8 [00:07<00:04,  1.66s/it]\u001b[A\n","loss: 0.09597, smth: 0.14927:  62%|   | 5/8 [00:08<00:04,  1.66s/it]\u001b[A\n","loss: 0.09597, smth: 0.14927:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.15991, smth: 0.15079:  75%|  | 6/8 [00:09<00:02,  1.37s/it]\u001b[A\n","loss: 0.15991, smth: 0.15079:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.25784, smth: 0.16417:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.25784, smth: 0.16417: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:27 2021 Epoch 155, lr: 0.0005300, train loss: 0.16417, train auc: 0.99682, val loss: 0.41544, val auc: 0.98686\n","Thu Jun 10 01:28:27 2021 Epoch: 156\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11765, smth: 0.11765:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.11765, smth: 0.11765:  12%|        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.17083, smth: 0.14424:  12%|        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.17083, smth: 0.14424:  25%|       | 2/8 [00:04<00:15,  2.51s/it]\u001b[A\n","loss: 0.24607, smth: 0.17818:  25%|       | 2/8 [00:05<00:15,  2.51s/it]\u001b[A\n","loss: 0.24607, smth: 0.17818:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.10413, smth: 0.15967:  38%|      | 3/8 [00:05<00:10,  2.09s/it]\u001b[A\n","loss: 0.10413, smth: 0.15967:  50%|     | 4/8 [00:05<00:06,  1.70s/it]\u001b[A\n","loss: 0.21883, smth: 0.17150:  50%|     | 4/8 [00:07<00:06,  1.70s/it]\u001b[A\n","loss: 0.21883, smth: 0.17150:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.13722, smth: 0.16579:  62%|   | 5/8 [00:07<00:04,  1.59s/it]\u001b[A\n","loss: 0.13722, smth: 0.16579:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.13460, smth: 0.16133:  75%|  | 6/8 [00:09<00:02,  1.33s/it]\u001b[A\n","loss: 0.13460, smth: 0.16133:  88%| | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.08201, smth: 0.15142:  88%| | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\n","loss: 0.08201, smth: 0.15142: 100%|| 8/8 [00:09<00:00,  1.24s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:38 2021 Epoch 156, lr: 0.0005034, train loss: 0.15142, train auc: 0.99744, val loss: 0.42526, val auc: 0.98642\n","Thu Jun 10 01:28:38 2021 Epoch: 157\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18129, smth: 0.18129:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18129, smth: 0.18129:  12%|        | 1/8 [00:02<00:18,  2.60s/it]\u001b[A\n","loss: 0.09824, smth: 0.13977:  12%|        | 1/8 [00:03<00:18,  2.60s/it]\u001b[A\n","loss: 0.09824, smth: 0.13977:  25%|       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.12510, smth: 0.13488:  25%|       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.12510, smth: 0.13488:  38%|      | 3/8 [00:04<00:08,  1.70s/it]\u001b[A\n","loss: 0.06927, smth: 0.11848:  38%|      | 3/8 [00:05<00:08,  1.70s/it]\u001b[A\n","loss: 0.06927, smth: 0.11848:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.15092, smth: 0.12497:  50%|     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.15092, smth: 0.12497:  62%|   | 5/8 [00:06<00:04,  1.40s/it]\u001b[A\n","loss: 0.05510, smth: 0.11332:  62%|   | 5/8 [00:07<00:04,  1.40s/it]\u001b[A\n","loss: 0.05510, smth: 0.11332:  75%|  | 6/8 [00:07<00:02,  1.38s/it]\u001b[A\n","loss: 0.10885, smth: 0.11268:  75%|  | 6/8 [00:08<00:02,  1.38s/it]\u001b[A\n","loss: 0.10885, smth: 0.11268:  88%| | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.16707, smth: 0.11948:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.16707, smth: 0.11948: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:48 2021 Epoch 157, lr: 0.0004775, train loss: 0.11948, train auc: 0.99832, val loss: 0.40530, val auc: 0.98776\n","Thu Jun 10 01:28:48 2021 Epoch: 158\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17455, smth: 0.17455:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17455, smth: 0.17455:  12%|        | 1/8 [00:02<00:17,  2.51s/it]\u001b[A\n","loss: 0.18603, smth: 0.18029:  12%|        | 1/8 [00:03<00:17,  2.51s/it]\u001b[A\n","loss: 0.18603, smth: 0.18029:  25%|       | 2/8 [00:03<00:11,  2.00s/it]\u001b[A\n","loss: 0.20528, smth: 0.18862:  25%|       | 2/8 [00:04<00:11,  2.00s/it]\u001b[A\n","loss: 0.20528, smth: 0.18862:  38%|      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.14396, smth: 0.17745:  38%|      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.14396, smth: 0.17745:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.21772, smth: 0.18551:  50%|     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.21772, smth: 0.18551:  62%|   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.10678, smth: 0.17239:  62%|   | 5/8 [00:07<00:04,  1.39s/it]\u001b[A\n","loss: 0.10678, smth: 0.17239:  75%|  | 6/8 [00:07<00:02,  1.18s/it]\u001b[A\n","loss: 0.10046, smth: 0.16211:  75%|  | 6/8 [00:08<00:02,  1.18s/it]\u001b[A\n","loss: 0.10046, smth: 0.16211:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.26906, smth: 0.17548:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.26906, smth: 0.17548: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:28:57 2021 Epoch 158, lr: 0.0004521, train loss: 0.17548, train auc: 0.99770, val loss: 0.43307, val auc: 0.98606\n","Thu Jun 10 01:28:57 2021 Epoch: 159\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16595, smth: 0.16595:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16595, smth: 0.16595:  12%|        | 1/8 [00:02<00:20,  2.87s/it]\u001b[A\n","loss: 0.09170, smth: 0.12882:  12%|        | 1/8 [00:03<00:20,  2.87s/it]\u001b[A\n","loss: 0.09170, smth: 0.12882:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.09601, smth: 0.11789:  25%|       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.09601, smth: 0.11789:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.18445, smth: 0.13453:  38%|      | 3/8 [00:05<00:09,  1.99s/it]\u001b[A\n","loss: 0.18445, smth: 0.13453:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.20636, smth: 0.14889:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.20636, smth: 0.14889:  62%|   | 5/8 [00:07<00:04,  1.62s/it]\u001b[A\n","loss: 0.20376, smth: 0.15804:  62%|   | 5/8 [00:08<00:04,  1.62s/it]\u001b[A\n","loss: 0.20376, smth: 0.15804:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37308, smth: 0.18876:  75%|  | 6/8 [00:08<00:02,  1.37s/it]\u001b[A\n","loss: 0.37308, smth: 0.18876:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.18590, smth: 0.18840:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.18590, smth: 0.18840: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:07 2021 Epoch 159, lr: 0.0004274, train loss: 0.18840, train auc: 0.99730, val loss: 0.42855, val auc: 0.98634\n","Thu Jun 10 01:29:07 2021 Epoch: 160\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.05978, smth: 0.05978:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.05978, smth: 0.05978:  12%|        | 1/8 [00:03<00:21,  3.12s/it]\u001b[A\n","loss: 0.25872, smth: 0.15925:  12%|        | 1/8 [00:03<00:21,  3.12s/it]\u001b[A\n","loss: 0.25872, smth: 0.15925:  25%|       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.15344, smth: 0.15731:  25%|       | 2/8 [00:04<00:14,  2.40s/it]\u001b[A\n","loss: 0.15344, smth: 0.15731:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.10755, smth: 0.14487:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.10755, smth: 0.14487:  50%|     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.13133, smth: 0.14216:  50%|     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.13133, smth: 0.14216:  62%|   | 5/8 [00:06<00:04,  1.45s/it]\u001b[A\n","loss: 0.21497, smth: 0.15430:  62%|   | 5/8 [00:07<00:04,  1.45s/it]\u001b[A\n","loss: 0.21497, smth: 0.15430:  75%|  | 6/8 [00:07<00:02,  1.36s/it]\u001b[A\n","loss: 0.14910, smth: 0.15356:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.14910, smth: 0.15356:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.11717, smth: 0.14901:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.11717, smth: 0.14901: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:17 2021 Epoch 160, lr: 0.0004033, train loss: 0.14901, train auc: 0.99817, val loss: 0.45808, val auc: 0.98601\n","Thu Jun 10 01:29:17 2021 Epoch: 161\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15011, smth: 0.15011:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15011, smth: 0.15011:  12%|        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.19186, smth: 0.17099:  12%|        | 1/8 [00:03<00:21,  3.08s/it]\u001b[A\n","loss: 0.19186, smth: 0.17099:  25%|       | 2/8 [00:03<00:14,  2.40s/it]\u001b[A\n","loss: 0.23841, smth: 0.19346:  25%|       | 2/8 [00:04<00:14,  2.40s/it]\u001b[A\n","loss: 0.23841, smth: 0.19346:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.12333, smth: 0.17593:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.12333, smth: 0.17593:  50%|     | 4/8 [00:05<00:06,  1.65s/it]\u001b[A\n","loss: 0.12436, smth: 0.16562:  50%|     | 4/8 [00:07<00:06,  1.65s/it]\u001b[A\n","loss: 0.12436, smth: 0.16562:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.10634, smth: 0.15574:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.10634, smth: 0.15574:  75%|  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.10841, smth: 0.14898:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.10841, smth: 0.14898:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.24309, smth: 0.16074:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.24309, smth: 0.16074: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.01it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.62it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:27 2021 Epoch 161, lr: 0.0003799, train loss: 0.16074, train auc: 0.99801, val loss: 0.42621, val auc: 0.98648\n","Thu Jun 10 01:29:27 2021 Epoch: 162\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17433, smth: 0.17433:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17433, smth: 0.17433:  12%|        | 1/8 [00:02<00:19,  2.82s/it]\u001b[A\n","loss: 0.11626, smth: 0.14530:  12%|        | 1/8 [00:03<00:19,  2.82s/it]\u001b[A\n","loss: 0.11626, smth: 0.14530:  25%|       | 2/8 [00:03<00:13,  2.21s/it]\u001b[A\n","loss: 0.21322, smth: 0.16794:  25%|       | 2/8 [00:04<00:13,  2.21s/it]\u001b[A\n","loss: 0.21322, smth: 0.16794:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.15383, smth: 0.16441:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.15383, smth: 0.16441:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.17023, smth: 0.16557:  50%|     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.17023, smth: 0.16557:  62%|   | 5/8 [00:06<00:04,  1.45s/it]\u001b[A\n","loss: 0.15574, smth: 0.16393:  62%|   | 5/8 [00:07<00:04,  1.45s/it]\u001b[A\n","loss: 0.15574, smth: 0.16393:  75%|  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.23092, smth: 0.17350:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.23092, smth: 0.17350:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.17951, smth: 0.17426:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.17951, smth: 0.17426: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.20it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.80it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:37 2021 Epoch 162, lr: 0.0003571, train loss: 0.17426, train auc: 0.99765, val loss: 0.39325, val auc: 0.98725\n","Thu Jun 10 01:29:37 2021 Epoch: 163\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.21194, smth: 0.21194:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.21194, smth: 0.21194:  12%|        | 1/8 [00:02<00:18,  2.67s/it]\u001b[A\n","loss: 0.16229, smth: 0.18712:  12%|        | 1/8 [00:03<00:18,  2.67s/it]\u001b[A\n","loss: 0.16229, smth: 0.18712:  25%|       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.25829, smth: 0.21084:  25%|       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.25829, smth: 0.21084:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.06937, smth: 0.17547:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.06937, smth: 0.17547:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.08162, smth: 0.15670:  50%|     | 4/8 [00:06<00:06,  1.60s/it]\u001b[A\n","loss: 0.08162, smth: 0.15670:  62%|   | 5/8 [00:06<00:04,  1.44s/it]\u001b[A\n","loss: 0.12643, smth: 0.15166:  62%|   | 5/8 [00:07<00:04,  1.44s/it]\u001b[A\n","loss: 0.12643, smth: 0.15166:  75%|  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 0.11691, smth: 0.14669:  75%|  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 0.11691, smth: 0.14669:  88%| | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n","loss: 0.11656, smth: 0.14293:  88%| | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n","loss: 0.11656, smth: 0.14293: 100%|| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:46 2021 Epoch 163, lr: 0.0003349, train loss: 0.14293, train auc: 0.99714, val loss: 0.41691, val auc: 0.98671\n","Thu Jun 10 01:29:46 2021 Epoch: 164\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14145, smth: 0.14145:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14145, smth: 0.14145:  12%|        | 1/8 [00:02<00:19,  2.80s/it]\u001b[A\n","loss: 0.17760, smth: 0.15952:  12%|        | 1/8 [00:03<00:19,  2.80s/it]\u001b[A\n","loss: 0.17760, smth: 0.15952:  25%|       | 2/8 [00:03<00:13,  2.19s/it]\u001b[A\n","loss: 0.12421, smth: 0.14775:  25%|       | 2/8 [00:04<00:13,  2.19s/it]\u001b[A\n","loss: 0.12421, smth: 0.14775:  38%|      | 3/8 [00:04<00:09,  1.91s/it]\u001b[A\n","loss: 0.25496, smth: 0.17455:  38%|      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.25496, smth: 0.17455:  50%|     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.10863, smth: 0.16137:  50%|     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.10863, smth: 0.16137:  62%|   | 5/8 [00:06<00:04,  1.47s/it]\u001b[A\n","loss: 0.21128, smth: 0.16969:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.21128, smth: 0.16969:  75%|  | 6/8 [00:07<00:02,  1.23s/it]\u001b[A\n","loss: 0.10825, smth: 0.16091:  75%|  | 6/8 [00:08<00:02,  1.23s/it]\u001b[A\n","loss: 0.10825, smth: 0.16091:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.21528, smth: 0.16771:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.21528, smth: 0.16771: 100%|| 8/8 [00:08<00:00,  1.12s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:29:56 2021 Epoch 164, lr: 0.0003135, train loss: 0.16771, train auc: 0.99742, val loss: 0.40619, val auc: 0.98658\n","Thu Jun 10 01:29:56 2021 Epoch: 165\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18675, smth: 0.18675:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18675, smth: 0.18675:  12%|        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 0.10816, smth: 0.14745:  12%|        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 0.10816, smth: 0.14745:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.12837, smth: 0.14109:  25%|       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.12837, smth: 0.14109:  38%|      | 3/8 [00:04<00:09,  1.90s/it]\u001b[A\n","loss: 0.18167, smth: 0.15124:  38%|      | 3/8 [00:05<00:09,  1.90s/it]\u001b[A\n","loss: 0.18167, smth: 0.15124:  50%|     | 4/8 [00:05<00:06,  1.55s/it]\u001b[A\n","loss: 0.11418, smth: 0.14382:  50%|     | 4/8 [00:07<00:06,  1.55s/it]\u001b[A\n","loss: 0.11418, smth: 0.14382:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.14832, smth: 0.14457:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.14832, smth: 0.14457:  75%|  | 6/8 [00:07<00:02,  1.30s/it]\u001b[A\n","loss: 0.11832, smth: 0.14082:  75%|  | 6/8 [00:08<00:02,  1.30s/it]\u001b[A\n","loss: 0.11832, smth: 0.14082:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.24058, smth: 0.15329:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.24058, smth: 0.15329: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.91it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:05 2021 Epoch 165, lr: 0.0002926, train loss: 0.15329, train auc: 0.99748, val loss: 0.42259, val auc: 0.98630\n","Thu Jun 10 01:30:05 2021 Epoch: 166\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16013, smth: 0.16013:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16013, smth: 0.16013:  12%|        | 1/8 [00:02<00:19,  2.75s/it]\u001b[A\n","loss: 0.10616, smth: 0.13315:  12%|        | 1/8 [00:03<00:19,  2.75s/it]\u001b[A\n","loss: 0.10616, smth: 0.13315:  25%|       | 2/8 [00:03<00:12,  2.15s/it]\u001b[A\n","loss: 0.20311, smth: 0.15647:  25%|       | 2/8 [00:04<00:12,  2.15s/it]\u001b[A\n","loss: 0.20311, smth: 0.15647:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.15022, smth: 0.15491:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.15022, smth: 0.15491:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.10059, smth: 0.14404:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.10059, smth: 0.14404:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.17702, smth: 0.14954:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.17702, smth: 0.14954:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.19346, smth: 0.15581:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.19346, smth: 0.15581:  88%| | 7/8 [00:08<00:01,  1.28s/it]\u001b[A\n","loss: 0.07390, smth: 0.14557:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.07390, smth: 0.14557: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.09it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:15 2021 Epoch 166, lr: 0.0002725, train loss: 0.14557, train auc: 0.99727, val loss: 0.44077, val auc: 0.98604\n","Thu Jun 10 01:30:15 2021 Epoch: 167\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12597, smth: 0.12597:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12597, smth: 0.12597:  12%|        | 1/8 [00:02<00:18,  2.60s/it]\u001b[A\n","loss: 0.09721, smth: 0.11159:  12%|        | 1/8 [00:03<00:18,  2.60s/it]\u001b[A\n","loss: 0.09721, smth: 0.11159:  25%|       | 2/8 [00:03<00:12,  2.04s/it]\u001b[A\n","loss: 0.06627, smth: 0.09649:  25%|       | 2/8 [00:04<00:12,  2.04s/it]\u001b[A\n","loss: 0.06627, smth: 0.09649:  38%|      | 3/8 [00:04<00:09,  1.89s/it]\u001b[A\n","loss: 0.27604, smth: 0.14137:  38%|      | 3/8 [00:05<00:09,  1.89s/it]\u001b[A\n","loss: 0.27604, smth: 0.14137:  50%|     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.14617, smth: 0.14233:  50%|     | 4/8 [00:07<00:06,  1.57s/it]\u001b[A\n","loss: 0.14617, smth: 0.14233:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.09448, smth: 0.13436:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.09448, smth: 0.13436:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.04615, smth: 0.12176:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.04615, smth: 0.12176:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.05943, smth: 0.11397:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.05943, smth: 0.11397: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:25 2021 Epoch 167, lr: 0.0002530, train loss: 0.11397, train auc: 0.99808, val loss: 0.46053, val auc: 0.98579\n","Thu Jun 10 01:30:25 2021 Epoch: 168\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10238, smth: 0.10238:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10238, smth: 0.10238:  12%|        | 1/8 [00:02<00:20,  2.99s/it]\u001b[A\n","loss: 0.19012, smth: 0.14625:  12%|        | 1/8 [00:03<00:20,  2.99s/it]\u001b[A\n","loss: 0.19012, smth: 0.14625:  25%|       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.17644, smth: 0.15632:  25%|       | 2/8 [00:05<00:13,  2.32s/it]\u001b[A\n","loss: 0.17644, smth: 0.15632:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.18848, smth: 0.16436:  38%|      | 3/8 [00:05<00:10,  2.07s/it]\u001b[A\n","loss: 0.18848, smth: 0.16436:  50%|     | 4/8 [00:05<00:06,  1.67s/it]\u001b[A\n","loss: 0.22597, smth: 0.17668:  50%|     | 4/8 [00:07<00:06,  1.67s/it]\u001b[A\n","loss: 0.22597, smth: 0.17668:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25844, smth: 0.19031:  62%|   | 5/8 [00:07<00:04,  1.56s/it]\u001b[A\n","loss: 0.25844, smth: 0.19031:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.10524, smth: 0.17815:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.10524, smth: 0.17815:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.16396, smth: 0.17638:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.16396, smth: 0.17638: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.21it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:35 2021 Epoch 168, lr: 0.0002342, train loss: 0.17638, train auc: 0.99776, val loss: 0.47086, val auc: 0.98580\n","Thu Jun 10 01:30:35 2021 Epoch: 169\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17320, smth: 0.17320:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17320, smth: 0.17320:  12%|        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.10856, smth: 0.14088:  12%|        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.10856, smth: 0.14088:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.05775, smth: 0.11317:  25%|       | 2/8 [00:04<00:13,  2.25s/it]\u001b[A\n","loss: 0.05775, smth: 0.11317:  38%|      | 3/8 [00:04<00:09,  1.97s/it]\u001b[A\n","loss: 0.08089, smth: 0.10510:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.08089, smth: 0.10510:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.17794, smth: 0.11967:  50%|     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.17794, smth: 0.11967:  62%|   | 5/8 [00:07<00:04,  1.65s/it]\u001b[A\n","loss: 0.12705, smth: 0.12090:  62%|   | 5/8 [00:08<00:04,  1.65s/it]\u001b[A\n","loss: 0.12705, smth: 0.12090:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.10137, smth: 0.11811:  75%|  | 6/8 [00:08<00:02,  1.36s/it]\u001b[A\n","loss: 0.10137, smth: 0.11811:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.16940, smth: 0.12452:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.16940, smth: 0.12452: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:46 2021 Epoch 169, lr: 0.0002161, train loss: 0.12452, train auc: 0.99844, val loss: 0.43357, val auc: 0.98675\n","Thu Jun 10 01:30:46 2021 Epoch: 170\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17333, smth: 0.17333:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17333, smth: 0.17333:  12%|        | 1/8 [00:02<00:18,  2.70s/it]\u001b[A\n","loss: 0.17184, smth: 0.17259:  12%|        | 1/8 [00:03<00:18,  2.70s/it]\u001b[A\n","loss: 0.17184, smth: 0.17259:  25%|       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.21688, smth: 0.18735:  25%|       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.21688, smth: 0.18735:  38%|      | 3/8 [00:04<00:09,  1.94s/it]\u001b[A\n","loss: 0.08344, smth: 0.16137:  38%|      | 3/8 [00:05<00:09,  1.94s/it]\u001b[A\n","loss: 0.08344, smth: 0.16137:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.24841, smth: 0.17878:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.24841, smth: 0.17878:  62%|   | 5/8 [00:06<00:04,  1.41s/it]\u001b[A\n","loss: 0.12654, smth: 0.17007:  62%|   | 5/8 [00:07<00:04,  1.41s/it]\u001b[A\n","loss: 0.12654, smth: 0.17007:  75%|  | 6/8 [00:07<00:02,  1.21s/it]\u001b[A\n","loss: 0.11995, smth: 0.16291:  75%|  | 6/8 [00:08<00:02,  1.21s/it]\u001b[A\n","loss: 0.11995, smth: 0.16291:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.27506, smth: 0.17693:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.27506, smth: 0.17693: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.79it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:30:55 2021 Epoch 170, lr: 0.0001987, train loss: 0.17693, train auc: 0.99828, val loss: 0.41823, val auc: 0.98698\n","Thu Jun 10 01:30:55 2021 Epoch: 171\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.28006, smth: 0.28006:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.28006, smth: 0.28006:  12%|        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.26611, smth: 0.27308:  12%|        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.26611, smth: 0.27308:  25%|       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.08185, smth: 0.20934:  25%|       | 2/8 [00:04<00:13,  2.26s/it]\u001b[A\n","loss: 0.08185, smth: 0.20934:  38%|      | 3/8 [00:04<00:09,  1.97s/it]\u001b[A\n","loss: 0.11097, smth: 0.18474:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.11097, smth: 0.18474:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.13629, smth: 0.17505:  50%|     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.13629, smth: 0.17505:  62%|   | 5/8 [00:06<00:04,  1.49s/it]\u001b[A\n","loss: 0.11988, smth: 0.16586:  62%|   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.11988, smth: 0.16586:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.23689, smth: 0.17600:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.23689, smth: 0.17600:  88%| | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.11294, smth: 0.16812:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.11294, smth: 0.16812: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.86it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.54it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:05 2021 Epoch 171, lr: 0.0001820, train loss: 0.16812, train auc: 0.99850, val loss: 0.40949, val auc: 0.98742\n","Thu Jun 10 01:31:05 2021 Epoch: 172\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11517, smth: 0.11517:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.11517, smth: 0.11517:  12%|        | 1/8 [00:02<00:19,  2.76s/it]\u001b[A\n","loss: 0.16426, smth: 0.13972:  12%|        | 1/8 [00:03<00:19,  2.76s/it]\u001b[A\n","loss: 0.16426, smth: 0.13972:  25%|       | 2/8 [00:03<00:13,  2.18s/it]\u001b[A\n","loss: 0.14588, smth: 0.14177:  25%|       | 2/8 [00:04<00:13,  2.18s/it]\u001b[A\n","loss: 0.14588, smth: 0.14177:  38%|      | 3/8 [00:05<00:09,  1.96s/it]\u001b[A\n","loss: 0.12788, smth: 0.13830:  38%|      | 3/8 [00:05<00:09,  1.96s/it]\u001b[A\n","loss: 0.12788, smth: 0.13830:  50%|     | 4/8 [00:05<00:06,  1.60s/it]\u001b[A\n","loss: 0.18738, smth: 0.14812:  50%|     | 4/8 [00:07<00:06,  1.60s/it]\u001b[A\n","loss: 0.18738, smth: 0.14812:  62%|   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.20935, smth: 0.15832:  62%|   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.20935, smth: 0.15832:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.06927, smth: 0.14560:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.06927, smth: 0.14560:  88%| | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 0.13458, smth: 0.14422:  88%| | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n","loss: 0.13458, smth: 0.14422: 100%|| 8/8 [00:09<00:00,  1.14s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:15 2021 Epoch 172, lr: 0.0001660, train loss: 0.14422, train auc: 0.99843, val loss: 0.42925, val auc: 0.98706\n","Thu Jun 10 01:31:15 2021 Epoch: 173\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13060, smth: 0.13060:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13060, smth: 0.13060:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.08340, smth: 0.10700:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.08340, smth: 0.10700:  25%|       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.12862, smth: 0.11421:  25%|       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.12862, smth: 0.11421:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.16907, smth: 0.12792:  38%|      | 3/8 [00:05<00:09,  2.00s/it]\u001b[A\n","loss: 0.16907, smth: 0.12792:  50%|     | 4/8 [00:05<00:06,  1.61s/it]\u001b[A\n","loss: 0.08822, smth: 0.11998:  50%|     | 4/8 [00:06<00:06,  1.61s/it]\u001b[A\n","loss: 0.08822, smth: 0.11998:  62%|   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.18740, smth: 0.13122:  62%|   | 5/8 [00:08<00:04,  1.51s/it]\u001b[A\n","loss: 0.18740, smth: 0.13122:  75%|  | 6/8 [00:08<00:02,  1.39s/it]\u001b[A\n","loss: 0.06526, smth: 0.12180:  75%|  | 6/8 [00:09<00:02,  1.39s/it]\u001b[A\n","loss: 0.06526, smth: 0.12180:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.14744, smth: 0.12500:  88%| | 7/8 [00:09<00:01,  1.28s/it]\u001b[A\n","loss: 0.14744, smth: 0.12500: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.76it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:25 2021 Epoch 173, lr: 0.0001508, train loss: 0.12500, train auc: 0.99800, val loss: 0.43995, val auc: 0.98684\n","Thu Jun 10 01:31:25 2021 Epoch: 174\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16859, smth: 0.16859:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16859, smth: 0.16859:  12%|        | 1/8 [00:02<00:18,  2.62s/it]\u001b[A\n","loss: 0.08533, smth: 0.12696:  12%|        | 1/8 [00:03<00:18,  2.62s/it]\u001b[A\n","loss: 0.08533, smth: 0.12696:  25%|       | 2/8 [00:03<00:12,  2.08s/it]\u001b[A\n","loss: 0.14345, smth: 0.13246:  25%|       | 2/8 [00:04<00:12,  2.08s/it]\u001b[A\n","loss: 0.14345, smth: 0.13246:  38%|      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.20164, smth: 0.14975:  38%|      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.20164, smth: 0.14975:  50%|     | 4/8 [00:05<00:06,  1.54s/it]\u001b[A\n","loss: 0.09462, smth: 0.13873:  50%|     | 4/8 [00:06<00:06,  1.54s/it]\u001b[A\n","loss: 0.09462, smth: 0.13873:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.10893, smth: 0.13376:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.10893, smth: 0.13376:  75%|  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.17023, smth: 0.13897:  75%|  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.17023, smth: 0.13897:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14275, smth: 0.13944:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14275, smth: 0.13944: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.17it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.84it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:35 2021 Epoch 174, lr: 0.0001362, train loss: 0.13944, train auc: 0.99779, val loss: 0.40915, val auc: 0.98781\n","Thu Jun 10 01:31:35 2021 Epoch: 175\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.26870, smth: 0.26870:   0%|          | 0/8 [00:03<?, ?it/s]\u001b[A\n","loss: 0.26870, smth: 0.26870:  12%|        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.20046, smth: 0.23458:  12%|        | 1/8 [00:03<00:22,  3.25s/it]\u001b[A\n","loss: 0.20046, smth: 0.23458:  25%|       | 2/8 [00:03<00:14,  2.49s/it]\u001b[A\n","loss: 0.15534, smth: 0.20816:  25%|       | 2/8 [00:04<00:14,  2.49s/it]\u001b[A\n","loss: 0.15534, smth: 0.20816:  38%|      | 3/8 [00:04<00:10,  2.05s/it]\u001b[A\n","loss: 0.13178, smth: 0.18907:  38%|      | 3/8 [00:05<00:10,  2.05s/it]\u001b[A\n","loss: 0.13178, smth: 0.18907:  50%|     | 4/8 [00:05<00:06,  1.68s/it]\u001b[A\n","loss: 0.12422, smth: 0.17610:  50%|     | 4/8 [00:06<00:06,  1.68s/it]\u001b[A\n","loss: 0.12422, smth: 0.17610:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.12179, smth: 0.16705:  62%|   | 5/8 [00:07<00:04,  1.55s/it]\u001b[A\n","loss: 0.12179, smth: 0.16705:  75%|  | 6/8 [00:07<00:02,  1.32s/it]\u001b[A\n","loss: 0.15937, smth: 0.16595:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.15937, smth: 0.16595:  88%| | 7/8 [00:08<00:01,  1.27s/it]\u001b[A\n","loss: 0.09338, smth: 0.15688:  88%| | 7/8 [00:09<00:01,  1.27s/it]\u001b[A\n","loss: 0.09338, smth: 0.15688: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.66it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:45 2021 Epoch 175, lr: 0.0001224, train loss: 0.15688, train auc: 0.99825, val loss: 0.42441, val auc: 0.98749\n","Thu Jun 10 01:31:45 2021 Epoch: 176\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14234, smth: 0.14234:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14234, smth: 0.14234:  12%|        | 1/8 [00:02<00:16,  2.38s/it]\u001b[A\n","loss: 0.12612, smth: 0.13423:  12%|        | 1/8 [00:03<00:16,  2.38s/it]\u001b[A\n","loss: 0.12612, smth: 0.13423:  25%|       | 2/8 [00:03<00:11,  1.89s/it]\u001b[A\n","loss: 0.09998, smth: 0.12281:  25%|       | 2/8 [00:04<00:11,  1.89s/it]\u001b[A\n","loss: 0.09998, smth: 0.12281:  38%|      | 3/8 [00:04<00:08,  1.67s/it]\u001b[A\n","loss: 0.17765, smth: 0.13652:  38%|      | 3/8 [00:05<00:08,  1.67s/it]\u001b[A\n","loss: 0.17765, smth: 0.13652:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.12065, smth: 0.13335:  50%|     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.12065, smth: 0.13335:  62%|   | 5/8 [00:06<00:03,  1.31s/it]\u001b[A\n","loss: 0.12230, smth: 0.13151:  62%|   | 5/8 [00:07<00:03,  1.31s/it]\u001b[A\n","loss: 0.12230, smth: 0.13151:  75%|  | 6/8 [00:07<00:02,  1.34s/it]\u001b[A\n","loss: 0.08323, smth: 0.12461:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.08323, smth: 0.12461:  88%| | 7/8 [00:08<00:01,  1.22s/it]\u001b[A\n","loss: 0.05122, smth: 0.11544:  88%| | 7/8 [00:09<00:01,  1.22s/it]\u001b[A\n","loss: 0.05122, smth: 0.11544: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.64it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:31:55 2021 Epoch 176, lr: 0.0001092, train loss: 0.11544, train auc: 0.99837, val loss: 0.42366, val auc: 0.98706\n","Thu Jun 10 01:31:55 2021 Epoch: 177\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19401, smth: 0.19401:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19401, smth: 0.19401:  12%|        | 1/8 [00:02<00:18,  2.65s/it]\u001b[A\n","loss: 0.27446, smth: 0.23424:  12%|        | 1/8 [00:03<00:18,  2.65s/it]\u001b[A\n","loss: 0.27446, smth: 0.23424:  25%|       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.25075, smth: 0.23974:  25%|       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.25075, smth: 0.23974:  38%|      | 3/8 [00:04<00:09,  1.83s/it]\u001b[A\n","loss: 0.07892, smth: 0.19953:  38%|      | 3/8 [00:05<00:09,  1.83s/it]\u001b[A\n","loss: 0.07892, smth: 0.19953:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.14758, smth: 0.18914:  50%|     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.14758, smth: 0.18914:  62%|   | 5/8 [00:06<00:04,  1.35s/it]\u001b[A\n","loss: 0.11287, smth: 0.17643:  62%|   | 5/8 [00:07<00:04,  1.35s/it]\u001b[A\n","loss: 0.11287, smth: 0.17643:  75%|  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.09969, smth: 0.16547:  75%|  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.09969, smth: 0.16547:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.16987, smth: 0.16602:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.16987, smth: 0.16602: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:05 2021 Epoch 177, lr: 0.0000968, train loss: 0.16602, train auc: 0.99767, val loss: 0.40045, val auc: 0.98806\n","Thu Jun 10 01:32:05 2021 Epoch: 178\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.19588, smth: 0.19588:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.19588, smth: 0.19588:  12%|        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.09092, smth: 0.14340:  12%|        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.09092, smth: 0.14340:  25%|       | 2/8 [00:03<00:13,  2.25s/it]\u001b[A\n","loss: 0.17918, smth: 0.15533:  25%|       | 2/8 [00:05<00:13,  2.25s/it]\u001b[A\n","loss: 0.17918, smth: 0.15533:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.11874, smth: 0.14618:  38%|      | 3/8 [00:06<00:10,  2.06s/it]\u001b[A\n","loss: 0.11874, smth: 0.14618:  50%|     | 4/8 [00:06<00:06,  1.69s/it]\u001b[A\n","loss: 0.17128, smth: 0.15120:  50%|     | 4/8 [00:07<00:06,  1.69s/it]\u001b[A\n","loss: 0.17128, smth: 0.15120:  62%|   | 5/8 [00:07<00:04,  1.63s/it]\u001b[A\n","loss: 0.15698, smth: 0.15216:  62%|   | 5/8 [00:08<00:04,  1.63s/it]\u001b[A\n","loss: 0.15698, smth: 0.15216:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.13845, smth: 0.15020:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.13845, smth: 0.15020:  88%| | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.22899, smth: 0.16005:  88%| | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.22899, smth: 0.16005: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:15 2021 Epoch 178, lr: 0.0000852, train loss: 0.16005, train auc: 0.99724, val loss: 0.43318, val auc: 0.98690\n","Thu Jun 10 01:32:15 2021 Epoch: 179\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10943, smth: 0.10943:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10943, smth: 0.10943:  12%|        | 1/8 [00:02<00:20,  2.98s/it]\u001b[A\n","loss: 0.12003, smth: 0.11473:  12%|        | 1/8 [00:03<00:20,  2.98s/it]\u001b[A\n","loss: 0.12003, smth: 0.11473:  25%|       | 2/8 [00:03<00:13,  2.32s/it]\u001b[A\n","loss: 0.19145, smth: 0.14030:  25%|       | 2/8 [00:04<00:13,  2.32s/it]\u001b[A\n","loss: 0.19145, smth: 0.14030:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.14797, smth: 0.14222:  38%|      | 3/8 [00:05<00:10,  2.03s/it]\u001b[A\n","loss: 0.14797, smth: 0.14222:  50%|     | 4/8 [00:05<00:06,  1.64s/it]\u001b[A\n","loss: 0.14171, smth: 0.14212:  50%|     | 4/8 [00:06<00:06,  1.64s/it]\u001b[A\n","loss: 0.14171, smth: 0.14212:  62%|   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.14754, smth: 0.14302:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.14754, smth: 0.14302:  75%|  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.19664, smth: 0.15068:  75%|  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.19664, smth: 0.15068:  88%| | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.09264, smth: 0.14343:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.09264, smth: 0.14343: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:25 2021 Epoch 179, lr: 0.0000743, train loss: 0.14343, train auc: 0.99728, val loss: 0.42815, val auc: 0.98652\n","Thu Jun 10 01:32:25 2021 Epoch: 180\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16585, smth: 0.16585:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16585, smth: 0.16585:  12%|        | 1/8 [00:02<00:17,  2.53s/it]\u001b[A\n","loss: 0.17763, smth: 0.17174:  12%|        | 1/8 [00:03<00:17,  2.53s/it]\u001b[A\n","loss: 0.17763, smth: 0.17174:  25%|       | 2/8 [00:03<00:12,  2.01s/it]\u001b[A\n","loss: 0.16556, smth: 0.16968:  25%|       | 2/8 [00:04<00:12,  2.01s/it]\u001b[A\n","loss: 0.16556, smth: 0.16968:  38%|      | 3/8 [00:04<00:08,  1.75s/it]\u001b[A\n","loss: 0.10474, smth: 0.15344:  38%|      | 3/8 [00:05<00:08,  1.75s/it]\u001b[A\n","loss: 0.10474, smth: 0.15344:  50%|     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.07794, smth: 0.13834:  50%|     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.07794, smth: 0.13834:  62%|   | 5/8 [00:06<00:04,  1.44s/it]\u001b[A\n","loss: 0.10095, smth: 0.13211:  62%|   | 5/8 [00:07<00:04,  1.44s/it]\u001b[A\n","loss: 0.10095, smth: 0.13211:  75%|  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.14927, smth: 0.13456:  75%|  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.14927, smth: 0.13456:  88%| | 7/8 [00:08<00:01,  1.21s/it]\u001b[A\n","loss: 0.17942, smth: 0.14017:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.17942, smth: 0.14017: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.70it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:34 2021 Epoch 180, lr: 0.0000641, train loss: 0.14017, train auc: 0.99831, val loss: 0.43070, val auc: 0.98661\n","Thu Jun 10 01:32:34 2021 Epoch: 181\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08492, smth: 0.08492:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08492, smth: 0.08492:  12%|        | 1/8 [00:02<00:19,  2.84s/it]\u001b[A\n","loss: 0.12036, smth: 0.10264:  12%|        | 1/8 [00:03<00:19,  2.84s/it]\u001b[A\n","loss: 0.12036, smth: 0.10264:  25%|       | 2/8 [00:03<00:13,  2.22s/it]\u001b[A\n","loss: 0.13501, smth: 0.11343:  25%|       | 2/8 [00:04<00:13,  2.22s/it]\u001b[A\n","loss: 0.13501, smth: 0.11343:  38%|      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.14055, smth: 0.12021:  38%|      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.14055, smth: 0.12021:  50%|     | 4/8 [00:05<00:06,  1.59s/it]\u001b[A\n","loss: 0.14451, smth: 0.12507:  50%|     | 4/8 [00:06<00:06,  1.59s/it]\u001b[A\n","loss: 0.14451, smth: 0.12507:  62%|   | 5/8 [00:06<00:04,  1.49s/it]\u001b[A\n","loss: 0.10274, smth: 0.12135:  62%|   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.10274, smth: 0.12135:  75%|  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.09083, smth: 0.11699:  75%|  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.09083, smth: 0.11699:  88%| | 7/8 [00:08<00:01,  1.24s/it]\u001b[A\n","loss: 0.11763, smth: 0.11707:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.11763, smth: 0.11707: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.74it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:44 2021 Epoch 181, lr: 0.0000546, train loss: 0.11707, train auc: 0.99794, val loss: 0.44045, val auc: 0.98613\n","Thu Jun 10 01:32:44 2021 Epoch: 182\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17050, smth: 0.17050:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17050, smth: 0.17050:  12%|        | 1/8 [00:02<00:17,  2.51s/it]\u001b[A\n","loss: 0.12084, smth: 0.14567:  12%|        | 1/8 [00:03<00:17,  2.51s/it]\u001b[A\n","loss: 0.12084, smth: 0.14567:  25%|       | 2/8 [00:03<00:11,  1.99s/it]\u001b[A\n","loss: 0.09647, smth: 0.12927:  25%|       | 2/8 [00:04<00:11,  1.99s/it]\u001b[A\n","loss: 0.09647, smth: 0.12927:  38%|      | 3/8 [00:04<00:08,  1.80s/it]\u001b[A\n","loss: 0.12085, smth: 0.12717:  38%|      | 3/8 [00:05<00:08,  1.80s/it]\u001b[A\n","loss: 0.12085, smth: 0.12717:  50%|     | 4/8 [00:05<00:05,  1.50s/it]\u001b[A\n","loss: 0.08144, smth: 0.11802:  50%|     | 4/8 [00:06<00:05,  1.50s/it]\u001b[A\n","loss: 0.08144, smth: 0.11802:  62%|   | 5/8 [00:06<00:04,  1.50s/it]\u001b[A\n","loss: 0.13071, smth: 0.12014:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.13071, smth: 0.12014:  75%|  | 6/8 [00:07<00:02,  1.25s/it]\u001b[A\n","loss: 0.11970, smth: 0.12007:  75%|  | 6/8 [00:08<00:02,  1.25s/it]\u001b[A\n","loss: 0.11970, smth: 0.12007:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.16779, smth: 0.12604:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.16779, smth: 0.12604: 100%|| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.16it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.72it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:32:54 2021 Epoch 182, lr: 0.0000459, train loss: 0.12604, train auc: 0.99651, val loss: 0.44270, val auc: 0.98645\n","Thu Jun 10 01:32:54 2021 Epoch: 183\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.09814, smth: 0.09814:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.09814, smth: 0.09814:  12%|        | 1/8 [00:02<00:17,  2.45s/it]\u001b[A\n","loss: 0.11978, smth: 0.10896:  12%|        | 1/8 [00:03<00:17,  2.45s/it]\u001b[A\n","loss: 0.11978, smth: 0.10896:  25%|       | 2/8 [00:03<00:11,  1.95s/it]\u001b[A\n","loss: 0.08619, smth: 0.10137:  25%|       | 2/8 [00:04<00:11,  1.95s/it]\u001b[A\n","loss: 0.08619, smth: 0.10137:  38%|      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.14054, smth: 0.11116:  38%|      | 3/8 [00:05<00:09,  1.91s/it]\u001b[A\n","loss: 0.14054, smth: 0.11116:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.16199, smth: 0.12133:  50%|     | 4/8 [00:06<00:06,  1.56s/it]\u001b[A\n","loss: 0.16199, smth: 0.12133:  62%|   | 5/8 [00:06<00:04,  1.34s/it]\u001b[A\n","loss: 0.15193, smth: 0.12643:  62%|   | 5/8 [00:07<00:04,  1.34s/it]\u001b[A\n","loss: 0.15193, smth: 0.12643:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.13302, smth: 0.12737:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.13302, smth: 0.12737:  88%| | 7/8 [00:08<00:01,  1.25s/it]\u001b[A\n","loss: 0.13681, smth: 0.12855:  88%| | 7/8 [00:09<00:01,  1.25s/it]\u001b[A\n","loss: 0.13681, smth: 0.12855: 100%|| 8/8 [00:09<00:00,  1.18s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.11it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:04 2021 Epoch 183, lr: 0.0000380, train loss: 0.12855, train auc: 0.99747, val loss: 0.45148, val auc: 0.98651\n","Thu Jun 10 01:33:04 2021 Epoch: 184\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08094, smth: 0.08094:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08094, smth: 0.08094:  12%|        | 1/8 [00:02<00:20,  2.91s/it]\u001b[A\n","loss: 0.14225, smth: 0.11160:  12%|        | 1/8 [00:03<00:20,  2.91s/it]\u001b[A\n","loss: 0.14225, smth: 0.11160:  25%|       | 2/8 [00:03<00:13,  2.27s/it]\u001b[A\n","loss: 0.06285, smth: 0.09535:  25%|       | 2/8 [00:04<00:13,  2.27s/it]\u001b[A\n","loss: 0.06285, smth: 0.09535:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.10863, smth: 0.09867:  38%|      | 3/8 [00:05<00:10,  2.01s/it]\u001b[A\n","loss: 0.10863, smth: 0.09867:  50%|     | 4/8 [00:05<00:06,  1.63s/it]\u001b[A\n","loss: 0.16364, smth: 0.11166:  50%|     | 4/8 [00:07<00:06,  1.63s/it]\u001b[A\n","loss: 0.16364, smth: 0.11166:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.10913, smth: 0.11124:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.10913, smth: 0.11124:  75%|  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.14499, smth: 0.11606:  75%|  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.14499, smth: 0.11606:  88%| | 7/8 [00:08<00:01,  1.24s/it]\u001b[A\n","loss: 0.04209, smth: 0.10681:  88%| | 7/8 [00:09<00:01,  1.24s/it]\u001b[A\n","loss: 0.04209, smth: 0.10681: 100%|| 8/8 [00:09<00:00,  1.19s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.97it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.59it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:14 2021 Epoch 184, lr: 0.0000308, train loss: 0.10681, train auc: 0.99756, val loss: 0.46452, val auc: 0.98634\n","Thu Jun 10 01:33:14 2021 Epoch: 185\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08871, smth: 0.08871:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08871, smth: 0.08871:  12%|        | 1/8 [00:02<00:20,  2.92s/it]\u001b[A\n","loss: 0.13433, smth: 0.11152:  12%|        | 1/8 [00:03<00:20,  2.92s/it]\u001b[A\n","loss: 0.13433, smth: 0.11152:  25%|       | 2/8 [00:03<00:13,  2.27s/it]\u001b[A\n","loss: 0.18420, smth: 0.13575:  25%|       | 2/8 [00:05<00:13,  2.27s/it]\u001b[A\n","loss: 0.18420, smth: 0.13575:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.10451, smth: 0.12794:  38%|      | 3/8 [00:05<00:10,  2.06s/it]\u001b[A\n","loss: 0.10451, smth: 0.12794:  50%|     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.08251, smth: 0.11885:  50%|     | 4/8 [00:06<00:06,  1.67s/it]\u001b[A\n","loss: 0.08251, smth: 0.11885:  62%|   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.10321, smth: 0.11625:  62%|   | 5/8 [00:07<00:04,  1.49s/it]\u001b[A\n","loss: 0.10321, smth: 0.11625:  75%|  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.10077, smth: 0.11403:  75%|  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.10077, smth: 0.11403:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.21700, smth: 0.12691:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.21700, smth: 0.12691: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.88it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.49it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:24 2021 Epoch 185, lr: 0.0000243, train loss: 0.12691, train auc: 0.99843, val loss: 0.46904, val auc: 0.98630\n","Thu Jun 10 01:33:24 2021 Epoch: 186\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.14755, smth: 0.14755:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.14755, smth: 0.14755:  12%|        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.13195, smth: 0.13975:  12%|        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.13195, smth: 0.13975:  25%|       | 2/8 [00:03<00:12,  2.16s/it]\u001b[A\n","loss: 0.13150, smth: 0.13700:  25%|       | 2/8 [00:04<00:12,  2.16s/it]\u001b[A\n","loss: 0.13150, smth: 0.13700:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.16763, smth: 0.14466:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.16763, smth: 0.14466:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.19337, smth: 0.15440:  50%|     | 4/8 [00:07<00:06,  1.62s/it]\u001b[A\n","loss: 0.19337, smth: 0.15440:  62%|   | 5/8 [00:07<00:04,  1.61s/it]\u001b[A\n","loss: 0.20188, smth: 0.16231:  62%|   | 5/8 [00:08<00:04,  1.61s/it]\u001b[A\n","loss: 0.20188, smth: 0.16231:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.12854, smth: 0.15749:  75%|  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\n","loss: 0.12854, smth: 0.15749:  88%| | 7/8 [00:08<00:01,  1.16s/it]\u001b[A\n","loss: 0.10996, smth: 0.15155:  88%| | 7/8 [00:09<00:01,  1.16s/it]\u001b[A\n","loss: 0.10996, smth: 0.15155: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.61it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:34 2021 Epoch 186, lr: 0.0000186, train loss: 0.15155, train auc: 0.99783, val loss: 0.47112, val auc: 0.98630\n","Thu Jun 10 01:33:34 2021 Epoch: 187\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12273, smth: 0.12273:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12273, smth: 0.12273:  12%|        | 1/8 [00:02<00:16,  2.38s/it]\u001b[A\n","loss: 0.16400, smth: 0.14337:  12%|        | 1/8 [00:03<00:16,  2.38s/it]\u001b[A\n","loss: 0.16400, smth: 0.14337:  25%|       | 2/8 [00:03<00:11,  1.96s/it]\u001b[A\n","loss: 0.14375, smth: 0.14349:  25%|       | 2/8 [00:04<00:11,  1.96s/it]\u001b[A\n","loss: 0.14375, smth: 0.14349:  38%|      | 3/8 [00:04<00:08,  1.71s/it]\u001b[A\n","loss: 0.09919, smth: 0.13242:  38%|      | 3/8 [00:05<00:08,  1.71s/it]\u001b[A\n","loss: 0.09919, smth: 0.13242:  50%|     | 4/8 [00:05<00:06,  1.57s/it]\u001b[A\n","loss: 0.11274, smth: 0.12848:  50%|     | 4/8 [00:06<00:06,  1.57s/it]\u001b[A\n","loss: 0.11274, smth: 0.12848:  62%|   | 5/8 [00:06<00:04,  1.34s/it]\u001b[A\n","loss: 0.24970, smth: 0.14869:  62%|   | 5/8 [00:08<00:04,  1.34s/it]\u001b[A\n","loss: 0.24970, smth: 0.14869:  75%|  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.09290, smth: 0.14072:  75%|  | 6/8 [00:08<00:02,  1.41s/it]\u001b[A\n","loss: 0.09290, smth: 0.14072:  88%| | 7/8 [00:08<00:01,  1.19s/it]\u001b[A\n","loss: 0.28994, smth: 0.15937:  88%| | 7/8 [00:09<00:01,  1.19s/it]\u001b[A\n","loss: 0.28994, smth: 0.15937: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.67it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:44 2021 Epoch 187, lr: 0.0000137, train loss: 0.15937, train auc: 0.99707, val loss: 0.47106, val auc: 0.98597\n","Thu Jun 10 01:33:44 2021 Epoch: 188\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.05052, smth: 0.05052:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.05052, smth: 0.05052:  12%|        | 1/8 [00:02<00:20,  2.88s/it]\u001b[A\n","loss: 0.24440, smth: 0.14746:  12%|        | 1/8 [00:03<00:20,  2.88s/it]\u001b[A\n","loss: 0.24440, smth: 0.14746:  25%|       | 2/8 [00:03<00:13,  2.26s/it]\u001b[A\n","loss: 0.16334, smth: 0.15275:  25%|       | 2/8 [00:04<00:13,  2.26s/it]\u001b[A\n","loss: 0.16334, smth: 0.15275:  38%|      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.09060, smth: 0.13722:  38%|      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.09060, smth: 0.13722:  50%|     | 4/8 [00:05<00:06,  1.54s/it]\u001b[A\n","loss: 0.07114, smth: 0.12400:  50%|     | 4/8 [00:06<00:06,  1.54s/it]\u001b[A\n","loss: 0.07114, smth: 0.12400:  62%|   | 5/8 [00:06<00:04,  1.51s/it]\u001b[A\n","loss: 0.13711, smth: 0.12619:  62%|   | 5/8 [00:07<00:04,  1.51s/it]\u001b[A\n","loss: 0.13711, smth: 0.12619:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.06510, smth: 0.11746:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.06510, smth: 0.11746:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.15644, smth: 0.12233:  88%| | 7/8 [00:08<00:01,  1.17s/it]\u001b[A\n","loss: 0.15644, smth: 0.12233: 100%|| 8/8 [00:09<00:00,  1.13s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.94it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.57it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:33:54 2021 Epoch 188, lr: 0.0000095, train loss: 0.12233, train auc: 0.99756, val loss: 0.45908, val auc: 0.98647\n","Thu Jun 10 01:33:54 2021 Epoch: 189\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12943, smth: 0.12943:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12943, smth: 0.12943:  12%|        | 1/8 [00:02<00:18,  2.65s/it]\u001b[A\n","loss: 0.12140, smth: 0.12542:  12%|        | 1/8 [00:03<00:18,  2.65s/it]\u001b[A\n","loss: 0.12140, smth: 0.12542:  25%|       | 2/8 [00:03<00:12,  2.10s/it]\u001b[A\n","loss: 0.15549, smth: 0.13544:  25%|       | 2/8 [00:04<00:12,  2.10s/it]\u001b[A\n","loss: 0.15549, smth: 0.13544:  38%|      | 3/8 [00:04<00:09,  1.85s/it]\u001b[A\n","loss: 0.12087, smth: 0.13180:  38%|      | 3/8 [00:05<00:09,  1.85s/it]\u001b[A\n","loss: 0.12087, smth: 0.13180:  50%|     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.13725, smth: 0.13289:  50%|     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.13725, smth: 0.13289:  62%|   | 5/8 [00:06<00:04,  1.39s/it]\u001b[A\n","loss: 0.22044, smth: 0.14748:  62%|   | 5/8 [00:07<00:04,  1.39s/it]\u001b[A\n","loss: 0.22044, smth: 0.14748:  75%|  | 6/8 [00:07<00:02,  1.29s/it]\u001b[A\n","loss: 0.14679, smth: 0.14738:  75%|  | 6/8 [00:08<00:02,  1.29s/it]\u001b[A\n","loss: 0.14679, smth: 0.14738:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.11926, smth: 0.14387:  88%| | 7/8 [00:08<00:01,  1.10s/it]\u001b[A\n","loss: 0.11926, smth: 0.14387: 100%|| 8/8 [00:08<00:00,  1.10s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:03 2021 Epoch 189, lr: 0.0000061, train loss: 0.14387, train auc: 0.99724, val loss: 0.45720, val auc: 0.98614\n","Thu Jun 10 01:34:03 2021 Epoch: 190\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11311, smth: 0.11311:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.11311, smth: 0.11311:  12%|        | 1/8 [00:02<00:16,  2.40s/it]\u001b[A\n","loss: 0.12304, smth: 0.11807:  12%|        | 1/8 [00:03<00:16,  2.40s/it]\u001b[A\n","loss: 0.12304, smth: 0.11807:  25%|       | 2/8 [00:03<00:11,  1.91s/it]\u001b[A\n","loss: 0.13092, smth: 0.12236:  25%|       | 2/8 [00:04<00:11,  1.91s/it]\u001b[A\n","loss: 0.13092, smth: 0.12236:  38%|      | 3/8 [00:04<00:08,  1.73s/it]\u001b[A\n","loss: 0.16317, smth: 0.13256:  38%|      | 3/8 [00:05<00:08,  1.73s/it]\u001b[A\n","loss: 0.16317, smth: 0.13256:  50%|     | 4/8 [00:05<00:05,  1.44s/it]\u001b[A\n","loss: 0.20620, smth: 0.14729:  50%|     | 4/8 [00:06<00:05,  1.44s/it]\u001b[A\n","loss: 0.20620, smth: 0.14729:  62%|   | 5/8 [00:06<00:04,  1.52s/it]\u001b[A\n","loss: 0.10472, smth: 0.14019:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.10472, smth: 0.14019:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.08195, smth: 0.13187:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.08195, smth: 0.13187:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.15464, smth: 0.13472:  88%| | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n","loss: 0.15464, smth: 0.13472: 100%|| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.97it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.58it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:12 2021 Epoch 190, lr: 0.0000034, train loss: 0.13472, train auc: 0.99795, val loss: 0.47983, val auc: 0.98618\n","Thu Jun 10 01:34:12 2021 Epoch: 191\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.12233, smth: 0.12233:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.12233, smth: 0.12233:  12%|        | 1/8 [00:02<00:18,  2.68s/it]\u001b[A\n","loss: 0.16417, smth: 0.14325:  12%|        | 1/8 [00:03<00:18,  2.68s/it]\u001b[A\n","loss: 0.16417, smth: 0.14325:  25%|       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.08460, smth: 0.12370:  25%|       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.08460, smth: 0.12370:  38%|      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.16192, smth: 0.13325:  38%|      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.16192, smth: 0.13325:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.12949, smth: 0.13250:  50%|     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.12949, smth: 0.13250:  62%|   | 5/8 [00:06<00:04,  1.42s/it]\u001b[A\n","loss: 0.17447, smth: 0.13950:  62%|   | 5/8 [00:07<00:04,  1.42s/it]\u001b[A\n","loss: 0.17447, smth: 0.13950:  75%|  | 6/8 [00:07<00:02,  1.20s/it]\u001b[A\n","loss: 0.06978, smth: 0.12954:  75%|  | 6/8 [00:08<00:02,  1.20s/it]\u001b[A\n","loss: 0.06978, smth: 0.12954:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.13284, smth: 0.12995:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.13284, smth: 0.12995: 100%|| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.92it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.53it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:22 2021 Epoch 191, lr: 0.0000015, train loss: 0.12995, train auc: 0.99747, val loss: 0.47076, val auc: 0.98591\n","Thu Jun 10 01:34:22 2021 Epoch: 192\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08355, smth: 0.08355:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08355, smth: 0.08355:  12%|        | 1/8 [00:02<00:18,  2.59s/it]\u001b[A\n","loss: 0.11079, smth: 0.09717:  12%|        | 1/8 [00:03<00:18,  2.59s/it]\u001b[A\n","loss: 0.11079, smth: 0.09717:  25%|       | 2/8 [00:03<00:12,  2.05s/it]\u001b[A\n","loss: 0.19303, smth: 0.12912:  25%|       | 2/8 [00:05<00:12,  2.05s/it]\u001b[A\n","loss: 0.19303, smth: 0.12912:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.12081, smth: 0.12704:  38%|      | 3/8 [00:05<00:09,  1.97s/it]\u001b[A\n","loss: 0.12081, smth: 0.12704:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.12538, smth: 0.12671:  50%|     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.12538, smth: 0.12671:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.14518, smth: 0.12979:  62%|   | 5/8 [00:07<00:04,  1.47s/it]\u001b[A\n","loss: 0.14518, smth: 0.12979:  75%|  | 6/8 [00:07<00:02,  1.24s/it]\u001b[A\n","loss: 0.21024, smth: 0.14128:  75%|  | 6/8 [00:08<00:02,  1.24s/it]\u001b[A\n","loss: 0.21024, smth: 0.14128:  88%| | 7/8 [00:08<00:01,  1.15s/it]\u001b[A\n","loss: 0.23773, smth: 0.15334:  88%| | 7/8 [00:09<00:01,  1.15s/it]\u001b[A\n","loss: 0.23773, smth: 0.15334: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.10it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.71it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:32 2021 Epoch 192, lr: 0.0000004, train loss: 0.15334, train auc: 0.99875, val loss: 0.46048, val auc: 0.98678\n","Thu Jun 10 01:34:32 2021 Epoch: 193\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.16677, smth: 0.16677:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.16677, smth: 0.16677:  12%|        | 1/8 [00:02<00:19,  2.78s/it]\u001b[A\n","loss: 0.17777, smth: 0.17227:  12%|        | 1/8 [00:03<00:19,  2.78s/it]\u001b[A\n","loss: 0.17777, smth: 0.17227:  25%|       | 2/8 [00:03<00:13,  2.19s/it]\u001b[A\n","loss: 0.15130, smth: 0.16528:  25%|       | 2/8 [00:04<00:13,  2.19s/it]\u001b[A\n","loss: 0.15130, smth: 0.16528:  38%|      | 3/8 [00:04<00:09,  1.92s/it]\u001b[A\n","loss: 0.10386, smth: 0.14993:  38%|      | 3/8 [00:05<00:09,  1.92s/it]\u001b[A\n","loss: 0.10386, smth: 0.14993:  50%|     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.04562, smth: 0.12907:  50%|     | 4/8 [00:07<00:06,  1.58s/it]\u001b[A\n","loss: 0.04562, smth: 0.12907:  62%|   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.21052, smth: 0.14264:  62%|   | 5/8 [00:07<00:04,  1.60s/it]\u001b[A\n","loss: 0.21052, smth: 0.14264:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.14517, smth: 0.14300:  75%|  | 6/8 [00:08<00:02,  1.33s/it]\u001b[A\n","loss: 0.14517, smth: 0.14300:  88%| | 7/8 [00:08<00:01,  1.14s/it]\u001b[A\n","loss: 0.05834, smth: 0.13242:  88%| | 7/8 [00:09<00:01,  1.14s/it]\u001b[A\n","loss: 0.05834, smth: 0.13242: 100%|| 8/8 [00:09<00:00,  1.16s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:42 2021 Epoch 193, lr: 0.0000000, train loss: 0.13242, train auc: 0.99752, val loss: 0.43660, val auc: 0.98644\n","Thu Jun 10 01:34:42 2021 Epoch: 194\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.13204, smth: 0.13204:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.13204, smth: 0.13204:  12%|        | 1/8 [00:02<00:18,  2.63s/it]\u001b[A\n","loss: 0.20079, smth: 0.16642:  12%|        | 1/8 [00:03<00:18,  2.63s/it]\u001b[A\n","loss: 0.20079, smth: 0.16642:  25%|       | 2/8 [00:03<00:12,  2.08s/it]\u001b[A\n","loss: 0.14923, smth: 0.16069:  25%|       | 2/8 [00:04<00:12,  2.08s/it]\u001b[A\n","loss: 0.14923, smth: 0.16069:  38%|      | 3/8 [00:04<00:09,  1.86s/it]\u001b[A\n","loss: 0.16200, smth: 0.16101:  38%|      | 3/8 [00:05<00:09,  1.86s/it]\u001b[A\n","loss: 0.16200, smth: 0.16101:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.12420, smth: 0.15365:  50%|     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.12420, smth: 0.15365:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.15022, smth: 0.15308:  62%|   | 5/8 [00:07<00:04,  1.52s/it]\u001b[A\n","loss: 0.15022, smth: 0.15308:  75%|  | 6/8 [00:07<00:02,  1.28s/it]\u001b[A\n","loss: 0.25446, smth: 0.16756:  75%|  | 6/8 [00:08<00:02,  1.28s/it]\u001b[A\n","loss: 0.25446, smth: 0.16756:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.08600, smth: 0.15737:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.08600, smth: 0.15737: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.06it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.68it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:34:51 2021 Epoch 194, lr: 0.0000004, train loss: 0.15737, train auc: 0.99805, val loss: 0.46612, val auc: 0.98578\n","Thu Jun 10 01:34:51 2021 Epoch: 195\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.15512, smth: 0.15512:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.15512, smth: 0.15512:  12%|        | 1/8 [00:02<00:18,  2.71s/it]\u001b[A\n","loss: 0.14965, smth: 0.15238:  12%|        | 1/8 [00:03<00:18,  2.71s/it]\u001b[A\n","loss: 0.14965, smth: 0.15238:  25%|       | 2/8 [00:03<00:12,  2.14s/it]\u001b[A\n","loss: 0.12200, smth: 0.14226:  25%|       | 2/8 [00:04<00:12,  2.14s/it]\u001b[A\n","loss: 0.12200, smth: 0.14226:  38%|      | 3/8 [00:04<00:09,  1.88s/it]\u001b[A\n","loss: 0.16390, smth: 0.14767:  38%|      | 3/8 [00:05<00:09,  1.88s/it]\u001b[A\n","loss: 0.16390, smth: 0.14767:  50%|     | 4/8 [00:05<00:06,  1.56s/it]\u001b[A\n","loss: 0.07952, smth: 0.13404:  50%|     | 4/8 [00:07<00:06,  1.56s/it]\u001b[A\n","loss: 0.07952, smth: 0.13404:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.19670, smth: 0.14448:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.19670, smth: 0.14448:  75%|  | 6/8 [00:08<00:02,  1.34s/it]\u001b[A\n","loss: 0.27658, smth: 0.16335:  75%|  | 6/8 [00:09<00:02,  1.34s/it]\u001b[A\n","loss: 0.27658, smth: 0.16335:  88%| | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.12312, smth: 0.15832:  88%| | 7/8 [00:09<00:01,  1.37s/it]\u001b[A\n","loss: 0.12312, smth: 0.15832: 100%|| 8/8 [00:09<00:00,  1.25s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.95it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.55it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:02 2021 Epoch 195, lr: 0.0000015, train loss: 0.15832, train auc: 0.99722, val loss: 0.45451, val auc: 0.98611\n","Thu Jun 10 01:35:02 2021 Epoch: 196\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.10895, smth: 0.10895:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.10895, smth: 0.10895:  12%|        | 1/8 [00:02<00:18,  2.65s/it]\u001b[A\n","loss: 0.12280, smth: 0.11587:  12%|        | 1/8 [00:03<00:18,  2.65s/it]\u001b[A\n","loss: 0.12280, smth: 0.11587:  25%|       | 2/8 [00:03<00:12,  2.09s/it]\u001b[A\n","loss: 0.26128, smth: 0.16434:  25%|       | 2/8 [00:04<00:12,  2.09s/it]\u001b[A\n","loss: 0.26128, smth: 0.16434:  38%|      | 3/8 [00:04<00:09,  1.82s/it]\u001b[A\n","loss: 0.09628, smth: 0.14733:  38%|      | 3/8 [00:05<00:09,  1.82s/it]\u001b[A\n","loss: 0.09628, smth: 0.14733:  50%|     | 4/8 [00:05<00:06,  1.51s/it]\u001b[A\n","loss: 0.10183, smth: 0.13823:  50%|     | 4/8 [00:06<00:06,  1.51s/it]\u001b[A\n","loss: 0.10183, smth: 0.13823:  62%|   | 5/8 [00:06<00:04,  1.48s/it]\u001b[A\n","loss: 0.10680, smth: 0.13299:  62%|   | 5/8 [00:07<00:04,  1.48s/it]\u001b[A\n","loss: 0.10680, smth: 0.13299:  75%|  | 6/8 [00:07<00:02,  1.26s/it]\u001b[A\n","loss: 0.21540, smth: 0.14476:  75%|  | 6/8 [00:08<00:02,  1.26s/it]\u001b[A\n","loss: 0.21540, smth: 0.14476:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14586, smth: 0.14490:  88%| | 7/8 [00:08<00:01,  1.13s/it]\u001b[A\n","loss: 0.14586, smth: 0.14490: 100%|| 8/8 [00:08<00:00,  1.11s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.13it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.73it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:12 2021 Epoch 196, lr: 0.0000034, train loss: 0.14490, train auc: 0.99841, val loss: 0.45959, val auc: 0.98620\n","Thu Jun 10 01:35:12 2021 Epoch: 197\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.18912, smth: 0.18912:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.18912, smth: 0.18912:  12%|        | 1/8 [00:02<00:20,  2.98s/it]\u001b[A\n","loss: 0.21479, smth: 0.20195:  12%|        | 1/8 [00:03<00:20,  2.98s/it]\u001b[A\n","loss: 0.21479, smth: 0.20195:  25%|       | 2/8 [00:03<00:13,  2.31s/it]\u001b[A\n","loss: 0.10449, smth: 0.16947:  25%|       | 2/8 [00:04<00:13,  2.31s/it]\u001b[A\n","loss: 0.10449, smth: 0.16947:  38%|      | 3/8 [00:04<00:09,  1.98s/it]\u001b[A\n","loss: 0.15986, smth: 0.16707:  38%|      | 3/8 [00:05<00:09,  1.98s/it]\u001b[A\n","loss: 0.15986, smth: 0.16707:  50%|     | 4/8 [00:05<00:06,  1.62s/it]\u001b[A\n","loss: 0.09481, smth: 0.15262:  50%|     | 4/8 [00:06<00:06,  1.62s/it]\u001b[A\n","loss: 0.09481, smth: 0.15262:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.12559, smth: 0.14811:  62%|   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\n","loss: 0.12559, smth: 0.14811:  75%|  | 6/8 [00:07<00:02,  1.31s/it]\u001b[A\n","loss: 0.16815, smth: 0.15097:  75%|  | 6/8 [00:08<00:02,  1.31s/it]\u001b[A\n","loss: 0.16815, smth: 0.15097:  88%| | 7/8 [00:08<00:01,  1.18s/it]\u001b[A\n","loss: 0.11770, smth: 0.14681:  88%| | 7/8 [00:09<00:01,  1.18s/it]\u001b[A\n","loss: 0.11770, smth: 0.14681: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.57it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:21 2021 Epoch 197, lr: 0.0000061, train loss: 0.14681, train auc: 0.99736, val loss: 0.44678, val auc: 0.98659\n","Thu Jun 10 01:35:21 2021 Epoch: 198\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.08704, smth: 0.08704:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.08704, smth: 0.08704:  12%|        | 1/8 [00:02<00:18,  2.70s/it]\u001b[A\n","loss: 0.10192, smth: 0.09448:  12%|        | 1/8 [00:03<00:18,  2.70s/it]\u001b[A\n","loss: 0.10192, smth: 0.09448:  25%|       | 2/8 [00:03<00:12,  2.11s/it]\u001b[A\n","loss: 0.11343, smth: 0.10079:  25%|       | 2/8 [00:04<00:12,  2.11s/it]\u001b[A\n","loss: 0.11343, smth: 0.10079:  38%|      | 3/8 [00:04<00:09,  1.93s/it]\u001b[A\n","loss: 0.09296, smth: 0.09884:  38%|      | 3/8 [00:05<00:09,  1.93s/it]\u001b[A\n","loss: 0.09296, smth: 0.09884:  50%|     | 4/8 [00:05<00:06,  1.58s/it]\u001b[A\n","loss: 0.18976, smth: 0.11702:  50%|     | 4/8 [00:06<00:06,  1.58s/it]\u001b[A\n","loss: 0.18976, smth: 0.11702:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.10176, smth: 0.11448:  62%|   | 5/8 [00:07<00:04,  1.50s/it]\u001b[A\n","loss: 0.10176, smth: 0.11448:  75%|  | 6/8 [00:07<00:02,  1.27s/it]\u001b[A\n","loss: 0.10106, smth: 0.11256:  75%|  | 6/8 [00:08<00:02,  1.27s/it]\u001b[A\n","loss: 0.10106, smth: 0.11256:  88%| | 7/8 [00:08<00:01,  1.23s/it]\u001b[A\n","loss: 0.19736, smth: 0.12316:  88%| | 7/8 [00:09<00:01,  1.23s/it]\u001b[A\n","loss: 0.19736, smth: 0.12316: 100%|| 8/8 [00:09<00:00,  1.17s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.14it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.75it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:31 2021 Epoch 198, lr: 0.0000095, train loss: 0.12316, train auc: 0.99783, val loss: 0.45003, val auc: 0.98630\n","Thu Jun 10 01:35:31 2021 Epoch: 199\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.11278, smth: 0.11278:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.11278, smth: 0.11278:  12%|        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.13626, smth: 0.12452:  12%|        | 1/8 [00:03<00:21,  3.04s/it]\u001b[A\n","loss: 0.13626, smth: 0.12452:  25%|       | 2/8 [00:03<00:14,  2.36s/it]\u001b[A\n","loss: 0.13262, smth: 0.12722:  25%|       | 2/8 [00:05<00:14,  2.36s/it]\u001b[A\n","loss: 0.13262, smth: 0.12722:  38%|      | 3/8 [00:05<00:10,  2.20s/it]\u001b[A\n","loss: 0.09638, smth: 0.11951:  38%|      | 3/8 [00:06<00:10,  2.20s/it]\u001b[A\n","loss: 0.09638, smth: 0.11951:  50%|     | 4/8 [00:06<00:07,  1.77s/it]\u001b[A\n","loss: 0.13961, smth: 0.12353:  50%|     | 4/8 [00:07<00:07,  1.77s/it]\u001b[A\n","loss: 0.13961, smth: 0.12353:  62%|   | 5/8 [00:07<00:04,  1.57s/it]\u001b[A\n","loss: 0.17691, smth: 0.13243:  62%|   | 5/8 [00:08<00:04,  1.57s/it]\u001b[A\n","loss: 0.17691, smth: 0.13243:  75%|  | 6/8 [00:08<00:02,  1.32s/it]\u001b[A\n","loss: 0.08848, smth: 0.12615:  75%|  | 6/8 [00:09<00:02,  1.32s/it]\u001b[A\n","loss: 0.08848, smth: 0.12615:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.12513, smth: 0.12602:  88%| | 7/8 [00:09<00:01,  1.21s/it]\u001b[A\n","loss: 0.12513, smth: 0.12602: 100%|| 8/8 [00:09<00:00,  1.21s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  3.15it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.77it/s]\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:42 2021 Epoch 199, lr: 0.0000137, train loss: 0.12602, train auc: 0.99794, val loss: 0.43351, val auc: 0.98699\n","Thu Jun 10 01:35:42 2021 Epoch: 200\n"],"name":"stdout"},{"output_type":"stream","text":["\n","loss: 0.17104, smth: 0.17104:   0%|          | 0/8 [00:02<?, ?it/s]\u001b[A\n","loss: 0.17104, smth: 0.17104:  12%|        | 1/8 [00:02<00:17,  2.54s/it]\u001b[A\n","loss: 0.11597, smth: 0.14350:  12%|        | 1/8 [00:03<00:17,  2.54s/it]\u001b[A\n","loss: 0.11597, smth: 0.14350:  25%|       | 2/8 [00:03<00:12,  2.03s/it]\u001b[A\n","loss: 0.18125, smth: 0.15609:  25%|       | 2/8 [00:04<00:12,  2.03s/it]\u001b[A\n","loss: 0.18125, smth: 0.15609:  38%|      | 3/8 [00:04<00:09,  1.87s/it]\u001b[A\n","loss: 0.10487, smth: 0.14328:  38%|      | 3/8 [00:05<00:09,  1.87s/it]\u001b[A\n","loss: 0.10487, smth: 0.14328:  50%|     | 4/8 [00:05<00:06,  1.53s/it]\u001b[A\n","loss: 0.16007, smth: 0.14664:  50%|     | 4/8 [00:06<00:06,  1.53s/it]\u001b[A\n","loss: 0.16007, smth: 0.14664:  62%|   | 5/8 [00:06<00:04,  1.46s/it]\u001b[A\n","loss: 0.22252, smth: 0.15929:  62%|   | 5/8 [00:07<00:04,  1.46s/it]\u001b[A\n","loss: 0.22252, smth: 0.15929:  75%|  | 6/8 [00:07<00:02,  1.22s/it]\u001b[A\n","loss: 0.14017, smth: 0.15656:  75%|  | 6/8 [00:08<00:02,  1.22s/it]\u001b[A\n","loss: 0.14017, smth: 0.15656:  88%| | 7/8 [00:08<00:01,  1.20s/it]\u001b[A\n","loss: 0.08360, smth: 0.14744:  88%| | 7/8 [00:09<00:01,  1.20s/it]\u001b[A\n","loss: 0.08360, smth: 0.14744: 100%|| 8/8 [00:09<00:00,  1.15s/it]\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n"," 50%|     | 1/2 [00:00<00:00,  2.96it/s]\u001b[A\n","100%|| 2/2 [00:00<00:00,  3.60it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Thu Jun 10 01:35:51 2021 Epoch 200, lr: 0.0000186, train loss: 0.14744, train auc: 0.99842, val loss: 0.45193, val auc: 0.98641\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOaBS7IhxkKk","executionInfo":{"status":"ok","timestamp":1623289671197,"user_tz":-480,"elapsed":313,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"553be18f-1e5e-4301-ef99-3219dedec094"},"source":["model = model.to(device)\n","model.load_state_dict(torch.load(os.path.join(f'{kernel_type}_best_fold{use_fold}.pth')))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"yJvIm17JzdmW"},"source":["# test_file_list = test['Filename'][:10000].values.tolist()\n","test_file_list = test['Filename'].values.tolist()\n","test_loader = data.DataLoader(PANNsDataset(test_file_list, None, training=False), \n","                             batch_size=16, \n","                             shuffle=False,\n","                             num_workers=2,\n","                             pin_memory=True,\n","                             drop_last=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UssJbITwxhqV"},"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"URDc17zoxt4b","executionInfo":{"status":"ok","timestamp":1623289747559,"user_tz":-480,"elapsed":68688,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"71d6a071-604b-4e17-86f0-0b5c73718eee"},"source":["PREDS = []\n","bar = tqdm(test_loader)\n","for d in bar:\n","    data = d['waveform']\n","    data = data.to(device)\n","    with torch.no_grad():\n","        logits = model(data)\n","\n","        # print(torch.exp(logits['clipwise_output']))\n","        # print(torch.exp(logits['clipwise_output']).sum(axis=-1))\n","        # input()\n","        \n","        PREDS.append(torch.exp(logits['clipwise_output']))\n","\n","        # if pred_num != target_num:\n","        #     plt.imshow(data.squeeze().cpu().numpy())\n","        #     plt.axis('off')\n","        #     plt.show()\n","        #     input('next...')\n","\n","PREDS = torch.cat(PREDS).detach().cpu().numpy()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","  0%|          | 0/1875 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 1/1875 [00:00<30:56,  1.01it/s]\u001b[A\n","  0%|          | 6/1875 [00:01<21:48,  1.43it/s]\u001b[A\n","  1%|          | 10/1875 [00:01<15:31,  2.00it/s]\u001b[A\n","  1%|          | 14/1875 [00:01<11:10,  2.78it/s]\u001b[A\n","  1%|          | 18/1875 [00:01<08:09,  3.80it/s]\u001b[A\n","  1%|          | 22/1875 [00:01<06:02,  5.12it/s]\u001b[A\n","  1%|         | 26/1875 [00:01<04:35,  6.71it/s]\u001b[A\n","  2%|         | 30/1875 [00:01<03:28,  8.87it/s]\u001b[A\n","  2%|         | 33/1875 [00:02<02:45, 11.11it/s]\u001b[A\n","  2%|         | 36/1875 [00:02<02:17, 13.39it/s]\u001b[A\n","  2%|         | 40/1875 [00:02<01:54, 15.97it/s]\u001b[A\n","  2%|         | 43/1875 [00:02<01:39, 18.46it/s]\u001b[A\n","  2%|         | 46/1875 [00:02<01:29, 20.38it/s]\u001b[A\n","  3%|         | 49/1875 [00:02<01:21, 22.48it/s]\u001b[A\n","  3%|         | 52/1875 [00:02<01:16, 23.80it/s]\u001b[A\n","  3%|         | 55/1875 [00:02<01:12, 25.23it/s]\u001b[A\n","  3%|         | 58/1875 [00:02<01:11, 25.37it/s]\u001b[A\n","  3%|         | 62/1875 [00:03<01:11, 25.32it/s]\u001b[A\n","  4%|         | 66/1875 [00:03<01:07, 26.63it/s]\u001b[A\n","  4%|         | 69/1875 [00:03<01:07, 26.95it/s]\u001b[A\n","  4%|         | 72/1875 [00:03<01:08, 26.45it/s]\u001b[A\n","  4%|         | 76/1875 [00:03<01:08, 26.18it/s]\u001b[A\n","  4%|         | 80/1875 [00:03<01:03, 28.09it/s]\u001b[A\n","  4%|         | 83/1875 [00:03<01:03, 28.06it/s]\u001b[A\n","  5%|         | 86/1875 [00:03<01:06, 26.73it/s]\u001b[A\n","  5%|         | 90/1875 [00:04<01:06, 26.89it/s]\u001b[A\n","  5%|         | 93/1875 [00:04<01:04, 27.57it/s]\u001b[A\n","  5%|         | 96/1875 [00:04<01:03, 27.94it/s]\u001b[A\n","  5%|         | 100/1875 [00:04<01:01, 28.77it/s]\u001b[A\n","  5%|         | 103/1875 [00:04<01:02, 28.35it/s]\u001b[A\n","  6%|         | 106/1875 [00:04<01:02, 28.29it/s]\u001b[A\n","  6%|         | 109/1875 [00:04<01:02, 28.34it/s]\u001b[A\n","  6%|         | 112/1875 [00:04<01:06, 26.34it/s]\u001b[A\n","  6%|         | 116/1875 [00:05<01:02, 28.09it/s]\u001b[A\n","  6%|         | 119/1875 [00:05<01:03, 27.84it/s]\u001b[A\n","  7%|         | 123/1875 [00:05<01:05, 26.59it/s]\u001b[A\n","  7%|         | 126/1875 [00:05<01:03, 27.42it/s]\u001b[A\n","  7%|         | 129/1875 [00:05<01:03, 27.61it/s]\u001b[A\n","  7%|         | 132/1875 [00:05<01:03, 27.56it/s]\u001b[A\n","  7%|         | 135/1875 [00:05<01:02, 27.74it/s]\u001b[A\n","  7%|         | 138/1875 [00:05<01:04, 26.95it/s]\u001b[A\n","  8%|         | 142/1875 [00:06<01:05, 26.27it/s]\u001b[A\n","  8%|         | 146/1875 [00:06<01:03, 27.11it/s]\u001b[A\n","  8%|         | 150/1875 [00:06<00:59, 28.76it/s]\u001b[A\n","  8%|         | 153/1875 [00:06<00:59, 29.02it/s]\u001b[A\n","  8%|         | 156/1875 [00:06<00:59, 28.85it/s]\u001b[A\n","  8%|         | 159/1875 [00:06<01:05, 26.30it/s]\u001b[A\n","  9%|         | 163/1875 [00:06<01:03, 27.04it/s]\u001b[A\n","  9%|         | 166/1875 [00:06<01:02, 27.13it/s]\u001b[A\n","  9%|         | 169/1875 [00:06<01:01, 27.78it/s]\u001b[A\n","  9%|         | 173/1875 [00:07<01:01, 27.50it/s]\u001b[A\n","  9%|         | 177/1875 [00:07<01:00, 27.91it/s]\u001b[A\n"," 10%|         | 181/1875 [00:07<01:03, 26.87it/s]\u001b[A\n"," 10%|         | 185/1875 [00:07<00:59, 28.25it/s]\u001b[A\n"," 10%|         | 188/1875 [00:07<01:00, 27.84it/s]\u001b[A\n"," 10%|         | 192/1875 [00:07<01:02, 27.10it/s]\u001b[A\n"," 10%|         | 196/1875 [00:07<01:00, 27.83it/s]\u001b[A\n"," 11%|         | 200/1875 [00:08<00:56, 29.79it/s]\u001b[A\n"," 11%|         | 204/1875 [00:08<01:01, 27.32it/s]\u001b[A\n"," 11%|         | 207/1875 [00:08<01:00, 27.80it/s]\u001b[A\n"," 11%|         | 210/1875 [00:08<01:00, 27.65it/s]\u001b[A\n"," 11%|        | 214/1875 [00:08<00:56, 29.55it/s]\u001b[A\n"," 12%|        | 218/1875 [00:08<01:01, 26.97it/s]\u001b[A\n"," 12%|        | 221/1875 [00:08<01:01, 26.97it/s]\u001b[A\n"," 12%|        | 224/1875 [00:08<01:01, 26.90it/s]\u001b[A\n"," 12%|        | 227/1875 [00:09<00:59, 27.58it/s]\u001b[A\n"," 12%|        | 231/1875 [00:09<00:56, 29.29it/s]\u001b[A\n"," 12%|        | 234/1875 [00:09<01:03, 25.95it/s]\u001b[A\n"," 13%|        | 237/1875 [00:09<01:02, 26.21it/s]\u001b[A\n"," 13%|        | 241/1875 [00:09<00:57, 28.31it/s]\u001b[A\n"," 13%|        | 244/1875 [00:09<00:56, 28.71it/s]\u001b[A\n"," 13%|        | 247/1875 [00:09<01:05, 24.87it/s]\u001b[A\n"," 13%|        | 251/1875 [00:09<00:59, 27.50it/s]\u001b[A\n"," 14%|        | 254/1875 [00:10<00:58, 27.54it/s]\u001b[A\n"," 14%|        | 257/1875 [00:10<00:57, 27.97it/s]\u001b[A\n"," 14%|        | 261/1875 [00:10<00:54, 29.51it/s]\u001b[A\n"," 14%|        | 265/1875 [00:10<00:57, 27.77it/s]\u001b[A\n"," 14%|        | 268/1875 [00:10<00:59, 26.83it/s]\u001b[A\n"," 15%|        | 272/1875 [00:10<01:04, 24.90it/s]\u001b[A\n"," 15%|        | 276/1875 [00:10<00:57, 28.01it/s]\u001b[A\n"," 15%|        | 279/1875 [00:10<00:56, 28.17it/s]\u001b[A\n"," 15%|        | 282/1875 [00:11<00:57, 27.63it/s]\u001b[A\n"," 15%|        | 285/1875 [00:11<00:57, 27.54it/s]\u001b[A\n"," 15%|        | 289/1875 [00:11<00:59, 26.76it/s]\u001b[A\n"," 16%|        | 292/1875 [00:11<00:57, 27.48it/s]\u001b[A\n"," 16%|        | 296/1875 [00:11<00:54, 29.09it/s]\u001b[A\n"," 16%|        | 299/1875 [00:11<00:54, 29.18it/s]\u001b[A\n"," 16%|        | 302/1875 [00:11<00:55, 28.55it/s]\u001b[A\n"," 16%|        | 305/1875 [00:11<00:56, 27.64it/s]\u001b[A\n"," 16%|        | 309/1875 [00:12<00:58, 26.57it/s]\u001b[A\n"," 17%|        | 312/1875 [00:12<00:58, 26.65it/s]\u001b[A\n"," 17%|        | 316/1875 [00:12<00:53, 29.01it/s]\u001b[A\n"," 17%|        | 319/1875 [00:12<01:05, 23.79it/s]\u001b[A\n"," 17%|        | 323/1875 [00:12<00:57, 26.90it/s]\u001b[A\n"," 17%|        | 326/1875 [00:12<00:58, 26.57it/s]\u001b[A\n"," 18%|        | 329/1875 [00:12<00:58, 26.22it/s]\u001b[A\n"," 18%|        | 333/1875 [00:12<00:58, 26.28it/s]\u001b[A\n"," 18%|        | 337/1875 [00:13<00:54, 28.38it/s]\u001b[A\n"," 18%|        | 340/1875 [00:13<00:54, 28.29it/s]\u001b[A\n"," 18%|        | 343/1875 [00:13<00:55, 27.41it/s]\u001b[A\n"," 19%|        | 347/1875 [00:13<00:57, 26.49it/s]\u001b[A\n"," 19%|        | 350/1875 [00:13<00:57, 26.49it/s]\u001b[A\n"," 19%|        | 354/1875 [00:13<00:53, 28.57it/s]\u001b[A\n"," 19%|        | 357/1875 [00:13<00:55, 27.12it/s]\u001b[A\n"," 19%|        | 361/1875 [00:13<00:57, 26.20it/s]\u001b[A\n"," 20%|        | 366/1875 [00:14<00:51, 29.52it/s]\u001b[A\n"," 20%|        | 370/1875 [00:14<00:56, 26.85it/s]\u001b[A\n"," 20%|        | 373/1875 [00:14<00:54, 27.65it/s]\u001b[A\n"," 20%|        | 377/1875 [00:14<00:59, 24.98it/s]\u001b[A\n"," 20%|        | 382/1875 [00:14<00:51, 28.86it/s]\u001b[A\n"," 21%|        | 386/1875 [00:14<00:55, 26.94it/s]\u001b[A\n"," 21%|        | 389/1875 [00:14<00:54, 27.43it/s]\u001b[A\n"," 21%|        | 392/1875 [00:15<00:52, 28.04it/s]\u001b[A\n"," 21%|        | 395/1875 [00:15<00:53, 27.53it/s]\u001b[A\n"," 21%|        | 398/1875 [00:15<00:54, 27.23it/s]\u001b[A\n"," 21%|       | 401/1875 [00:15<00:53, 27.72it/s]\u001b[A\n"," 22%|       | 404/1875 [00:15<00:52, 27.83it/s]\u001b[A\n"," 22%|       | 407/1875 [00:15<00:52, 28.20it/s]\u001b[A\n"," 22%|       | 411/1875 [00:15<00:54, 26.87it/s]\u001b[A\n"," 22%|       | 415/1875 [00:15<00:50, 29.05it/s]\u001b[A\n"," 22%|       | 418/1875 [00:15<00:51, 28.19it/s]\u001b[A\n"," 22%|       | 421/1875 [00:16<00:51, 28.44it/s]\u001b[A\n"," 23%|       | 424/1875 [00:16<00:50, 28.78it/s]\u001b[A\n"," 23%|       | 427/1875 [00:16<01:00, 24.01it/s]\u001b[A\n"," 23%|       | 432/1875 [00:16<00:52, 27.65it/s]\u001b[A\n"," 23%|       | 436/1875 [00:16<00:56, 25.34it/s]\u001b[A\n"," 23%|       | 440/1875 [00:16<00:51, 28.12it/s]\u001b[A\n"," 24%|       | 444/1875 [00:16<00:50, 28.56it/s]\u001b[A\n"," 24%|       | 448/1875 [00:17<00:53, 26.90it/s]\u001b[A\n"," 24%|       | 451/1875 [00:17<00:52, 27.25it/s]\u001b[A\n"," 24%|       | 455/1875 [00:17<00:48, 29.11it/s]\u001b[A\n"," 24%|       | 459/1875 [00:17<00:53, 26.55it/s]\u001b[A\n"," 25%|       | 462/1875 [00:17<00:52, 26.75it/s]\u001b[A\n"," 25%|       | 466/1875 [00:17<00:53, 26.13it/s]\u001b[A\n"," 25%|       | 470/1875 [00:17<00:49, 28.36it/s]\u001b[A\n"," 25%|       | 473/1875 [00:17<00:51, 27.43it/s]\u001b[A\n"," 25%|       | 476/1875 [00:18<00:51, 27.17it/s]\u001b[A\n"," 26%|       | 480/1875 [00:18<00:52, 26.36it/s]\u001b[A\n"," 26%|       | 483/1875 [00:18<00:52, 26.74it/s]\u001b[A\n"," 26%|       | 486/1875 [00:18<00:53, 26.11it/s]\u001b[A\n"," 26%|       | 490/1875 [00:18<00:54, 25.52it/s]\u001b[A\n"," 26%|       | 494/1875 [00:18<00:48, 28.49it/s]\u001b[A\n"," 27%|       | 497/1875 [00:18<00:49, 27.82it/s]\u001b[A\n"," 27%|       | 501/1875 [00:18<00:45, 30.36it/s]\u001b[A\n"," 27%|       | 505/1875 [00:19<00:46, 29.63it/s]\u001b[A\n"," 27%|       | 509/1875 [00:19<00:48, 28.04it/s]\u001b[A\n"," 27%|       | 512/1875 [00:19<00:48, 28.39it/s]\u001b[A\n"," 27%|       | 515/1875 [00:19<00:47, 28.50it/s]\u001b[A\n"," 28%|       | 518/1875 [00:19<00:56, 24.09it/s]\u001b[A\n"," 28%|       | 521/1875 [00:19<00:54, 24.68it/s]\u001b[A\n"," 28%|       | 525/1875 [00:19<00:48, 27.60it/s]\u001b[A\n"," 28%|       | 529/1875 [00:19<00:45, 29.76it/s]\u001b[A\n"," 28%|       | 533/1875 [00:20<00:49, 27.28it/s]\u001b[A\n"," 29%|       | 536/1875 [00:20<00:49, 27.19it/s]\u001b[A\n"," 29%|       | 540/1875 [00:20<00:44, 29.83it/s]\u001b[A\n"," 29%|       | 544/1875 [00:20<00:51, 25.62it/s]\u001b[A\n"," 29%|       | 549/1875 [00:20<00:50, 26.02it/s]\u001b[A\n"," 30%|       | 554/1875 [00:20<00:44, 29.47it/s]\u001b[A\n"," 30%|       | 558/1875 [00:21<00:48, 27.23it/s]\u001b[A\n"," 30%|       | 561/1875 [00:21<00:47, 27.38it/s]\u001b[A\n"," 30%|       | 564/1875 [00:21<00:47, 27.53it/s]\u001b[A\n"," 30%|       | 568/1875 [00:21<00:44, 29.37it/s]\u001b[A\n"," 31%|       | 572/1875 [00:21<00:49, 26.09it/s]\u001b[A\n"," 31%|       | 575/1875 [00:21<00:57, 22.56it/s]\u001b[A\n"," 31%|       | 580/1875 [00:21<00:49, 26.20it/s]\u001b[A\n"," 31%|       | 584/1875 [00:22<00:51, 25.07it/s]\u001b[A\n"," 31%|      | 588/1875 [00:22<00:46, 27.95it/s]\u001b[A\n"," 32%|      | 592/1875 [00:22<00:43, 29.71it/s]\u001b[A\n"," 32%|      | 596/1875 [00:22<00:44, 28.84it/s]\u001b[A\n"," 32%|      | 600/1875 [00:22<00:45, 28.15it/s]\u001b[A\n"," 32%|      | 603/1875 [00:22<00:44, 28.27it/s]\u001b[A\n"," 32%|      | 606/1875 [00:22<00:47, 26.69it/s]\u001b[A\n"," 33%|      | 610/1875 [00:22<00:48, 26.25it/s]\u001b[A\n"," 33%|      | 614/1875 [00:23<00:44, 28.48it/s]\u001b[A\n"," 33%|      | 617/1875 [00:23<00:44, 28.34it/s]\u001b[A\n"," 33%|      | 620/1875 [00:23<00:53, 23.55it/s]\u001b[A\n"," 33%|      | 625/1875 [00:23<00:46, 27.14it/s]\u001b[A\n"," 34%|      | 629/1875 [00:23<00:48, 25.45it/s]\u001b[A\n"," 34%|      | 632/1875 [00:23<00:49, 24.93it/s]\u001b[A\n"," 34%|      | 636/1875 [00:23<00:47, 26.12it/s]\u001b[A\n"," 34%|      | 640/1875 [00:24<00:46, 26.59it/s]\u001b[A\n"," 34%|      | 644/1875 [00:24<00:46, 26.47it/s]\u001b[A\n"," 35%|      | 648/1875 [00:24<00:44, 27.33it/s]\u001b[A\n"," 35%|      | 652/1875 [00:24<00:44, 27.55it/s]\u001b[A\n"," 35%|      | 655/1875 [00:24<00:43, 28.18it/s]\u001b[A\n"," 35%|      | 658/1875 [00:24<00:45, 26.83it/s]\u001b[A\n"," 35%|      | 662/1875 [00:24<00:45, 26.74it/s]\u001b[A\n"," 35%|      | 665/1875 [00:24<00:44, 27.34it/s]\u001b[A\n"," 36%|      | 668/1875 [00:25<00:44, 26.95it/s]\u001b[A\n"," 36%|      | 672/1875 [00:25<00:41, 29.26it/s]\u001b[A\n"," 36%|      | 676/1875 [00:25<00:42, 28.18it/s]\u001b[A\n"," 36%|      | 680/1875 [00:25<00:42, 28.12it/s]\u001b[A\n"," 36%|      | 684/1875 [00:25<00:40, 29.71it/s]\u001b[A\n"," 37%|      | 688/1875 [00:25<00:41, 28.34it/s]\u001b[A\n"," 37%|      | 692/1875 [00:25<00:41, 28.71it/s]\u001b[A\n"," 37%|      | 696/1875 [00:26<00:42, 28.01it/s]\u001b[A\n"," 37%|      | 700/1875 [00:26<00:42, 27.87it/s]\u001b[A\n"," 38%|      | 704/1875 [00:26<00:43, 26.94it/s]\u001b[A\n"," 38%|      | 708/1875 [00:26<00:42, 27.69it/s]\u001b[A\n"," 38%|      | 712/1875 [00:26<00:40, 29.03it/s]\u001b[A\n"," 38%|      | 715/1875 [00:26<00:41, 27.90it/s]\u001b[A\n"," 38%|      | 719/1875 [00:26<00:39, 29.27it/s]\u001b[A\n"," 39%|      | 722/1875 [00:26<00:42, 27.00it/s]\u001b[A\n"," 39%|      | 725/1875 [00:27<00:42, 26.98it/s]\u001b[A\n"," 39%|      | 728/1875 [00:27<00:44, 25.85it/s]\u001b[A\n"," 39%|      | 731/1875 [00:27<00:43, 26.54it/s]\u001b[A\n"," 39%|      | 734/1875 [00:27<00:44, 25.65it/s]\u001b[A\n"," 39%|      | 738/1875 [00:27<00:42, 26.90it/s]\u001b[A\n"," 40%|      | 742/1875 [00:27<00:40, 27.68it/s]\u001b[A\n"," 40%|      | 746/1875 [00:27<00:41, 26.89it/s]\u001b[A\n"," 40%|      | 750/1875 [00:27<00:39, 28.56it/s]\u001b[A\n"," 40%|      | 754/1875 [00:28<00:40, 27.54it/s]\u001b[A\n"," 40%|      | 758/1875 [00:28<00:39, 28.04it/s]\u001b[A\n"," 41%|      | 761/1875 [00:28<00:38, 28.57it/s]\u001b[A\n"," 41%|      | 764/1875 [00:28<00:39, 27.82it/s]\u001b[A\n"," 41%|      | 767/1875 [00:28<00:39, 27.74it/s]\u001b[A\n"," 41%|      | 770/1875 [00:28<00:39, 27.81it/s]\u001b[A\n"," 41%|     | 774/1875 [00:28<00:41, 26.76it/s]\u001b[A\n"," 41%|     | 778/1875 [00:28<00:39, 27.54it/s]\u001b[A\n"," 42%|     | 782/1875 [00:29<00:39, 27.79it/s]\u001b[A\n"," 42%|     | 786/1875 [00:29<00:37, 28.75it/s]\u001b[A\n"," 42%|     | 789/1875 [00:29<00:37, 28.98it/s]\u001b[A\n"," 42%|     | 792/1875 [00:29<00:39, 27.64it/s]\u001b[A\n"," 42%|     | 796/1875 [00:29<00:39, 27.26it/s]\u001b[A\n"," 43%|     | 800/1875 [00:29<00:38, 27.61it/s]\u001b[A\n"," 43%|     | 804/1875 [00:29<00:36, 29.47it/s]\u001b[A\n"," 43%|     | 807/1875 [00:29<00:36, 29.57it/s]\u001b[A\n"," 43%|     | 810/1875 [00:30<00:39, 27.16it/s]\u001b[A\n"," 43%|     | 814/1875 [00:30<00:38, 27.28it/s]\u001b[A\n"," 44%|     | 817/1875 [00:30<00:38, 27.62it/s]\u001b[A\n"," 44%|     | 820/1875 [00:30<00:37, 28.14it/s]\u001b[A\n"," 44%|     | 824/1875 [00:30<00:38, 26.99it/s]\u001b[A\n"," 44%|     | 827/1875 [00:30<00:38, 27.29it/s]\u001b[A\n"," 44%|     | 830/1875 [00:30<00:38, 27.42it/s]\u001b[A\n"," 44%|     | 834/1875 [00:30<00:36, 28.67it/s]\u001b[A\n"," 45%|     | 838/1875 [00:31<00:36, 28.06it/s]\u001b[A\n"," 45%|     | 842/1875 [00:31<00:36, 28.14it/s]\u001b[A\n"," 45%|     | 846/1875 [00:31<00:38, 26.52it/s]\u001b[A\n"," 45%|     | 850/1875 [00:31<00:36, 28.39it/s]\u001b[A\n"," 45%|     | 853/1875 [00:31<00:36, 27.96it/s]\u001b[A\n"," 46%|     | 856/1875 [00:31<00:37, 27.36it/s]\u001b[A\n"," 46%|     | 860/1875 [00:31<00:36, 27.55it/s]\u001b[A\n"," 46%|     | 864/1875 [00:32<00:35, 28.63it/s]\u001b[A\n"," 46%|     | 867/1875 [00:32<00:35, 28.76it/s]\u001b[A\n"," 46%|     | 870/1875 [00:32<00:35, 28.36it/s]\u001b[A\n"," 47%|     | 874/1875 [00:32<00:33, 30.01it/s]\u001b[A\n"," 47%|     | 878/1875 [00:32<00:35, 28.39it/s]\u001b[A\n"," 47%|     | 881/1875 [00:32<00:34, 28.50it/s]\u001b[A\n"," 47%|     | 884/1875 [00:32<00:35, 28.25it/s]\u001b[A\n"," 47%|     | 887/1875 [00:32<00:34, 28.67it/s]\u001b[A\n"," 48%|     | 891/1875 [00:32<00:32, 30.20it/s]\u001b[A\n"," 48%|     | 895/1875 [00:33<00:35, 27.59it/s]\u001b[A\n"," 48%|     | 898/1875 [00:33<00:34, 28.24it/s]\u001b[A\n"," 48%|     | 901/1875 [00:33<00:34, 28.07it/s]\u001b[A\n"," 48%|     | 904/1875 [00:33<00:36, 26.86it/s]\u001b[A\n"," 48%|     | 907/1875 [00:33<00:35, 27.41it/s]\u001b[A\n"," 49%|     | 911/1875 [00:33<00:33, 28.91it/s]\u001b[A\n"," 49%|     | 914/1875 [00:33<00:37, 25.89it/s]\u001b[A\n"," 49%|     | 917/1875 [00:33<00:35, 26.72it/s]\u001b[A\n"," 49%|     | 920/1875 [00:34<00:34, 27.57it/s]\u001b[A\n"," 49%|     | 923/1875 [00:34<00:34, 27.70it/s]\u001b[A\n"," 49%|     | 926/1875 [00:34<00:34, 27.78it/s]\u001b[A\n"," 50%|     | 929/1875 [00:34<00:33, 28.21it/s]\u001b[A\n"," 50%|     | 933/1875 [00:34<00:31, 30.05it/s]\u001b[A\n"," 50%|     | 937/1875 [00:34<00:34, 26.92it/s]\u001b[A\n"," 50%|     | 941/1875 [00:34<00:32, 28.97it/s]\u001b[A\n"," 50%|     | 945/1875 [00:34<00:34, 26.86it/s]\u001b[A\n"," 51%|     | 949/1875 [00:35<00:31, 29.15it/s]\u001b[A\n"," 51%|     | 953/1875 [00:35<00:30, 29.77it/s]\u001b[A\n"," 51%|     | 957/1875 [00:35<00:33, 27.46it/s]\u001b[A\n"," 51%|    | 961/1875 [00:35<00:31, 28.94it/s]\u001b[A\n"," 51%|    | 964/1875 [00:35<00:35, 25.79it/s]\u001b[A\n"," 52%|    | 968/1875 [00:35<00:32, 28.19it/s]\u001b[A\n"," 52%|    | 971/1875 [00:35<00:32, 27.91it/s]\u001b[A\n"," 52%|    | 975/1875 [00:35<00:30, 29.68it/s]\u001b[A\n"," 52%|    | 979/1875 [00:36<00:32, 27.66it/s]\u001b[A\n"," 52%|    | 983/1875 [00:36<00:30, 29.30it/s]\u001b[A\n"," 53%|    | 987/1875 [00:36<00:33, 26.37it/s]\u001b[A\n"," 53%|    | 991/1875 [00:36<00:31, 28.46it/s]\u001b[A\n"," 53%|    | 995/1875 [00:36<00:32, 26.96it/s]\u001b[A\n"," 53%|    | 998/1875 [00:36<00:32, 26.65it/s]\u001b[A\n"," 53%|    | 1002/1875 [00:36<00:31, 28.07it/s]\u001b[A\n"," 54%|    | 1005/1875 [00:37<00:30, 28.10it/s]\u001b[A\n"," 54%|    | 1008/1875 [00:37<00:30, 28.14it/s]\u001b[A\n"," 54%|    | 1011/1875 [00:37<00:31, 27.44it/s]\u001b[A\n"," 54%|    | 1014/1875 [00:37<00:33, 25.68it/s]\u001b[A\n"," 54%|    | 1018/1875 [00:37<00:31, 27.20it/s]\u001b[A\n"," 55%|    | 1022/1875 [00:37<00:29, 29.10it/s]\u001b[A\n"," 55%|    | 1025/1875 [00:37<00:29, 28.40it/s]\u001b[A\n"," 55%|    | 1028/1875 [00:37<00:29, 28.84it/s]\u001b[A\n"," 55%|    | 1031/1875 [00:37<00:29, 28.34it/s]\u001b[A\n"," 55%|    | 1035/1875 [00:38<00:30, 27.73it/s]\u001b[A\n"," 55%|    | 1038/1875 [00:38<00:30, 27.78it/s]\u001b[A\n"," 56%|    | 1041/1875 [00:38<00:29, 27.83it/s]\u001b[A\n"," 56%|    | 1044/1875 [00:38<00:29, 27.98it/s]\u001b[A\n"," 56%|    | 1047/1875 [00:38<00:29, 27.95it/s]\u001b[A\n"," 56%|    | 1051/1875 [00:38<00:30, 27.23it/s]\u001b[A\n"," 56%|    | 1055/1875 [00:38<00:29, 27.81it/s]\u001b[A\n"," 56%|    | 1059/1875 [00:38<00:27, 29.39it/s]\u001b[A\n"," 57%|    | 1062/1875 [00:39<00:28, 28.55it/s]\u001b[A\n"," 57%|    | 1065/1875 [00:39<00:31, 25.50it/s]\u001b[A\n"," 57%|    | 1069/1875 [00:39<00:29, 27.78it/s]\u001b[A\n"," 57%|    | 1072/1875 [00:39<00:28, 27.71it/s]\u001b[A\n"," 57%|    | 1076/1875 [00:39<00:27, 29.59it/s]\u001b[A\n"," 58%|    | 1080/1875 [00:39<00:28, 27.54it/s]\u001b[A\n"," 58%|    | 1083/1875 [00:39<00:29, 27.28it/s]\u001b[A\n"," 58%|    | 1087/1875 [00:39<00:26, 29.34it/s]\u001b[A\n"," 58%|    | 1091/1875 [00:40<00:29, 27.02it/s]\u001b[A\n"," 58%|    | 1094/1875 [00:40<00:29, 26.60it/s]\u001b[A\n"," 59%|    | 1097/1875 [00:40<00:29, 26.40it/s]\u001b[A\n"," 59%|    | 1100/1875 [00:40<00:28, 26.83it/s]\u001b[A\n"," 59%|    | 1104/1875 [00:40<00:28, 26.73it/s]\u001b[A\n"," 59%|    | 1108/1875 [00:40<00:26, 28.43it/s]\u001b[A\n"," 59%|    | 1111/1875 [00:40<00:27, 27.85it/s]\u001b[A\n"," 59%|    | 1114/1875 [00:40<00:27, 27.90it/s]\u001b[A\n"," 60%|    | 1118/1875 [00:41<00:27, 27.39it/s]\u001b[A\n"," 60%|    | 1122/1875 [00:41<00:26, 28.41it/s]\u001b[A\n"," 60%|    | 1125/1875 [00:41<00:26, 28.08it/s]\u001b[A\n"," 60%|    | 1128/1875 [00:41<00:26, 28.59it/s]\u001b[A\n"," 60%|    | 1132/1875 [00:41<00:27, 26.80it/s]\u001b[A\n"," 61%|    | 1136/1875 [00:41<00:26, 28.41it/s]\u001b[A\n"," 61%|    | 1139/1875 [00:41<00:25, 28.68it/s]\u001b[A\n"," 61%|    | 1142/1875 [00:41<00:26, 27.25it/s]\u001b[A\n"," 61%|    | 1146/1875 [00:42<00:26, 27.29it/s]\u001b[A\n"," 61%|   | 1149/1875 [00:42<00:26, 27.27it/s]\u001b[A\n"," 61%|   | 1153/1875 [00:42<00:24, 29.22it/s]\u001b[A\n"," 62%|   | 1156/1875 [00:42<00:25, 28.05it/s]\u001b[A\n"," 62%|   | 1160/1875 [00:42<00:26, 26.59it/s]\u001b[A\n"," 62%|   | 1164/1875 [00:42<00:25, 28.34it/s]\u001b[A\n"," 62%|   | 1167/1875 [00:42<00:24, 28.44it/s]\u001b[A\n"," 62%|   | 1170/1875 [00:42<00:25, 27.94it/s]\u001b[A\n"," 63%|   | 1174/1875 [00:43<00:26, 26.88it/s]\u001b[A\n"," 63%|   | 1177/1875 [00:43<00:25, 27.11it/s]\u001b[A\n"," 63%|   | 1180/1875 [00:43<00:25, 27.65it/s]\u001b[A\n"," 63%|   | 1183/1875 [00:43<00:24, 28.14it/s]\u001b[A\n"," 63%|   | 1186/1875 [00:43<00:24, 28.15it/s]\u001b[A\n"," 63%|   | 1189/1875 [00:43<00:24, 28.57it/s]\u001b[A\n"," 64%|   | 1192/1875 [00:43<00:24, 27.61it/s]\u001b[A\n"," 64%|   | 1196/1875 [00:43<00:25, 26.26it/s]\u001b[A\n"," 64%|   | 1199/1875 [00:44<00:24, 27.17it/s]\u001b[A\n"," 64%|   | 1203/1875 [00:44<00:23, 28.68it/s]\u001b[A\n"," 64%|   | 1206/1875 [00:44<00:23, 29.03it/s]\u001b[A\n"," 64%|   | 1209/1875 [00:44<00:26, 24.79it/s]\u001b[A\n"," 65%|   | 1213/1875 [00:44<00:24, 27.24it/s]\u001b[A\n"," 65%|   | 1216/1875 [00:44<00:24, 27.06it/s]\u001b[A\n"," 65%|   | 1219/1875 [00:44<00:24, 27.06it/s]\u001b[A\n"," 65%|   | 1223/1875 [00:44<00:25, 25.98it/s]\u001b[A\n"," 65%|   | 1226/1875 [00:45<00:24, 26.16it/s]\u001b[A\n"," 66%|   | 1230/1875 [00:45<00:22, 28.25it/s]\u001b[A\n"," 66%|   | 1233/1875 [00:45<00:22, 28.14it/s]\u001b[A\n"," 66%|   | 1236/1875 [00:45<00:22, 27.82it/s]\u001b[A\n"," 66%|   | 1239/1875 [00:45<00:25, 25.01it/s]\u001b[A\n"," 66%|   | 1243/1875 [00:45<00:22, 27.82it/s]\u001b[A\n"," 66%|   | 1246/1875 [00:45<00:23, 26.97it/s]\u001b[A\n"," 67%|   | 1250/1875 [00:45<00:21, 28.91it/s]\u001b[A\n"," 67%|   | 1254/1875 [00:46<00:23, 26.57it/s]\u001b[A\n"," 67%|   | 1258/1875 [00:46<00:21, 28.70it/s]\u001b[A\n"," 67%|   | 1262/1875 [00:46<00:22, 27.41it/s]\u001b[A\n"," 67%|   | 1265/1875 [00:46<00:21, 28.13it/s]\u001b[A\n"," 68%|   | 1268/1875 [00:46<00:21, 28.08it/s]\u001b[A\n"," 68%|   | 1271/1875 [00:46<00:21, 28.03it/s]\u001b[A\n"," 68%|   | 1274/1875 [00:46<00:21, 27.84it/s]\u001b[A\n"," 68%|   | 1278/1875 [00:46<00:20, 29.57it/s]\u001b[A\n"," 68%|   | 1282/1875 [00:47<00:22, 26.65it/s]\u001b[A\n"," 69%|   | 1285/1875 [00:47<00:23, 25.60it/s]\u001b[A\n"," 69%|   | 1289/1875 [00:47<00:22, 25.62it/s]\u001b[A\n"," 69%|   | 1293/1875 [00:47<00:20, 28.46it/s]\u001b[A\n"," 69%|   | 1296/1875 [00:47<00:20, 27.88it/s]\u001b[A\n"," 69%|   | 1300/1875 [00:47<00:19, 30.26it/s]\u001b[A\n"," 70%|   | 1304/1875 [00:47<00:22, 25.93it/s]\u001b[A\n"," 70%|   | 1307/1875 [00:47<00:21, 26.41it/s]\u001b[A\n"," 70%|   | 1311/1875 [00:48<00:22, 25.61it/s]\u001b[A\n"," 70%|   | 1316/1875 [00:48<00:19, 29.18it/s]\u001b[A\n"," 70%|   | 1320/1875 [00:48<00:19, 28.54it/s]\u001b[A\n"," 71%|   | 1324/1875 [00:48<00:20, 27.39it/s]\u001b[A\n"," 71%|   | 1327/1875 [00:48<00:19, 27.72it/s]\u001b[A\n"," 71%|   | 1331/1875 [00:48<00:20, 26.26it/s]\u001b[A\n"," 71%|   | 1335/1875 [00:48<00:18, 28.54it/s]\u001b[A\n"," 71%|  | 1338/1875 [00:49<00:19, 27.56it/s]\u001b[A\n"," 72%|  | 1341/1875 [00:49<00:22, 23.97it/s]\u001b[A\n"," 72%|  | 1345/1875 [00:49<00:19, 26.51it/s]\u001b[A\n"," 72%|  | 1348/1875 [00:49<00:19, 27.41it/s]\u001b[A\n"," 72%|  | 1351/1875 [00:49<00:18, 28.05it/s]\u001b[A\n"," 72%|  | 1354/1875 [00:49<00:18, 27.68it/s]\u001b[A\n"," 72%|  | 1357/1875 [00:49<00:18, 27.34it/s]\u001b[A\n"," 73%|  | 1361/1875 [00:49<00:19, 26.21it/s]\u001b[A\n"," 73%|  | 1366/1875 [00:50<00:17, 29.54it/s]\u001b[A\n"," 73%|  | 1370/1875 [00:50<00:18, 27.17it/s]\u001b[A\n"," 73%|  | 1373/1875 [00:50<00:18, 27.28it/s]\u001b[A\n"," 73%|  | 1376/1875 [00:50<00:18, 27.40it/s]\u001b[A\n"," 74%|  | 1380/1875 [00:50<00:16, 29.74it/s]\u001b[A\n"," 74%|  | 1384/1875 [00:50<00:17, 27.49it/s]\u001b[A\n"," 74%|  | 1387/1875 [00:50<00:17, 28.07it/s]\u001b[A\n"," 74%|  | 1390/1875 [00:50<00:17, 27.08it/s]\u001b[A\n"," 74%|  | 1394/1875 [00:51<00:16, 29.11it/s]\u001b[A\n"," 75%|  | 1397/1875 [00:51<00:19, 24.00it/s]\u001b[A\n"," 75%|  | 1402/1875 [00:51<00:17, 27.63it/s]\u001b[A\n"," 75%|  | 1406/1875 [00:51<00:17, 26.10it/s]\u001b[A\n"," 75%|  | 1409/1875 [00:51<00:17, 26.21it/s]\u001b[A\n"," 75%|  | 1412/1875 [00:51<00:17, 26.71it/s]\u001b[A\n"," 75%|  | 1415/1875 [00:51<00:17, 26.07it/s]\u001b[A\n"," 76%|  | 1418/1875 [00:51<00:17, 26.24it/s]\u001b[A\n"," 76%|  | 1422/1875 [00:52<00:15, 28.64it/s]\u001b[A\n"," 76%|  | 1425/1875 [00:52<00:17, 26.07it/s]\u001b[A\n"," 76%|  | 1429/1875 [00:52<00:15, 28.81it/s]\u001b[A\n"," 76%|  | 1433/1875 [00:52<00:17, 25.62it/s]\u001b[A\n"," 77%|  | 1437/1875 [00:52<00:15, 28.48it/s]\u001b[A\n"," 77%|  | 1441/1875 [00:52<00:15, 28.71it/s]\u001b[A\n"," 77%|  | 1445/1875 [00:52<00:15, 27.25it/s]\u001b[A\n"," 77%|  | 1448/1875 [00:53<00:15, 26.83it/s]\u001b[A\n"," 77%|  | 1451/1875 [00:53<00:15, 26.67it/s]\u001b[A\n"," 78%|  | 1455/1875 [00:53<00:16, 25.82it/s]\u001b[A\n"," 78%|  | 1458/1875 [00:53<00:15, 26.12it/s]\u001b[A\n"," 78%|  | 1462/1875 [00:53<00:14, 28.38it/s]\u001b[A\n"," 78%|  | 1465/1875 [00:53<00:17, 23.86it/s]\u001b[A\n"," 78%|  | 1470/1875 [00:53<00:14, 27.31it/s]\u001b[A\n"," 79%|  | 1474/1875 [00:53<00:13, 29.80it/s]\u001b[A\n"," 79%|  | 1478/1875 [00:54<00:15, 25.94it/s]\u001b[A\n"," 79%|  | 1483/1875 [00:54<00:14, 26.80it/s]\u001b[A\n"," 79%|  | 1487/1875 [00:54<00:13, 28.93it/s]\u001b[A\n"," 80%|  | 1491/1875 [00:54<00:14, 26.25it/s]\u001b[A\n"," 80%|  | 1495/1875 [00:54<00:13, 27.64it/s]\u001b[A\n"," 80%|  | 1499/1875 [00:54<00:12, 29.42it/s]\u001b[A\n"," 80%|  | 1503/1875 [00:55<00:13, 27.06it/s]\u001b[A\n"," 80%|  | 1506/1875 [00:55<00:13, 27.05it/s]\u001b[A\n"," 80%|  | 1509/1875 [00:55<00:13, 27.07it/s]\u001b[A\n"," 81%|  | 1513/1875 [00:55<00:13, 26.31it/s]\u001b[A\n"," 81%|  | 1517/1875 [00:55<00:12, 28.87it/s]\u001b[A\n"," 81%|  | 1520/1875 [00:55<00:12, 29.02it/s]\u001b[A\n"," 81%|  | 1523/1875 [00:55<00:12, 29.04it/s]\u001b[A\n"," 81%| | 1526/1875 [00:55<00:12, 28.78it/s]\u001b[A\n"," 82%| | 1529/1875 [00:55<00:12, 28.64it/s]\u001b[A\n"," 82%| | 1532/1875 [00:56<00:11, 28.62it/s]\u001b[A\n"," 82%| | 1535/1875 [00:56<00:14, 23.80it/s]\u001b[A\n"," 82%| | 1540/1875 [00:56<00:12, 27.42it/s]\u001b[A\n"," 82%| | 1544/1875 [00:56<00:13, 25.15it/s]\u001b[A\n"," 83%| | 1547/1875 [00:56<00:12, 25.78it/s]\u001b[A\n"," 83%| | 1551/1875 [00:56<00:11, 28.59it/s]\u001b[A\n"," 83%| | 1555/1875 [00:56<00:10, 29.70it/s]\u001b[A\n"," 83%| | 1559/1875 [00:57<00:11, 27.61it/s]\u001b[A\n"," 83%| | 1562/1875 [00:57<00:11, 27.43it/s]\u001b[A\n"," 84%| | 1566/1875 [00:57<00:10, 29.16it/s]\u001b[A\n"," 84%| | 1570/1875 [00:57<00:11, 26.78it/s]\u001b[A\n"," 84%| | 1573/1875 [00:57<00:11, 26.10it/s]\u001b[A\n"," 84%| | 1577/1875 [00:57<00:11, 25.28it/s]\u001b[A\n"," 84%| | 1582/1875 [00:57<00:10, 29.26it/s]\u001b[A\n"," 85%| | 1586/1875 [00:58<00:11, 25.70it/s]\u001b[A\n"," 85%| | 1590/1875 [00:58<00:10, 28.29it/s]\u001b[A\n"," 85%| | 1594/1875 [00:58<00:10, 25.69it/s]\u001b[A\n"," 85%| | 1598/1875 [00:58<00:09, 28.33it/s]\u001b[A\n"," 85%| | 1602/1875 [00:58<00:10, 26.18it/s]\u001b[A\n"," 86%| | 1606/1875 [00:58<00:09, 28.55it/s]\u001b[A\n"," 86%| | 1610/1875 [00:58<00:10, 25.96it/s]\u001b[A\n"," 86%| | 1614/1875 [00:59<00:09, 28.13it/s]\u001b[A\n"," 86%| | 1617/1875 [00:59<00:11, 23.23it/s]\u001b[A\n"," 87%| | 1622/1875 [00:59<00:09, 26.80it/s]\u001b[A\n"," 87%| | 1626/1875 [00:59<00:08, 29.26it/s]\u001b[A\n"," 87%| | 1630/1875 [00:59<00:08, 27.32it/s]\u001b[A\n"," 87%| | 1633/1875 [00:59<00:09, 26.33it/s]\u001b[A\n"," 87%| | 1637/1875 [00:59<00:08, 28.97it/s]\u001b[A\n"," 88%| | 1641/1875 [01:00<00:09, 25.99it/s]\u001b[A\n"," 88%| | 1645/1875 [01:00<00:08, 28.11it/s]\u001b[A\n"," 88%| | 1649/1875 [01:00<00:07, 28.90it/s]\u001b[A\n"," 88%| | 1653/1875 [01:00<00:08, 27.34it/s]\u001b[A\n"," 88%| | 1656/1875 [01:00<00:08, 26.37it/s]\u001b[A\n"," 89%| | 1660/1875 [01:00<00:08, 25.88it/s]\u001b[A\n"," 89%| | 1663/1875 [01:00<00:07, 26.55it/s]\u001b[A\n"," 89%| | 1666/1875 [01:00<00:07, 27.25it/s]\u001b[A\n"," 89%| | 1670/1875 [01:01<00:07, 29.00it/s]\u001b[A\n"," 89%| | 1674/1875 [01:01<00:07, 27.03it/s]\u001b[A\n"," 89%| | 1677/1875 [01:01<00:07, 27.24it/s]\u001b[A\n"," 90%| | 1681/1875 [01:01<00:06, 29.74it/s]\u001b[A\n"," 90%| | 1685/1875 [01:01<00:07, 25.74it/s]\u001b[A\n"," 90%| | 1689/1875 [01:01<00:06, 28.18it/s]\u001b[A\n"," 90%| | 1693/1875 [01:01<00:06, 29.53it/s]\u001b[A\n"," 91%| | 1697/1875 [01:02<00:06, 28.04it/s]\u001b[A\n"," 91%| | 1700/1875 [01:02<00:07, 23.24it/s]\u001b[A\n"," 91%| | 1704/1875 [01:02<00:06, 26.29it/s]\u001b[A\n"," 91%| | 1708/1875 [01:02<00:05, 29.20it/s]\u001b[A\n"," 91%|| 1712/1875 [01:02<00:06, 25.66it/s]\u001b[A\n"," 92%|| 1716/1875 [01:02<00:05, 28.47it/s]\u001b[A\n"," 92%|| 1720/1875 [01:02<00:06, 25.65it/s]\u001b[A\n"," 92%|| 1724/1875 [01:03<00:05, 28.47it/s]\u001b[A\n"," 92%|| 1728/1875 [01:03<00:05, 28.56it/s]\u001b[A\n"," 92%|| 1732/1875 [01:03<00:05, 27.38it/s]\u001b[A\n"," 93%|| 1735/1875 [01:03<00:05, 27.49it/s]\u001b[A\n"," 93%|| 1738/1875 [01:03<00:05, 25.84it/s]\u001b[A\n"," 93%|| 1742/1875 [01:03<00:05, 25.75it/s]\u001b[A\n"," 93%|| 1747/1875 [01:03<00:04, 29.24it/s]\u001b[A\n"," 93%|| 1751/1875 [01:03<00:04, 28.76it/s]\u001b[A\n"," 94%|| 1755/1875 [01:04<00:04, 26.94it/s]\u001b[A\n"," 94%|| 1760/1875 [01:04<00:04, 27.18it/s]\u001b[A\n"," 94%|| 1763/1875 [01:04<00:04, 27.57it/s]\u001b[A\n"," 94%|| 1767/1875 [01:04<00:03, 29.48it/s]\u001b[A\n"," 94%|| 1771/1875 [01:04<00:03, 26.42it/s]\u001b[A\n"," 95%|| 1775/1875 [01:04<00:03, 28.25it/s]\u001b[A\n"," 95%|| 1779/1875 [01:05<00:03, 26.88it/s]\u001b[A\n"," 95%|| 1783/1875 [01:05<00:03, 29.21it/s]\u001b[A\n"," 95%|| 1787/1875 [01:05<00:03, 25.79it/s]\u001b[A\n"," 96%|| 1792/1875 [01:05<00:02, 29.48it/s]\u001b[A\n"," 96%|| 1796/1875 [01:05<00:02, 26.59it/s]\u001b[A\n"," 96%|| 1799/1875 [01:05<00:02, 26.48it/s]\u001b[A\n"," 96%|| 1802/1875 [01:05<00:02, 26.71it/s]\u001b[A\n"," 96%|| 1806/1875 [01:06<00:02, 26.03it/s]\u001b[A\n"," 97%|| 1810/1875 [01:06<00:02, 28.61it/s]\u001b[A\n"," 97%|| 1814/1875 [01:06<00:02, 25.30it/s]\u001b[A\n"," 97%|| 1819/1875 [01:06<00:01, 28.94it/s]\u001b[A\n"," 97%|| 1823/1875 [01:06<00:01, 28.44it/s]\u001b[A\n"," 97%|| 1827/1875 [01:06<00:01, 27.45it/s]\u001b[A\n"," 98%|| 1830/1875 [01:06<00:01, 22.99it/s]\u001b[A\n"," 98%|| 1835/1875 [01:07<00:01, 26.70it/s]\u001b[A\n"," 98%|| 1839/1875 [01:07<00:01, 27.10it/s]\u001b[A\n"," 98%|| 1842/1875 [01:07<00:01, 24.99it/s]\u001b[A\n"," 99%|| 1847/1875 [01:07<00:00, 28.37it/s]\u001b[A\n"," 99%|| 1851/1875 [01:07<00:00, 28.15it/s]\u001b[A\n"," 99%|| 1855/1875 [01:07<00:00, 27.58it/s]\u001b[A\n"," 99%|| 1859/1875 [01:07<00:00, 29.13it/s]\u001b[A\n"," 99%|| 1863/1875 [01:08<00:00, 26.80it/s]\u001b[A\n","100%|| 1866/1875 [01:08<00:00, 25.69it/s]\u001b[A\n","100%|| 1870/1875 [01:08<00:00, 25.48it/s]\u001b[A\n","100%|| 1875/1875 [01:08<00:00, 27.38it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"BdQ558ooxwQJ","executionInfo":{"status":"ok","timestamp":1623289748009,"user_tz":-480,"elapsed":460,"user":{"displayName":"newman chiang","photoUrl":"","userId":"07326278778598898302"}},"outputId":"e22387c1-adb1-4c49-8df5-b2da32baef6f"},"source":["# test.iloc[:10000, 1:] = F.softmax(torch.tensor(PREDS), dim=-1).numpy()\n","# test.iloc[:10000, 1:] = torch.tensor(PREDS).numpy()\n","test.iloc[:, 1:] = torch.tensor(PREDS).numpy()\n","test['Filename'] = test['Filename'].str.replace('.wav', '')\n","test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Filename</th>\n","      <th>Barking</th>\n","      <th>Howling</th>\n","      <th>Crying</th>\n","      <th>COSmoke</th>\n","      <th>GlassBreaking</th>\n","      <th>Other</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>public_00001</td>\n","      <td>6.144940e-02</td>\n","      <td>5.453053e-01</td>\n","      <td>3.228939e-01</td>\n","      <td>1.126858e-02</td>\n","      <td>7.859051e-03</td>\n","      <td>0.051224</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>public_00002</td>\n","      <td>2.654521e-07</td>\n","      <td>3.957806e-07</td>\n","      <td>1.911393e-07</td>\n","      <td>5.934786e-10</td>\n","      <td>1.516890e-05</td>\n","      <td>0.999984</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>public_00003</td>\n","      <td>9.999226e-01</td>\n","      <td>3.045627e-05</td>\n","      <td>1.043902e-05</td>\n","      <td>7.652030e-08</td>\n","      <td>1.474413e-05</td>\n","      <td>0.000022</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>public_00004</td>\n","      <td>2.964883e-01</td>\n","      <td>1.444015e-01</td>\n","      <td>2.364393e-01</td>\n","      <td>8.931077e-02</td>\n","      <td>8.475937e-02</td>\n","      <td>0.148601</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>public_00005</td>\n","      <td>3.901064e-02</td>\n","      <td>1.406257e-01</td>\n","      <td>7.239787e-01</td>\n","      <td>1.721349e-02</td>\n","      <td>3.508842e-02</td>\n","      <td>0.044083</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29995</th>\n","      <td>private_19996</td>\n","      <td>2.312374e-03</td>\n","      <td>2.550300e-05</td>\n","      <td>2.088708e-03</td>\n","      <td>9.953142e-01</td>\n","      <td>1.351796e-04</td>\n","      <td>0.000124</td>\n","    </tr>\n","    <tr>\n","      <th>29996</th>\n","      <td>private_19997</td>\n","      <td>1.420989e-01</td>\n","      <td>1.024586e-01</td>\n","      <td>1.133607e-01</td>\n","      <td>6.746402e-02</td>\n","      <td>2.826460e-01</td>\n","      <td>0.291972</td>\n","    </tr>\n","    <tr>\n","      <th>29997</th>\n","      <td>private_19998</td>\n","      <td>8.714300e-10</td>\n","      <td>2.151173e-09</td>\n","      <td>7.284987e-10</td>\n","      <td>1.612620e-13</td>\n","      <td>1.130452e-07</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>29998</th>\n","      <td>private_19999</td>\n","      <td>1.540704e-03</td>\n","      <td>1.168827e-02</td>\n","      <td>9.866976e-01</td>\n","      <td>3.251455e-06</td>\n","      <td>6.343484e-06</td>\n","      <td>0.000064</td>\n","    </tr>\n","    <tr>\n","      <th>29999</th>\n","      <td>private_20000</td>\n","      <td>5.067265e-05</td>\n","      <td>9.946993e-01</td>\n","      <td>5.197752e-03</td>\n","      <td>1.310457e-08</td>\n","      <td>5.470592e-07</td>\n","      <td>0.000052</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30000 rows  7 columns</p>\n","</div>"],"text/plain":["            Filename       Barking  ...  GlassBreaking     Other\n","0       public_00001  6.144940e-02  ...   7.859051e-03  0.051224\n","1       public_00002  2.654521e-07  ...   1.516890e-05  0.999984\n","2       public_00003  9.999226e-01  ...   1.474413e-05  0.000022\n","3       public_00004  2.964883e-01  ...   8.475937e-02  0.148601\n","4       public_00005  3.901064e-02  ...   3.508842e-02  0.044083\n","...              ...           ...  ...            ...       ...\n","29995  private_19996  2.312374e-03  ...   1.351796e-04  0.000124\n","29996  private_19997  1.420989e-01  ...   2.826460e-01  0.291972\n","29997  private_19998  8.714300e-10  ...   1.130452e-07  1.000000\n","29998  private_19999  1.540704e-03  ...   6.343484e-06  0.000064\n","29999  private_20000  5.067265e-05  ...   5.470592e-07  0.000052\n","\n","[30000 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"vILIh030xx_J"},"source":["test.to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2020-08-14T10:26:59.844806Z","iopub.status.busy":"2020-08-14T10:26:59.843872Z","iopub.status.idle":"2020-08-14T11:06:08.770335Z","shell.execute_reply":"2020-08-14T11:06:08.767243Z"},"papermill":{"duration":2348.968697,"end_time":"2020-08-14T11:06:08.770508","exception":false,"start_time":"2020-08-14T10:26:59.801811","status":"completed"},"tags":[],"id":"leCrNXYxEIAo"},"source":["# from catalyst import dl\n","\n","# warnings.simplefilter(\"ignore\")\n","\n","# runner = SupervisedRunner(\n","#     input_key=\"waveform\",\n","#     output_key=\"logits\",\n","#     target_key=\"targets\",\n","#     loss_key=\"loss\"\n","# )\n","\n","# runner.train(\n","#     model=model,\n","#     criterion=criterion,\n","#     loaders=loaders,\n","#     optimizer=optimizer,\n","#     scheduler=scheduler,\n","#     num_epochs=10,\n","#     verbose=True,\n","#     logdir=None, #f\"fold0\",\n","#     callbacks=[\n","#         dl.AUCCallback(input_key=\"logits\", target_key=\"targets\")  #callbacks,\n","#     ],\n","#     valid_loader=\"valid\",\n","#     valid_metric=\"loss\"\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.484059,"end_time":"2020-08-14T11:06:25.607820","exception":false,"start_time":"2020-08-14T11:06:25.123761","status":"completed"},"tags":[],"id":"sSmzxzEfEIAo"},"source":["Seems it's learning something.\n","\n","Now I'll show how this model works in the inference phase. I'll use trained model of this which I trained by myself using the data of this competition in my local environment.\n","\n","Since [several concerns](https://www.kaggle.com/c/birdsong-recognition/discussion/172356) are expressed about over-sharing of top solutions during competition, and since I do respect those people who have worked hard to improve their scores, I would not make trained weight in common and would not share how I trained this model."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.970323,"end_time":"2020-08-14T11:06:27.858184","exception":false,"start_time":"2020-08-14T11:06:26.887861","status":"completed"},"tags":[],"id":"aorWjX4EEIAo"},"source":["## Prediction with SED model"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:28.697434Z","iopub.status.busy":"2020-08-14T11:06:28.695751Z","iopub.status.idle":"2020-08-14T11:06:28.698107Z","shell.execute_reply":"2020-08-14T11:06:28.698557Z"},"papermill":{"duration":0.413231,"end_time":"2020-08-14T11:06:28.698692","exception":false,"start_time":"2020-08-14T11:06:28.285461","status":"completed"},"tags":[],"id":"uO1KiQjcEIAo"},"source":["model_config = {\n","    \"sample_rate\": 32000,\n","    \"window_size\": 1024,\n","    \"hop_size\": 320,\n","    \"mel_bins\": 64,\n","    \"fmin\": 50,\n","    \"fmax\": 14000,\n","    \"classes_num\": 264\n","}\n","\n","weights_path = \"../input/birdcall-pannsatt-aux-weak/best.pth\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:29.628436Z","iopub.status.busy":"2020-08-14T11:06:29.626386Z","iopub.status.idle":"2020-08-14T11:06:29.629299Z","shell.execute_reply":"2020-08-14T11:06:29.630949Z"},"papermill":{"duration":0.538009,"end_time":"2020-08-14T11:06:29.631164","exception":false,"start_time":"2020-08-14T11:06:29.093155","status":"completed"},"tags":[],"id":"CmczvMtCEIAo"},"source":["def get_model(config: dict, weights_path: str):\n","    model = PANNsCNN14Att(**config)\n","    checkpoint = torch.load(weights_path)\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    device = torch.device(\"cuda\")\n","    model.to(device)\n","    model.eval()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:30.517376Z","iopub.status.busy":"2020-08-14T11:06:30.515796Z","iopub.status.idle":"2020-08-14T11:06:30.518163Z","shell.execute_reply":"2020-08-14T11:06:30.518675Z"},"papermill":{"duration":0.436469,"end_time":"2020-08-14T11:06:30.518838","exception":false,"start_time":"2020-08-14T11:06:30.082369","status":"completed"},"tags":[],"id":"7va6HNrCEIAo"},"source":["def prediction_for_clip(test_df: pd.DataFrame,\n","                        clip: np.ndarray, \n","                        model: PANNsCNN14Att,\n","                        threshold=0.5):\n","    PERIOD = 30\n","    audios = []\n","    y = clip.astype(np.float32)\n","    len_y = len(y)\n","    start = 0\n","    end = PERIOD * SR\n","    while True:\n","        y_batch = y[start:end].astype(np.float32)\n","        if len(y_batch) != PERIOD * SR:\n","            y_pad = np.zeros(PERIOD * SR, dtype=np.float32)\n","            y_pad[:len(y_batch)] = y_batch\n","            audios.append(y_pad)\n","            break\n","        start = end\n","        end += PERIOD * SR\n","        audios.append(y_batch)\n","        \n","    array = np.asarray(audios)\n","    tensors = torch.from_numpy(array)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    model.eval()\n","    estimated_event_list = []\n","    global_time = 0.0\n","    site = test_df[\"site\"].values[0]\n","    audio_id = test_df[\"audio_id\"].values[0]\n","    for image in progress_bar(tensors):\n","        image = image.view(1, image.size(0))\n","        image = image.to(device)\n","\n","        with torch.no_grad():\n","            prediction = model(image)\n","            framewise_outputs = prediction[\"framewise_output\"].detach(\n","                ).cpu().numpy()[0]\n","                \n","        thresholded = framewise_outputs >= threshold\n","\n","        for target_idx in range(thresholded.shape[1]):\n","            if thresholded[:, target_idx].mean() == 0:\n","                pass\n","            else:\n","                detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n","                head_idx = 0\n","                tail_idx = 0\n","                while True:\n","                    if (tail_idx + 1 == len(detected)) or (\n","                            detected[tail_idx + 1] - \n","                            detected[tail_idx] != 1):\n","                        onset = 0.01 * detected[\n","                            head_idx] + global_time\n","                        offset = 0.01 * detected[\n","                            tail_idx] + global_time\n","                        onset_idx = detected[head_idx]\n","                        offset_idx = detected[tail_idx]\n","                        max_confidence = framewise_outputs[\n","                            onset_idx:offset_idx, target_idx].max()\n","                        mean_confidence = framewise_outputs[\n","                            onset_idx:offset_idx, target_idx].mean()\n","                        estimated_event = {\n","                            \"site\": site,\n","                            \"audio_id\": audio_id,\n","                            \"ebird_code\": INV_BIRD_CODE[target_idx],\n","                            \"onset\": onset,\n","                            \"offset\": offset,\n","                            \"max_confidence\": max_confidence,\n","                            \"mean_confidence\": mean_confidence\n","                        }\n","                        estimated_event_list.append(estimated_event)\n","                        head_idx = tail_idx + 1\n","                        tail_idx = tail_idx + 1\n","                        if head_idx >= len(detected):\n","                            break\n","                    else:\n","                        tail_idx += 1\n","        global_time += PERIOD\n","        \n","    prediction_df = pd.DataFrame(estimated_event_list)\n","    return prediction_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:31.416619Z","iopub.status.busy":"2020-08-14T11:06:31.415703Z","iopub.status.idle":"2020-08-14T11:06:31.418878Z","shell.execute_reply":"2020-08-14T11:06:31.418386Z"},"papermill":{"duration":0.411637,"end_time":"2020-08-14T11:06:31.418987","exception":false,"start_time":"2020-08-14T11:06:31.007350","status":"completed"},"tags":[],"id":"aHRriBcgEIAp"},"source":["def prediction(test_df: pd.DataFrame,\n","               test_audio: Path,\n","               model_config: dict,\n","               weights_path: str,\n","               threshold=0.5):\n","    model = get_model(model_config, weights_path)\n","    unique_audio_id = test_df.audio_id.unique()\n","\n","    warnings.filterwarnings(\"ignore\")\n","    prediction_dfs = []\n","    for audio_id in unique_audio_id:\n","        with timer(f\"Loading {audio_id}\"):\n","            clip, _ = librosa.load(test_audio / (audio_id + \".mp3\"),\n","                                   sr=SR,\n","                                   mono=True,\n","                                   res_type=\"kaiser_fast\")\n","        \n","        test_df_for_audio_id = test_df.query(\n","            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n","        with timer(f\"Prediction on {audio_id}\"):\n","            prediction_df = prediction_for_clip(test_df_for_audio_id,\n","                                                clip=clip,\n","                                                model=model,\n","                                                threshold=threshold)\n","\n","        prediction_dfs.append(prediction_df)\n","    \n","    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n","    return prediction_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:32.264948Z","iopub.status.busy":"2020-08-14T11:06:32.263886Z","iopub.status.idle":"2020-08-14T11:06:52.100417Z","shell.execute_reply":"2020-08-14T11:06:52.101005Z"},"papermill":{"duration":20.287908,"end_time":"2020-08-14T11:06:52.101194","exception":false,"start_time":"2020-08-14T11:06:31.813286","status":"completed"},"tags":[],"id":"BAmgqI6wEIAp","outputId":"2276b18e-3018-4611-b39b-88d6f3747382"},"source":["prediction_df = prediction(test_df=test,\n","                           test_audio=TEST_AUDIO_DIR,\n","                           model_config=model_config,\n","                           weights_path=weights_path,\n","                           threshold=0.5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Loading 41e6fe6504a34bf6846938ba78d13df1] start\n","[Loading 41e6fe6504a34bf6846938ba78d13df1] done in 2.31 s\n","[Prediction on 41e6fe6504a34bf6846938ba78d13df1] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 41e6fe6504a34bf6846938ba78d13df1] done in 0.60 s\n","[Loading cce64fffafed40f2b2f3d3413ec1c4c2] start\n","[Loading cce64fffafed40f2b2f3d3413ec1c4c2] done in 0.81 s\n","[Prediction on cce64fffafed40f2b2f3d3413ec1c4c2] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on cce64fffafed40f2b2f3d3413ec1c4c2] done in 0.07 s\n","[Loading 99af324c881246949408c0b1ae54271f] start\n","[Loading 99af324c881246949408c0b1ae54271f] done in 0.82 s\n","[Prediction on 99af324c881246949408c0b1ae54271f] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 99af324c881246949408c0b1ae54271f] done in 0.08 s\n","[Loading 6ab74e177aa149468a39ca10beed6222] start\n","[Loading 6ab74e177aa149468a39ca10beed6222] done in 0.76 s\n","[Prediction on 6ab74e177aa149468a39ca10beed6222] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 6ab74e177aa149468a39ca10beed6222] done in 0.08 s\n","[Loading b2fd3f01e9284293a1e33f9c811a2ed6] start\n","[Loading b2fd3f01e9284293a1e33f9c811a2ed6] done in 0.78 s\n","[Prediction on b2fd3f01e9284293a1e33f9c811a2ed6] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on b2fd3f01e9284293a1e33f9c811a2ed6] done in 0.07 s\n","[Loading de62b37ebba749d2abf29d4a493ea5d4] start\n","[Loading de62b37ebba749d2abf29d4a493ea5d4] done in 0.44 s\n","[Prediction on de62b37ebba749d2abf29d4a493ea5d4] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on de62b37ebba749d2abf29d4a493ea5d4] done in 0.04 s\n","[Loading 8680a8dd845d40f296246dbed0d37394] start\n","[Loading 8680a8dd845d40f296246dbed0d37394] done in 0.88 s\n","[Prediction on 8680a8dd845d40f296246dbed0d37394] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 8680a8dd845d40f296246dbed0d37394] done in 0.07 s\n","[Loading 940d546e5eb745c9a74bce3f35efa1f9] start\n","[Loading 940d546e5eb745c9a74bce3f35efa1f9] done in 1.22 s\n","[Prediction on 940d546e5eb745c9a74bce3f35efa1f9] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='3' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [3/3 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 940d546e5eb745c9a74bce3f35efa1f9] done in 0.11 s\n","[Loading 07ab324c602e4afab65ddbcc746c31b5] start\n","[Loading 07ab324c602e4afab65ddbcc746c31b5] done in 0.66 s\n","[Prediction on 07ab324c602e4afab65ddbcc746c31b5] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 07ab324c602e4afab65ddbcc746c31b5] done in 0.04 s\n","[Loading 899616723a32409c996f6f3441646c2a] start\n","[Loading 899616723a32409c996f6f3441646c2a] done in 1.17 s\n","[Prediction on 899616723a32409c996f6f3441646c2a] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 899616723a32409c996f6f3441646c2a] done in 0.08 s\n","[Loading 9cc5d9646f344f1bbb52640a988fe902] start\n","[Loading 9cc5d9646f344f1bbb52640a988fe902] done in 3.42 s\n","[Prediction on 9cc5d9646f344f1bbb52640a988fe902] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='9' class='' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [9/9 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 9cc5d9646f344f1bbb52640a988fe902] done in 0.32 s\n","[Loading a56e20a518684688a9952add8a9d5213] start\n","[Loading a56e20a518684688a9952add8a9d5213] done in 0.74 s\n","[Prediction on a56e20a518684688a9952add8a9d5213] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='2' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2/2 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on a56e20a518684688a9952add8a9d5213] done in 0.08 s\n","[Loading 96779836288745728306903d54e264dd] start\n","[Loading 96779836288745728306903d54e264dd] done in 0.59 s\n","[Prediction on 96779836288745728306903d54e264dd] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 96779836288745728306903d54e264dd] done in 0.04 s\n","[Loading f77783ba4c6641bc918b034a18c23e53] start\n","[Loading f77783ba4c6641bc918b034a18c23e53] done in 0.47 s\n","[Prediction on f77783ba4c6641bc918b034a18c23e53] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on f77783ba4c6641bc918b034a18c23e53] done in 0.04 s\n","[Loading 856b194b097441958697c2bcd1f63982] start\n","[Loading 856b194b097441958697c2bcd1f63982] done in 0.71 s\n","[Prediction on 856b194b097441958697c2bcd1f63982] start\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[Prediction on 856b194b097441958697c2bcd1f63982] done in 0.04 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:52.923275Z","iopub.status.busy":"2020-08-14T11:06:52.922629Z","iopub.status.idle":"2020-08-14T11:06:52.941913Z","shell.execute_reply":"2020-08-14T11:06:52.942405Z"},"papermill":{"duration":0.437985,"end_time":"2020-08-14T11:06:52.942559","exception":false,"start_time":"2020-08-14T11:06:52.504574","status":"completed"},"tags":[],"id":"n_psXKK6EIAp","outputId":"d279790c-1bb5-4dd9-d512-bdaf669659cf"},"source":["prediction_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>site</th>\n","      <th>audio_id</th>\n","      <th>ebird_code</th>\n","      <th>onset</th>\n","      <th>offset</th>\n","      <th>max_confidence</th>\n","      <th>mean_confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>0.96</td>\n","      <td>2.23</td>\n","      <td>0.985395</td>\n","      <td>0.897037</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>7.04</td>\n","      <td>7.67</td>\n","      <td>0.526611</td>\n","      <td>0.519470</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>11.20</td>\n","      <td>12.15</td>\n","      <td>0.956318</td>\n","      <td>0.928820</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>14.40</td>\n","      <td>15.03</td>\n","      <td>0.809643</td>\n","      <td>0.805571</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site_1</td>\n","      <td>41e6fe6504a34bf6846938ba78d13df1</td>\n","      <td>aldfly</td>\n","      <td>20.16</td>\n","      <td>21.43</td>\n","      <td>0.987058</td>\n","      <td>0.917030</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>18.24</td>\n","      <td>19.19</td>\n","      <td>0.885321</td>\n","      <td>0.789641</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>19.84</td>\n","      <td>22.07</td>\n","      <td>0.983291</td>\n","      <td>0.913020</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>23.04</td>\n","      <td>23.67</td>\n","      <td>0.669237</td>\n","      <td>0.624711</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>25.92</td>\n","      <td>26.87</td>\n","      <td>0.950051</td>\n","      <td>0.897717</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>site_3</td>\n","      <td>856b194b097441958697c2bcd1f63982</td>\n","      <td>aldfly</td>\n","      <td>27.84</td>\n","      <td>28.79</td>\n","      <td>0.991087</td>\n","      <td>0.899111</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows  7 columns</p>\n","</div>"],"text/plain":["       site                          audio_id ebird_code  onset  offset  \\\n","0    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly   0.96    2.23   \n","1    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly   7.04    7.67   \n","2    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly  11.20   12.15   \n","3    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly  14.40   15.03   \n","4    site_1  41e6fe6504a34bf6846938ba78d13df1     aldfly  20.16   21.43   \n","..      ...                               ...        ...    ...     ...   \n","195  site_3  856b194b097441958697c2bcd1f63982     aldfly  18.24   19.19   \n","196  site_3  856b194b097441958697c2bcd1f63982     aldfly  19.84   22.07   \n","197  site_3  856b194b097441958697c2bcd1f63982     aldfly  23.04   23.67   \n","198  site_3  856b194b097441958697c2bcd1f63982     aldfly  25.92   26.87   \n","199  site_3  856b194b097441958697c2bcd1f63982     aldfly  27.84   28.79   \n","\n","     max_confidence  mean_confidence  \n","0          0.985395         0.897037  \n","1          0.526611         0.519470  \n","2          0.956318         0.928820  \n","3          0.809643         0.805571  \n","4          0.987058         0.917030  \n","..              ...              ...  \n","195        0.885321         0.789641  \n","196        0.983291         0.913020  \n","197        0.669237         0.624711  \n","198        0.950051         0.897717  \n","199        0.991087         0.899111  \n","\n","[200 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.448352,"end_time":"2020-08-14T11:06:53.824923","exception":false,"start_time":"2020-08-14T11:06:53.376571","status":"completed"},"tags":[],"id":"xpta1cQPEIAq"},"source":["## Postprocess\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:54.667626Z","iopub.status.busy":"2020-08-14T11:06:54.666862Z","iopub.status.idle":"2020-08-14T11:06:54.711744Z","shell.execute_reply":"2020-08-14T11:06:54.711222Z"},"papermill":{"duration":0.476478,"end_time":"2020-08-14T11:06:54.711863","exception":false,"start_time":"2020-08-14T11:06:54.235385","status":"completed"},"tags":[],"id":"bQHVLzN7EIAq"},"source":["labels = {}\n","\n","for audio_id, sub_df in prediction_df.groupby(\"audio_id\"):\n","    events = sub_df[[\"ebird_code\", \"onset\", \"offset\", \"max_confidence\", \"site\"]].values\n","    n_events = len(events)\n","    removed_event = []\n","    # Overlap deletion: this part may not be necessary\n","    # I deleted this part in other model and found there's no difference on the public LB score.\n","    for i in range(n_events):\n","        for j in range(n_events):\n","            if i == j:\n","                continue\n","            if i in removed_event:\n","                continue\n","            if j in removed_event:\n","                continue\n","            \n","            event_i = events[i]\n","            event_j = events[j]\n","            \n","            if (event_i[1] - event_j[2] >= 0) or (event_j[1] - event_i[2] >= 0):\n","                pass\n","            else:\n","                later_onset = max(event_i[1], event_j[1])\n","                sooner_onset = min(event_i[1], event_j[1])\n","                sooner_offset = min(event_i[2], event_j[2])\n","                later_offset = max(event_i[2], event_j[2])\n","\n","                intersection = sooner_offset - later_onset\n","                union = later_offset - sooner_onset\n","                \n","                iou = intersection / union\n","                if iou > 0.4:\n","                    if event_i[3] > event_j[3]:\n","                        removed_event.append(j)\n","                    else:\n","                        removed_event.append(i)\n","\n","    site = events[0][4]\n","    for i in range(n_events):\n","        if i in removed_event:\n","            continue\n","        event = events[i][0]\n","        onset = events[i][1]\n","        offset = events[i][2]\n","        if site in {\"site_1\", \"site_2\"}:\n","            start_section = int((onset // 5) * 5) + 5\n","            end_section = int((offset // 5) * 5) + 5\n","            cur_section = start_section\n","\n","            row_id = f\"{site}_{audio_id}_{start_section}\"\n","            if labels.get(row_id) is not None:\n","                labels[row_id].add(event)\n","            else:\n","                labels[row_id] = set()\n","                labels[row_id].add(event)\n","\n","            while cur_section != end_section:\n","                cur_section += 5\n","                row_id = f\"{site}_{audio_id}_{cur_section}\"\n","                if labels.get(row_id) is not None:\n","                    labels[row_id].add(event)\n","                else:\n","                    labels[row_id] = set()\n","                    labels[row_id].add(event)\n","        else:\n","            row_id = f\"{site}_{audio_id}\"\n","            if labels.get(row_id) is not None:\n","                labels[row_id].add(event)\n","            else:\n","                labels[row_id] = set()\n","                labels[row_id].add(event)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:55.584674Z","iopub.status.busy":"2020-08-14T11:06:55.583531Z","iopub.status.idle":"2020-08-14T11:06:55.588323Z","shell.execute_reply":"2020-08-14T11:06:55.587801Z"},"papermill":{"duration":0.421735,"end_time":"2020-08-14T11:06:55.588428","exception":false,"start_time":"2020-08-14T11:06:55.166693","status":"completed"},"tags":[],"id":"n4bVOnKMEIAq","outputId":"d3a77ae3-ac79-4ae9-e334-eb4d1fa6df2b"},"source":["for key in labels:\n","    labels[key] = \" \".join(sorted(list(labels[key])))\n","    \n","    \n","row_ids = list(labels.keys())\n","birds = list(labels.values())\n","post_processed = pd.DataFrame({\n","    \"row_id\": row_ids,\n","    \"birds\": birds\n","})\n","post_processed.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>birds</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_10</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_15</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>site_2_07ab324c602e4afab65ddbcc746c31b5_25</td>\n","      <td>redcro</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       row_id   birds\n","0   site_2_07ab324c602e4afab65ddbcc746c31b5_5  aldfly\n","1  site_2_07ab324c602e4afab65ddbcc746c31b5_10  aldfly\n","2  site_2_07ab324c602e4afab65ddbcc746c31b5_15  aldfly\n","3  site_2_07ab324c602e4afab65ddbcc746c31b5_25  redcro\n","4   site_1_41e6fe6504a34bf6846938ba78d13df1_5  aldfly"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-08-14T11:06:56.691972Z","iopub.status.busy":"2020-08-14T11:06:56.690948Z","iopub.status.idle":"2020-08-14T11:06:56.903190Z","shell.execute_reply":"2020-08-14T11:06:56.902599Z"},"papermill":{"duration":0.819428,"end_time":"2020-08-14T11:06:56.903303","exception":false,"start_time":"2020-08-14T11:06:56.083875","status":"completed"},"tags":[],"id":"0twOoktlEIAq","outputId":"6db86e93-4c17-46b5-e8cd-ddf51dd3dd03"},"source":["all_row_id = test[[\"row_id\"]]\n","submission = all_row_id.merge(post_processed, on=\"row_id\", how=\"left\")\n","submission = submission.fillna(\"nocall\")\n","submission.to_csv(\"submission.csv\", index=False)\n","submission.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>birds</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_10</td>\n","      <td>aldfly fiespa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_15</td>\n","      <td>aldfly moudov</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_20</td>\n","      <td>aldfly chswar</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site_1_41e6fe6504a34bf6846938ba78d13df1_25</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_10</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_15</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_20</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_25</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_30</td>\n","      <td>nocall</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>site_1_cce64fffafed40f2b2f3d3413ec1c4c2_35</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_5</td>\n","      <td>hamfly</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_10</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_15</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_20</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_25</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_30</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>site_1_99af324c881246949408c0b1ae54271f_35</td>\n","      <td>aldfly</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>site_1_6ab74e177aa149468a39ca10beed6222_5</td>\n","      <td>aldfly</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        row_id          birds\n","0    site_1_41e6fe6504a34bf6846938ba78d13df1_5         aldfly\n","1   site_1_41e6fe6504a34bf6846938ba78d13df1_10  aldfly fiespa\n","2   site_1_41e6fe6504a34bf6846938ba78d13df1_15  aldfly moudov\n","3   site_1_41e6fe6504a34bf6846938ba78d13df1_20  aldfly chswar\n","4   site_1_41e6fe6504a34bf6846938ba78d13df1_25         aldfly\n","5    site_1_cce64fffafed40f2b2f3d3413ec1c4c2_5         aldfly\n","6   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_10         nocall\n","7   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_15         aldfly\n","8   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_20         nocall\n","9   site_1_cce64fffafed40f2b2f3d3413ec1c4c2_25         nocall\n","10  site_1_cce64fffafed40f2b2f3d3413ec1c4c2_30         nocall\n","11  site_1_cce64fffafed40f2b2f3d3413ec1c4c2_35         aldfly\n","12   site_1_99af324c881246949408c0b1ae54271f_5         hamfly\n","13  site_1_99af324c881246949408c0b1ae54271f_10         aldfly\n","14  site_1_99af324c881246949408c0b1ae54271f_15         aldfly\n","15  site_1_99af324c881246949408c0b1ae54271f_20         aldfly\n","16  site_1_99af324c881246949408c0b1ae54271f_25         aldfly\n","17  site_1_99af324c881246949408c0b1ae54271f_30         aldfly\n","18  site_1_99af324c881246949408c0b1ae54271f_35         aldfly\n","19   site_1_6ab74e177aa149468a39ca10beed6222_5         aldfly"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.406457,"end_time":"2020-08-14T11:06:57.714194","exception":false,"start_time":"2020-08-14T11:06:57.307737","status":"completed"},"tags":[],"id":"BNGASdj8EIAr"},"source":["## EOF"]},{"cell_type":"code","metadata":{"papermill":{"duration":0.454931,"end_time":"2020-08-14T11:06:58.586733","exception":false,"start_time":"2020-08-14T11:06:58.131802","status":"completed"},"tags":[],"id":"xXXdz-RPEIAr"},"source":[""],"execution_count":null,"outputs":[]}]}